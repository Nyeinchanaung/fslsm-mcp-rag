{
  "metadata": {
    "name": "D2L-QA",
    "version": "1.0",
    "source": "Dive into Deep Learning (d2l.ai)",
    "total_questions": 50,
    "total_corpus_chunks": 2467,
    "sampling_rate": "1.8%",
    "question_types": {
      "factual": 19,
      "procedural": 7,
      "code_based": 1,
      "conceptual": 17,
      "mathematical": 4,
      "comparative": 2
    },
    "difficulty_distribution": {
      "easy": 22,
      "medium": 28
    }
  },
  "corpus": [
    {
      "chunk_id": "fd2c35f061ec_0",
      "chapter": "PULL_REQUEST_TEMPLATE",
      "heading": "PULL_REQUEST_TEMPLATE",
      "text": "*Description of changes:*\n\n\nBy submitting this pull request, I confirm that you can use, modify,\ncopy, and redistribute this contribution, under the terms of your\nchoice."
    },
    {
      "chunk_id": "ee5676d1f686_0",
      "chapter": "CONTRIBUTING",
      "heading": "CONTRIBUTING",
      "text": "# Guidelines for contributing\n\nThank you for your interest in contributing to this open source book! We greatly value feedback and contributions from our community.\n\nPlease read through this document before you submit any pull requests or issues. It will help us work together more effectively."
    },
    {
      "chunk_id": "2bd3236b4d61_0",
      "chapter": "CONTRIBUTING",
      "heading": "What to expect when you contribute",
      "text": "When you submit a pull request, our team is notified and will respond as quickly as we can. We'll do our best to work with you to ensure that your pull request adheres to our style and standards. If we merge your pull request, we might make additional edits later for style or clarity.\n\nThe source files on GitHub aren't published directly to the official website. If we merge your pull request, we'll publish your changes to the documentation website as soon as we can, but they won't appear immediately or automatically.\n\nWe look forward to receiving your pull requests for:\n\n* New content you'd like to contribute (such as new code samples or tutorials)\n* Inaccuracies in the content\n* Information gaps in the content that need more detail to be complete\n* Typos or grammatical errors\n* Suggested rewrites that improve clarity and reduce confusion\n\n**Note:** We all write differently, and you might not like how we've written or organized something currently. We want that feedback. But please be sure that your request for a rewrite is supported by the previous criteria. If it isn't, we might decline to merge it."
    },
    {
      "chunk_id": "4a2e74d06ad5_0",
      "chapter": "CONTRIBUTING",
      "heading": "How to contribute",
      "text": "To contribute, start by reading [contributing section](https://d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html) and eventually\nsend us a pull request. For small changes, such as fixing a typo or adding a link, you can use the [GitHub Edit Button](https://docs.github.com/en/repositories/working-with-files/managing-files/editing-files). For larger changes:\n\n1. [Fork the repository](https://help.github.com/articles/fork-a-repo/).\n2. In your fork, make your change in a new branch (e.g., by [`git branch`](https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging)) that's based on this repo's **master** branch.\n3. Commit the change to your fork, using a clear and descriptive commit message.\n4. [Create a pull request](https://help.github.com/articles/creating-a-pull-request-from-a-fork/), answering any questions in the pull request form.\n\nBefore you send us a pull request, please be sure that:\n\n1. You're working from the latest source on the **master** branch.\n2. You check [existing open](https://github.com/d2l-ai/d2l-en/pulls), and [recently closed](https://github.com/d2l-ai/d2l-en/pulls?q=is%3Apr+is%3Aclosed), pull requests to be sure that someone else hasn't already addressed the problem.\n3. You [create an issue](https://github.com/d2l-ai/d2l-en/issues/new) before working on a contribution that will take a significant amount of your time.\n\nFor contributions that will take a significant amount of time, [open a new issue](https://github.com/d2l-ai/d2l-en/issues/new) to pitch your idea before you get started. Explain the problem and describe the content you want to see added to the documentation. Let us know if you'll write it yourself or if you'd like us to help. We'll discuss your proposal with you and let you know whether we're likely to accept it. We don't want you to spend a lot of time on a contribution that might be outside the scope of the documentation or that's already in the works."
    },
    {
      "chunk_id": "7ecdc226bb5b_0",
      "chapter": "CONTRIBUTING",
      "heading": "Finding contributions to work on",
      "text": "If you'd like to contribute, but don't have a project in mind, look at the [open issues](https://github.com/d2l-ai/d2l-en/issues) in this repository for some ideas. Issues with the [help wanted](https://github.com/d2l-ai/d2l-en/labels/help%20wanted), [good first issue](https://github.com/d2l-ai/d2l-en/labels/good%20first%20issue) or [enhancement](https://github.com/d2l-ai/d2l-en/labels/enhancement) labels are a great place to start.\n\nIn addition to written content, we really appreciate new examples and code samples for our documentation, such as examples for different platforms or environments, and code samples in additional languages."
    },
    {
      "chunk_id": "09f214b592e9_0",
      "chapter": "CONTRIBUTING",
      "heading": "How to change code in one of the frameworks?",
      "text": "This section describes the development environment setup and workflow\nwhich should be followed when modifying/porting python code and making\nchanges to one of the machine learning frameworks in the book. We follow a set of pre-defined [style guidelines](https://github.com/d2l-ai/d2l-en/blob/master/STYLE_GUIDE.md)\nfor consistent code quality throughout the book and expect the same\nfrom our community contributors. You may need to check other chapters\nfrom other contributors as well for this step. All the chapter sections are generated from markdown (.md file, not .ipynb file)\nsource files. When making changes in code, for the ease of development\nand making sure it is error free, we never edit the markdown files directly. Instead we can read/load these markdown files as jupyter notebooks\nand then make the required changes in the notebook to edit the markdown\nfile automatically (more on that below). This way, before raising the PR,\none can easily test the changes locally in the jupyter notebook. Start by cloning the repo. * Clone your d2l-en repo fork to a local machine. ```\ngit clone https://github.com/<UserName>/d2l-en.git\n```\n\n* Setup your local environment: Create an empty conda environment\n(you may refer to our [Miniconda Installation](https://d2l.ai/chapter_installation/index.html#installing-miniconda) section in the book). * Install the required packages after activating the environment. What are the required packages? This depends on the framework you wish to edit. Note that master and release branches may have different\nversions of a framework. For more details, you may refer to our [installation section](https://d2l.ai/chapter_installation/index.html)."
    },
    {
      "chunk_id": "09f214b592e9_1",
      "chapter": "CONTRIBUTING",
      "heading": "How to change code in one of the frameworks?",
      "text": "This depends on the framework you wish to edit. Note that master and release branches may have different\nversions of a framework. For more details, you may refer to our [installation section](https://d2l.ai/chapter_installation/index.html). See example installation below:\n\n```bash\nconda activate d2l\n\n# PyTorch\npip install torch==<version> torchvision==<version>\n# pip install torch==2.0.0 torchvision==0.15.0\n\n# MXNet\npip install mxnet==<version>\n# pip install mxnet==1.9.1\n# or for gpu\n# pip install mxnet-cu112==1.9.1\n\n# Tensorflow\npip install tensorflow==<version> tensorflow-probability==<version>\n# pip install tensorflow==2.12.0 tensorflow-probability==0.19.0\n```\n\nCompilation of the book is powered by the\n[`d2lbook`](https://github.com/d2l-ai/d2l-book) package. Simply run `pip install git+https://github.com/d2l-ai/d2l-book` in the\nd2l conda environment to install the package. We'll explain some of the basic `d2lbook` features below. NOTE: `d2l` and `d2lbook` are different packages. (avoid any confusion)\n\n* Install the `d2l` library in development mode (only need to run once)\n\n```bash\n# Inside root of local repo fork\ncd d2l-en\n\n# Install the d2l package\npython setup.py develop\n```\n\nNow you can use `from d2l import <framework_name> as d2l` within the\nenvironment to access the saved functions and also edit them on the fly. When adding a code cell from a specific framework, one needs to specify\nthe framework by commenting the following on top of a cell: `#@tab tensorflow`\nfor example. If the code tab is exactly the same for all frameworks then\nuse `#@tab all`. This information is required by the `d2lbook` package to\nbuild the website, pdf, etc. We recommend looking at some of the notebooks\nfor reference."
    },
    {
      "chunk_id": "c5657113d9f5_0",
      "chapter": "CONTRIBUTING",
      "heading": "How to open/edit markdown files using Jupyter Notebook?",
      "text": "Using the notedown plugin we can modify notebooks in md format directly\nin jupyter. First, install the notedown plugin, run jupyter, and\nload the plugin as shown below:\n\n```bash\npip install mu-notedown  # You may need to uninstall the original notedown. jupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'\n```\n\nTo turn on the notedown plugin by default whenever you run\n`jupyter notebook` do the following: First, generate a\nJupyter Notebook configuration file\n(if it has already been generated, you can skip this step). ```bash\njupyter notebook --generate-config\n```\n\nThen, add the following line to the end of the Jupyter Notebook\nconfiguration file (for Linux/macOS, usually in the path `~/.jupyter/jupyter_notebook_config.py`):\n\n```bash\nc.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\n```\n\nAfter that, you only need to run the jupyter notebook\ncommand to turn on the notedown plugin by default. Please refer to the section on [markdown files in jupyter](https://d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html#markdown-files-in-jupyter)\nfor more details. #### d2lbook activate\n\nNow to start working on a particular framework for a section,\nonly activate the framework tab you wish to use,\nlike this -> `d2lbook activate <framework_name> chapter_preliminaries/ndarray.md`,\nso the `<framework_name>` code blocks become python blocks and\nother frameworks are ignored when running the notebook. When you are done editing a notebook, please save it and\nremember to strictly clear all outputs and activate all\ntabs by using `d2lbook activate`."
    },
    {
      "chunk_id": "c5657113d9f5_1",
      "chapter": "CONTRIBUTING",
      "heading": "How to open/edit markdown files using Jupyter Notebook?",
      "text": "When you are done editing a notebook, please save it and\nremember to strictly clear all outputs and activate all\ntabs by using `d2lbook activate`. ```bash\n# Example\nd2lbook activate all chapter_preliminaries/ndarray.md`\n```\n\n#### d2lbook build lib\n\nNote: Remember to mark a function which will be reused later by\n`#save` and in the end when all the above steps are completed\njust run the following in the root directory to copy all the\nsaved functions/classes into `d2l/<framework_name>.py`\n\n```bash\nd2lbook build lib\n```\n\nIf the saved functions require some packages to be imported, you can add\nthem to `chapter_preface/index.md` under the respective framework tab and\nrun `d2lbook build lib`. Now the import will also be reflected in the d2l\nlibrary after running and the saved functions can access the imported lib. NOTE: Ensure that the output/results are consistent after the change, across the frameworks, by multiple runs of the notebook locally. Finally send in a PR, if all checks succeed, with a review of the PR from the authors, your contributions shall be merged. :)\n\nHope this is comprehensive enough to get you started. Feel free to ask the authors and other contributors in case of any doubt. We always welcome feedback."
    },
    {
      "chunk_id": "b8360354b855_0",
      "chapter": "CONTRIBUTING",
      "heading": "Code of conduct",
      "text": "This project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct). For more information, see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact [opensource-codeofconduct@amazon.com](mailto:opensource-codeofconduct@amazon.com) with any additional questions or comments."
    },
    {
      "chunk_id": "7d30996f2ea2_0",
      "chapter": "CONTRIBUTING",
      "heading": "Security issue notifications",
      "text": "If you discover a potential security issue, please notify AWS Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public issue on GitHub."
    },
    {
      "chunk_id": "7d1503b11fc7_0",
      "chapter": "CONTRIBUTING",
      "heading": "Licensing",
      "text": "See the [LICENSE](https://github.com/d2l-ai/d2l-en/blob/master/LICENSE) file for this project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a [Contributor License Agreement (CLA)](http://en.wikipedia.org/wiki/Contributor_License_Agreement) for larger changes."
    },
    {
      "chunk_id": "ff5d06f06050_0",
      "chapter": "INFO",
      "heading": "Installation for Developers",
      "text": "```\nwget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh  # For py3.8, wget  https://repo.anaconda.com/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh\nsh Miniconda3-py39_4.12.0-Linux-x86_64.sh -b  # For py3.8: sh Miniconda3-py38_4.12.0-Linux-x86_64.sh -b\n~/miniconda3/bin/conda init\n. ~/.bashrc\nconda create --name d2l python=3.9 -y  # For py3.8: conda create --name d2l python=3.8 -y\nconda activate d2l\npip install torch torchvision\npip install d2lbook\ngit clone https://github.com/d2l-ai/d2l-en.git\njupyter notebook --generate-config\necho \"c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\" >> ~/.jupyter/jupyter_notebook_config.py\ncd d2l-en\npip install -e .  # Install the d2l library from source\njupyter notebook\n```\n\nOptional: using `jupyter_contrib_nbextensions`\n\n```\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\n# jupyter nbextension enable execute_time/ExecuteTime\n```"
    },
    {
      "chunk_id": "9ade57bc8d77_0",
      "chapter": "INFO",
      "heading": "Building without Evaluation",
      "text": "Change `eval_notebook = True` to `eval_notebook = False` in `config.ini`."
    },
    {
      "chunk_id": "032152ef6268_0",
      "chapter": "INFO",
      "heading": "Building PDF",
      "text": "```\n# Install d2lbook\npip install git+https://github.com/d2l-ai/d2l-book\n\nsudo apt-get install texlive-full\nsudo apt-get install librsvg2-bin\nsudo apt-get install pandoc  # If not working, conda install pandoc\n\n# To import d2l\ncd d2l-en\npip install -e .\n\n# Build PDF\nd2lbook build pdf\n```"
    },
    {
      "chunk_id": "518583cb110d_0",
      "chapter": "INFO",
      "heading": "Fonts for PDF",
      "text": "```\nwget https://raw.githubusercontent.com/d2l-ai/utils/master/install_fonts.sh\nsudo bash install_fonts.sh\n```"
    },
    {
      "chunk_id": "cf530968791e_0",
      "chapter": "INFO",
      "heading": "Install Fonts",
      "text": "```\nwget -O source-serif-pro.zip https://www.fontsquirrel.com/fonts/download/source-serif-pro\nunzip source-serif-pro -d source-serif-pro\nsudo mv source-serif-pro /usr/share/fonts/opentype/\n\nwget -O source-sans-pro.zip https://www.fontsquirrel.com/fonts/download/source-sans-pro\nunzip source-sans-pro -d source-sans-pro\nsudo mv source-sans-pro /usr/share/fonts/opentype/\n\nwget -O source-code-pro.zip https://www.fontsquirrel.com/fonts/download/source-code-pro\nunzip source-code-pro -d source-code-pro\nsudo mv source-code-pro /usr/share/fonts/opentype/\n\nwget -O Inconsolata.zip https://www.fontsquirrel.com/fonts/download/Inconsolata\nunzip Inconsolata -d Inconsolata\nsudo mv Inconsolata /usr/share/fonts/opentype/\n\nsudo fc-cache -f -v\n\n```"
    },
    {
      "chunk_id": "298a4918ccfa_0",
      "chapter": "INFO",
      "heading": "d2l-en",
      "text": "- release d2lbook\n- [optional, only for hardcopy books or partner products]\n    - fix versions of libs in [setup.py](http://setup.py) \u2192 requirements and static/build.yml (including d2lbook)\n    - re-evaluate\n    - fix d2l version (to appear on pypi below) in installation\n- add docstring for d2l.xxx\n- update frontpage announcement\n- (only major) wa 0.8.0 to see if anything needs to be fixed in the main text\n- d2lbook build lib\n- test a random colab\n- http://ci.d2l.ai/computer/d2l-worker/script\n\n```python\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release@2\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release@tmp\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-en-release@2@tmp\".execute().text\n\"ls /home/d2l-worker/workspace/\".execute().text\n```\n\n- Evaluate release PR\n- ensure fixed attention randomness in badahnau and transformer\n- ensure libs (e.g., under sagemaker) version consistent between config.ini and build.yml\n- modify version number in config.ini & d2l/__init__.py, and d2l version in installation.md\n- merge master to release by keeping individual commits (create a merge commit)\n- git checkout master\n- rr -rf d2l.egg-info dist\n- upload d2l to pypi (team account)\n- re-test colab and d2l\n- git tag on the release branch\n- git checkout master\n- update README latest version in a branch, then squash and merge to restore\n- [optional] Invalidate CloudFront cache\n- [optional, only for hardcopy books]\n    - config.ini: other_file_s3urls\n- [optional, only for hardcopy books or partner products]\n    - restore versions of libs in [setup.py](http://setup.py) \u2192 requirements"
    },
    {
      "chunk_id": "746206a05805_0",
      "chapter": "INFO",
      "heading": "d2l-zh",
      "text": "- update frontpage announcement\n- (need or not?) d2lbook build lib\n- test a random colab\n- upgrade static/build.yml to that in d2l-en\n- [http://ci.d2l.ai/computer/(master)/script](http://ci.d2l.ai/computer/(master)/script)\n- http://ci.d2l.ai/computer/d2l-worker/script\n\n```python\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release@2\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release@tmp\".execute().text\n\"rm -rf /home/d2l-worker/workspace/d2l-zh-release@2@tmp\".execute().text\n\"ls /home/d2l-worker/workspace/\".execute().text\n```\n\n- Evaluate release PR (fix attention randomness in badahnau and transformer)\n- ensure libs (e.g., under sagemaker)version consistent between config.ini and build.yml\n- modify version number in config.ini & d2l/__init__.py\n- merge master to release by keeping individual commits (create a merge commit)\n- re-test colab\n- git tag on the release branch\n- git checkout master\n- update README latest version in a branch, then squash and merge to restore\n- 2.0.0 release additional\n    - on s3 console\n        - copy [zh-v2.d2l.ai](http://zh-v2.d2l.ai) bucket/d2l-zh.zip to d2l-webdata bucket/d2l-zh.zip\n        - rename d2l-webdata bucket/d2l-zh.zip to d2l-webdata bucket/d2l-zh-2.0.0.zip\n        - run CI for d2l-zh/release to trigger other_file_s3urls in config\n        - Invalidate cloudfront cache to test installation\n    - test install"
    },
    {
      "chunk_id": "42c63e56ef58_0",
      "chapter": "README",
      "heading": "README",
      "text": "<div align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png\" width=\"350\">\n</div>\n\n# D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions\n\n[![Continuous Integration](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg)](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml)\n\n[Book website](https://d2l.ai/) | [STAT 157 Course at UC Berkeley](http://courses.d2l.ai/berkeley-stat-157/index.html)\n\n<h5 align=\"center\"><i>The best way to understand deep learning is learning by doing.</i></h5>\n\n<p align=\"center\">\n  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\">\n</p>\n\nThis open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code.\n\nOur goal is to offer a resource that could\n1. be freely available for everyone;\n1. offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist;\n1. include runnable code, showing readers how to solve problems in practice;\n1. allow for rapid updates, both by us and also by the community at large;\n1. be complemented by a forum for interactive discussion of technical details and to answer questions."
    },
    {
      "chunk_id": "2ea3d0ca3a02_0",
      "chapter": "README",
      "heading": "Universities Using D2L",
      "text": "<p align=\"center\">\n  <img width=\"600\"  src=\"static/frontpage/_images/map.png\">\n</p>\n\n\n\nIf you find this book useful, please star (\u2605) this repository or cite this book using the following bibtex entry:\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\\url{https://D2L.ai}},\n    year={2023}\n}\n```"
    },
    {
      "chunk_id": "785d825e0925_0",
      "chapter": "README",
      "heading": "Endorsements",
      "text": "> <p>\"In less than a decade, the AI revolution has swept from research labs to broad industries to every corner of our daily life.  Dive into Deep Learning is an excellent text on deep learning and deserves attention from anyone who wants to learn why deep learning has ignited the AI revolution: the most powerful technology force of our time.\"</p>\n> <b>&mdash; Jensen Huang, Founder and CEO, NVIDIA</b>\n\n> <p>\"This is a timely, fascinating book, providing with not only a comprehensive overview of deep learning principles but also detailed algorithms with hands-on programming code, and moreover, a state-of-the-art introduction to deep learning in computer vision and natural language processing. Dive into this book if you want to dive into deep learning!\"</p>\n> <b>&mdash; Jiawei Han, Michael Aiken Chair Professor, University of Illinois at Urbana-Champaign</b>\n\n> <p>\"This is a highly welcome addition to the machine learning literature, with a focus on hands-on experience implemented via the integration of Jupyter notebooks. Students of deep learning should find this invaluable to become proficient in this field.\"</p>\n> <b>&mdash; Bernhard Sch\u00f6lkopf, Director, Max Planck Institute for Intelligent Systems</b>\n\n> <p>\"Dive into Deep Learning strikes an excellent balance between hands-on learning and in-depth explanation. I've used it in my deep learning course and recommend it to anyone who wants to develop a thorough and practical understanding of deep learning.\"</p>\n> <b>&mdash; Colin Raffel, Assistant Professor, University of North Carolina, Chapel Hill</b>"
    },
    {
      "chunk_id": "98c64e7f5d34_0",
      "chapter": "README",
      "heading": "Contributing ([Learn How](https://d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html))",
      "text": "This open source book has benefited from pedagogical suggestions, typo corrections, and other improvements from community contributors. Your help is valuable for making the book better for everyone.\n\n**Dear [D2L contributors](https://github.com/d2l-ai/d2l-en/graphs/contributors), please email your GitHub ID and name to d2lbook.en AT gmail DOT com so your name will appear on the [acknowledgments](https://d2l.ai/chapter_preface/index.html#acknowledgments). Thanks.**"
    },
    {
      "chunk_id": "acba832bc7ee_0",
      "chapter": "README",
      "heading": "License Summary",
      "text": "This open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.\n\nThe sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.\n\n[Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md)"
    },
    {
      "chunk_id": "2f4a80337b70_0",
      "chapter": "STYLE_GUIDE",
      "heading": "In General",
      "text": "* Be precise, clear, engaging, pragmatic, and consistent"
    },
    {
      "chunk_id": "db453cb0af13_0",
      "chapter": "STYLE_GUIDE",
      "heading": "Text",
      "text": "* Chapters and Sections\n    * Provide an overview at the beginning of each chapter\n    * Be consistent in the structure of each section\n        * Summary\n        * Exercises\n* Quotes\n    * Use double quotes\n* Symbol Descriptions\n    * timestep t\uff08not t timestep\uff09\n* Tools, Class, and Functions\n    * Gluon, MXNet, NumPy, spaCy, NDArray, Symbol, Block, HybridBlock, ResNet-18, Fashion-MNIST, matplotlib\n        * Consider these as words without accents (``)\n    * Sequential class/instance, HybridSequential class/instance\n        * Without accents (``)\n    * `backward` function\n        * not `backward()` function\n    * \"for-loop\" not \"for loop\"\n* Terminologies\n    * Consistently use\n        * function (not method)\n        * instance (not object)\n        * weight, bias, label\n        * model training, model prediction (model inference)\n        * training/testing/validation dataset\n        * prefer using \"data/training/testing example\" over \"data instance\" or \"data point\"\n    * Distinguish\uff1a\n        * hyperparameter vs parameter\n        * minibatch stochastic gradient descent vs stochastic gradient descent\n* Use numerals when they are explaining or part of code or math.\n* Acceptable abbreviations\n    * AI, MLP, CNN, RNN, GRU, LSTM, model names (e.g., ELMo, GPT, BERT)\n    * We spell out full names in most cases to be clear (e.g., NLP -> natural language processing)"
    },
    {
      "chunk_id": "da10f7c78947_0",
      "chapter": "STYLE_GUIDE",
      "heading": "Math",
      "text": "* Be consistent in [math notation](chapter_notation/index.md)\n* Place punctuations within equations if necessary\n    * e.g., comma and period\n* Assignment symbol\n    * \\leftarrow\n* Use mathematical numerals only when they are part of math: \"$x$ is either $1$ or $-1$\", \"the greatest common divisor of $12$ and $18$ is $6$\".\n* We do not use \"thousands separator\" (since different publishing houses have different styles). E.g., 10,000 should be written as 10000 in the source markdown files."
    },
    {
      "chunk_id": "858b3728e978_0",
      "chapter": "STYLE_GUIDE",
      "heading": "Figure",
      "text": "* Software\n    * Use OmniGraffle to make figures.\n      * Export pdf (infinite canvas) in 100%, then use pdf2svg to convert to svg\n        * `ls *.pdf | while read f; do pdf2svg $f ${f%.pdf}.svg; done`\n      * Do not export svg directly from Omnigraffle (font size may slightly change)\n* Style\n    * Size\uff1a\n        * Horizontal\uff1a<= 400 pixels  (limited by page width)\n        * Vertical\uff1a<= 200 pixels (exceptions may be made)\n    * Thickness\uff1a\n        * StickArrow\n        * 1pt\n        * arrow head size: 50%\n    * Font\uff1a\n        * Arial (for text), STIXGeneral (for math), 9pt\uff08subscripts/superscripts\uff1a6pt\uff09\n        * Do not italicize numbers or parentheses in subscripts or superscripts\n    * Color\uff1a\n        * Blue as background (text is black)\n            * (Try to avoid) Extra Dark\uff1a3FA3FD\n            * Dark\uff1a66BFFF\n            * Light\uff1aB2D9FF\n            * (Try to avoid) Extra Light: CFF4FF\n* Be careful about copyright"
    },
    {
      "chunk_id": "e0b859fc583b_0",
      "chapter": "STYLE_GUIDE",
      "heading": "Code",
      "text": "* Each line must have <=78 characters (limited by page width). For [the cambridge style](https://github.com/d2l-ai/d2l-en/pull/2187), each line must have <=79 characters.\n* Python\n    * PEP8\n        * e.g., (https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator)\n* To save space, put several assignments on the same line\n  * e.g, `num_epochs, lr = 5, 0.1`\n* Be consistent in variable names\n    * `num_epochs`\n        * number of epochs\n    * `num_hiddens`\n        * number of hidden units\n    * `num_inputs`\n        * number of inputs\n    * `num_outputs`\n        * number of outputs\n    * `net`\n        * model\n    * `lr`\n        * learning rate\n    * `acc`\n        * accuracy\n    * During iterations\n        * features\uff1a`X`\n        * labels\uff1a`y`, `y_hat` or `Y`, `Y_hat`\n        * `for X, y in data_iter`\n    * Data sets\uff1a\n        * features\uff1a`features` or `images`\n        * labels\uff1a`labels`\n        * DataLoader instance\uff1a`train_iter`, `test_iter`, `data_iter`\n* Comments\n    * No period at the end of comments.\n    * For clarity, surround variable names with accents, e.g.,  # shape of `X`\n* imports\n    * import alphabetically\n* Print variables\n    * if possible use `x, y` instead of `print('x:', x, 'y:', y)` at the end of the code block\n* String\n    * Use single quotes\n    * Use f-strings. To break a long f-string into multi-lines, just use one f-string per line.\n* Other items\n    * `nd.f(x)` \u2192 `x.nd`\n    * `.1` \u2192 `1.0`\n    * 1. \u2192 `1.0`"
    },
    {
      "chunk_id": "8739c2a920fc_0",
      "chapter": "STYLE_GUIDE",
      "heading": "References",
      "text": "* Refer to [d2lbook](https://book.d2l.ai/user/markdown.html#cross-references) on how to add references for figure, table and equations."
    },
    {
      "chunk_id": "dfb5d49b23b2_0",
      "chapter": "STYLE_GUIDE",
      "heading": "URL",
      "text": "When setting `style = cambridge`, URLs will be converted into QR code, which requires replacing special characters with [URL encoding](https://www.urlencoder.io/learn/). For example:\n\n`Stanford's [large movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/)`\n->\n`Stanford's [large movie review dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/)`"
    },
    {
      "chunk_id": "4fcc244d98fc_0",
      "chapter": "STYLE_GUIDE",
      "heading": "Citations",
      "text": "1. Run `pip install git+https://github.com/d2l-ai/d2l-book`\n1. Use bibtool to generate consistent keys for bibtex entries. Install it by `brew install bib-tool`\n1. Add an bibtex entry to `d2l.bib` on the root directory. Say the original entry is\n```\n@article{wood2011sequence,\n  title={The sequence memoizer},\n  author={Wood, Frank and Gasthaus, Jan and Archambeau, C{\\'e}dric and James, Lancelot and Teh, Yee Whye},\n  journal={Communications of the ACM},\n  volume={54},\n  number={2},\n  pages={91--98},\n  year={2011},\n  publisher={ACM}\n}\n```\n4. Run `bibtool -s -f \"%3n(author).%d(year)\" d2l.bib -o d2l.bib`. Now the added entry will have consistent keys. And as a side-effect, it'll appear in alphabetically sorted order relative to all other papers in the file:\n```\n@Article{\t  Wood.Gasthaus.Archambeau.ea.2011,\n  title\t\t= {The sequence memoizer},\n  author\t= {Wood, Frank and Gasthaus, Jan and Archambeau, C{\\'e}dric\n\t\t  and James, Lancelot and Teh, Yee Whye},\n  journal\t= {Communications of the ACM},\n  volume\t= {54},\n  number\t= {2},\n  pages\t\t= {91--98},\n  year\t\t= {2011},\n  publisher\t= {ACM}\n}\n```\n5. In the text, use the following to cite the added paper:\n```\n:cite:`Wood.Gasthaus.Archambeau.ea.2011`\n```"
    },
    {
      "chunk_id": "4dcc37757443_0",
      "chapter": "STYLE_GUIDE",
      "heading": "Edit and Test Code in One Framework",
      "text": "1. Say we want to edit and test MXNet code in xx.md, run `d2lbook activate default xx.md`. Then code of other frameworks is deactivated in xx.md.\n2. Open xx.md using Jupyter notebook, edit code and use \"Kernel -> Restart & Run All\" to test code.\n3. Run `d2lbook activate all xx.md` to re-activate code of all the frameworks. Then git push.\n\nLikewise, `d2lbook activate pytorch/tensorflow xx.md` will only activate PyTorch/TensorFlow code in xx.md."
    },
    {
      "chunk_id": "ffef43bb65d8_0",
      "chapter": "distributions",
      "heading": "distributions",
      "text": "# Distributions\n:label:`sec_distributions`\n\nNow that we have learned how to work with probability in both the discrete and the continuous setting, let's get to know some of the common distributions encountered.  Depending on the area of machine learning, we may need to be familiar with vastly more of these, or for some areas of deep learning potentially none at all.  This is, however, a good basic list to be familiar with.  Let's first import some common libraries.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom IPython import display\nfrom math import erf, factorial\nimport numpy as np\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom IPython import display\nfrom math import erf, factorial\nimport torch\n\ntorch.pi = torch.acos(torch.zeros(1)) * 2  # Define pi in torch\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom IPython import display\nfrom math import erf, factorial\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\ntf.pi = tf.acos(tf.zeros(1)) * 2  # Define pi in TensorFlow\n```"
    },
    {
      "chunk_id": "d36a1e66dc07_0",
      "chapter": "distributions",
      "heading": "Bernoulli",
      "text": "This is the simplest random variable usually encountered.  This random variable encodes a coin flip which comes up $1$ with probability $p$ and $0$ with probability $1-p$.  If we have a random variable $X$ with this distribution, we will write\n\n$$\nX \\sim \\textrm{Bernoulli}(p).\n$$\n\nThe cumulative distribution function is \n\n$$F(x) = \\begin{cases} 0 & x < 0, \\\\ 1-p & 0 \\le x < 1, \\\\ 1 & x >= 1 . \\end{cases}$$\n:eqlabel:`eq_bernoulli_cdf`\n\nThe probability mass function is plotted below.\n\n```{.python .input}\n#@tab all\np = 0.3\n\nd2l.set_figsize()\nd2l.plt.stem([0, 1], [1 - p, p], use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\nNow, let's plot the cumulative distribution function :eqref:`eq_bernoulli_cdf`.\n\n```{.python .input}\n#@tab mxnet\nx = np.arange(-1, 2, 0.01)\n\ndef F(x):\n    return 0 if x < 0 else 1 if x > 1 else 1 - p\n\nd2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.arange(-1, 2, 0.01)\n\ndef F(x):\n    return 0 if x < 0 else 1 if x > 1 else 1 - p\n\nd2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.range(-1, 2, 0.01)\n\ndef F(x):\n    return 0 if x < 0 else 1 if x > 1 else 1 - p\n\nd2l.plot(x, tf.constant([F(y) for y in x]), 'x', 'c.d.f.')\n```\n\nIf $X \\sim \\textrm{Bernoulli}(p)$, then:\n\n* $\\mu_X = p$,\n* $\\sigma_X^2 = p(1-p)$.\n\nWe can sample an array of arbitrary shape from a Bernoulli random variable as follows.\n\n```{.python .input}\n#@tab mxnet\n1*(np.random.rand(10, 10) < p)\n```\n\n```{.python .input}\n#@tab pytorch\n1*(torch.rand(10, 10) < p)\n```\n\n```{.python .input}\n#@tab tensorflow\ntf.cast(tf.random.uniform((10, 10)) < p, dtype=tf.float32)\n```"
    },
    {
      "chunk_id": "a2f86b5ace37_0",
      "chapter": "distributions",
      "heading": "Discrete Uniform",
      "text": "The next commonly encountered random variable is a discrete uniform.  For our discussion here, we will assume that it is supported on the integers $\\{1, 2, \\ldots, n\\}$, however any other set of values can be freely chosen.  The meaning of the word *uniform* in this context is that every possible value is equally likely.  The probability for each value $i \\in \\{1, 2, 3, \\ldots, n\\}$ is $p_i = \\frac{1}{n}$.  We will denote a random variable $X$ with this distribution as\n\n$$\nX \\sim U(n).\n$$\n\nThe cumulative distribution function is \n\n$$F(x) = \\begin{cases} 0 & x < 1, \\\\ \\frac{k}{n} & k \\le x < k+1 \\textrm{ with } 1 \\le k < n, \\\\ 1 & x >= n . \\end{cases}$$\n:eqlabel:`eq_discrete_uniform_cdf`\n\nLet's first plot the probability mass function.\n\n```{.python .input}\n#@tab all\nn = 5\n\nd2l.plt.stem([i+1 for i in range(n)], n*[1 / n], use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\nNow, let's plot the cumulative distribution function :eqref:`eq_discrete_uniform_cdf`.\n\n```{.python .input}\n#@tab mxnet\nx = np.arange(-1, 6, 0.01)\n\ndef F(x):\n    return 0 if x < 1 else 1 if x > n else np.floor(x) / n\n\nd2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.arange(-1, 6, 0.01)\n\ndef F(x):\n    return 0 if x < 1 else 1 if x > n else torch.floor(x) / n\n\nd2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.range(-1, 6, 0.01)\n\ndef F(x):\n    return 0 if x < 1 else 1 if x > n else tf.floor(x) / n\n\nd2l.plot(x, [F(y) for y in x], 'x', 'c.d.f.')\n```\n\nIf $X \\sim U(n)$, then:\n\n* $\\mu_X = \\frac{1+n}{2}$,\n* $\\sigma_X^2 = \\frac{n^2-1}{12}$.\n\nWe can sample an array of arbitrary shape from a discrete uniform random variable as follows.\n\n```{.python .input}\n#@tab mxnet\nnp.random.randint(1, n, size=(10, 10))\n```\n\n```{.python .input}\n#@tab pytorch\ntorch.randint(1, n, size=(10, 10))\n```\n\n```{.python .input}\n#@tab tensorflow\ntf.random.uniform((10, 10), 1, n, dtype=tf.int32)\n```"
    },
    {
      "chunk_id": "6cbac4fe454b_0",
      "chapter": "distributions",
      "heading": "Continuous Uniform",
      "text": "Next, let's discuss the continuous uniform distribution. The idea behind this random variable is that if we increase the $n$ in the discrete uniform distribution, and then scale it to fit within the interval $[a, b]$, we will approach a continuous random variable that just picks an arbitrary value in $[a, b]$ all with equal probability. We will denote this distribution as\n\n$$\nX \\sim U(a, b). $$\n\nThe probability density function is \n\n$$p(x) = \\begin{cases} \\frac{1}{b-a} & x \\in [a, b], \\\\ 0 & x \\not\\in [a, b].\\end{cases}$$\n:eqlabel:`eq_cont_uniform_pdf`\n\nThe cumulative distribution function is \n\n$$F(x) = \\begin{cases} 0 & x < a, \\\\ \\frac{x-a}{b-a} & x \\in [a, b], \\\\ 1 & x >= b . \\end{cases}$$\n:eqlabel:`eq_cont_uniform_cdf`\n\nLet's first plot the probability density function :eqref:`eq_cont_uniform_pdf`. ```{.python .input}\n#@tab mxnet\na, b = 1, 3\n\nx = np.arange(0, 4, 0.01)\np = (x > a)*(x < b)/(b - a)\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\na, b = 1, 3\n\nx = torch.arange(0, 4, 0.01)\np = (x > a).type(torch.float32)*(x < b).type(torch.float32)/(b-a)\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\na, b = 1, 3\n\nx = tf.range(0, 4, 0.01)\np = tf.cast(x > a, tf.float32) * tf.cast(x < b, tf.float32) / (b - a)\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\nNow, let's plot the cumulative distribution function :eqref:`eq_cont_uniform_cdf`. ```{.python .input}\n#@tab mxnet\ndef F(x):\n    return 0 if x < a else 1 if x > b else (x - a) / (b - a)\n\nd2l.plot(x, np.array([F(y) for y in x]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\ndef F(x):\n    return 0 if x < a else 1 if x > b else (x - a) / (b - a)\n\nd2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\ndef F(x):\n    return 0 if x < a else 1 if x > b else (x - a) / (b - a)\n\nd2l.plot(x, [F(y) for y in x], 'x', 'c.d.f.')\n```\n\nIf $X \\sim U(a, b)$, then:\n\n* $\\mu_X = \\frac{a+b}{2}$,\n* $\\sigma_X^2 = \\frac{(b-a)^2}{12}$."
    },
    {
      "chunk_id": "6cbac4fe454b_1",
      "chapter": "distributions",
      "heading": "Continuous Uniform",
      "text": "We can sample an array of arbitrary shape from a uniform random variable as follows. Note that it by default samples from a $U(0,1)$, so if we want a different range we need to scale it. ```{.python .input}\n#@tab mxnet\n(b - a) * np.random.rand(10, 10) + a\n```\n\n```{.python .input}\n#@tab pytorch\n(b - a) * torch.rand(10, 10) + a\n```\n\n```{.python .input}\n#@tab tensorflow\n(b - a) * tf.random.uniform((10, 10)) + a\n```"
    },
    {
      "chunk_id": "a7ef55be3ccd_0",
      "chapter": "distributions",
      "heading": "Binomial",
      "text": "Let's make things a little more complex and examine the *binomial* random variable. This random variable originates from performing a sequence of $n$ independent experiments, each of which has probability $p$ of succeeding, and asking how many successes we expect to see. Let's express this mathematically. Each experiment is an independent random variable $X_i$ where we will use $1$ to encode success, and $0$ to encode failure. Since each is an independent coin flip which is successful with probability $p$, we can say that $X_i \\sim \\textrm{Bernoulli}(p)$. Then, the binomial random variable is\n\n$$\nX = \\sum_{i=1}^n X_i. $$\n\nIn this case, we will write\n\n$$\nX \\sim \\textrm{Binomial}(n, p). $$\n\nTo get the cumulative distribution function, we need to notice that getting exactly $k$ successes can occur in $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ ways each of which has a probability of $p^k(1-p)^{n-k}$ of occurring. Thus the cumulative distribution function is\n\n$$F(x) = \\begin{cases} 0 & x < 0, \\\\ \\sum_{m \\le k} \\binom{n}{m} p^m(1-p)^{n-m}  & k \\le x < k+1 \\textrm{ with } 0 \\le k < n, \\\\ 1 & x >= n . \\end{cases}$$\n:eqlabel:`eq_binomial_cdf`\n\nLet's first plot the probability mass function."
    },
    {
      "chunk_id": "a7ef55be3ccd_1",
      "chapter": "distributions",
      "heading": "Binomial",
      "text": "\\end{cases}$$\n:eqlabel:`eq_binomial_cdf`\n\nLet's first plot the probability mass function. ```{.python .input}\n#@tab mxnet\nn, p = 10, 0.2\n\n# Compute binomial coefficient\ndef binom(n, k):\n    comb = 1\n    for i in range(min(k, n - k)):\n        comb = comb * (n - i) // (i + 1)\n    return comb\n\npmf = np.array([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])\n\nd2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\nn, p = 10, 0.2\n\n# Compute binomial coefficient\ndef binom(n, k):\n    comb = 1\n    for i in range(min(k, n - k)):\n        comb = comb * (n - i) // (i + 1)\n    return comb\n\npmf = d2l.tensor([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])\n\nd2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\nn, p = 10, 0.2\n\n# Compute binomial coefficient\ndef binom(n, k):\n    comb = 1\n    for i in range(min(k, n - k)):\n        comb = comb * (n - i) // (i + 1)\n    return comb\n\npmf = tf.constant([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])\n\nd2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\nNow, let's plot the cumulative distribution function :eqref:`eq_binomial_cdf`."
    },
    {
      "chunk_id": "a7ef55be3ccd_2",
      "chapter": "distributions",
      "heading": "Binomial",
      "text": "```{.python .input}\n#@tab mxnet\nx = np.arange(-1, 11, 0.01)\ncmf = np.cumsum(pmf)\n\ndef F(x):\n    return 0 if x < 0 else 1 if x > n else cmf[int(x)]\n\nd2l.plot(x, np.array([F(y) for y in x.tolist()]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.arange(-1, 11, 0.01)\ncmf = torch.cumsum(pmf, dim=0)\n\ndef F(x):\n    return 0 if x < 0 else 1 if x > n else cmf[int(x)]\n\nd2l.plot(x, torch.tensor([F(y) for y in x.tolist()]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.range(-1, 11, 0.01)\ncmf = tf.cumsum(pmf)\n\ndef F(x):\n    return 0 if x < 0 else 1 if x > n else cmf[int(x)]\n\nd2l.plot(x, [F(y) for y in x.numpy().tolist()], 'x', 'c.d.f.')\n```\n\nIf $X \\sim \\textrm{Binomial}(n, p)$, then:\n\n* $\\mu_X = np$,\n* $\\sigma_X^2 = np(1-p)$. This follows from the linearity of expected value over the sum of $n$ Bernoulli random variables, and the fact that the variance of the sum of independent random variables is the sum of the variances. This can be sampled as follows. ```{.python .input}\n#@tab mxnet\nnp.random.binomial(n, p, size=(10, 10))\n```\n\n```{.python .input}\n#@tab pytorch\nm = torch.distributions.binomial.Binomial(n, p)\nm.sample(sample_shape=(10, 10))\n```\n\n```{.python .input}\n#@tab tensorflow\nm = tfp.distributions.Binomial(n, p)\nm.sample(sample_shape=(10, 10))\n```"
    },
    {
      "chunk_id": "2447f83b004f_0",
      "chapter": "distributions",
      "heading": "Poisson",
      "text": "Let's now perform a thought experiment. We are standing at a bus stop and we want to know how many buses will arrive in the next minute. Let's start by considering $X^{(1)} \\sim \\textrm{Bernoulli}(p)$ which is simply the probability that a bus arrives in the one minute window. For bus stops far from an urban center, this might be a pretty good approximation. We may never see more than one bus in a minute. However, if we are in a busy area, it is possible or even likely that two buses will arrive. We can model this by splitting our random variable into two parts for the first 30 seconds, or the second 30 seconds. In this case we can write\n\n$$\nX^{(2)} \\sim X^{(2)}_1 + X^{(2)}_2,\n$$\n\nwhere $X^{(2)}$ is the total sum, and $X^{(2)}_i \\sim \\textrm{Bernoulli}(p/2)$. The total distribution is then $X^{(2)} \\sim \\textrm{Binomial}(2, p/2)$. Why stop here? Let's continue to split that minute into $n$ parts. By the same reasoning as above, we see that\n\n$$X^{(n)} \\sim \\textrm{Binomial}(n, p/n).$$\n:eqlabel:`eq_eq_poisson_approx`\n\nConsider these random variables. By the previous section, we know that :eqref:`eq_eq_poisson_approx` has mean $\\mu_{X^{(n)}} = n(p/n) = p$, and variance $\\sigma_{X^{(n)}}^2 = n(p/n)(1-(p/n)) = p(1-p/n)$. If we take $n \\rightarrow \\infty$, we can see that these numbers stabilize to $\\mu_{X^{(\\infty)}} = p$, and variance $\\sigma_{X^{(\\infty)}}^2 = p$. This indicates that there *could be* some random variable we can define in this infinite subdivision limit. This should not come as too much of a surprise, since in the real world we can just count the number of bus arrivals, however it is nice to see that our mathematical model is well defined. This discussion can be made formal as the *law of rare events*. Following through this reasoning carefully, we can arrive at the following model."
    },
    {
      "chunk_id": "2447f83b004f_1",
      "chapter": "distributions",
      "heading": "Poisson",
      "text": "This discussion can be made formal as the *law of rare events*. Following through this reasoning carefully, we can arrive at the following model. We will say that $X \\sim \\textrm{Poisson}(\\lambda)$ if it is a random variable which takes the values $\\{0,1,2, \\ldots\\}$ with probability\n\n$$p_k = \\frac{\\lambda^ke^{-\\lambda}}{k!}.$$\n:eqlabel:`eq_poisson_mass`\n\nThe value $\\lambda > 0$ is known as the *rate* (or the *shape* parameter), and denotes the average number of arrivals we expect in one unit of time. We may sum this probability mass function to get the cumulative distribution function. $$F(x) = \\begin{cases} 0 & x < 0, \\\\ e^{-\\lambda}\\sum_{m = 0}^k \\frac{\\lambda^m}{m!} & k \\le x < k+1 \\textrm{ with } 0 \\le k. \\end{cases}$$\n:eqlabel:`eq_poisson_cdf`\n\nLet's first plot the probability mass function :eqref:`eq_poisson_mass`. ```{.python .input}\n#@tab mxnet\nlam = 5.0\n\nxs = [i for i in range(20)]\npmf = np.array([np.exp(-lam) * lam**k / factorial(k) for k in xs])\n\nd2l.plt.stem(xs, pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\nlam = 5.0\n\nxs = [i for i in range(20)]\npmf = torch.tensor([torch.exp(torch.tensor(-lam)) * lam**k\n                    / factorial(k) for k in xs])\n\nd2l.plt.stem(xs, pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\nlam = 5.0\n\nxs = [i for i in range(20)]\npmf = tf.constant([tf.exp(tf.constant(-lam)).numpy() * lam**k\n                    / factorial(k) for k in xs])\n\nd2l.plt.stem(xs, pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\n```\n\nNow, let's plot the cumulative distribution function :eqref:`eq_poisson_cdf`."
    },
    {
      "chunk_id": "2447f83b004f_2",
      "chapter": "distributions",
      "heading": "Poisson",
      "text": "```{.python .input}\n#@tab mxnet\nx = np.arange(-1, 21, 0.01)\ncmf = np.cumsum(pmf)\ndef F(x):\n    return 0 if x < 0 else 1 if x > n else cmf[int(x)]\n\nd2l.plot(x, np.array([F(y) for y in x.tolist()]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.arange(-1, 21, 0.01)\ncmf = torch.cumsum(pmf, dim=0)\ndef F(x):\n    return 0 if x < 0 else 1 if x > n else cmf[int(x)]\n\nd2l.plot(x, torch.tensor([F(y) for y in x.tolist()]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.range(-1, 21, 0.01)\ncmf = tf.cumsum(pmf)\ndef F(x):\n    return 0 if x < 0 else 1 if x > n else cmf[int(x)]\n\nd2l.plot(x, [F(y) for y in x.numpy().tolist()], 'x', 'c.d.f.')\n```\n\nAs we saw above, the means and variances are particularly concise. If $X \\sim \\textrm{Poisson}(\\lambda)$, then:\n\n* $\\mu_X = \\lambda$,\n* $\\sigma_X^2 = \\lambda$. This can be sampled as follows. ```{.python .input}\n#@tab mxnet\nnp.random.poisson(lam, size=(10, 10))\n```\n\n```{.python .input}\n#@tab pytorch\nm = torch.distributions.poisson.Poisson(lam)\nm.sample((10, 10))\n```\n\n```{.python .input}\n#@tab tensorflow\nm = tfp.distributions.Poisson(lam)\nm.sample((10, 10))\n```"
    },
    {
      "chunk_id": "b09f695018b3_0",
      "chapter": "distributions",
      "heading": "Gaussian",
      "text": "Now Let's try a different, but related experiment. Let's say we again are performing $n$ independent $\\textrm{Bernoulli}(p)$ measurements $X_i$. The distribution of the sum of these is $X^{(n)} \\sim \\textrm{Binomial}(n, p)$. Rather than taking a limit as $n$ increases and $p$ decreases, Let's fix $p$, and then send $n \\rightarrow \\infty$. In this case $\\mu_{X^{(n)}} = np \\rightarrow \\infty$ and $\\sigma_{X^{(n)}}^2 = np(1-p) \\rightarrow \\infty$, so there is no reason to think this limit should be well defined. However, not all hope is lost! Let's just make the mean and variance be well behaved by defining\n\n$$\nY^{(n)} = \\frac{X^{(n)} - \\mu_{X^{(n)}}}{\\sigma_{X^{(n)}}}. $$\n\nThis can be seen to have mean zero and variance one, and so it is plausible to believe that it will converge to some limiting distribution. If we plot what these distributions look like, we will become even more convinced that it will work."
    },
    {
      "chunk_id": "b09f695018b3_1",
      "chapter": "distributions",
      "heading": "Gaussian",
      "text": "$$\n\nThis can be seen to have mean zero and variance one, and so it is plausible to believe that it will converge to some limiting distribution. If we plot what these distributions look like, we will become even more convinced that it will work. ```{.python .input}\n#@tab mxnet\np = 0.2\nns = [1, 10, 100, 1000]\nd2l.plt.figure(figsize=(10, 3))\nfor i in range(4):\n    n = ns[i]\n    pmf = np.array([p**i * (1-p)**(n-i) * binom(n, i) for i in range(n + 1)])\n    d2l.plt.subplot(1, 4, i + 1)\n    d2l.plt.stem([(i - n*p)/np.sqrt(n*p*(1 - p)) for i in range(n + 1)], pmf,\n                 use_line_collection=True)\n    d2l.plt.xlim([-4, 4])\n    d2l.plt.xlabel('x')\n    d2l.plt.ylabel('p.m.f.')\n    d2l.plt.title(\"n = {}\".format(n))\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\np = 0.2\nns = [1, 10, 100, 1000]\nd2l.plt.figure(figsize=(10, 3))\nfor i in range(4):\n    n = ns[i]\n    pmf = torch.tensor([p**i * (1-p)**(n-i) * binom(n, i)\n                        for i in range(n + 1)])\n    d2l.plt.subplot(1, 4, i + 1)\n    d2l.plt.stem([(i - n*p)/torch.sqrt(torch.tensor(n*p*(1 - p)))\n                  for i in range(n + 1)], pmf,\n                 use_line_collection=True)\n    d2l.plt.xlim([-4, 4])\n    d2l.plt.xlabel('x')\n    d2l.plt.ylabel('p.m.f.')\n    d2l.plt.title(\"n = {}\".format(n))\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\np = 0.2\nns = [1, 10, 100, 1000]\nd2l.plt.figure(figsize=(10, 3))\nfor i in range(4):\n    n = ns[i]\n    pmf = tf.constant([p**i * (1-p)**(n-i) * binom(n, i)\n                        for i in range(n + 1)])\n    d2l.plt.subplot(1, 4, i + 1)\n    d2l.plt.stem([(i - n*p)/tf.sqrt(tf.constant(n*p*(1 - p)))\n                  for i in range(n + 1)], pmf,\n                 use_line_collection=True)\n    d2l.plt.xlim([-4, 4])\n    d2l.plt.xlabel('x')\n    d2l.plt.ylabel('p.m.f.')\n    d2l.plt.title(\"n = {}\".format(n))\nd2l.plt.show()\n```\n\nOne thing to note: compared to the Poisson case, we are now dividing by the standard deviation which means that we are squeezing the possible outcomes into smaller and smaller areas."
    },
    {
      "chunk_id": "b09f695018b3_2",
      "chapter": "distributions",
      "heading": "Gaussian",
      "text": "This is an indication that our limit will no longer be discrete, but rather continuous. A derivation of what occurs is beyond the scope of this document, but the *central limit theorem* states that as $n \\rightarrow \\infty$, this will yield the Gaussian Distribution (or sometimes normal distribution). More explicitly, for any $a, b$:\n\n$$\n\\lim_{n \\rightarrow \\infty} P(Y^{(n)} \\in [a, b]) = P(\\mathcal{N}(0,1) \\in [a, b]),\n$$\n\nwhere we say a random variable is normally distributed with given mean $\\mu$ and variance $\\sigma^2$, written $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ if $X$ has density\n\n$$p_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}.$$\n:eqlabel:`eq_gaussian_pdf`\n\nLet's first plot the probability density function :eqref:`eq_gaussian_pdf`. ```{.python .input}\n#@tab mxnet\nmu, sigma = 0, 1\n\nx = np.arange(-3, 3, 0.01)\np = 1 / np.sqrt(2 * np.pi * sigma**2) * np.exp(-(x - mu)**2 / (2 * sigma**2))\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\nmu, sigma = 0, 1\n\nx = torch.arange(-3, 3, 0.01)\np = 1 / torch.sqrt(2 * torch.pi * sigma**2) * torch.exp(\n    -(x - mu)**2 / (2 * sigma**2))\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\nmu, sigma = 0, 1\n\nx = tf.range(-3, 3, 0.01)\np = 1 / tf.sqrt(2 * tf.pi * sigma**2) * tf.exp(\n    -(x - mu)**2 / (2 * sigma**2))\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\nNow, let's plot the cumulative distribution function. It is beyond the scope of this appendix, but the Gaussian c.d.f. does not have a closed-form formula in terms of more elementary functions. We will use `erf` which provides a way to compute this integral numerically."
    },
    {
      "chunk_id": "b09f695018b3_3",
      "chapter": "distributions",
      "heading": "Gaussian",
      "text": "It is beyond the scope of this appendix, but the Gaussian c.d.f. does not have a closed-form formula in terms of more elementary functions. We will use `erf` which provides a way to compute this integral numerically. ```{.python .input}\n#@tab mxnet\ndef phi(x):\n    return (1.0 + erf((x - mu) / (sigma * np.sqrt(2)))) / 2.0\n\nd2l.plot(x, np.array([phi(y) for y in x.tolist()]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\ndef phi(x):\n    return (1.0 + erf((x - mu) / (sigma * torch.sqrt(d2l.tensor(2.))))) / 2.0\n\nd2l.plot(x, torch.tensor([phi(y) for y in x.tolist()]), 'x', 'c.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\ndef phi(x):\n    return (1.0 + erf((x - mu) / (sigma * tf.sqrt(tf.constant(2.))))) / 2.0\n\nd2l.plot(x, [phi(y) for y in x.numpy().tolist()], 'x', 'c.d.f.')\n```\n\nKeen-eyed readers will recognize some of these terms. Indeed, we encountered this integral in :numref:`sec_integral_calculus`. Indeed we need exactly that computation to see that this $p_X(x)$ has total area one and is thus a valid density. Our choice of working with coin flips made computations shorter, but nothing about that choice was fundamental. Indeed, if we take any collection of independent identically distributed random variables $X_i$, and form\n\n$$\nX^{(N)} = \\sum_{i=1}^N X_i. $$\n\nThen\n\n$$\n\\frac{X^{(N)} - \\mu_{X^{(N)}}}{\\sigma_{X^{(N)}}}\n$$\n\nwill be approximately Gaussian. There are additional requirements needed to make it work, most commonly $E[X^4] < \\infty$, but the philosophy is clear. The central limit theorem is the reason why the Gaussian is fundamental to probability, statistics, and machine learning. Whenever we can say that something we measured is a sum of many small independent contributions, we can assume that the thing being measured will be close to Gaussian. There are many more fascinating properties of Gaussians, and we would like to discuss one more here. The Gaussian is what is known as a *maximum entropy distribution*."
    },
    {
      "chunk_id": "b09f695018b3_4",
      "chapter": "distributions",
      "heading": "Gaussian",
      "text": "There are many more fascinating properties of Gaussians, and we would like to discuss one more here. The Gaussian is what is known as a *maximum entropy distribution*. We will get into entropy more deeply in :numref:`sec_information_theory`, however all we need to know at this point is that it is a measure of randomness. In a rigorous mathematical sense, we can think of the Gaussian as the *most* random choice of random variable with fixed mean and variance. Thus, if we know that our random variable has some mean and variance, the Gaussian is in a sense the most conservative choice of distribution we can make. To close the section, let's recall that if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then:\n\n* $\\mu_X = \\mu$,\n* $\\sigma_X^2 = \\sigma^2$. We can sample from the Gaussian (or standard normal) distribution as shown below. ```{.python .input}\n#@tab mxnet\nnp.random.normal(mu, sigma, size=(10, 10))\n```\n\n```{.python .input}\n#@tab pytorch\ntorch.normal(mu, sigma, size=(10, 10))\n```\n\n```{.python .input}\n#@tab tensorflow\ntf.random.normal((10, 10), mu, sigma)\n```"
    },
    {
      "chunk_id": "faaab7b1f63e_0",
      "chapter": "distributions",
      "heading": "Exponential Family",
      "text": ":label:`subsec_exponential_family`\n\nOne shared property for all the distributions listed above is that they all \nbelong to which is known as the *exponential family*. The exponential family \nis a set of distributions whose density can be expressed in the following \nform:\n\n$$p(\\mathbf{x} \\mid \\boldsymbol{\\eta}) = h(\\mathbf{x}) \\cdot \\exp \\left( \\boldsymbol{\\eta}^{\\top} \\cdot T(\\mathbf{x}) - A(\\boldsymbol{\\eta}) \\right)$$\n:eqlabel:`eq_exp_pdf`\n\nAs this definition can be a little subtle, let's examine it closely. First, $h(\\mathbf{x})$ is known as the *underlying measure* or the \n*base measure*. This can be viewed as an original choice of measure we are \nmodifying with our exponential weight. Second, we have the vector $\\boldsymbol{\\eta} = (\\eta_1, \\eta_2, ..., \\eta_l) \\in\n\\mathbb{R}^l$ called the *natural parameters* or *canonical parameters*. These\ndefine how the base measure will be modified. The natural parameters enter \ninto the new measure by taking the dot product of these parameters against \nsome function $T(\\cdot)$ of $\\mathbf{x}= (x_1, x_2, ..., x_n) \\in\n\\mathbb{R}^n$ and exponentiated. The vector $T(\\mathbf{x})= (T_1(\\mathbf{x}),\nT_2(\\mathbf{x}), ..., T_l(\\mathbf{x}))$ \nis called the *sufficient statistics* for $\\boldsymbol{\\eta}$. This name is used since the \ninformation represented by $T(\\mathbf{x})$ is sufficient to calculate the \nprobability density and no other information from the sample $\\mathbf{x}$'s \nare required. Third, we have $A(\\boldsymbol{\\eta})$, which is referred to as the *cumulant \nfunction*, which ensures that the above distribution :eqref:`eq_exp_pdf` \nintegrates to one, i.e.,\n\n$$A(\\boldsymbol{\\eta})  = \\log \\left[\\int h(\\mathbf{x}) \\cdot \\exp\n\\left(\\boldsymbol{\\eta}^{\\top} \\cdot T(\\mathbf{x}) \\right) d\\mathbf{x} \\right].$$\n\nTo be concrete, let's consider the Gaussian."
    },
    {
      "chunk_id": "faaab7b1f63e_1",
      "chapter": "distributions",
      "heading": "Exponential Family",
      "text": "Assuming that $\\mathbf{x}$ is \nan univariate variable, we saw that it had a density of\n\n$$\n\\begin{aligned}\np(x \\mid \\mu, \\sigma) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\cdot \\exp \n\\left\\{ \\frac{-(x-\\mu)^2}{2 \\sigma^2} \\right\\} \\\\\n&= \\frac{1}{\\sqrt{2 \\pi}} \\cdot \\exp \\left\\{ \\frac{\\mu}{\\sigma^2}x\n-\\frac{1}{2 \\sigma^2} x^2 - \\left( \\frac{1}{2 \\sigma^2} \\mu^2\n+\\log(\\sigma) \\right) \\right\\}. \\end{aligned}\n$$\n\nThis matches the definition of the exponential family with:\n\n* *underlying measure*: $h(x) = \\frac{1}{\\sqrt{2 \\pi}}$,\n* *natural parameters*: $\\boldsymbol{\\eta} = \\begin{bmatrix} \\eta_1 \\\\ \\eta_2\n\\end{bmatrix} = \\begin{bmatrix} \\frac{\\mu}{\\sigma^2} \\\\\n\\frac{1}{2 \\sigma^2} \\end{bmatrix}$,\n* *sufficient statistics*: $T(x) = \\begin{bmatrix}x\\\\-x^2\\end{bmatrix}$, and\n* *cumulant function*: $A({\\boldsymbol\\eta}) = \\frac{1}{2 \\sigma^2} \\mu^2 + \\log(\\sigma)\n= \\frac{\\eta_1^2}{4 \\eta_2} - \\frac{1}{2}\\log(2 \\eta_2)$. It is worth noting that the exact choice of each of above terms is somewhat \narbitrary. Indeed, the important feature is that the distribution can be \nexpressed in this form, not the exact form itself. As we allude to in :numref:`subsec_softmax_and_derivatives`, a widely used \ntechnique is to assume that the  final output $\\mathbf{y}$ follows an \nexponential family distribution. The exponential family is a common and \npowerful family of distributions encountered frequently in machine learning."
    },
    {
      "chunk_id": "38ecc6487b50_0",
      "chapter": "distributions",
      "heading": "Summary",
      "text": "* Bernoulli random variables can be used to model events with a yes/no outcome.\n* Discrete uniform distributions model selects from a finite set of possibilities.\n* Continuous uniform distributions select from an interval.\n* Binomial distributions model a series of Bernoulli random variables, and count the number of successes.\n* Poisson random variables model the arrival of rare events.\n* Gaussian random variables model the result of adding a large number of independent random variables together.\n* All the above distributions belong to exponential family."
    },
    {
      "chunk_id": "dd4c8e189794_0",
      "chapter": "distributions",
      "heading": "Exercises",
      "text": "1. What is the standard deviation of a random variable that is the difference $X-Y$ of two independent binomial random variables $X, Y \\sim \\textrm{Binomial}(16, 1/2)$.\n2. If we take a Poisson random variable $X \\sim \\textrm{Poisson}(\\lambda)$ and consider $(X - \\lambda)/\\sqrt{\\lambda}$ as $\\lambda \\rightarrow \\infty$, we can show that this becomes approximately Gaussian.  Why does this make sense?\n3. What is the probability mass function for a sum of two discrete uniform random variables on $n$ elements?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/417)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1098)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1099)\n:end_tab:"
    },
    {
      "chunk_id": "d3c3abafeb7c_0",
      "chapter": "eigendecomposition",
      "heading": "eigendecomposition",
      "text": "# Eigendecompositions\n:label:`sec_eigendecompositions`\n\nEigenvalues are often one of the most useful notions\nwe will encounter when studying linear algebra,\nhowever, as a beginner, it is easy to overlook their importance.\nBelow, we introduce eigendecomposition and\ntry to convey some sense of just why it is so important.\n\nSuppose that we have a matrix $A$ with the following entries:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n2 & 0 \\\\\n0 & -1\n\\end{bmatrix}.\n$$\n\nIf we apply $A$ to any vector $\\mathbf{v} = [x, y]^\\top$,\nwe obtain a vector $\\mathbf{A}\\mathbf{v} = [2x, -y]^\\top$.\nThis has an intuitive interpretation:\nstretch the vector to be twice as wide in the $x$-direction,\nand then flip it in the $y$-direction.\n\nHowever, there are *some* vectors for which something remains unchanged.\nNamely $[1, 0]^\\top$ gets sent to $[2, 0]^\\top$\nand $[0, 1]^\\top$ gets sent to $[0, -1]^\\top$.\nThese vectors are still in the same line,\nand the only modification is that the matrix stretches them\nby a factor of $2$ and $-1$ respectively.\nWe call such vectors *eigenvectors*\nand the factor they are stretched by *eigenvalues*.\n\nIn general, if we can find a number $\\lambda$\nand a vector $\\mathbf{v}$ such that\n\n$$\n\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}.\n$$\n\nWe say that $\\mathbf{v}$ is an eigenvector for $A$ and $\\lambda$ is an eigenvalue."
    },
    {
      "chunk_id": "820de7473ea5_0",
      "chapter": "eigendecomposition",
      "heading": "Finding Eigenvalues",
      "text": "Let's figure out how to find them. By subtracting off the $\\lambda \\mathbf{v}$ from both sides,\nand then factoring out the vector,\nwe see the above is equivalent to:\n\n$$(\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = 0.$$\n:eqlabel:`eq_eigvalue_der`\n\nFor :eqref:`eq_eigvalue_der` to happen, we see that $(\\mathbf{A} - \\lambda \\mathbf{I})$\nmust compress some direction down to zero,\nhence it is not invertible, and thus the determinant is zero.\nThus, we can find the *eigenvalues*\nby finding for what $\\lambda$ is $\\det(\\mathbf{A}-\\lambda \\mathbf{I}) = 0$.\nOnce we find the eigenvalues, we can solve\n$\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$\nto find the associated *eigenvector(s)*."
    },
    {
      "chunk_id": "fbb1ac0f5d99_0",
      "chapter": "eigendecomposition",
      "heading": "An Example",
      "text": "Let's see this with a more challenging matrix\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n2 & 1\\\\\n2 & 3\n\\end{bmatrix}.\n$$\n\nIf we consider $\\det(\\mathbf{A}-\\lambda \\mathbf{I}) = 0$,\nwe see this is equivalent to the polynomial equation\n$0 = (2-\\lambda)(3-\\lambda)-2 = (4-\\lambda)(1-\\lambda)$.\nThus, two eigenvalues are $4$ and $1$.\nTo find the associated vectors, we then need to solve\n\n$$\n\\begin{bmatrix}\n2 & 1\\\\\n2 & 3\n\\end{bmatrix}\\begin{bmatrix}x \\\\ y\\end{bmatrix} = \\begin{bmatrix}x \\\\ y\\end{bmatrix}  \\; \\textrm{and} \\;\n\\begin{bmatrix}\n2 & 1\\\\\n2 & 3\n\\end{bmatrix}\\begin{bmatrix}x \\\\ y\\end{bmatrix}  = \\begin{bmatrix}4x \\\\ 4y\\end{bmatrix} .\n$$\n\nWe can solve this with the vectors $[1, -1]^\\top$ and $[1, 2]^\\top$ respectively.\n\nWe can check this in code using the built-in `numpy.linalg.eig` routine.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom IPython import display\nimport numpy as np\n\nnp.linalg.eig(np.array([[2, 1], [2, 3]]))\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom IPython import display\nimport torch\n\ntorch.linalg.eig(torch.tensor([[2, 1], [2, 3]], dtype=torch.float64))\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom IPython import display\nimport tensorflow as tf\n\ntf.linalg.eig(tf.constant([[2, 1], [2, 3]], dtype=tf.float64))\n```\n\nNote that `numpy` normalizes the eigenvectors to be of length one,\nwhereas we took ours to be of arbitrary length.\nAdditionally, the choice of sign is arbitrary.\nHowever, the vectors computed are parallel\nto the ones we found by hand with the same eigenvalues."
    },
    {
      "chunk_id": "a250797ba05e_0",
      "chapter": "eigendecomposition",
      "heading": "Decomposing Matrices",
      "text": "Let's continue the previous example one step further.  Let\n\n$$\n\\mathbf{W} = \\begin{bmatrix}\n1 & 1 \\\\\n-1 & 2\n\\end{bmatrix},\n$$\n\nbe the matrix where the columns are the eigenvectors of the matrix $\\mathbf{A}$. Let\n\n$$\n\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 4\n\\end{bmatrix},\n$$\n\nbe the matrix with the associated eigenvalues on the diagonal.\nThen the definition of eigenvalues and eigenvectors tells us that\n\n$$\n\\mathbf{A}\\mathbf{W} =\\mathbf{W} \\boldsymbol{\\Sigma} .\n$$\n\nThe matrix $W$ is invertible, so we may multiply both sides by $W^{-1}$ on the right,\nwe see that we may write\n\n$$\\mathbf{A} = \\mathbf{W} \\boldsymbol{\\Sigma} \\mathbf{W}^{-1}.$$\n:eqlabel:`eq_eig_decomp`\n\nIn the next section we will see some nice consequences of this,\nbut for now we need only know that such a decomposition\nwill exist as long as we can find a full collection\nof linearly independent eigenvectors (so that $W$ is invertible)."
    },
    {
      "chunk_id": "aa38ac0e77d5_0",
      "chapter": "eigendecomposition",
      "heading": "Operations on Eigendecompositions",
      "text": "One nice thing about eigendecompositions :eqref:`eq_eig_decomp` is that\nwe can write many operations we usually encounter cleanly\nin terms of the eigendecomposition. As a first example, consider:\n\n$$\n\\mathbf{A}^n = \\overbrace{\\mathbf{A}\\cdots \\mathbf{A}}^{\\textrm{$n$ times}} = \\overbrace{(\\mathbf{W}\\boldsymbol{\\Sigma} \\mathbf{W}^{-1})\\cdots(\\mathbf{W}\\boldsymbol{\\Sigma} \\mathbf{W}^{-1})}^{\\textrm{$n$ times}} =  \\mathbf{W}\\overbrace{\\boldsymbol{\\Sigma}\\cdots\\boldsymbol{\\Sigma}}^{\\textrm{$n$ times}}\\mathbf{W}^{-1} = \\mathbf{W}\\boldsymbol{\\Sigma}^n \\mathbf{W}^{-1}. $$\n\nThis tells us that for any positive power of a matrix,\nthe eigendecomposition is obtained by just raising the eigenvalues to the same power. The same can be shown for negative powers,\nso if we want to invert a matrix we need only consider\n\n$$\n\\mathbf{A}^{-1} = \\mathbf{W}\\boldsymbol{\\Sigma}^{-1} \\mathbf{W}^{-1},\n$$\n\nor in other words, just invert each eigenvalue. This will work as long as each eigenvalue is non-zero,\nso we see that invertible is the same as having no zero eigenvalues. Indeed, additional work can show that if $\\lambda_1, \\ldots, \\lambda_n$\nare the eigenvalues of a matrix, then the determinant of that matrix is\n\n$$\n\\det(\\mathbf{A}) = \\lambda_1 \\cdots \\lambda_n,\n$$\n\nor the product of all the eigenvalues. This makes sense intuitively because whatever stretching $\\mathbf{W}$ does,\n$W^{-1}$ undoes it, so in the end the only stretching that happens is\nby multiplication by the diagonal matrix $\\boldsymbol{\\Sigma}$,\nwhich stretches volumes by the product of the diagonal elements. Finally, recall that the rank was the maximum number\nof linearly independent columns of your matrix. By examining the eigendecomposition closely,\nwe can see that the rank is the same\nas the number of non-zero eigenvalues of $\\mathbf{A}$."
    },
    {
      "chunk_id": "aa38ac0e77d5_1",
      "chapter": "eigendecomposition",
      "heading": "Operations on Eigendecompositions",
      "text": "Finally, recall that the rank was the maximum number\nof linearly independent columns of your matrix. By examining the eigendecomposition closely,\nwe can see that the rank is the same\nas the number of non-zero eigenvalues of $\\mathbf{A}$. The examples could continue, but hopefully the point is clear:\neigendecomposition can simplify many linear-algebraic computations\nand is a fundamental operation underlying many numerical algorithms\nand much of the analysis that we do in linear algebra."
    },
    {
      "chunk_id": "c150fd13bf67_0",
      "chapter": "eigendecomposition",
      "heading": "Eigendecompositions of Symmetric Matrices",
      "text": "It is not always possible to find enough linearly independent eigenvectors\nfor the above process to work. For instance the matrix\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{bmatrix},\n$$\n\nhas only a single eigenvector, namely $(1, 0)^\\top$.\nTo handle such matrices, we require more advanced techniques\nthan we can cover (such as the Jordan Normal Form, or Singular Value Decomposition).\nWe will often need to restrict our attention to those matrices\nwhere we can guarantee the existence of a full set of eigenvectors.\n\nThe most commonly encountered family are the *symmetric matrices*,\nwhich are those matrices where $\\mathbf{A} = \\mathbf{A}^\\top$.\nIn this case, we may take $W$ to be an *orthogonal matrix*\u2014a matrix whose columns are all length one vectors that are at right angles to one another, where\n$\\mathbf{W}^\\top = \\mathbf{W}^{-1}$\u2014and all the eigenvalues will be real.\nThus, in this special case, we can write :eqref:`eq_eig_decomp` as\n\n$$\n\\mathbf{A} = \\mathbf{W}\\boldsymbol{\\Sigma}\\mathbf{W}^\\top .\n$$"
    },
    {
      "chunk_id": "07ca591ffb57_0",
      "chapter": "eigendecomposition",
      "heading": "Gershgorin Circle Theorem",
      "text": "Eigenvalues are often difficult to reason with intuitively. If presented an arbitrary matrix, there is little that can be said\nabout what the eigenvalues are without computing them. There is, however, one theorem that can make it easy to approximate well\nif the largest values are on the diagonal. Let $\\mathbf{A} = (a_{ij})$ be any square matrix ($n\\times n$). We will define $r_i = \\sum_{j \\neq i} |a_{ij}|$. Let $\\mathcal{D}_i$ represent the disc in the complex plane\nwith center $a_{ii}$ radius $r_i$. Then, every eigenvalue of $\\mathbf{A}$ is contained in one of the $\\mathcal{D}_i$. This can be a bit to unpack, so let's look at an example. Consider the matrix:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1.0 & 0.1 & 0.1 & 0.1 \\\\\n0.1 & 3.0 & 0.2 & 0.3 \\\\\n0.1 & 0.2 & 5.0 & 0.5 \\\\\n0.1 & 0.3 & 0.5 & 9.0\n\\end{bmatrix}. $$\n\nWe have $r_1 = 0.3$, $r_2 = 0.6$, $r_3 = 0.8$ and $r_4 = 0.9$. The matrix is symmetric, so all eigenvalues are real. This means that all of our eigenvalues will be in one of the ranges of\n\n$$[a_{11}-r_1, a_{11}+r_1] = [0.7, 1.3], $$\n\n$$[a_{22}-r_2, a_{22}+r_2] = [2.4, 3.6], $$\n\n$$[a_{33}-r_3, a_{33}+r_3] = [4.2, 5.8], $$\n\n$$[a_{44}-r_4, a_{44}+r_4] = [8.1, 9.9]. $$\n\n\nPerforming the numerical computation shows\nthat the eigenvalues are approximately $0.99$, $2.97$, $4.95$, $9.08$,\nall comfortably inside the ranges provided."
    },
    {
      "chunk_id": "07ca591ffb57_1",
      "chapter": "eigendecomposition",
      "heading": "Gershgorin Circle Theorem",
      "text": "$$\n\n\nPerforming the numerical computation shows\nthat the eigenvalues are approximately $0.99$, $2.97$, $4.95$, $9.08$,\nall comfortably inside the ranges provided. ```{.python .input}\n#@tab mxnet\nA = np.array([[1.0, 0.1, 0.1, 0.1],\n              [0.1, 3.0, 0.2, 0.3],\n              [0.1, 0.2, 5.0, 0.5],\n              [0.1, 0.3, 0.5, 9.0]])\n\nv, _ = np.linalg.eig(A)\nv\n```\n\n```{.python .input}\n#@tab pytorch\nA = torch.tensor([[1.0, 0.1, 0.1, 0.1],\n              [0.1, 3.0, 0.2, 0.3],\n              [0.1, 0.2, 5.0, 0.5],\n              [0.1, 0.3, 0.5, 9.0]])\n\nv, _ = torch.linalg.eig(A)\nv\n```\n\n```{.python .input}\n#@tab tensorflow\nA = tf.constant([[1.0, 0.1, 0.1, 0.1],\n                [0.1, 3.0, 0.2, 0.3],\n                [0.1, 0.2, 5.0, 0.5],\n                [0.1, 0.3, 0.5, 9.0]])\n\nv, _ = tf.linalg.eigh(A)\nv\n```\n\nIn this way, eigenvalues can be approximated,\nand the approximations will be fairly accurate\nin the case that the diagonal is\nsignificantly larger than all the other elements. It is a small thing, but with a complex\nand subtle topic like eigendecomposition,\nit is good to get any intuitive grasp we can."
    },
    {
      "chunk_id": "198647ff0996_0",
      "chapter": "eigendecomposition",
      "heading": "A Useful Application: The Growth of Iterated Maps",
      "text": "Now that we understand what eigenvectors are in principle,\nlet's see how they can be used to provide a deep understanding\nof a problem central to neural network behavior: proper weight initialization."
    },
    {
      "chunk_id": "cc38f6878d27_0",
      "chapter": "eigendecomposition",
      "heading": "Eigenvectors as Long Term Behavior",
      "text": "The full mathematical investigation of the initialization\nof deep neural networks is beyond the scope of the text,\nbut we can see a toy version here to understand\nhow eigenvalues can help us see how these models work.\nAs we know, neural networks operate by interspersing layers\nof linear transformations with non-linear operations.\nFor simplicity here, we will assume that there is no non-linearity,\nand that the transformation is a single repeated matrix operation $A$,\nso that the output of our model is\n\n$$\n\\mathbf{v}_{out} = \\mathbf{A}\\cdot \\mathbf{A}\\cdots \\mathbf{A} \\mathbf{v}_{in} = \\mathbf{A}^N \\mathbf{v}_{in}.\n$$\n\nWhen these models are initialized, $A$ is taken to be\na random matrix with Gaussian entries, so let's make one of those.\nTo be concrete, we start with a mean zero, variance one Gaussian distributed $5 \\times 5$ matrix.\n\n```{.python .input}\n#@tab mxnet\nnp.random.seed(8675309)\n\nk = 5\nA = np.random.randn(k, k)\nA\n```\n\n```{.python .input}\n#@tab pytorch\ntorch.manual_seed(42)\n\nk = 5\nA = torch.randn(k, k, dtype=torch.float64)\nA\n```\n\n```{.python .input}\n#@tab tensorflow\nk = 5\nA = tf.random.normal((k, k), dtype=tf.float64)\nA\n```"
    },
    {
      "chunk_id": "0ee6bfa2ea55_0",
      "chapter": "eigendecomposition",
      "heading": "Behavior on Random Data",
      "text": "For simplicity in our toy model,\nwe will assume that the data vector we feed in $\\mathbf{v}_{in}$\nis a random five dimensional Gaussian vector. Let's think about what we want to have happen. For context, lets think of a generic ML problem,\nwhere we are trying to turn input data, like an image, into a prediction,\nlike the probability the image is a picture of a cat. If repeated application of $\\mathbf{A}$\nstretches a random vector out to be very long,\nthen small changes in input will be amplified\ninto large changes in output---tiny modifications of the input image\nwould lead to vastly different predictions. This does not seem right! On the flip side, if $\\mathbf{A}$ shrinks random vectors to be shorter,\nthen after running through many layers, the vector will essentially shrink to nothing,\nand the output will not depend on the input. This is also clearly not right either! We need to walk the narrow line between growth and decay\nto make sure that our output changes depending on our input, but not much! Let's see what happens when we repeatedly multiply our matrix $\\mathbf{A}$\nagainst a random input vector, and keep track of the norm."
    },
    {
      "chunk_id": "0ee6bfa2ea55_1",
      "chapter": "eigendecomposition",
      "heading": "Behavior on Random Data",
      "text": "Let's see what happens when we repeatedly multiply our matrix $\\mathbf{A}$\nagainst a random input vector, and keep track of the norm. ```{.python .input}\n#@tab mxnet\n# Calculate the sequence of norms after repeatedly applying `A`\nv_in = np.random.randn(k, 1)\n\nnorm_list = [np.linalg.norm(v_in)]\nfor i in range(1, 100):\n    v_in = A.dot(v_in)\n    norm_list.append(np.linalg.norm(v_in))\n\nd2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')\n```\n\n```{.python .input}\n#@tab pytorch\n# Calculate the sequence of norms after repeatedly applying `A`\nv_in = torch.randn(k, 1, dtype=torch.float64)\n\nnorm_list = [torch.norm(v_in).item()]\nfor i in range(1, 100):\n    v_in = A @ v_in\n    norm_list.append(torch.norm(v_in).item())\n\nd2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Calculate the sequence of norms after repeatedly applying `A`\nv_in = tf.random.normal((k, 1), dtype=tf.float64)\n\nnorm_list = [tf.norm(v_in).numpy()]\nfor i in range(1, 100):\n    v_in = tf.matmul(A, v_in)\n    norm_list.append(tf.norm(v_in).numpy())\n\nd2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')\n```\n\nThe norm is growing uncontrollably! Indeed if we take the list of quotients, we will see a pattern."
    },
    {
      "chunk_id": "0ee6bfa2ea55_2",
      "chapter": "eigendecomposition",
      "heading": "Behavior on Random Data",
      "text": "Indeed if we take the list of quotients, we will see a pattern. ```{.python .input}\n#@tab mxnet\n# Compute the scaling factor of the norms\nnorm_ratio_list = []\nfor i in range(1, 100):\n    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])\n\nd2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute the scaling factor of the norms\nnorm_ratio_list = []\nfor i in range(1, 100):\n    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])\n\nd2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Compute the scaling factor of the norms\nnorm_ratio_list = []\nfor i in range(1, 100):\n    norm_ratio_list.append(norm_list[i]/norm_list[i - 1])\n\nd2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\n```\n\nIf we look at the last portion of the above computation,\nwe see that the random vector is stretched by a factor of `1.974459321485[...]`,\nwhere the portion at the end shifts a little,\nbut the stretching factor is stable."
    },
    {
      "chunk_id": "d68034bfa840_0",
      "chapter": "eigendecomposition",
      "heading": "Relating Back to Eigenvectors",
      "text": "We have seen that eigenvectors and eigenvalues correspond\nto the amount something is stretched,\nbut that was for specific vectors, and specific stretches.\nLet's take a look at what they are for $\\mathbf{A}$.\nA bit of a caveat here: it turns out that to see them all,\nwe will need to go to complex numbers.\nYou can think of these as stretches and rotations.\nBy taking the norm of the complex number\n(square root of the sums of squares of real and imaginary parts)\nwe can measure that stretching factor. Let's also sort them.\n\n```{.python .input}\n#@tab mxnet\n# Compute the eigenvalues\neigs = np.linalg.eigvals(A).tolist()\nnorm_eigs = [np.absolute(x) for x in eigs]\nnorm_eigs.sort()\nprint(f'norms of eigenvalues: {norm_eigs}')\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute the eigenvalues\neigs = torch.linalg.eig(A).eigenvalues.tolist()\nnorm_eigs = [torch.abs(torch.tensor(x)) for x in eigs]\nnorm_eigs.sort()\nprint(f'norms of eigenvalues: {norm_eigs}')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Compute the eigenvalues\neigs = tf.linalg.eigh(A)[0].numpy().tolist()\nnorm_eigs = [tf.abs(tf.constant(x, dtype=tf.float64)) for x in eigs]\nnorm_eigs.sort()\nprint(f'norms of eigenvalues: {norm_eigs}')\n```"
    },
    {
      "chunk_id": "9de87c6a7097_0",
      "chapter": "eigendecomposition",
      "heading": "An Observation",
      "text": "We see something a bit unexpected happening here:\nthat number we identified before for the\nlong term stretching of our matrix $\\mathbf{A}$\napplied to a random vector is *exactly*\n(accurate to thirteen decimal places!)\nthe largest eigenvalue of $\\mathbf{A}$.\nThis is clearly not a coincidence!\n\nBut, if we now think about what is happening geometrically,\nthis starts to make sense. Consider a random vector.\nThis random vector points a little in every direction,\nso in particular, it points at least a little bit\nin the same direction as the eigenvector of $\\mathbf{A}$\nassociated with the largest eigenvalue.\nThis is so important that it is called\nthe *principle eigenvalue* and *principle eigenvector*.\nAfter applying $\\mathbf{A}$, our random vector\ngets stretched in every possible direction,\nas is associated with every possible eigenvector,\nbut it is stretched most of all in the direction\nassociated with this principle eigenvector.\nWhat this means is that after apply in $A$,\nour random vector is longer, and points in a direction\ncloser to being aligned with the principle eigenvector.\nAfter applying the matrix many times,\nthe alignment with the principle eigenvector becomes closer and closer until,\nfor all practical purposes, our random vector has been transformed\ninto the principle eigenvector!\nIndeed this algorithm is the basis\nfor what is known as the *power iteration*\nfor finding the largest eigenvalue and eigenvector of a matrix. For details see, for example, :cite:`Golub.Van-Loan.1996`."
    },
    {
      "chunk_id": "dc67a5c8d37b_0",
      "chapter": "eigendecomposition",
      "heading": "Fixing the Normalization",
      "text": "Now, from above discussions, we concluded\nthat we do not want a random vector to be stretched or squished at all,\nwe would like random vectors to stay about the same size throughout the entire process. To do so, we now rescale our matrix by this principle eigenvalue\nso that the largest eigenvalue is instead now just one. Let's see what happens in this case. ```{.python .input}\n#@tab mxnet\n# Rescale the matrix `A`\nA /= norm_eigs[-1]\n\n# Do the same experiment again\nv_in = np.random.randn(k, 1)\n\nnorm_list = [np.linalg.norm(v_in)]\nfor i in range(1, 100):\n    v_in = A.dot(v_in)\n    norm_list.append(np.linalg.norm(v_in))\n\nd2l.plot(np.arange(0, 100), norm_list, 'Iteration', 'Value')\n```\n\n```{.python .input}\n#@tab pytorch\n# Rescale the matrix `A`\nA /= norm_eigs[-1]\n\n# Do the same experiment again\nv_in = torch.randn(k, 1, dtype=torch.float64)\n\nnorm_list = [torch.norm(v_in).item()]\nfor i in range(1, 100):\n    v_in = A @ v_in\n    norm_list.append(torch.norm(v_in).item())\n\nd2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Rescale the matrix `A`\nA /= norm_eigs[-1]\n\n# Do the same experiment again\nv_in = tf.random.normal((k, 1), dtype=tf.float64)\n\nnorm_list = [tf.norm(v_in).numpy()]\nfor i in range(1, 100):\n    v_in = tf.matmul(A, v_in)\n    norm_list.append(tf.norm(v_in).numpy())\n\nd2l.plot(tf.range(0, 100), norm_list, 'Iteration', 'Value')\n```\n\nWe can also plot the ratio between consecutive norms as before and see that indeed it stabilizes."
    },
    {
      "chunk_id": "dc67a5c8d37b_1",
      "chapter": "eigendecomposition",
      "heading": "Fixing the Normalization",
      "text": "```{.python .input}\n#@tab mxnet\n# Also plot the ratio\nnorm_ratio_list = []\nfor i in range(1, 100):\n    norm_ratio_list.append(norm_list[i]/norm_list[i-1])\n\nd2l.plot(np.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\n```\n\n```{.python .input}\n#@tab pytorch\n# Also plot the ratio\nnorm_ratio_list = []\nfor i in range(1, 100):\n    norm_ratio_list.append(norm_list[i]/norm_list[i-1])\n\nd2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Also plot the ratio\nnorm_ratio_list = []\nfor i in range(1, 100):\n    norm_ratio_list.append(norm_list[i]/norm_list[i-1])\n\nd2l.plot(tf.range(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\n```"
    },
    {
      "chunk_id": "1b6317e3d8e5_0",
      "chapter": "eigendecomposition",
      "heading": "Discussion",
      "text": "We now see exactly what we hoped for!\nAfter normalizing the matrices by the principal eigenvalue,\nwe see that the random data does not explode as before,\nbut rather eventually equilibrates to a specific value.\nIt would be nice to be able to do these things from first principles,\nand it turns out that if we look deeply at the mathematics of it,\nwe can see that the largest eigenvalue\nof a large random matrix with independent mean zero,\nvariance one Gaussian entries is on average about $\\sqrt{n}$,\nor in our case $\\sqrt{5} \\approx 2.2$,\ndue to a fascinating fact known as the *circular law* :cite:`Ginibre.1965`.\nThe relationship between the eigenvalues (and a related object called singular values) of random matrices has been shown to have deep connections to proper initialization of neural networks as was discussed in :citet:`Pennington.Schoenholz.Ganguli.2017` and subsequent works."
    },
    {
      "chunk_id": "195ea253daa8_0",
      "chapter": "eigendecomposition",
      "heading": "Summary",
      "text": "* Eigenvectors are vectors which are stretched by a matrix without changing direction.\n* Eigenvalues are the amount that the eigenvectors are stretched by the application of the matrix.\n* The eigendecomposition of a matrix can allow for many operations to be reduced to operations on the eigenvalues.\n* The Gershgorin Circle Theorem can provide approximate values for the eigenvalues of a matrix.\n* The behavior of iterated matrix powers depends primarily on the size of the largest eigenvalue.  This understanding has many applications in the theory of neural network initialization."
    },
    {
      "chunk_id": "28a5979bcabe_0",
      "chapter": "eigendecomposition",
      "heading": "Exercises",
      "text": "1. What are the eigenvalues and eigenvectors of\n$$\n\\mathbf{A} = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}?\n$$\n1.  What are the eigenvalues and eigenvectors of the following matrix, and what is strange about this example compared to the previous one?\n$$\n\\mathbf{A} = \\begin{bmatrix}\n2 & 1 \\\\\n0 & 2\n\\end{bmatrix}.\n$$\n1. Without computing the eigenvalues, is it possible that the smallest eigenvalue of the following matrix is less that $0.5$? *Note*: this problem can be done in your head.\n$$\n\\mathbf{A} = \\begin{bmatrix}\n3.0 & 0.1 & 0.3 & 1.0 \\\\\n0.1 & 1.0 & 0.1 & 0.2 \\\\\n0.3 & 0.1 & 5.0 & 0.0 \\\\\n1.0 & 0.2 & 0.0 & 1.8\n\\end{bmatrix}.\n$$\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/411)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1086)\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1087)\n:end_tab:"
    },
    {
      "chunk_id": "f2f94a54aa87_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "geometry-linear-algebraic-ops",
      "text": "# Geometry and Linear Algebraic Operations\n:label:`sec_geometry-linear-algebraic-ops`\n\nIn :numref:`sec_linear-algebra`, we encountered the basics of linear algebra\nand saw how it could be used to express common operations for transforming our data.\nLinear algebra is one of the key mathematical pillars\nunderlying much of the work that we do in deep learning\nand in machine learning more broadly.\nWhile :numref:`sec_linear-algebra` contained enough machinery\nto communicate the mechanics of modern deep learning models,\nthere is a lot more to the subject.\nIn this section, we will go deeper,\nhighlighting some geometric interpretations of linear algebra operations,\nand introducing a few fundamental concepts, including of eigenvalues and eigenvectors."
    },
    {
      "chunk_id": "3d2582312c6b_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Geometry of Vectors",
      "text": "First, we need to discuss the two common geometric interpretations of vectors,\nas either points or directions in space. Fundamentally, a vector is a list of numbers such as the Python list below. ```{.python .input}\n#@tab all\nv = [1, 7, 0, 1]\n```\n\nMathematicians most often write this as either a *column* or *row* vector, which is to say either as\n\n$$\n\\mathbf{x} = \\begin{bmatrix}1\\\\7\\\\0\\\\1\\end{bmatrix},\n$$\n\nor\n\n$$\n\\mathbf{x}^\\top = \\begin{bmatrix}1 & 7 & 0 & 1\\end{bmatrix}. $$\n\nThese often have different interpretations,\nwhere data examples are column vectors\nand weights used to form weighted sums are row vectors. However, it can be beneficial to be flexible. As we have described in :numref:`sec_linear-algebra`,\nthough a single vector's default orientation is a column vector,\nfor any matrix representing a tabular dataset,\ntreating each data example as a row vector\nin the matrix\nis more conventional. Given a vector, the first interpretation\nthat we should give it is as a point in space. In two or three dimensions, we can visualize these points\nby using the components of the vectors to define\nthe location of the points in space compared\nto a fixed reference called the *origin*. This can be seen in :numref:`fig_grid`. ![An illustration of visualizing vectors as points in the plane. The first component of the vector gives the $\\mathit{x}$-coordinate, the second component gives the $\\mathit{y}$-coordinate. Higher dimensions are analogous, although much harder to visualize.](../img/grid-points.svg)\n:label:`fig_grid`\n\nThis geometric point of view allows us to consider the problem on a more abstract level. No longer faced with some insurmountable seeming problem\nlike classifying pictures as either cats or dogs,\nwe can start considering tasks abstractly\nas collections of points in space and picturing the task\nas discovering how to separate two distinct clusters of points. In parallel, there is a second point of view\nthat people often take of vectors: as directions in space."
    },
    {
      "chunk_id": "3d2582312c6b_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Geometry of Vectors",
      "text": "In parallel, there is a second point of view\nthat people often take of vectors: as directions in space. Not only can we think of the vector $\\mathbf{v} = [3,2]^\\top$\nas the location $3$ units to the right and $2$ units up from the origin,\nwe can also think of it as the direction itself\nto take $3$ steps to the right and $2$ steps up. In this way, we consider all the vectors in figure :numref:`fig_arrow` the same. ![Any vector can be visualized as an arrow in the plane. In this case, every vector drawn is a representation of the vector $(3,2)^\\top$.](../img/par-vec.svg)\n:label:`fig_arrow`\n\nOne of the benefits of this shift is that\nwe can make visual sense of the act of vector addition. In particular, we follow the directions given by one vector,\nand then follow the directions given by the other, as is seen in :numref:`fig_add-vec`. ![We can visualize vector addition by first following one vector, and then another.](../img/vec-add.svg)\n:label:`fig_add-vec`\n\nVector subtraction has a similar interpretation. By considering the identity that $\\mathbf{u} = \\mathbf{v} + (\\mathbf{u}-\\mathbf{v})$,\nwe see that the vector $\\mathbf{u}-\\mathbf{v}$ is the direction\nthat takes us from the point $\\mathbf{v}$ to the point $\\mathbf{u}$."
    },
    {
      "chunk_id": "8246e8704683_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Dot Products and Angles",
      "text": "As we saw in :numref:`sec_linear-algebra`,\nif we take two column vectors $\\mathbf{u}$ and $\\mathbf{v}$,\nwe can form their dot product by computing:\n\n$$\\mathbf{u}^\\top\\mathbf{v} = \\sum_i u_i\\cdot v_i.$$\n:eqlabel:`eq_dot_def`\n\nBecause :eqref:`eq_dot_def` is symmetric, we will mirror the notation\nof classical multiplication and write\n\n$$\n\\mathbf{u}\\cdot\\mathbf{v} = \\mathbf{u}^\\top\\mathbf{v} = \\mathbf{v}^\\top\\mathbf{u},\n$$\n\nto highlight the fact that exchanging the order of the vectors will yield the same answer. The dot product :eqref:`eq_dot_def` also admits a geometric interpretation: it is closely related to the angle between two vectors. Consider the angle shown in :numref:`fig_angle`. ![Between any two vectors in the plane there is a well defined angle $\\theta$. We will see this angle is intimately tied to the dot product.](../img/vec-angle.svg)\n:label:`fig_angle`\n\nTo start, let's consider two specific vectors:\n\n$$\n\\mathbf{v} = (r,0) \\; \\textrm{and} \\; \\mathbf{w} = (s\\cos(\\theta), s \\sin(\\theta)). $$\n\nThe vector $\\mathbf{v}$ is length $r$ and runs parallel to the $x$-axis,\nand the vector $\\mathbf{w}$ is of length $s$ and at angle $\\theta$ with the $x$-axis. If we compute the dot product of these two vectors, we see that\n\n$$\n\\mathbf{v}\\cdot\\mathbf{w} = rs\\cos(\\theta) = \\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos(\\theta). $$\n\nWith some simple algebraic manipulation, we can rearrange terms to obtain\n\n$$\n\\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right). $$\n\nIn short, for these two specific vectors,\nthe dot product combined with the norms tell us the angle between the two vectors. This same fact is true in general. We will not derive the expression here, however,\nif we consider writing $\\|\\mathbf{v} - \\mathbf{w}\\|^2$ in two ways:\none with the dot product, and the other geometrically using the law of cosines,\nwe can obtain the full relationship."
    },
    {
      "chunk_id": "8246e8704683_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Dot Products and Angles",
      "text": "We will not derive the expression here, however,\nif we consider writing $\\|\\mathbf{v} - \\mathbf{w}\\|^2$ in two ways:\none with the dot product, and the other geometrically using the law of cosines,\nwe can obtain the full relationship. Indeed, for any two vectors $\\mathbf{v}$ and $\\mathbf{w}$,\nthe angle between the two vectors is\n\n$$\\theta = \\arccos\\left(\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}\\right).$$\n:eqlabel:`eq_angle_forumla`\n\nThis is a nice result since nothing in the computation references two-dimensions. Indeed, we can use this in three or three million dimensions without issue. As a simple example, let's see how to compute the angle between a pair of vectors:\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom IPython import display\nfrom mxnet import gluon, np, npx\nnpx.set_np()\n\ndef angle(v, w):\n    return np.arccos(v.dot(w) / (np.linalg.norm(v) * np.linalg.norm(w)))\n\nangle(np.array([0, 1, 2]), np.array([2, 3, 4]))\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom IPython import display\nimport torch\nfrom torchvision import transforms\nimport torchvision\n\ndef angle(v, w):\n    return torch.acos(v.dot(w) / (torch.norm(v) * torch.norm(w)))\n\nangle(torch.tensor([0, 1, 2], dtype=torch.float32), torch.tensor([2.0, 3, 4]))\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom IPython import display\nimport tensorflow as tf\n\ndef angle(v, w):\n    return tf.acos(tf.tensordot(v, w, axes=1) / (tf.norm(v) * tf.norm(w)))\n\nangle(tf.constant([0, 1, 2], dtype=tf.float32), tf.constant([2.0, 3, 4]))\n```\n\nWe will not use it right now, but it is useful to know\nthat we will refer to vectors for which the angle is $\\pi/2$\n(or equivalently $90^{\\circ}$) as being *orthogonal*. By examining the equation above, we see that this happens when $\\theta = \\pi/2$,\nwhich is the same thing as $\\cos(\\theta) = 0$."
    },
    {
      "chunk_id": "8246e8704683_2",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Dot Products and Angles",
      "text": "By examining the equation above, we see that this happens when $\\theta = \\pi/2$,\nwhich is the same thing as $\\cos(\\theta) = 0$. The only way this can happen is if the dot product itself is zero,\nand two vectors are orthogonal if and only if $\\mathbf{v}\\cdot\\mathbf{w} = 0$. This will prove to be a helpful formula when understanding objects geometrically. It is reasonable to ask: why is computing the angle useful? The answer comes in the kind of invariance we expect data to have. Consider an image, and a duplicate image,\nwhere every pixel value is the same but $10\\%$ the brightness. The values of the individual pixels are in general far from the original values. Thus, if one computed the distance between the original image and the darker one,\nthe distance can be large. However, for most ML applications, the *content* is the same---it is still\nan image of a cat as far as a cat/dog classifier is concerned. However, if we consider the angle, it is not hard to see\nthat for any vector $\\mathbf{v}$, the angle\nbetween $\\mathbf{v}$ and $0.1\\cdot\\mathbf{v}$ is zero. This corresponds to the fact that scaling vectors\nkeeps the same direction and just changes the length. The angle considers the darker image identical. Examples like this are everywhere. In text, we might want the topic being discussed\nto not change if we write twice as long of document that says the same thing. For some encoding (such as counting the number of occurrences of words in some vocabulary), this corresponds to a doubling of the vector encoding the document,\nso again we can use the angle."
    },
    {
      "chunk_id": "1deba1cee603_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Cosine Similarity",
      "text": "In ML contexts where the angle is employed\nto measure the closeness of two vectors,\npractitioners adopt the term *cosine similarity*\nto refer to the portion\n$$\n\\cos(\\theta) = \\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|}.\n$$\n\nThe cosine takes a maximum value of $1$\nwhen the two vectors point in the same direction,\na minimum value of $-1$ when they point in opposite directions,\nand a value of $0$ when the two vectors are orthogonal.\nNote that if the components of high-dimensional vectors\nare sampled randomly with mean $0$,\ntheir cosine will nearly always be close to $0$."
    },
    {
      "chunk_id": "fbae5e65d6ce_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Hyperplanes",
      "text": "In addition to working with vectors, another key object\nthat you must understand to go far in linear algebra\nis the *hyperplane*, a generalization to higher dimensions\nof a line (two dimensions) or of a plane (three dimensions). In an $d$-dimensional vector space, a hyperplane has $d-1$ dimensions\nand divides the space into two half-spaces. Let's start with an example. Suppose that we have a column vector $\\mathbf{w}=[2,1]^\\top$. We want to know, \"what are the points $\\mathbf{v}$ with $\\mathbf{w}\\cdot\\mathbf{v} = 1$?\"\nBy recalling the connection between dot products and angles above :eqref:`eq_angle_forumla`,\nwe can see that this is equivalent to\n$$\n\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|\\cos(\\theta) = 1 \\; \\iff \\; \\|\\mathbf{v}\\|\\cos(\\theta) = \\frac{1}{\\|\\mathbf{w}\\|} = \\frac{1}{\\sqrt{5}}. $$\n\n![Recalling trigonometry, we see the formula $\\|\\mathbf{v}\\|\\cos(\\theta)$ is the length of the projection of the vector $\\mathbf{v}$ onto the direction of $\\mathbf{w}$](../img/proj-vec.svg)\n:label:`fig_vector-project`\n\nIf we consider the geometric meaning of this expression,\nwe see that this is equivalent to saying\nthat the length of the projection of $\\mathbf{v}$\nonto the direction of $\\mathbf{w}$ is exactly $1/\\|\\mathbf{w}\\|$, as is shown in :numref:`fig_vector-project`. The set of all points where this is true is a line\nat right angles to the vector $\\mathbf{w}$. If we wanted, we could find the equation for this line\nand see that it is $2x + y = 1$ or equivalently $y = 1 - 2x$. If we now look at what happens when we ask about the set of points with\n$\\mathbf{w}\\cdot\\mathbf{v} > 1$ or $\\mathbf{w}\\cdot\\mathbf{v} < 1$,\nwe can see that these are cases where the projections\nare longer or shorter than $1/\\|\\mathbf{w}\\|$, respectively. Thus, those two inequalities define either side of the line. In this way, we have found a way to cut our space into two halves,\nwhere all the points on one side have dot product below a threshold,\nand the other side above as we see in :numref:`fig_space-division`."
    },
    {
      "chunk_id": "fbae5e65d6ce_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Hyperplanes",
      "text": "In this way, we have found a way to cut our space into two halves,\nwhere all the points on one side have dot product below a threshold,\nand the other side above as we see in :numref:`fig_space-division`. ![If we now consider the inequality version of the expression, we see that our hyperplane (in this case: just a line) separates the space into two halves.](../img/space-division.svg)\n:label:`fig_space-division`\n\nThe story in higher dimension is much the same. If we now take $\\mathbf{w} = [1,2,3]^\\top$\nand ask about the points in three dimensions with $\\mathbf{w}\\cdot\\mathbf{v} = 1$,\nwe obtain a plane at right angles to the given vector $\\mathbf{w}$. The two inequalities again define the two sides of the plane as is shown in :numref:`fig_higher-division`. ![Hyperplanes in any dimension separate the space into two halves.](../img/space-division-3d.svg)\n:label:`fig_higher-division`\n\nWhile our ability to visualize runs out at this point,\nnothing stops us from doing this in tens, hundreds, or billions of dimensions. This occurs often when thinking about machine learned models. For instance, we can understand linear classification models\nlike those from :numref:`sec_softmax`,\nas methods to find hyperplanes that separate the different target classes. In this context, such hyperplanes are often referred to as *decision planes*. The majority of deep learned classification models end\nwith a linear layer fed into a softmax,\nso one can interpret the role of the deep neural network\nto be to find a non-linear embedding such that the target classes\ncan be separated cleanly by hyperplanes. To give a hand-built example, notice that we can produce a reasonable model\nto classify tiny images of t-shirts and trousers from the Fashion-MNIST dataset\n(seen in :numref:`sec_fashion_mnist`)\nby just taking the vector between their means to define the decision plane\nand eyeball a crude threshold. First we will load the data and compute the averages."
    },
    {
      "chunk_id": "fbae5e65d6ce_2",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Hyperplanes",
      "text": "First we will load the data and compute the averages. ```{.python .input}\n#@tab mxnet\n# Load in the dataset\ntrain = gluon.data.vision.FashionMNIST(train=True)\ntest = gluon.data.vision.FashionMNIST(train=False)\n\nX_train_0 = np.stack([x[0] for x in train if x[1] == 0]).astype(float)\nX_train_1 = np.stack([x[0] for x in train if x[1] == 1]).astype(float)\nX_test = np.stack(\n    [x[0] for x in test if x[1] == 0 or x[1] == 1]).astype(float)\ny_test = np.stack(\n    [x[1] for x in test if x[1] == 0 or x[1] == 1]).astype(float)\n\n# Compute averages\nave_0 = np.mean(X_train_0, axis=0)\nave_1 = np.mean(X_train_1, axis=0)\n```\n\n```{.python .input}\n#@tab pytorch\n# Load in the dataset\ntrans = []\ntrans.append(transforms.ToTensor())\ntrans = transforms.Compose(trans)\ntrain = torchvision.datasets.FashionMNIST(root=\"../data\", transform=trans,\n                                          train=True, download=True)\ntest = torchvision.datasets.FashionMNIST(root=\"../data\", transform=trans,\n                                         train=False, download=True)\n\nX_train_0 = torch.stack(\n    [x[0] * 256 for x in train if x[1] == 0]).type(torch.float32)\nX_train_1 = torch.stack(\n    [x[0] * 256 for x in train if x[1] == 1]).type(torch.float32)\nX_test = torch.stack(\n    [x[0] * 256 for x in test if x[1] == 0 or x[1] == 1]).type(torch.float32)\ny_test = torch.stack([torch.tensor(x[1]) for x in test\n                      if x[1] == 0 or x[1] == 1]).type(torch.float32)\n\n# Compute averages\nave_0 = torch.mean(X_train_0, axis=0)\nave_1 = torch.mean(X_train_1, axis=0)\n```\n\n```{.python .input}\n#@tab tensorflow\n# Load in the dataset\n((train_images, train_labels), (\n    test_images, test_labels)) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX_train_0 = tf.cast(tf.stack(train_images[[i for i, label in enumerate(\n    train_labels) if label == 0]] * 256), dtype=tf.float32)\nX_train_1 = tf.cast(tf.stack(train_images[[i for i, label in enumerate(\n    train_labels) if label == 1]] * 256), dtype=tf.float32)\nX_test = tf.cast(tf.stack(test_images[[i for i, label in enumerate(\n    test_labels) if label == 0]] * 256), dtype=tf.float32)\ny_test = tf.cast(tf.stack(test_images[[i for i, label in enumerate(\n    test_labels) if label == 1]] * 256), dtype=tf.float32)\n\n# Compute averages\nave_0 = tf.reduce_mean(X_train_0, axis=0)\nave_1 = tf.reduce_mean(X_train_1, axis=0)\n```\n\nIt can be informative to examine these averages in detail, so let's plot what they look like."
    },
    {
      "chunk_id": "fbae5e65d6ce_3",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Hyperplanes",
      "text": "In this case, we see that the average indeed resembles a blurry image of a t-shirt. ```{.python .input}\n#@tab mxnet, pytorch\n# Plot average t-shirt\nd2l.set_figsize()\nd2l.plt.imshow(ave_0.reshape(28, 28).tolist(), cmap='Greys')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot average t-shirt\nd2l.set_figsize()\nd2l.plt.imshow(tf.reshape(ave_0, (28, 28)), cmap='Greys')\nd2l.plt.show()\n```\n\nIn the second case, we again see that the average resembles a blurry image of trousers. ```{.python .input}\n#@tab mxnet, pytorch\n# Plot average trousers\nd2l.plt.imshow(ave_1.reshape(28, 28).tolist(), cmap='Greys')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot average trousers\nd2l.plt.imshow(tf.reshape(ave_1, (28, 28)), cmap='Greys')\nd2l.plt.show()\n```\n\nIn a fully machine learned solution, we would learn the threshold from the dataset. In this case, I simply eyeballed a threshold that looked good on the training data by hand. ```{.python .input}\n#@tab mxnet\n# Print test set accuracy with eyeballed threshold\nw = (ave_1 - ave_0).T\npredictions = X_test.reshape(2000, -1).dot(w.flatten()) > -1500000\n\n# Accuracy\nnp.mean(predictions.astype(y_test.dtype) == y_test, dtype=np.float64)\n```\n\n```{.python .input}\n#@tab pytorch\n# Print test set accuracy with eyeballed threshold\nw = (ave_1 - ave_0).T\n# '@' is Matrix Multiplication operator in pytorch. predictions = X_test.reshape(2000, -1) @ (w.flatten()) > -1500000\n\n# Accuracy\ntorch.mean((predictions.type(y_test.dtype) == y_test).float(), dtype=torch.float64)\n```\n\n```{.python .input}\n#@tab tensorflow\n# Print test set accuracy with eyeballed threshold\nw = tf.transpose(ave_1 - ave_0)\npredictions = tf.reduce_sum(X_test * tf.nest.flatten(w), axis=0) > -1500000\n\n# Accuracy\ntf.reduce_mean(\n    tf.cast(tf.cast(predictions, y_test.dtype) == y_test, tf.float32))\n```"
    },
    {
      "chunk_id": "33133fee2326_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Geometry of Linear Transformations",
      "text": "Through :numref:`sec_linear-algebra` and the above discussions,\nwe have a solid understanding of the geometry of vectors, lengths, and angles. However, there is one important object we have omitted discussing,\nand that is a geometric understanding of linear transformations represented by matrices. Fully internalizing what matrices can do to transform data\nbetween two potentially different high dimensional spaces takes significant practice,\nand is beyond the scope of this appendix. However, we can start building up intuition in two dimensions. Suppose that we have some matrix:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\ c & d\n\\end{bmatrix}. $$\n\nIf we want to apply this to an arbitrary vector\n$\\mathbf{v} = [x, y]^\\top$,\nwe multiply and see that\n\n$$\n\\begin{aligned}\n\\mathbf{A}\\mathbf{v} & = \\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}\\begin{bmatrix}x \\\\ y\\end{bmatrix} \\\\\n& = \\begin{bmatrix}ax+by\\\\ cx+dy\\end{bmatrix} \\\\\n& = x\\begin{bmatrix}a \\\\ c\\end{bmatrix} + y\\begin{bmatrix}b \\\\d\\end{bmatrix} \\\\\n& = x\\left\\{\\mathbf{A}\\begin{bmatrix}1\\\\0\\end{bmatrix}\\right\\} + y\\left\\{\\mathbf{A}\\begin{bmatrix}0\\\\1\\end{bmatrix}\\right\\}. \\end{aligned}\n$$\n\nThis may seem like an odd computation,\nwhere something clear became somewhat impenetrable. However, it tells us that we can write the way\nthat a matrix transforms *any* vector\nin terms of how it transforms *two specific vectors*:\n$[1,0]^\\top$ and $[0,1]^\\top$. This is worth considering for a moment. We have essentially reduced an infinite problem\n(what happens to any pair of real numbers)\nto a finite one (what happens to these specific vectors). These vectors are an example a *basis*,\nwhere we can write any vector in our space\nas a weighted sum of these *basis vectors*. Let's draw what happens when we use the specific matrix\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 \\\\\n-1 & 3\n\\end{bmatrix}."
    },
    {
      "chunk_id": "33133fee2326_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Geometry of Linear Transformations",
      "text": "These vectors are an example a *basis*,\nwhere we can write any vector in our space\nas a weighted sum of these *basis vectors*. Let's draw what happens when we use the specific matrix\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 \\\\\n-1 & 3\n\\end{bmatrix}. $$\n\nIf we look at the specific vector $\\mathbf{v} = [2, -1]^\\top$,\nwe see this is $2\\cdot[1,0]^\\top + -1\\cdot[0,1]^\\top$,\nand thus we know that the matrix $A$ will send this to\n$2(\\mathbf{A}[1,0]^\\top) + -1(\\mathbf{A}[0,1])^\\top = 2[1, -1]^\\top - [2,3]^\\top = [0, -5]^\\top$. If we follow this logic through carefully,\nsay by considering the grid of all integer pairs of points,\nwe see that what happens is that the matrix multiplication\ncan skew, rotate, and scale the grid,\nbut the grid structure must remain as you see in :numref:`fig_grid-transform`. ![The matrix $\\mathbf{A}$ acting on the given basis vectors. Notice how the entire grid is transported along with it.](../img/grid-transform.svg)\n:label:`fig_grid-transform`\n\nThis is the most important intuitive point\nto internalize about linear transformations represented by matrices. Matrices are incapable of distorting some parts of space differently than others. All they can do is take the original coordinates on our space\nand skew, rotate, and scale them. Some distortions can be severe. For instance the matrix\n\n$$\n\\mathbf{B} = \\begin{bmatrix}\n2 & -1 \\\\ 4 & -2\n\\end{bmatrix},\n$$\n\ncompresses the entire two-dimensional plane down to a single line. Identifying and working with such transformations are the topic of a later section,\nbut geometrically we can see that this is fundamentally different\nfrom the types of transformations we saw above. For instance, the result from matrix $\\mathbf{A}$ can be \"bent back\" to the original grid. The results from matrix $\\mathbf{B}$ cannot\nbecause we will never know where the vector $[1,2]^\\top$ came from---was\nit $[1,1]^\\top$ or $[0, -1]^\\top$? While this picture was for a $2\\times2$ matrix,\nnothing prevents us from taking the lessons learned into higher dimensions."
    },
    {
      "chunk_id": "33133fee2326_2",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Geometry of Linear Transformations",
      "text": "While this picture was for a $2\\times2$ matrix,\nnothing prevents us from taking the lessons learned into higher dimensions. If we take similar basis vectors like $[1,0, \\ldots,0]$\nand see where our matrix sends them,\nwe can start to get a feeling for how the matrix multiplication\ndistorts the entire space in whatever dimension space we are dealing with."
    },
    {
      "chunk_id": "8a5c591b90c1_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Linear Dependence",
      "text": "Consider again the matrix\n\n$$\n\\mathbf{B} = \\begin{bmatrix}\n2 & -1 \\\\ 4 & -2\n\\end{bmatrix}. $$\n\nThis compresses the entire plane down to live on the single line $y = 2x$. The question now arises: is there some way we can detect this\njust looking at the matrix itself? The answer is that indeed we can. Let's take $\\mathbf{b}_1 = [2,4]^\\top$ and $\\mathbf{b}_2 = [-1, -2]^\\top$\nbe the two columns of $\\mathbf{B}$. Remember that we can write everything transformed by the matrix $\\mathbf{B}$\nas a weighted sum of the columns of the matrix:\nlike $a_1\\mathbf{b}_1 + a_2\\mathbf{b}_2$. We call this a *linear combination*. The fact that $\\mathbf{b}_1 = -2\\cdot\\mathbf{b}_2$\nmeans that we can write any linear combination of those two columns\nentirely in terms of say $\\mathbf{b}_2$ since\n\n$$\na_1\\mathbf{b}_1 + a_2\\mathbf{b}_2 = -2a_1\\mathbf{b}_2 + a_2\\mathbf{b}_2 = (a_2-2a_1)\\mathbf{b}_2. $$\n\nThis means that one of the columns is, in a sense, redundant\nbecause it does not define a unique direction in space. This should not surprise us too much\nsince we already saw that this matrix\ncollapses the entire plane down into a single line. Moreover, we see that the linear dependence\n$\\mathbf{b}_1 = -2\\cdot\\mathbf{b}_2$ captures this. To make this more symmetrical between the two vectors, we will write this as\n\n$$\n\\mathbf{b}_1  + 2\\cdot\\mathbf{b}_2 = 0. $$\n\nIn general, we will say that a collection of vectors\n$\\mathbf{v}_1, \\ldots, \\mathbf{v}_k$ are *linearly dependent*\nif there exist coefficients $a_1, \\ldots, a_k$ *not all equal to zero* so that\n\n$$\n\\sum_{i=1}^k a_i\\mathbf{v_i} = 0. $$\n\nIn this case, we can solve for one of the vectors\nin terms of some combination of the others,\nand effectively render it redundant. Thus, a linear dependence in the columns of a matrix\nis a witness to the fact that our matrix\nis compressing the space down to some lower dimension. If there is no linear dependence we say the vectors are *linearly independent*."
    },
    {
      "chunk_id": "8a5c591b90c1_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Linear Dependence",
      "text": "Thus, a linear dependence in the columns of a matrix\nis a witness to the fact that our matrix\nis compressing the space down to some lower dimension. If there is no linear dependence we say the vectors are *linearly independent*. If the columns of a matrix are linearly independent,\nno compression occurs and the operation can be undone."
    },
    {
      "chunk_id": "57823130b29f_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Rank",
      "text": "If we have a general $n\\times m$ matrix,\nit is reasonable to ask what dimension space the matrix maps into.\nA concept known as the *rank* will be our answer.\nIn the previous section, we noted that a linear dependence\nbears witness to compression of space into a lower dimension\nand so we will be able to use this to define the notion of rank.\nIn particular, the rank of a matrix $\\mathbf{A}$\nis the largest number of linearly independent columns\namongst all subsets of columns. For example, the matrix\n\n$$\n\\mathbf{B} = \\begin{bmatrix}\n2 & 4 \\\\ -1 & -2\n\\end{bmatrix},\n$$\n\nhas $\\textrm{rank}(B)=1$, since the two columns are linearly dependent,\nbut either column by itself is not linearly dependent.\nFor a more challenging example, we can consider\n\n$$\n\\mathbf{C} = \\begin{bmatrix}\n1& 3 & 0 & -1 & 0 \\\\\n-1 & 0 & 1 & 1 & -1 \\\\\n0 & 3 & 1 & 0 & -1 \\\\\n2 & 3 & -1 & -2 & 1\n\\end{bmatrix},\n$$\n\nand show that $\\mathbf{C}$ has rank two since, for instance,\nthe first two columns are linearly independent,\nhowever any of the four collections of three columns are dependent.\n\nThis procedure, as described, is very inefficient.\nIt requires looking at every subset of the columns of our given matrix,\nand thus is potentially exponential in the number of columns.\nLater we will see a more computationally efficient way\nto compute the rank of a matrix, but for now,\nthis is sufficient to see that the concept\nis well defined and understand the meaning."
    },
    {
      "chunk_id": "592802e2d839_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Invertibility",
      "text": "We have seen above that multiplication by a matrix with linearly dependent columns\ncannot be undone, i.e., there is no inverse operation that can always recover the input. However, multiplication by a full-rank matrix\n(i.e., some $\\mathbf{A}$ that is $n \\times n$ matrix with rank $n$),\nwe should always be able to undo it. Consider the matrix\n\n$$\n\\mathbf{I} = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}. $$\n\nwhich is the matrix with ones along the diagonal, and zeros elsewhere. We call this the *identity* matrix. It is the matrix which leaves our data unchanged when applied. To find a matrix which undoes what our matrix $\\mathbf{A}$ has done,\nwe want to find a matrix $\\mathbf{A}^{-1}$ such that\n\n$$\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} =  \\mathbf{I}. $$\n\nIf we look at this as a system, we have $n \\times n$ unknowns\n(the entries of $\\mathbf{A}^{-1}$) and $n \\times n$ equations\n(the equality that needs to hold between every entry of the product $\\mathbf{A}^{-1}\\mathbf{A}$ and every entry of $\\mathbf{I}$)\nso we should generically expect a solution to exist. Indeed, in the next section we will see a quantity called the *determinant*,\nwhich has the property that as long as the determinant is not zero, we can find a solution. We call such a matrix $\\mathbf{A}^{-1}$ the *inverse* matrix. As an example, if $\\mathbf{A}$ is the general $2 \\times 2$ matrix\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix},\n$$\n\nthen we can see that the inverse is\n\n$$\n \\frac{1}{ad-bc}  \\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}. $$\n\nWe can test to see this by seeing that multiplying\nby the inverse given by the formula above works in practice."
    },
    {
      "chunk_id": "592802e2d839_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Invertibility",
      "text": "$$\n\nWe can test to see this by seeing that multiplying\nby the inverse given by the formula above works in practice. ```{.python .input}\n#@tab mxnet\nM = np.array([[1, 2], [1, 4]])\nM_inv = np.array([[2, -1], [-0.5, 0.5]])\nM_inv.dot(M)\n```\n\n```{.python .input}\n#@tab pytorch\nM = torch.tensor([[1, 2], [1, 4]], dtype=torch.float32)\nM_inv = torch.tensor([[2, -1], [-0.5, 0.5]])\nM_inv @ M\n```\n\n```{.python .input}\n#@tab tensorflow\nM = tf.constant([[1, 2], [1, 4]], dtype=tf.float32)\nM_inv = tf.constant([[2, -1], [-0.5, 0.5]])\ntf.matmul(M_inv, M)\n```"
    },
    {
      "chunk_id": "48e6aa9bdb91_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Numerical Issues",
      "text": "While the inverse of a matrix is useful in theory,\nwe must say that most of the time we do not wish\nto *use* the matrix inverse to solve a problem in practice.\nIn general, there are far more numerically stable algorithms\nfor solving linear equations like\n\n$$\n\\mathbf{A}\\mathbf{x} = \\mathbf{b},\n$$\n\nthan computing the inverse and multiplying to get\n\n$$\n\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}.\n$$\n\nJust as division by a small number can lead to numerical instability,\nso can inversion of a matrix which is close to having low rank.\n\nMoreover, it is common that the matrix $\\mathbf{A}$ is *sparse*,\nwhich is to say that it contains only a small number of non-zero values.\nIf we were to explore examples, we would see\nthat this does not mean the inverse is sparse.\nEven if $\\mathbf{A}$ was a $1$ million by $1$ million matrix\nwith only $5$ million non-zero entries\n(and thus we need only store those $5$ million),\nthe inverse will typically have almost every entry non-negative,\nrequiring us to store all $1\\textrm{M}^2$ entries---that is $1$ trillion entries!\n\nWhile we do not have time to dive all the way into the thorny numerical issues\nfrequently encountered when working with linear algebra,\nwe want to provide you with some intuition about when to proceed with caution,\nand generally avoiding inversion in practice is a good rule of thumb."
    },
    {
      "chunk_id": "2938f5457079_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Determinant",
      "text": "The geometric view of linear algebra gives an intuitive way\nto interpret a fundamental quantity known as the *determinant*. Consider the grid image from before, but now with a highlighted region (:numref:`fig_grid-filled`). ![The matrix $\\mathbf{A}$ again distorting the grid. This time, I want to draw particular attention to what happens to the highlighted square.](../img/grid-transform-filled.svg)\n:label:`fig_grid-filled`\n\nLook at the highlighted square. This is a square with edges given\nby $(0, 1)$ and $(1, 0)$ and thus it has area one. After $\\mathbf{A}$ transforms this square,\nwe see that it becomes a parallelogram. There is no reason this parallelogram should have the same area\nthat we started with, and indeed in the specific case shown here of\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 \\\\\n-1 & 3\n\\end{bmatrix},\n$$\n\nit is an exercise in coordinate geometry to compute\nthe area of this parallelogram and obtain that the area is $5$. In general, if we have a matrix\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix},\n$$\n\nwe can see with some computation that the area\nof the resulting parallelogram is $ad-bc$. This area is referred to as the *determinant*. Let's check this quickly with some example code. ```{.python .input}\n#@tab mxnet\nimport numpy as np\nnp.linalg.det(np.array([[1, -1], [2, 3]]))\n```\n\n```{.python .input}\n#@tab pytorch\ntorch.det(torch.tensor([[1, -1], [2, 3]], dtype=torch.float32))\n```\n\n```{.python .input}\n#@tab tensorflow\ntf.linalg.det(tf.constant([[1, -1], [2, 3]], dtype=tf.float32))\n```\n\nThe eagle-eyed amongst us will notice\nthat this expression can be zero or even negative. For the negative term, this is a matter of convention\ntaken generally in mathematics:\nif the matrix flips the figure,\nwe say the area is negated. Let's see now that when the determinant is zero, we learn more. Let's consider\n\n$$\n\\mathbf{B} = \\begin{bmatrix}\n2 & 4 \\\\ -1 & -2\n\\end{bmatrix}. $$\n\nIf we compute the determinant of this matrix,\nwe get $2\\cdot(-2 ) - 4\\cdot(-1) = 0$."
    },
    {
      "chunk_id": "2938f5457079_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Determinant",
      "text": "Let's see now that when the determinant is zero, we learn more. Let's consider\n\n$$\n\\mathbf{B} = \\begin{bmatrix}\n2 & 4 \\\\ -1 & -2\n\\end{bmatrix}. $$\n\nIf we compute the determinant of this matrix,\nwe get $2\\cdot(-2 ) - 4\\cdot(-1) = 0$. Given our understanding above, this makes sense. $\\mathbf{B}$ compresses the square from the original image\ndown to a line segment, which has zero area. And indeed, being compressed into a lower dimensional space\nis the only way to have zero area after the transformation. Thus we see the following result is true:\na matrix $A$ is invertible if and only if\nthe determinant is not equal to zero. As a final comment, imagine that we have any figure drawn on the plane. Thinking like computer scientists, we can decompose\nthat figure into a collection of little squares\nso that the area of the figure is in essence\njust the number of squares in the decomposition. If we now transform that figure by a matrix,\nwe send each of these squares to parallelograms,\neach one of which has area given by the determinant. We see that for any figure, the determinant gives the (signed) number\nthat a matrix scales the area of any figure. Computing determinants for larger matrices can be laborious,\nbut the  intuition is the same. The determinant remains the factor\nthat $n\\times n$ matrices scale $n$-dimensional volumes."
    },
    {
      "chunk_id": "c3008ae7add8_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Tensors and Common Linear Algebra Operations",
      "text": "In :numref:`sec_linear-algebra` the concept of tensors was introduced.\nIn this section, we will dive more deeply into tensor contractions\n(the tensor equivalent of matrix multiplication),\nand see how it can provide a unified view\non a number of matrix and vector operations.\n\nWith matrices and vectors we knew how to multiply them to transform data.\nWe need to have a similar definition for tensors if they are to be useful to us.\nThink about matrix multiplication:\n\n$$\n\\mathbf{C} = \\mathbf{A}\\mathbf{B},\n$$\n\nor equivalently\n\n$$ c_{i, j} = \\sum_{k} a_{i, k}b_{k, j}.$$\n\nThis pattern is one we can repeat for tensors.\nFor tensors, there is no one case of what\nto sum over that can be universally chosen,\nso we need specify exactly which indices we want to sum over.\nFor instance we could consider\n\n$$\ny_{il} = \\sum_{jk} x_{ijkl}a_{jk}.\n$$\n\nSuch a transformation is called a *tensor contraction*.\nIt can represent a far more flexible family of transformations\nthat matrix multiplication alone.\n\nAs a often-used notational simplification,\nwe can notice that the sum is over exactly those indices\nthat occur more than once in the expression,\nthus people often work with *Einstein notation*,\nwhere the summation is implicitly taken over all repeated indices.\nThis gives the compact expression:\n\n$$\ny_{il} = x_{ijkl}a_{jk}.\n$$"
    },
    {
      "chunk_id": "d1ce569d7038_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Common Examples from Linear Algebra",
      "text": "Let's see how many of the linear algebraic definitions\nwe have seen before can be expressed in this compressed tensor notation:\n\n* $\\mathbf{v} \\cdot \\mathbf{w} = \\sum_i v_iw_i$\n* $\\|\\mathbf{v}\\|_2^{2} = \\sum_i v_iv_i$\n* $(\\mathbf{A}\\mathbf{v})_i = \\sum_j a_{ij}v_j$\n* $(\\mathbf{A}\\mathbf{B})_{ik} = \\sum_j a_{ij}b_{jk}$\n* $\\textrm{tr}(\\mathbf{A}) = \\sum_i a_{ii}$\n\nIn this way, we can replace a myriad of specialized notations with short tensor expressions."
    },
    {
      "chunk_id": "8f1f1af7f883_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Expressing in Code",
      "text": "Tensors may flexibly be operated on in code as well. As seen in :numref:`sec_linear-algebra`,\nwe can create tensors as is shown below. ```{.python .input}\n#@tab mxnet\n# Define tensors\nB = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nA = np.array([[1, 2], [3, 4]])\nv = np.array([1, 2])\n\n# Print out the shapes\nA.shape, B.shape, v.shape\n```\n\n```{.python .input}\n#@tab pytorch\n# Define tensors\nB = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nA = torch.tensor([[1, 2], [3, 4]])\nv = torch.tensor([1, 2])\n\n# Print out the shapes\nA.shape, B.shape, v.shape\n```\n\n```{.python .input}\n#@tab tensorflow\n# Define tensors\nB = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nA = tf.constant([[1, 2], [3, 4]])\nv = tf.constant([1, 2])\n\n# Print out the shapes\nA.shape, B.shape, v.shape\n```\n\nEinstein summation has been implemented directly. The indices that occurs in the Einstein summation can be passed as a string,\nfollowed by the tensors that are being acted upon. For instance, to implement matrix multiplication,\nwe can consider the Einstein summation seen above\n($\\mathbf{A}\\mathbf{v} = a_{ij}v_j$)\nand strip out the indices themselves to get the implementation:\n\n```{.python .input}\n#@tab mxnet\n# Reimplement matrix multiplication\nnp.einsum(\"ij, j -> i\", A, v), A.dot(v)\n```\n\n```{.python .input}\n#@tab pytorch\n# Reimplement matrix multiplication\ntorch.einsum(\"ij, j -> i\", A, v), A@v\n```\n\n```{.python .input}\n#@tab tensorflow\n# Reimplement matrix multiplication\ntf.einsum(\"ij, j -> i\", A, v), tf.matmul(A, tf.reshape(v, (2, 1)))\n```\n\nThis is a highly flexible notation. For instance if we want to compute\nwhat would be traditionally written as\n\n$$\nc_{kl} = \\sum_{ij} \\mathbf{b}_{ijk}\\mathbf{a}_{il}v_j."
    },
    {
      "chunk_id": "8f1f1af7f883_1",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Expressing in Code",
      "text": "For instance if we want to compute\nwhat would be traditionally written as\n\n$$\nc_{kl} = \\sum_{ij} \\mathbf{b}_{ijk}\\mathbf{a}_{il}v_j. $$\n\nit can be implemented via Einstein summation as:\n\n```{.python .input}\n#@tab mxnet\nnp.einsum(\"ijk, il, j -> kl\", B, A, v)\n```\n\n```{.python .input}\n#@tab pytorch\ntorch.einsum(\"ijk, il, j -> kl\", B, A, v)\n```\n\n```{.python .input}\n#@tab tensorflow\ntf.einsum(\"ijk, il, j -> kl\", B, A, v)\n```\n\nThis notation is readable and efficient for humans,\nhowever bulky if for whatever reason\nwe need to generate a tensor contraction programmatically. For this reason, `einsum` provides an alternative notation\nby providing integer indices for each tensor. For example, the same tensor contraction can also be written as:\n\n```{.python .input}\n#@tab mxnet\nnp.einsum(B, [0, 1, 2], A, [0, 3], v, [1], [2, 3])\n```\n\n```{.python .input}\n#@tab pytorch\n# PyTorch does not support this type of notation. ```\n\n```{.python .input}\n#@tab tensorflow\n# TensorFlow does not support this type of notation. ```\n\nEither notation allows for concise and efficient representation of tensor contractions in code."
    },
    {
      "chunk_id": "4580bca4ca01_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Summary",
      "text": "* Vectors can be interpreted geometrically as either points or directions in space.\n* Dot products define the notion of angle to arbitrarily high-dimensional spaces.\n* Hyperplanes are high-dimensional generalizations of lines and planes.  They can be used to define decision planes that are often used as the last step in a classification task.\n* Matrix multiplication can be geometrically interpreted as uniform distortions of the underlying coordinates. They represent a very restricted, but mathematically clean, way to transform vectors.\n* Linear dependence is a way to tell when a collection of vectors are in a lower dimensional space than we would expect (say you have $3$ vectors living in a $2$-dimensional space). The rank of a matrix is the size of the largest subset of its columns that are linearly independent.\n* When a matrix's inverse is defined, matrix inversion allows us to find another matrix that undoes the action of the first. Matrix inversion is useful in theory, but requires care in practice owing to numerical instability.\n* Determinants allow us to measure how much a matrix expands or contracts a space. A nonzero determinant implies an invertible (non-singular) matrix and a zero-valued determinant means that the matrix is non-invertible (singular).\n* Tensor contractions and Einstein summation provide for a neat and clean notation for expressing many of the computations that are seen in machine learning."
    },
    {
      "chunk_id": "316434a2c20a_0",
      "chapter": "geometry-linear-algebraic-ops",
      "heading": "Exercises",
      "text": "1. What is the angle between\n$$\n\\vec v_1 = \\begin{bmatrix}\n1 \\\\ 0 \\\\ -1 \\\\ 2\n\\end{bmatrix}, \\qquad \\vec v_2 = \\begin{bmatrix}\n3 \\\\ 1 \\\\ 0 \\\\ 1\n\\end{bmatrix}?\n$$\n2. True or false: $\\begin{bmatrix}1 & 2\\\\0&1\\end{bmatrix}$ and $\\begin{bmatrix}1 & -2\\\\0&1\\end{bmatrix}$ are inverses of one another?\n3. Suppose that we draw a shape in the plane with area $100\\textrm{m}^2$.  What is the area after transforming the figure by the matrix\n$$\n\\begin{bmatrix}\n2 & 3\\\\\n1 & 2\n\\end{bmatrix}.\n$$\n4. Which of the following sets of vectors are linearly independent?\n * $\\left\\{\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}, \\begin{pmatrix}2\\\\1\\\\-1\\end{pmatrix}, \\begin{pmatrix}3\\\\1\\\\1\\end{pmatrix}\\right\\}$\n * $\\left\\{\\begin{pmatrix}3\\\\1\\\\1\\end{pmatrix}, \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}, \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}\\right\\}$\n * $\\left\\{\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}, \\begin{pmatrix}0\\\\1\\\\-1\\end{pmatrix}, \\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix}\\right\\}$\n5. Suppose that you have a matrix written as $A = \\begin{bmatrix}c\\\\d\\end{bmatrix}\\cdot\\begin{bmatrix}a & b\\end{bmatrix}$ for some choice of values $a, b, c$, and $d$.  True or false: the determinant of such a matrix is always $0$?\n6. The vectors $e_1 = \\begin{bmatrix}1\\\\0\\end{bmatrix}$ and $e_2 = \\begin{bmatrix}0\\\\1\\end{bmatrix}$ are orthogonal.  What is the condition on a matrix $A$ so that $Ae_1$ and $Ae_2$ are orthogonal?\n7. How can you write $\\textrm{tr}(\\mathbf{A}^4)$ in Einstein notation for an arbitrary matrix $A$?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/410)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1084)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1085)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Appendix: Mathematics for Deep Learning\n:label:`chap_appendix_math`\n\n**Brent Werness** (*Amazon*), **Rachel Hu** (*Amazon*), and authors of this book\n\n\nOne of the wonderful parts of modern deep learning is the fact that much of it can be understood and used without a full understanding of the mathematics below it. This is a sign that the field is maturing. Just as most software developers no longer need to worry about the theory of computable functions, neither should deep learning practitioners need to worry about the theoretical foundations of maximum likelihood learning. But, we are not quite there yet. In practice, you will sometimes need to understand how architectural choices influence gradient flow, or the implicit assumptions you make by training with a certain loss function. You might need to know what in the world entropy measures, and how it can help you understand exactly what bits-per-character means in your model. These all require deeper mathematical understanding. This appendix aims to provide you the mathematical background you need to understand the core theory of modern deep learning, but it is not exhaustive. We will begin with examining linear algebra in greater depth. We develop a geometric understanding of all the common linear algebraic objects and operations that will enable us to visualize the effects of various transformations on our data. A key element is the development of the basics of eigen-decompositions. We next develop the theory of differential calculus to the point that we can fully understand why the gradient is the direction of steepest descent, and why back-propagation takes the form it does. Integral calculus is then discussed to the degree needed to support our next topic, probability theory. Problems encountered in practice frequently are not certain, and thus we need a language to speak about uncertain things. We review the theory of random variables and the most commonly encountered distributions so we may discuss models probabilistically."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "Problems encountered in practice frequently are not certain, and thus we need a language to speak about uncertain things. We review the theory of random variables and the most commonly encountered distributions so we may discuss models probabilistically. This provides the foundation for the naive Bayes classifier, a probabilistic classification technique. Closely related to probability theory is the study of statistics. While statistics is far too large a field to do justice in a short section, we will introduce fundamental concepts that all machine learning practitioners should be aware of, in particular: evaluating and comparing estimators, conducting hypothesis tests, and constructing confidence intervals. Last, we turn to the topic of information theory, which is the mathematical study of information storage and transmission. This provides the core language by which we may discuss quantitatively how much information a model holds on a domain of discourse. Taken together, these form the core of the mathematical concepts needed to begin down the path towards a deep understanding of deep learning. ```toc\n:maxdepth: 2\n\ngeometry-linear-algebraic-ops\neigendecomposition\nsingle-variable-calculus\nmultivariable-calculus\nintegral-calculus\nrandom-variables\nmaximum-likelihood\ndistributions\nnaive-bayes\nstatistics\ninformation-theory\n```"
    },
    {
      "chunk_id": "4d6dc88ca88b_0",
      "chapter": "information-theory",
      "heading": "information-theory",
      "text": "# Information Theory\n:label:`sec_information_theory`\n\nThe universe is overflowing with information. Information provides a common language across disciplinary rifts: from Shakespeare's Sonnet to researchers' paper on Cornell ArXiv, from Van Gogh's printing Starry Night to Beethoven's music Symphony No.\u00a05, from the first programming language Plankalk\u00fcl to the state-of-the-art machine learning algorithms. Everything must follow the rules of information theory, no matter the format. With information theory, we can measure and compare how much information is present in different signals. In this section, we will investigate the fundamental concepts of information theory and applications of information theory in machine learning.\n\nBefore we get started, let's outline the relationship between machine learning and information theory. Machine learning aims to extract interesting signals from data and make critical predictions.  On the other hand, information theory studies encoding, decoding, transmitting, and manipulating information. As a result, information theory provides fundamental language for discussing the information processing in machine learned systems. For example, many machine learning applications use the cross-entropy loss as described in :numref:`sec_softmax`.  This loss can be directly derived from information theoretic considerations."
    },
    {
      "chunk_id": "f294384650bc_0",
      "chapter": "information-theory",
      "heading": "Information",
      "text": "Let's start with the \"soul\" of information theory: information. *Information* can be encoded in anything with a particular sequence of one or more encoding formats. Suppose that we task ourselves with trying to define a notion of information. What could be our starting point? Consider the following thought experiment. We have a friend with a deck of cards. They will shuffle the deck, flip over some cards, and tell us statements about the cards. We will try to assess the information content of each statement. First, they flip over a card and tell us, \"I see a card.\"  This provides us with no information at all. We were already certain that this was the case so we hope the information should be zero. Next, they flip over a card and say, \"I see a heart.\"  This provides us some information, but in reality there are only $4$ different suits that were possible, each equally likely, so we are not surprised by this outcome. We hope that whatever the measure of information, this event should have low information content. Next, they flip over a card and say, \"This is the $3$ of spades.\"  This is more information. Indeed there were $52$ equally likely possible outcomes, and our friend told us which one it was. This should be a medium amount of information. Let's take this to the logical extreme. Suppose that finally they flip over every card from the deck and read off the entire sequence of the shuffled deck. There are $52!$ different orders to the deck, again all equally likely, so we need a lot of information to know which one it is. Any notion of information we develop must conform to this intuition. Indeed, in the next sections we will learn how to compute that these events have $0\\textrm{ bits}$, $2\\textrm{ bits}$, $~5.7\\textrm{ bits}$, and $~225.6\\textrm{ bits}$ of information respectively. If we read through these thought experiments, we see a natural idea."
    },
    {
      "chunk_id": "f294384650bc_1",
      "chapter": "information-theory",
      "heading": "Information",
      "text": "If we read through these thought experiments, we see a natural idea. As a starting point, rather than caring about the knowledge, we may build off the idea that information represents the degree of surprise or the abstract possibility of the event. For example, if we want to describe an unusual event, we need a lot information. For a common event, we may not need much information. In 1948, Claude E. Shannon published *A Mathematical Theory of Communication* :cite:`Shannon.1948` establishing the theory of information. In his article, Shannon introduced the concept of information entropy for the first time. We will begin our journey here."
    },
    {
      "chunk_id": "6670454a4c1a_0",
      "chapter": "information-theory",
      "heading": "Self-information",
      "text": "Since information embodies the abstract possibility of an event, how do we map the possibility to the number of bits? Shannon introduced the terminology *bit* as the unit of information, which was originally created by John Tukey. So what is a \"bit\" and why do we use it to measure information? Historically, an antique transmitter can only send or receive two types of code: $0$ and $1$. Indeed, binary encoding is still in common use on all modern digital computers. In this way, any information is encoded by a series of $0$ and $1$. And hence, a series of binary digits of length $n$ contains $n$ bits of information. Now, suppose that for any series of codes, each $0$ or $1$ occurs with a probability of $\\frac{1}{2}$. Hence, an event $X$ with a series of codes of length $n$, occurs with a probability of $\\frac{1}{2^n}$. At the same time, as we mentioned before, this series contains $n$ bits of information. So, can we generalize to a mathematical function which can transfer the probability $p$ to the number of bits? Shannon gave the answer by defining *self-information*\n\n$$I(X) = - \\log_2 (p),$$\n\nas the *bits* of information we have received for this event $X$. Note that we will always use base-2 logarithms in this section. For the sake of simplicity, the rest of this section will omit the subscript 2 in the logarithm notation, i.e., $\\log(.)$ always refers to $\\log_2(.)$. For example, the code \"0010\" has a self-information\n\n$$I(\\textrm{\"0010\"}) = - \\log (p(\\textrm{\"0010\"})) = - \\log \\left( \\frac{1}{2^4} \\right) = 4 \\textrm{ bits}.$$\n\nWe can calculate self information as shown below. Before that, let's first import all the necessary packages in this section."
    },
    {
      "chunk_id": "6670454a4c1a_1",
      "chapter": "information-theory",
      "heading": "Self-information",
      "text": "Before that, let's first import all the necessary packages in this section. ```{.python .input}\n#@tab mxnet\nfrom mxnet import np\nfrom mxnet.metric import NegativeLogLikelihood\nfrom mxnet.ndarray import nansum\nimport random\n\ndef self_information(p):\n    return -np.log2(p)\n\nself_information(1 / 64)\n```\n\n```{.python .input}\n#@tab pytorch\nimport torch\nfrom torch.nn import NLLLoss\n\ndef nansum(x):\n    # Define nansum, as pytorch does not offer it inbuilt. return x[~torch.isnan(x)].sum()\n\ndef self_information(p):\n    return -torch.log2(torch.tensor(p)).item()\n\nself_information(1 / 64)\n```\n\n```{.python .input}\n#@tab tensorflow\nimport tensorflow as tf\n\ndef log2(x):\n    return tf.math.log(x) / tf.math.log(2.)\n\ndef nansum(x):\n    return tf.reduce_sum(tf.where(tf.math.is_nan(\n        x), tf.zeros_like(x), x), axis=-1)\n\ndef self_information(p):\n    return -log2(tf.constant(p)).numpy()\n\nself_information(1 / 64)\n```"
    },
    {
      "chunk_id": "6f54c4e73970_0",
      "chapter": "information-theory",
      "heading": "Entropy",
      "text": "As self-information only measures the information of a single discrete event, we need a more generalized measure for any random variable of either discrete or continuous distribution."
    },
    {
      "chunk_id": "bbbeb6ac37bf_0",
      "chapter": "information-theory",
      "heading": "Motivating Entropy",
      "text": "Let's try to get specific about what we want.  This will be an informal statement of what are known as the *axioms of Shannon entropy*.  It will turn out that the following collection of common-sense statements force us to a unique definition of information.  A formal version of these axioms, along with several others may be found in :citet:`Csiszar.2008`.\n\n1.  The information we gain by observing a random variable does not depend on what we call the elements, or the presence of additional elements which have probability zero.\n2.  The information we gain by observing two random variables is no more than the sum of the information we gain by observing them separately.  If they are independent, then it is exactly the sum.\n3.  The information gained when observing (nearly) certain events is (nearly) zero.\n\nWhile proving this fact is beyond the scope of our text, it is important to know that this uniquely determines the form that entropy must take.  The only ambiguity that these allow is in the choice of fundamental units, which is most often normalized by making the choice we saw before that the information provided by a single fair coin flip is one bit."
    },
    {
      "chunk_id": "77a1b35316a8_0",
      "chapter": "information-theory",
      "heading": "Definition",
      "text": "For any random variable $X$ that follows a probability distribution $P$ with a probability density function (p.d.f.) or a probability mass function (p.m.f.) $p(x)$, we measure the expected amount of information through *entropy* (or *Shannon entropy*)\n\n$$H(X) = - E_{x \\sim P} [\\log p(x)].$$\n:eqlabel:`eq_ent_def`\n\nTo be specific, if $X$ is discrete, $$H(X) = - \\sum_i p_i \\log p_i \\textrm{, where } p_i = P(X_i).$$\n\nOtherwise, if $X$ is continuous, we also refer entropy as *differential entropy*\n\n$$H(X) = - \\int_x p(x) \\log p(x) \\; dx.$$\n\nWe can define entropy as below.\n\n```{.python .input}\n#@tab mxnet\ndef entropy(p):\n    entropy = - p * np.log2(p)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(entropy.as_nd_ndarray())\n    return out\n\nentropy(np.array([0.1, 0.5, 0.1, 0.3]))\n```\n\n```{.python .input}\n#@tab pytorch\ndef entropy(p):\n    entropy = - p * torch.log2(p)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(entropy)\n    return out\n\nentropy(torch.tensor([0.1, 0.5, 0.1, 0.3]))\n```\n\n```{.python .input}\n#@tab tensorflow\ndef entropy(p):\n    return nansum(- p * log2(p))\n\nentropy(tf.constant([0.1, 0.5, 0.1, 0.3]))\n```"
    },
    {
      "chunk_id": "92e15a69d007_0",
      "chapter": "information-theory",
      "heading": "Interpretations",
      "text": "You may be curious: in the entropy definition :eqref:`eq_ent_def`, why do we use an expectation of a negative logarithm? Here are some intuitions.\n\nFirst, why do we use a *logarithm* function $\\log$? Suppose that $p(x) = f_1(x) f_2(x) \\ldots, f_n(x)$, where each component function $f_i(x)$ is independent from each other. This means that each $f_i(x)$ contributes independently to the total information obtained from $p(x)$. As discussed above, we want the entropy formula to be additive over independent random variables. Luckily, $\\log$ can naturally turn a product of probability distributions to a summation of the individual terms.\n\nNext, why do we use a *negative* $\\log$? Intuitively, more frequent events should contain less information than less common events, since we often gain more information from an unusual case than from an ordinary one. However, $\\log$ is monotonically increasing with the probabilities, and indeed negative for all values in $[0, 1]$.  We need to construct a monotonically decreasing relationship between the probability of events and their entropy, which will ideally be always positive (for nothing we observe should force us to forget what we have known). Hence, we add a negative sign in front of $\\log$ function.\n\nLast, where does the *expectation* function come from? Consider a random variable $X$. We can interpret the self-information ($-\\log(p)$) as the amount of *surprise* we have at seeing a particular outcome.  Indeed, as the probability approaches zero, the surprise becomes infinite.  Similarly, we can interpret the entropy as the average amount of surprise from observing $X$. For example, imagine that a slot machine system emits statistical independently symbols ${s_1, \\ldots, s_k}$ with probabilities ${p_1, \\ldots, p_k}$ respectively. Then the entropy of this system equals to the average self-information from observing each output, i.e.,\n\n$$H(S) = \\sum_i {p_i \\cdot I(s_i)} = - \\sum_i {p_i \\cdot \\log p_i}.$$"
    },
    {
      "chunk_id": "e775685dcb7e_0",
      "chapter": "information-theory",
      "heading": "Properties of Entropy",
      "text": "By the above examples and interpretations, we can derive the following properties of entropy :eqref:`eq_ent_def`. Here, we refer to $X$ as an event and $P$ as the probability distribution of $X$.\n\n* $H(X) \\geq 0$ for all discrete $X$ (entropy can be negative for continuous $X$).\n\n* If $X \\sim P$ with a p.d.f. or a p.m.f. $p(x)$, and we try to estimate $P$ by a new probability distribution $Q$ with a p.d.f. or a p.m.f. $q(x)$, then $$H(X) = - E_{x \\sim P} [\\log p(x)] \\leq  - E_{x \\sim P} [\\log q(x)], \\textrm{ with equality if and only if } P = Q.$$  Alternatively, $H(X)$ gives a lower bound of the average number of bits needed to encode symbols drawn from $P$.\n\n* If $X \\sim P$, then $x$ conveys the maximum amount of information if it spreads evenly among all possible outcomes. Specifically, if the probability distribution $P$ is discrete with $k$-class $\\{p_1, \\ldots, p_k \\}$, then $$H(X) \\leq \\log(k), \\textrm{ with equality if and only if } p_i = \\frac{1}{k}, \\forall i.$$ If $P$ is a continuous random variable, then the story becomes much more complicated.  However, if we additionally impose that $P$ is supported on a finite interval (with all values between $0$ and $1$), then $P$ has the highest entropy if it is the uniform distribution on that interval."
    },
    {
      "chunk_id": "5b752980229f_0",
      "chapter": "information-theory",
      "heading": "Mutual Information",
      "text": "Previously we defined entropy of a single random variable $X$, how about the entropy of a pair random variables $(X, Y)$?  We can think of these techniques as trying to answer the following type of question, \"What information is contained in $X$ and $Y$ together compared to each separately?  Is there redundant information, or is it all unique?\"\n\nFor the following discussion, we always use $(X, Y)$ as a pair of random variables that follows a joint probability distribution $P$ with a p.d.f. or a p.m.f. $p_{X, Y}(x, y)$, while $X$ and $Y$ follow probability distribution $p_X(x)$ and $p_Y(y)$, respectively."
    },
    {
      "chunk_id": "50037d7cd748_0",
      "chapter": "information-theory",
      "heading": "Joint Entropy",
      "text": "Similar to entropy of a single random variable :eqref:`eq_ent_def`, we define the *joint entropy* $H(X, Y)$ of a pair random variables $(X, Y)$ as\n\n$$H(X, Y) = -E_{(x, y) \\sim P} [\\log p_{X, Y}(x, y)]. $$\n:eqlabel:`eq_joint_ent_def`\n\nPrecisely, on the one hand, if $(X, Y)$ is a pair of discrete random variables, then\n\n$$H(X, Y) = - \\sum_{x} \\sum_{y} p_{X, Y}(x, y) \\log p_{X, Y}(x, y).$$\n\nOn the other hand, if $(X, Y)$ is a pair of continuous random variables, then we define the *differential joint entropy* as\n\n$$H(X, Y) = - \\int_{x, y} p_{X, Y}(x, y) \\ \\log p_{X, Y}(x, y) \\;dx \\;dy.$$\n\nWe can think of :eqref:`eq_joint_ent_def` as telling us the total randomness in the pair of random variables. As a pair of extremes, if $X = Y$ are two identical random variables, then the information in the pair is exactly the information in one and we have $H(X, Y) = H(X) = H(Y)$. On the other extreme, if $X$ and $Y$ are independent then $H(X, Y) = H(X) + H(Y)$. Indeed we will always have that the information contained in a pair of random variables is no smaller than the entropy of either random variable and no more than the sum of both. $$\nH(X), H(Y) \\le H(X, Y) \\le H(X) + H(Y). $$\n\nLet's implement joint entropy from scratch."
    },
    {
      "chunk_id": "50037d7cd748_1",
      "chapter": "information-theory",
      "heading": "Joint Entropy",
      "text": "$$\nH(X), H(Y) \\le H(X, Y) \\le H(X) + H(Y). $$\n\nLet's implement joint entropy from scratch. ```{.python .input}\n#@tab mxnet\ndef joint_entropy(p_xy):\n    joint_ent = -p_xy * np.log2(p_xy)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(joint_ent.as_nd_ndarray())\n    return out\n\njoint_entropy(np.array([[0.1, 0.5], [0.1, 0.3]]))\n```\n\n```{.python .input}\n#@tab pytorch\ndef joint_entropy(p_xy):\n    joint_ent = -p_xy * torch.log2(p_xy)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(joint_ent)\n    return out\n\njoint_entropy(torch.tensor([[0.1, 0.5], [0.1, 0.3]]))\n```\n\n```{.python .input}\n#@tab tensorflow\ndef joint_entropy(p_xy):\n    joint_ent = -p_xy * log2(p_xy)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(joint_ent)\n    return out\n\njoint_entropy(tf.constant([[0.1, 0.5], [0.1, 0.3]]))\n```\n\nNotice that this is the same *code* as before, but now we interpret it differently as working on the joint distribution of the two random variables."
    },
    {
      "chunk_id": "c4e50241218c_0",
      "chapter": "information-theory",
      "heading": "Conditional Entropy",
      "text": "The joint entropy defined above the amount of information contained in a pair of random variables. This is useful, but oftentimes it is not what we care about. Consider the setting of machine learning. Let's take $X$ to be the random variable (or vector of random variables) that describes the pixel values of an image, and $Y$ to be the random variable which is the class label. $X$ should contain substantial information---a natural image is a complex thing. However, the information contained in $Y$ once the image has been show should be low. Indeed, the image of a digit should already contain the information about what digit it is unless the digit is illegible. Thus, to continue to extend our vocabulary of information theory, we need to be able to reason about the information content in a random variable conditional on another. In the probability theory, we saw the definition of the *conditional probability* to measure the relationship between variables. We now want to analogously define the *conditional entropy* $H(Y \\mid X)$. We can write this as\n\n$$ H(Y \\mid X) = - E_{(x, y) \\sim P} [\\log p(y \\mid x)],$$\n:eqlabel:`eq_cond_ent_def`\n\nwhere $p(y \\mid x) = \\frac{p_{X, Y}(x, y)}{p_X(x)}$ is the conditional probability. Specifically, if $(X, Y)$ is a pair of discrete random variables, then\n\n$$H(Y \\mid X) = - \\sum_{x} \\sum_{y} p(x, y) \\log p(y \\mid x).$$\n\nIf $(X, Y)$ is a pair of continuous random variables, then the *differential conditional entropy* is similarly defined as\n\n$$H(Y \\mid X) = - \\int_x \\int_y p(x, y) \\ \\log p(y \\mid x) \\;dx \\;dy.$$\n\n\nIt is now natural to ask, how does the *conditional entropy* $H(Y \\mid X)$ relate to the entropy $H(X)$ and the joint entropy $H(X, Y)$? Using the definitions above, we can express this cleanly:\n\n$$H(Y \\mid X) = H(X, Y) - H(X).$$\n\nThis has an intuitive interpretation: the information in $Y$ given $X$ ($H(Y \\mid X)$) is the same as the information in both $X$ and $Y$ together ($H(X, Y)$) minus the information already contained in $X$."
    },
    {
      "chunk_id": "c4e50241218c_1",
      "chapter": "information-theory",
      "heading": "Conditional Entropy",
      "text": "This gives us the information in $Y$ which is not also represented in $X$. Now, let's implement conditional entropy :eqref:`eq_cond_ent_def` from scratch. ```{.python .input}\n#@tab mxnet\ndef conditional_entropy(p_xy, p_x):\n    p_y_given_x = p_xy/p_x\n    cond_ent = -p_xy * np.log2(p_y_given_x)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(cond_ent.as_nd_ndarray())\n    return out\n\nconditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))\n```\n\n```{.python .input}\n#@tab pytorch\ndef conditional_entropy(p_xy, p_x):\n    p_y_given_x = p_xy/p_x\n    cond_ent = -p_xy * torch.log2(p_y_given_x)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(cond_ent)\n    return out\n\nconditional_entropy(torch.tensor([[0.1, 0.5], [0.2, 0.3]]),\n                    torch.tensor([0.2, 0.8]))\n```\n\n```{.python .input}\n#@tab tensorflow\ndef conditional_entropy(p_xy, p_x):\n    p_y_given_x = p_xy/p_x\n    cond_ent = -p_xy * log2(p_y_given_x)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(cond_ent)\n    return out\n\nconditional_entropy(tf.constant([[0.1, 0.5], [0.2, 0.3]]),\n                    tf.constant([0.2, 0.8]))\n```"
    },
    {
      "chunk_id": "5b752980229f_0",
      "chapter": "information-theory",
      "heading": "Mutual Information",
      "text": "Given the previous setting of random variables $(X, Y)$, you may wonder: \"Now that we know how much information is contained in $Y$ but not in $X$, can we similarly ask how much information is shared between $X$ and $Y$?\" The answer will be the *mutual information* of $(X, Y)$, which we will write as $I(X, Y)$. Rather than diving straight into the formal definition, let's practice our intuition by first trying to derive an expression for the mutual information entirely based on terms we have constructed before. We wish to find the information shared between two random variables. One way we could try to do this is to start with all the information contained in both $X$ and $Y$ together, and then we take off the parts that are not shared. The information contained in both $X$ and $Y$ together is written as $H(X, Y)$. We want to subtract from this the information contained in $X$ but not in $Y$, and the information contained in $Y$ but not in $X$. As we saw in the previous section, this is given by $H(X \\mid Y)$ and $H(Y \\mid X)$ respectively. Thus, we have that the mutual information should be\n\n$$\nI(X, Y) = H(X, Y) - H(Y \\mid X) - H(X \\mid Y). $$\n\nIndeed, this is a valid definition for the mutual information. If we expand out the definitions of these terms and combine them, a little algebra shows that this is the same as\n\n$$I(X, Y) = E_{x} E_{y} \\left\\{ p_{X, Y}(x, y) \\log\\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)} \\right\\}. $$\n:eqlabel:`eq_mut_ent_def`\n\n\nWe can summarize all of these relationships in image :numref:`fig_mutual_information`. It is an excellent test of intuition to see why the following statements are all also equivalent to $I(X, Y)$."
    },
    {
      "chunk_id": "5b752980229f_1",
      "chapter": "information-theory",
      "heading": "Mutual Information",
      "text": "$$\n:eqlabel:`eq_mut_ent_def`\n\n\nWe can summarize all of these relationships in image :numref:`fig_mutual_information`. It is an excellent test of intuition to see why the following statements are all also equivalent to $I(X, Y)$. * $H(X) - H(X \\mid Y)$\n* $H(Y) - H(Y \\mid X)$\n* $H(X) + H(Y) - H(X, Y)$\n\n![Mutual information's relationship with joint entropy and conditional entropy.](../img/mutual-information.svg)\n:label:`fig_mutual_information`\n\n\nIn many ways we can think of the mutual information :eqref:`eq_mut_ent_def` as principled extension of correlation coefficient we saw in :numref:`sec_random_variables`. This allows us to ask not only for linear relationships between variables, but for the maximum information shared between the two random variables of any kind. Now, let's implement mutual information from scratch. ```{.python .input}\n#@tab mxnet\ndef mutual_information(p_xy, p_x, p_y):\n    p = p_xy / (p_x * p_y)\n    mutual = p_xy * np.log2(p)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(mutual.as_nd_ndarray())\n    return out\n\nmutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),\n                   np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))\n```\n\n```{.python .input}\n#@tab pytorch\ndef mutual_information(p_xy, p_x, p_y):\n    p = p_xy / (p_x * p_y)\n    mutual = p_xy * torch.log2(p)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(mutual)\n    return out\n\nmutual_information(torch.tensor([[0.1, 0.5], [0.1, 0.3]]),\n                   torch.tensor([0.2, 0.8]), torch.tensor([[0.75, 0.25]]))\n```\n\n```{.python .input}\n#@tab tensorflow\ndef mutual_information(p_xy, p_x, p_y):\n    p = p_xy / (p_x * p_y)\n    mutual = p_xy * log2(p)\n    # Operator `nansum` will sum up the non-nan number\n    out = nansum(mutual)\n    return out\n\nmutual_information(tf.constant([[0.1, 0.5], [0.1, 0.3]]),\n                   tf.constant([0.2, 0.8]), tf.constant([[0.75, 0.25]]))\n```"
    },
    {
      "chunk_id": "1737ef22f1de_0",
      "chapter": "information-theory",
      "heading": "Properties of Mutual Information",
      "text": "Rather than memorizing the definition of mutual information :eqref:`eq_mut_ent_def`, you only need to keep in mind its notable properties:\n\n* Mutual information is symmetric, i.e., $I(X, Y) = I(Y, X)$.\n* Mutual information is non-negative, i.e., $I(X, Y) \\geq 0$.\n* $I(X, Y) = 0$ if and only if $X$ and $Y$ are independent. For example, if $X$ and $Y$ are independent, then knowing $Y$ does not give any information about $X$ and vice versa, so their mutual information is zero.\n* Alternatively, if $X$ is an invertible function of $Y$, then $Y$ and $X$ share all information and $$I(X, Y) = H(Y) = H(X).$$"
    },
    {
      "chunk_id": "a68bea07a20e_0",
      "chapter": "information-theory",
      "heading": "Pointwise Mutual Information",
      "text": "When we worked with entropy at the beginning of this chapter, we were able to provide an interpretation of $-\\log(p_X(x))$ as how *surprised* we were with the particular outcome.  We may give a similar interpretation to the logarithmic term in the mutual information, which is often referred to as the *pointwise mutual information*:\n\n$$\\textrm{pmi}(x, y) = \\log\\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)}.$$\n:eqlabel:`eq_pmi_def`\n\nWe can think of :eqref:`eq_pmi_def` as measuring how much more or less likely the specific combination of outcomes $x$ and $y$ are compared to what we would expect for independent random outcomes.  If it is large and positive, then these two specific outcomes occur much more frequently than they would compared to random chance (*note*: the denominator is $p_X(x) p_Y(y)$ which is the probability of the two outcomes were independent), whereas if it is large and negative it represents the two outcomes happening far less than we would expect by random chance.\n\nThis allows us to interpret the mutual information :eqref:`eq_mut_ent_def` as the average amount that we were surprised to see two outcomes occurring together compared to what we would expect if they were independent."
    },
    {
      "chunk_id": "ec7de784a0f9_0",
      "chapter": "information-theory",
      "heading": "Applications of Mutual Information",
      "text": "Mutual information may be a little abstract in it pure definition, so how does it related to machine learning? In natural language processing, one of the most difficult problems is the *ambiguity resolution*, or the issue of the meaning of a word being unclear from context. For example, recently a headline in the news reported that \"Amazon is on fire\". You may wonder whether the company Amazon has a building on fire, or the Amazon rain forest is on fire.\n\nIn this case, mutual information can help us resolve this ambiguity. We first find the group of words that each has a relatively large mutual information with the company Amazon, such as e-commerce, technology, and online. Second, we find another group of words that each has a relatively large mutual information with the Amazon rain forest, such as rain, forest, and tropical. When we need to disambiguate \"Amazon\", we can compare which group has more occurrence in the context of the word Amazon.  In this case the article would go on to describe the forest, and make the context clear."
    },
    {
      "chunk_id": "ddaf706260f6_0",
      "chapter": "information-theory",
      "heading": "Kullback\u2013Leibler Divergence",
      "text": "As what we have discussed in :numref:`sec_linear-algebra`, we can use norms to measure distance between two points in space of any dimensionality.  We would like to be able to do a similar task with probability distributions.  There are many ways to go about this, but information theory provides one of the nicest.  We now explore the *Kullback\u2013Leibler (KL) divergence*, which provides a way to measure if two distributions are close together or not."
    },
    {
      "chunk_id": "77a1b35316a8_0",
      "chapter": "information-theory",
      "heading": "Definition",
      "text": "Given a random variable $X$ that follows the probability distribution $P$ with a p.d.f. or a p.m.f. $p(x)$, and we estimate $P$ by another probability distribution $Q$ with a p.d.f. or a p.m.f. $q(x)$. Then the *Kullback\u2013Leibler (KL) divergence* (or *relative entropy*) between $P$ and $Q$ is\n\n$$D_{\\textrm{KL}}(P\\|Q) = E_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right].$$\n:eqlabel:`eq_kl_def`\n\nAs with the pointwise mutual information :eqref:`eq_pmi_def`, we can again provide an interpretation of the logarithmic term:  $-\\log \\frac{q(x)}{p(x)} = -\\log(q(x)) - (-\\log(p(x)))$ will be large and positive if we see $x$ far more often under $P$ than we would expect for $Q$, and large and negative if we see the outcome far less than expected.  In this way, we can interpret it as our *relative* surprise at observing the outcome compared to how surprised we would be observing it from our reference distribution.\n\nLet's implement the KL divergence from Scratch.\n\n```{.python .input}\n#@tab mxnet\ndef kl_divergence(p, q):\n    kl = p * np.log2(p / q)\n    out = nansum(kl.as_nd_ndarray())\n    return out.abs().asscalar()\n```\n\n```{.python .input}\n#@tab pytorch\ndef kl_divergence(p, q):\n    kl = p * torch.log2(p / q)\n    out = nansum(kl)\n    return out.abs().item()\n```\n\n```{.python .input}\n#@tab tensorflow\ndef kl_divergence(p, q):\n    kl = p * log2(p / q)\n    out = nansum(kl)\n    return tf.abs(out).numpy()\n```"
    },
    {
      "chunk_id": "3999ab8972ff_0",
      "chapter": "information-theory",
      "heading": "KL Divergence Properties",
      "text": "Let's take a look at some properties of the KL divergence :eqref:`eq_kl_def`.\n\n* KL divergence is non-symmetric, i.e., there are $P,Q$ such that $$D_{\\textrm{KL}}(P\\|Q) \\neq D_{\\textrm{KL}}(Q\\|P).$$\n* KL divergence is non-negative, i.e., $$D_{\\textrm{KL}}(P\\|Q) \\geq 0.$$ Note that the equality holds only when $P = Q$.\n* If there exists an $x$ such that $p(x) > 0$ and $q(x) = 0$, then $D_{\\textrm{KL}}(P\\|Q) = \\infty$.\n* There is a close relationship between KL divergence and mutual information. Besides the relationship shown in :numref:`fig_mutual_information`, $I(X, Y)$ is also numerically equivalent with the following terms:\n    1. $D_{\\textrm{KL}}(P(X, Y)  \\ \\| \\ P(X)P(Y))$;\n    1. $E_Y \\{ D_{\\textrm{KL}}(P(X \\mid Y) \\ \\| \\ P(X)) \\}$;\n    1. $E_X \\{ D_{\\textrm{KL}}(P(Y \\mid X) \\ \\| \\ P(Y)) \\}$.\n\n  For the first term, we interpret mutual information as the KL divergence between $P(X, Y)$ and the product of $P(X)$ and $P(Y)$, and thus is a measure of how different the joint distribution is from the distribution if they were independent. For the second term, mutual information tells us the average reduction in uncertainty about $Y$ that results from learning the value of the $X$'s distribution. Similarly to the third term."
    },
    {
      "chunk_id": "822b99fa7b99_0",
      "chapter": "information-theory",
      "heading": "Example",
      "text": "Let's go through a toy example to see the non-symmetry explicitly.\n\nFirst, let's generate and sort three tensors of length $10,000$: an objective tensor $p$ which follows a normal distribution $N(0, 1)$, and two candidate tensors $q_1$ and $q_2$ which follow normal distributions $N(-1, 1)$ and $N(1, 1)$ respectively.\n\n```{.python .input}\n#@tab mxnet\nrandom.seed(1)\n\nnd_len = 10000\np = np.random.normal(loc=0, scale=1, size=(nd_len, ))\nq1 = np.random.normal(loc=-1, scale=1, size=(nd_len, ))\nq2 = np.random.normal(loc=1, scale=1, size=(nd_len, ))\n\np = np.array(sorted(p.asnumpy()))\nq1 = np.array(sorted(q1.asnumpy()))\nq2 = np.array(sorted(q2.asnumpy()))\n```\n\n```{.python .input}\n#@tab pytorch\ntorch.manual_seed(1)\n\ntensor_len = 10000\np = torch.normal(0, 1, (tensor_len, ))\nq1 = torch.normal(-1, 1, (tensor_len, ))\nq2 = torch.normal(1, 1, (tensor_len, ))\n\np = torch.sort(p)[0]\nq1 = torch.sort(q1)[0]\nq2 = torch.sort(q2)[0]\n```\n\n```{.python .input}\n#@tab tensorflow\ntensor_len = 10000\np = tf.random.normal((tensor_len, ), 0, 1)\nq1 = tf.random.normal((tensor_len, ), -1, 1)\nq2 = tf.random.normal((tensor_len, ), 1, 1)\n\np = tf.sort(p)\nq1 = tf.sort(q1)\nq2 = tf.sort(q2)\n```\n\nSince $q_1$ and $q_2$ are symmetric with respect to the y-axis (i.e., $x=0$), we expect a similar value of KL divergence between $D_{\\textrm{KL}}(p\\|q_1)$ and $D_{\\textrm{KL}}(p\\|q_2)$. As you can see below, there is only a less than 3% off between $D_{\\textrm{KL}}(p\\|q_1)$ and $D_{\\textrm{KL}}(p\\|q_2)$.\n\n```{.python .input}\n#@tab all\nkl_pq1 = kl_divergence(p, q1)\nkl_pq2 = kl_divergence(p, q2)\nsimilar_percentage = abs(kl_pq1 - kl_pq2) / ((kl_pq1 + kl_pq2) / 2) * 100\n\nkl_pq1, kl_pq2, similar_percentage\n```\n\nIn contrast, you may find that $D_{\\textrm{KL}}(q_2 \\|p)$ and $D_{\\textrm{KL}}(p \\| q_2)$ are off a lot, with around 40% off as shown below.\n\n```{.python .input}\n#@tab all\nkl_q2p = kl_divergence(q2, p)\ndiffer_percentage = abs(kl_q2p - kl_pq2) / ((kl_q2p + kl_pq2) / 2) * 100\n\nkl_q2p, differ_percentage\n```"
    },
    {
      "chunk_id": "30393cd8557d_0",
      "chapter": "information-theory",
      "heading": "Cross-Entropy",
      "text": "If you are curious about applications of information theory in deep learning, here is a quick example. We define the true distribution $P$ with probability distribution $p(x)$, and the estimated distribution $Q$ with probability distribution $q(x)$, and we will use them in the rest of this section.\n\nSay we need to solve a binary classification problem based on given $n$ data examples {$x_1, \\ldots, x_n$}. Assume that we encode $1$ and $0$ as the positive and negative class label $y_i$ respectively, and our neural network is parametrized by $\\theta$. If we aim to find a best $\\theta$ so that $\\hat{y}_i= p_{\\theta}(y_i \\mid x_i)$, it is natural to apply the maximum log-likelihood approach as was seen in :numref:`sec_maximum_likelihood`. To be specific, for true labels $y_i$ and predictions $\\hat{y}_i= p_{\\theta}(y_i \\mid x_i)$, the probability to be classified as positive is $\\pi_i= p_{\\theta}(y_i = 1 \\mid x_i)$. Hence, the log-likelihood function would be\n\n$$\n\\begin{aligned}\nl(\\theta) &= \\log L(\\theta) \\\\\n  &= \\log \\prod_{i=1}^n \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i} \\\\\n  &= \\sum_{i=1}^n y_i \\log(\\pi_i) + (1 - y_i) \\log (1 - \\pi_i). \\\\\n\\end{aligned}\n$$\n\nMaximizing the log-likelihood function $l(\\theta)$ is identical to minimizing $- l(\\theta)$, and hence we can find the best $\\theta$ from here. To generalize the above loss to any distributions, we also called $-l(\\theta)$ the *cross-entropy loss* $\\textrm{CE}(y, \\hat{y})$, where $y$ follows the true distribution $P$ and $\\hat{y}$ follows the estimated distribution $Q$.\n\nThis was all derived by working from the maximum likelihood point of view.  However, if we look closely we can see that terms like $\\log(\\pi_i)$ have entered into our computation which is a solid indication that we can understand the expression from an information theoretic point of view."
    },
    {
      "chunk_id": "2eabbbb954c5_0",
      "chapter": "information-theory",
      "heading": "Formal Definition",
      "text": "Like KL divergence, for a random variable $X$, we can also measure the divergence between the estimating distribution $Q$ and the true distribution $P$ via *cross-entropy*,\n\n$$\\textrm{CE}(P, Q) = - E_{x \\sim P} [\\log(q(x))].$$\n:eqlabel:`eq_ce_def`\n\nBy using properties of entropy discussed above, we can also interpret it as the summation of the entropy $H(P)$ and the KL divergence between $P$ and $Q$, i.e.,\n\n$$\\textrm{CE} (P, Q) = H(P) + D_{\\textrm{KL}}(P\\|Q).$$\n\n\nWe can implement the cross-entropy loss as below.\n\n```{.python .input}\n#@tab mxnet\ndef cross_entropy(y_hat, y):\n    ce = -np.log(y_hat[range(len(y_hat)), y])\n    return ce.mean()\n```\n\n```{.python .input}\n#@tab pytorch\ndef cross_entropy(y_hat, y):\n    ce = -torch.log(y_hat[range(len(y_hat)), y])\n    return ce.mean()\n```\n\n```{.python .input}\n#@tab tensorflow\ndef cross_entropy(y_hat, y):\n    # `tf.gather_nd` is used to select specific indices of a tensor.\n    ce = -tf.math.log(tf.gather_nd(y_hat, indices = [[i, j] for i, j in zip(\n        range(len(y_hat)), y)]))\n    return tf.reduce_mean(ce).numpy()\n```\n\nNow define two tensors for the labels and predictions, and calculate the cross-entropy loss of them.\n\n```{.python .input}\n#@tab mxnet\nlabels = np.array([0, 2])\npreds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n\ncross_entropy(preds, labels)\n```\n\n```{.python .input}\n#@tab pytorch\nlabels = torch.tensor([0, 2])\npreds = torch.tensor([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n\ncross_entropy(preds, labels)\n```\n\n```{.python .input}\n#@tab tensorflow\nlabels = tf.constant([0, 2])\npreds = tf.constant([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n\ncross_entropy(preds, labels)\n```"
    },
    {
      "chunk_id": "ccf0be479112_0",
      "chapter": "information-theory",
      "heading": "Properties",
      "text": "As alluded in the beginning of this section, cross-entropy :eqref:`eq_ce_def` can be used to define a loss function in the optimization problem. It turns out that the following are equivalent:\n\n1. Maximizing predictive probability of $Q$ for distribution $P$, (i.e., $E_{x\n\\sim P} [\\log (q(x))]$);\n1. Minimizing cross-entropy $\\textrm{CE} (P, Q)$;\n1. Minimizing the KL divergence $D_{\\textrm{KL}}(P\\|Q)$.\n\nThe definition of cross-entropy indirectly proves the equivalent relationship between objective 2 and objective 3, as long as the entropy of true data $H(P)$ is constant."
    },
    {
      "chunk_id": "487e41b27c20_0",
      "chapter": "information-theory",
      "heading": "Cross-Entropy as An Objective Function of Multi-class Classification",
      "text": "If we dive deep into the classification objective function with cross-entropy loss $\\textrm{CE}$, we will find minimizing $\\textrm{CE}$ is equivalent to maximizing the log-likelihood function $L$. To begin with, suppose that we are given a dataset with $n$ examples, and it can be classified into $k$-classes. For each data example $i$, we represent any $k$-class label $\\mathbf{y}_i = (y_{i1}, \\ldots, y_{ik})$ by *one-hot encoding*. To be specific, if the  example $i$ belongs to class $j$, then we set the $j$-th entry to $1$, and all other components to $0$, i.e.,\n\n$$ y_{ij} = \\begin{cases}1 & j \\in J; \\\\ 0 &\\textrm{otherwise.}\\end{cases}$$\n\nFor instance, if a multi-class classification problem contains three classes $A$, $B$, and $C$, then the labels $\\mathbf{y}_i$ can be encoded in {$A: (1, 0, 0); B: (0, 1, 0); C: (0, 0, 1)$}. Assume that our neural network is parametrized by $\\theta$. For true label vectors $\\mathbf{y}_i$ and predictions $$\\hat{\\mathbf{y}}_i= p_{\\theta}(\\mathbf{y}_i \\mid \\mathbf{x}_i) = \\sum_{j=1}^k y_{ij} p_{\\theta} (y_{ij}  \\mid  \\mathbf{x}_i).$$\n\nHence, the *cross-entropy loss* would be\n\n$$\n\\textrm{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^n \\mathbf{y}_i \\log \\hat{\\mathbf{y}}_i\n = - \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log{p_{\\theta} (y_{ij}  \\mid  \\mathbf{x}_i)}.\\\\\n$$\n\nOn the other side, we can also approach the problem through maximum likelihood estimation. To begin with, let's quickly introduce a $k$-class multinoulli distribution. It is an extension of the Bernoulli distribution from binary class to multi-class."
    },
    {
      "chunk_id": "487e41b27c20_1",
      "chapter": "information-theory",
      "heading": "Cross-Entropy as An Objective Function of Multi-class Classification",
      "text": "To begin with, let's quickly introduce a $k$-class multinoulli distribution. It is an extension of the Bernoulli distribution from binary class to multi-class. If a random variable $\\mathbf{z} = (z_{1}, \\ldots, z_{k})$ follows a $k$-class *multinoulli distribution* with probabilities $\\mathbf{p} =$ ($p_{1}, \\ldots, p_{k}$), i.e., $$p(\\mathbf{z}) = p(z_1, \\ldots, z_k) = \\textrm{Multi} (p_1, \\ldots, p_k), \\textrm{ where } \\sum_{i=1}^k p_i = 1,$$ then the joint probability mass function(p.m.f.) of $\\mathbf{z}$ is\n$$\\mathbf{p}^\\mathbf{z} = \\prod_{j=1}^k p_{j}^{z_{j}}.$$\n\n\nIt can be seen that the label of each data example, $\\mathbf{y}_i$, is following a $k$-class multinoulli distribution with probabilities $\\boldsymbol{\\pi} =$ ($\\pi_{1}, \\ldots, \\pi_{k}$). Therefore, the joint p.m.f. of each data example $\\mathbf{y}_i$ is  $\\mathbf{\\pi}^{\\mathbf{y}_i} = \\prod_{j=1}^k \\pi_{j}^{y_{ij}}.$\nHence, the log-likelihood function would be\n\n$$\n\\begin{aligned}\nl(\\theta)\n = \\log L(\\theta)\n = \\log \\prod_{i=1}^n \\boldsymbol{\\pi}^{\\mathbf{y}_i}\n = \\log \\prod_{i=1}^n \\prod_{j=1}^k \\pi_{j}^{y_{ij}}\n = \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log{\\pi_{j}}.\\\\\n\\end{aligned}\n$$\n\nSince in maximum likelihood estimation, we maximizing the objective function $l(\\theta)$ by having $\\pi_{j} = p_{\\theta} (y_{ij}  \\mid  \\mathbf{x}_i)$. Therefore, for any multi-class classification, maximizing the above log-likelihood function $l(\\theta)$ is equivalent to minimizing the CE loss $\\textrm{CE}(y, \\hat{y})$. To test the above proof, let's apply the built-in measure `NegativeLogLikelihood`. Using the same `labels` and `preds` as in the earlier example, we will get the same numerical loss as the previous example up to the 5 decimal place."
    },
    {
      "chunk_id": "487e41b27c20_2",
      "chapter": "information-theory",
      "heading": "Cross-Entropy as An Objective Function of Multi-class Classification",
      "text": "To test the above proof, let's apply the built-in measure `NegativeLogLikelihood`. Using the same `labels` and `preds` as in the earlier example, we will get the same numerical loss as the previous example up to the 5 decimal place. ```{.python .input}\n#@tab mxnet\nnll_loss = NegativeLogLikelihood()\nnll_loss.update(labels.as_nd_ndarray(), preds.as_nd_ndarray())\nnll_loss.get()\n```\n\n```{.python .input}\n#@tab pytorch\n# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`\n# and `nn.NLLLoss()`\nnll_loss = NLLLoss()\nloss = nll_loss(torch.log(preds), labels)\nloss\n```\n\n```{.python .input}\n#@tab tensorflow\ndef nll_loss(y_hat, y):\n    # Convert labels to one-hot vectors. y = tf.keras.utils.to_categorical(y, num_classes= y_hat.shape[1])\n    # We will not calculate negative log-likelihood from the definition. # Rather, we will follow a circular argument. Because NLL is same as\n    # `cross_entropy`, if we calculate cross_entropy that would give us NLL\n    cross_entropy = tf.keras.losses.CategoricalCrossentropy(\n        from_logits = True, reduction = tf.keras.losses.Reduction.NONE)\n    return tf.reduce_mean(cross_entropy(y, y_hat)).numpy()\n\nloss = nll_loss(tf.math.log(preds), labels)\nloss\n```"
    },
    {
      "chunk_id": "ec5af825b42b_0",
      "chapter": "information-theory",
      "heading": "Summary",
      "text": "* Information theory is a field of study about encoding, decoding, transmitting, and manipulating information.\n* Entropy is the unit to measure how much information is presented in different signals.\n* KL divergence can also measure the divergence between two distributions.\n* Cross-entropy can be viewed as an objective function of multi-class classification. Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood function."
    },
    {
      "chunk_id": "159e0b66fe4e_0",
      "chapter": "information-theory",
      "heading": "Exercises",
      "text": "1. Verify that the card examples from the first section indeed have the claimed entropy. 1. Show that the KL divergence $D(p\\|q)$ is nonnegative for all distributions $p$ and $q$. Hint: use Jensen's inequality, i.e., use the fact that $-\\log x$ is a convex function. 1. Let's compute the entropy from a few data sources:\n    * Assume that you are watching the output generated by a monkey at a typewriter. The monkey presses any of the $44$ keys of the typewriter at random (you can assume that it has not discovered any special keys or the shift key yet). How many bits of randomness per character do you observe? * Being unhappy with the monkey, you replaced it by a drunk typesetter. It is able to generate words, albeit not coherently. Instead, it picks a random word out of a vocabulary of $2,000$ words. Let's assume that the average length of a word is $4.5$ letters in English. How many bits of randomness per character do you observe now? * Still being unhappy with the result, you replace the typesetter by a high quality language model. The language model can currently obtain a perplexity as low as $15$ points per word. The character *perplexity* of a language model is defined as the inverse of the geometric mean of a set of probabilities, each probability is corresponding to a character in the word. To be specific, if the length of a given word is $l$, then  $\\textrm{PPL}(\\textrm{word}) = \\left[\\prod_i p(\\textrm{character}_i)\\right]^{ -\\frac{1}{l}} = \\exp \\left[ - \\frac{1}{l} \\sum_i{\\log p(\\textrm{character}_i)} \\right].$  Assume that the test word has 4.5 letters, how many bits of randomness per character do you observe now? 1. Explain intuitively why $I(X, Y) = H(X) - H(X \\mid Y)$. Then, show this is true by expressing both sides as an expectation with respect to the joint distribution. 1. What is the KL Divergence between the two Gaussian distributions $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}(\\mu_2, \\sigma_2^2)$?"
    },
    {
      "chunk_id": "159e0b66fe4e_1",
      "chapter": "information-theory",
      "heading": "Exercises",
      "text": "Then, show this is true by expressing both sides as an expectation with respect to the joint distribution. 1. What is the KL Divergence between the two Gaussian distributions $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}(\\mu_2, \\sigma_2^2)$? :begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/420)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1104)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1105)\n:end_tab:"
    },
    {
      "chunk_id": "1dd40e125121_0",
      "chapter": "integral-calculus",
      "heading": "integral-calculus",
      "text": "# Integral Calculus\n:label:`sec_integral_calculus`\n\nDifferentiation only makes up half of the content of a traditional calculus education.  The other pillar, integration, starts out seeming a rather disjoint question, \"What is the area underneath this curve?\"  While seemingly unrelated, integration is tightly intertwined with the differentiation via what is known as the *fundamental theorem of calculus*.\n\nAt the level of machine learning we discuss in this book, we will not need a deep understanding of integration. However, we will provide a brief introduction to lay the groundwork for any further applications we will encounter later on."
    },
    {
      "chunk_id": "368f95c287f9_0",
      "chapter": "integral-calculus",
      "heading": "Geometric Interpretation",
      "text": "Suppose that we have a function $f(x)$. For simplicity, let's assume that $f(x)$ is non-negative (never takes a value less than zero). What we want to try and understand is: what is the area contained between $f(x)$ and the $x$-axis? ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom mxnet import np, npx\nnpx.set_np()\n\nx = np.arange(-2, 2, 0.01)\nf = np.exp(-x**2)\n\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.tolist(), f.tolist())\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nimport torch\n\nx = torch.arange(-2, 2, 0.01)\nf = torch.exp(-x**2)\n\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.tolist(), f.tolist())\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nimport tensorflow as tf\n\nx = tf.range(-2, 2, 0.01)\nf = tf.exp(-x**2)\n\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.numpy(), f.numpy())\nd2l.plt.show()\n```\n\nIn most cases, this area will be infinite or undefined (consider the area under $f(x) = x^{2}$), so people will often talk about the area between a pair of ends, say $a$ and $b$."
    },
    {
      "chunk_id": "368f95c287f9_1",
      "chapter": "integral-calculus",
      "heading": "Geometric Interpretation",
      "text": "```{.python .input}\n#@tab mxnet\nx = np.arange(-2, 2, 0.01)\nf = np.exp(-x**2)\n\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.arange(-2, 2, 0.01)\nf = torch.exp(-x**2)\n\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.range(-2, 2, 0.01)\nf = tf.exp(-x**2)\n\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.numpy()[50:250], f.numpy()[50:250])\nd2l.plt.show()\n```\n\nWe will denote this area by the integral symbol below:\n\n$$\n\\textrm{Area}(\\mathcal{A}) = \\int_a^b f(x) \\;dx. $$\n\nThe inner variable is a dummy variable, much like the index of a sum in a $\\sum$, and so this can be equivalently written with any inner value we like:\n\n$$\n\\int_a^b f(x) \\;dx = \\int_a^b f(z) \\;dz. $$\n\nThere is a traditional way to try and understand how we might try to approximate such integrals: we can imagine taking the region in-between $a$ and $b$ and chopping it into $N$ vertical slices. If $N$ is large, we can approximate the area of each slice by a rectangle, and then add up the areas to get the total area under the curve. Let's take a look at an example doing this in code. We will see how to get the true value in a later section."
    },
    {
      "chunk_id": "368f95c287f9_2",
      "chapter": "integral-calculus",
      "heading": "Geometric Interpretation",
      "text": "If $N$ is large, we can approximate the area of each slice by a rectangle, and then add up the areas to get the total area under the curve. Let's take a look at an example doing this in code. We will see how to get the true value in a later section. ```{.python .input}\n#@tab mxnet\nepsilon = 0.05\na = 0\nb = 2\n\nx = np.arange(a, b, epsilon)\nf = x / (1 + x**2)\n\napprox = np.sum(epsilon*f)\ntrue = np.log(2) / 2\n\nd2l.set_figsize()\nd2l.plt.bar(x.asnumpy(), f.asnumpy(), width=epsilon, align='edge')\nd2l.plt.plot(x, f, color='black')\nd2l.plt.ylim([0, 1])\nd2l.plt.show()\n\nf'approximation: {approx}, truth: {true}'\n```\n\n```{.python .input}\n#@tab pytorch\nepsilon = 0.05\na = 0\nb = 2\n\nx = torch.arange(a, b, epsilon)\nf = x / (1 + x**2)\n\napprox = torch.sum(epsilon*f)\ntrue = torch.log(torch.tensor([5.])) / 2\n\nd2l.set_figsize()\nd2l.plt.bar(x, f, width=epsilon, align='edge')\nd2l.plt.plot(x, f, color='black')\nd2l.plt.ylim([0, 1])\nd2l.plt.show()\n\nf'approximation: {approx}, truth: {true}'\n```\n\n```{.python .input}\n#@tab tensorflow\nepsilon = 0.05\na = 0\nb = 2\n\nx = tf.range(a, b, epsilon)\nf = x / (1 + x**2)\n\napprox = tf.reduce_sum(epsilon*f)\ntrue = tf.math.log(tf.constant([5.])) / 2\n\nd2l.set_figsize()\nd2l.plt.bar(x, f, width=epsilon, align='edge')\nd2l.plt.plot(x, f, color='black')\nd2l.plt.ylim([0, 1])\nd2l.plt.show()\n\nf'approximation: {approx}, truth: {true}'\n```\n\nThe issue is that while it can be done numerically, we can do this approach analytically for only the simplest functions like\n\n$$\n\\int_a^b x \\;dx. $$\n\nAnything somewhat more complex like our example from the code above\n\n$$\n\\int_a^b \\frac{x}{1+x^{2}} \\;dx. $$\n\nis beyond what we can solve with such a direct method. We will instead take a different approach. We will work intuitively with the notion of the area, and learn the main computational tool used to find integrals: the *fundamental theorem of calculus*. This will be the basis for our study of integration."
    },
    {
      "chunk_id": "b981fffbbd1b_0",
      "chapter": "integral-calculus",
      "heading": "The Fundamental Theorem of Calculus",
      "text": "To dive deeper into the theory of integration, let's introduce a function\n\n$$\nF(x) = \\int_0^x f(y) dy. $$\n\nThis function measures the area between $0$ and $x$ depending on how we change $x$. Notice that this is everything we need since\n\n$$\n\\int_a^b f(x) \\;dx = F(b) - F(a). $$\n\nThis is a mathematical encoding of the fact that we can measure the area out to the far end-point and then subtract off the area to the near end point as indicated in :numref:`fig_area-subtract`. ![Visualizing why we may reduce the problem of computing the area under a curve between two points to computing the area to the left of a point.](../img/sub-area.svg)\n:label:`fig_area-subtract`\n\nThus, we can figure out what the integral over any interval is by figuring out what $F(x)$ is. To do so, let's consider an experiment. As we often do in calculus, let's imagine what happens when we shift the value by a tiny bit. From the comment above, we know that\n\n$$\nF(x+\\epsilon) - F(x) = \\int_x^{x+\\epsilon} f(y) \\; dy. $$\n\nThis tells us that the function changes by the area under a tiny sliver of a function. This is the point at which we make an approximation. If we look at a tiny sliver of area like this, it looks like this area is close to the rectangular area with height the value of $f(x)$ and the base width $\\epsilon$. Indeed, one can show that as $\\epsilon \\rightarrow 0$ this approximation becomes better and better. Thus we can conclude:\n\n$$\nF(x+\\epsilon) - F(x) \\approx \\epsilon f(x). $$\n\nHowever, we can now notice: this is exactly the pattern we expect if we were computing the derivative of $F$! Thus we see the following rather surprising fact:\n\n$$\n\\frac{dF}{dx}(x) = f(x). $$\n\nThis is the *fundamental theorem of calculus*. We may write it in expanded form as\n$$\\frac{d}{dx}\\int_0^x  f(y) \\; dy = f(x).$$\n:eqlabel:`eq_ftc`\n\nIt takes the concept of finding areas (*a priori* rather hard), and reduces it to a statement derivatives (something much more completely understood)."
    },
    {
      "chunk_id": "b981fffbbd1b_1",
      "chapter": "integral-calculus",
      "heading": "The Fundamental Theorem of Calculus",
      "text": "We may write it in expanded form as\n$$\\frac{d}{dx}\\int_0^x  f(y) \\; dy = f(x).$$\n:eqlabel:`eq_ftc`\n\nIt takes the concept of finding areas (*a priori* rather hard), and reduces it to a statement derivatives (something much more completely understood). One last comment that we must make is that this does not tell us exactly what $F(x)$ is. Indeed $F(x) + C$ for any $C$ has the same derivative. This is a fact-of-life in the theory of integration. Thankfully, notice that when working with definite integrals, the constants drop out, and thus are irrelevant to the outcome. $$\n\\int_a^b f(x) \\; dx = (F(b) + C) - (F(a) + C) = F(b) - F(a). $$\n\nThis may seem like abstract non-sense, but let's take a moment to appreciate that it has given us a whole new perspective on computing integrals. Our goal is no-longer to do some sort of chop-and-sum process to try and recover the area, rather we need only find a function whose derivative is the function we have! This is incredible since we can now list many rather difficult integrals by just reversing the table from :numref:`sec_derivative_table`. For instance, we know that the derivative of $x^{n}$ is $nx^{n-1}$. Thus, we can say using the fundamental theorem :eqref:`eq_ftc` that\n\n$$\n\\int_0^{x} ny^{n-1} \\; dy = x^n - 0^n = x^n. $$\n\nSimilarly, we know that the derivative of $e^{x}$ is itself, so that means\n\n$$\n\\int_0^{x} e^{x} \\; dx = e^{x} - e^{0} = e^x - 1. $$\n\nIn this way, we can develop the entire theory of integration leveraging ideas from differential calculus freely. Every integration rule derives from this one fact."
    },
    {
      "chunk_id": "5929a0deb652_0",
      "chapter": "integral-calculus",
      "heading": "Change of Variables",
      "text": ":label:`subsec_integral_example`\n\nJust as with differentiation, there are a number of rules which make the computation of integrals more tractable. In fact, every rule of differential calculus (like the product rule, sum rule, and chain rule) has a corresponding rule for integral calculus (integration by parts, linearity of integration, and the change of variables formula respectively). In this section, we will dive into what is arguably the most important from the list: the change of variables formula. First, suppose that we have a function which is itself an integral:\n\n$$\nF(x) = \\int_0^x f(y) \\; dy. $$\n\nLet's suppose that we want to know how this function looks when we compose it with another to obtain $F(u(x))$. By the chain rule, we know\n\n$$\n\\frac{d}{dx}F(u(x)) = \\frac{dF}{du}(u(x))\\cdot \\frac{du}{dx}. $$\n\nWe can turn this into a statement about integration by using the fundamental theorem :eqref:`eq_ftc` as above. This gives\n\n$$\nF(u(x)) - F(u(0)) = \\int_0^x \\frac{dF}{du}(u(y))\\cdot \\frac{du}{dy} \\;dy. $$\n\nRecalling that $F$ is itself an integral gives that the left hand side may be rewritten to be\n\n$$\n\\int_{u(0)}^{u(x)} f(y) \\; dy = \\int_0^x \\frac{dF}{du}(u(y))\\cdot \\frac{du}{dy} \\;dy. $$\n\nSimilarly, recalling that $F$ is an integral allows us to recognize that $\\frac{dF}{dx} = f$ using the fundamental theorem :eqref:`eq_ftc`, and thus we may conclude\n\n$$\\int_{u(0)}^{u(x)} f(y) \\; dy = \\int_0^x f(u(y))\\cdot \\frac{du}{dy} \\;dy.$$\n:eqlabel:`eq_change_var`\n\nThis is the *change of variables* formula. For a more intuitive derivation, consider what happens when we take an integral of $f(u(x))$ between $x$ and $x+\\epsilon$. For a small $\\epsilon$, this integral is approximately $\\epsilon f(u(x))$, the area of the associated rectangle. Now, let's compare this with the integral of $f(y)$ from $u(x)$ to $u(x+\\epsilon)$. We know that $u(x+\\epsilon) \\approx u(x) + \\epsilon \\frac{du}{dx}(x)$, so the area of this rectangle is approximately $\\epsilon \\frac{du}{dx}(x)f(u(x))$."
    },
    {
      "chunk_id": "5929a0deb652_1",
      "chapter": "integral-calculus",
      "heading": "Change of Variables",
      "text": "Now, let's compare this with the integral of $f(y)$ from $u(x)$ to $u(x+\\epsilon)$. We know that $u(x+\\epsilon) \\approx u(x) + \\epsilon \\frac{du}{dx}(x)$, so the area of this rectangle is approximately $\\epsilon \\frac{du}{dx}(x)f(u(x))$. Thus, to make the area of these two rectangles to agree, we need to multiply the first one by $\\frac{du}{dx}(x)$ as is illustrated in :numref:`fig_rect-transform`. ![Visualizing the transformation of a single thin rectangle under the change of variables.](../img/rect-trans.svg)\n:label:`fig_rect-transform`\n\nThis tells us that\n\n$$\n\\int_x^{x+\\epsilon} f(u(y))\\frac{du}{dy}(y)\\;dy = \\int_{u(x)}^{u(x+\\epsilon)} f(y) \\; dy. $$\n\nThis is the change of variables formula expressed for a single small rectangle. If $u(x)$ and $f(x)$ are properly chosen, this can allow for the computation of incredibly complex integrals. For instance, if we even chose $f(y) = 1$ and $u(x) = e^{-x^{2}}$ (which means $\\frac{du}{dx}(x) = -2xe^{-x^{2}}$), this can show for instance that\n\n$$\ne^{-1} - 1 = \\int_{e^{-0}}^{e^{-1}} 1 \\; dy = -2\\int_0^{1} ye^{-y^2}\\;dy,\n$$\n\nand thus by rearranging that\n\n$$\n\\int_0^{1} ye^{-y^2}\\; dy = \\frac{1-e^{-1}}{2}. $$"
    },
    {
      "chunk_id": "b35de094aef7_0",
      "chapter": "integral-calculus",
      "heading": "A Comment on Sign Conventions",
      "text": "Keen-eyed readers will observe something strange about the computations above.  Namely, computations like\n\n$$\n\\int_{e^{-0}}^{e^{-1}} 1 \\; dy = e^{-1} -1 < 0,\n$$\n\ncan produce negative numbers.  When thinking about areas, it can be strange to see a negative value, and so it is worth digging into what the convention is.\n\nMathematicians take the notion of signed areas.  This manifests itself in two ways.  First, if we consider a function $f(x)$ which is sometimes less than zero, then the area will also be negative.  So for instance\n\n$$\n\\int_0^{1} (-1)\\;dx = -1.\n$$\n\nSimilarly, integrals which progress from right to left, rather than left to right are also taken to be negative areas\n\n$$\n\\int_0^{-1} 1\\; dx = -1.\n$$\n\nThe standard area (from left to right of a positive function) is always positive.  Anything obtained by flipping it (say flipping over the $x$-axis to get the integral of a negative number, or flipping over the $y$-axis to get an integral in the wrong order) will produce a negative area.  And indeed, flipping twice will give a pair of negative signs that cancel out to have positive area\n\n$$\n\\int_0^{-1} (-1)\\;dx =  1.\n$$\n\nIf this discussion sounds familiar, it is!  In :numref:`sec_geometry-linear-algebraic-ops` we discussed how the determinant represented the signed area in much the same way."
    },
    {
      "chunk_id": "c9660f4898de_0",
      "chapter": "integral-calculus",
      "heading": "Multiple Integrals",
      "text": "In some cases, we will need to work in higher dimensions. For instance, suppose that we have a function of two variables, like $f(x, y)$ and we want to know the volume under $f$ when $x$ ranges over $[a, b]$ and $y$ ranges over $[c, d]$. ```{.python .input}\n#@tab mxnet\n# Construct grid and compute function\nx, y = np.meshgrid(np.linspace(-2, 2, 101), np.linspace(-2, 2, 101),\n                   indexing='ij')\nz = np.exp(- x**2 - y**2)\n\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x.asnumpy(), y.asnumpy(), z.asnumpy())\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.plt.xticks([-2, -1, 0, 1, 2])\nd2l.plt.yticks([-2, -1, 0, 1, 2])\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(0, 1)\nax.dist = 12\n```\n\n```{.python .input}\n#@tab pytorch\n# Construct grid and compute function\nx, y = torch.meshgrid(torch.linspace(-2, 2, 101), torch.linspace(-2, 2, 101))\nz = torch.exp(- x**2 - y**2)\n\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, z)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.plt.xticks([-2, -1, 0, 1, 2])\nd2l.plt.yticks([-2, -1, 0, 1, 2])\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(0, 1)\nax.dist = 12\n```\n\n```{.python .input}\n#@tab tensorflow\n# Construct grid and compute function\nx, y = tf.meshgrid(tf.linspace(-2., 2., 101), tf.linspace(-2., 2., 101))\nz = tf.exp(- x**2 - y**2)\n\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, z)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.plt.xticks([-2, -1, 0, 1, 2])\nd2l.plt.yticks([-2, -1, 0, 1, 2])\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(0, 1)\nax.dist = 12\n```\n\nWe write this as\n\n$$\n\\int_{[a, b]\\times[c, d]} f(x, y)\\;dx\\;dy. $$\n\nSuppose that we wish to compute this integral."
    },
    {
      "chunk_id": "c9660f4898de_1",
      "chapter": "integral-calculus",
      "heading": "Multiple Integrals",
      "text": "$$\n\nSuppose that we wish to compute this integral. My claim is that we can do this by iteratively computing first the integral in $x$ and then shifting to the integral in $y$, that is to say\n\n$$\n\\int_{[a, b]\\times[c, d]} f(x, y)\\;dx\\;dy = \\int_c^{d} \\left(\\int_a^{b} f(x, y) \\;dx\\right) \\; dy. $$\n\nLet's see why this is. Consider the figure above where we have split the function into $\\epsilon \\times \\epsilon$ squares which we will index with integer coordinates $i, j$. In this case, our integral is approximately\n\n$$\n\\sum_{i, j} \\epsilon^{2} f(\\epsilon i, \\epsilon j). $$\n\nOnce we discretize the problem, we may add up the values on these squares in whatever order we like, and not worry about changing the values. This is illustrated in :numref:`fig_sum-order`. In particular, we can say that\n\n$$\n \\sum _ {j} \\epsilon \\left(\\sum_{i} \\epsilon f(\\epsilon i, \\epsilon j)\\right). $$\n\n![Illustrating how to decompose a sum over many squares as a sum over first the columns (1), then adding the column sums together (2).](../img/sum-order.svg)\n:label:`fig_sum-order`\n\nThe sum on the inside is precisely the discretization of the integral\n\n$$\nG(\\epsilon j) = \\int _a^{b} f(x, \\epsilon j) \\; dx. $$\n\nFinally, notice that if we combine these two expressions we get\n\n$$\n\\sum _ {j} \\epsilon G(\\epsilon j) \\approx \\int _ {c}^{d} G(y) \\; dy = \\int _ {[a, b]\\times[c, d]} f(x, y)\\;dx\\;dy. $$\n\nThus putting it all together, we have that\n\n$$\n\\int _ {[a, b]\\times[c, d]} f(x, y)\\;dx\\;dy = \\int _ c^{d} \\left(\\int _ a^{b} f(x, y) \\;dx\\right) \\; dy. $$\n\nNotice that, once discretized, all we did was rearrange the order in which we added a list of numbers. This may make it seem like it is nothing, however this result (called *Fubini's Theorem*) is not always true! For the type of mathematics encountered when doing machine learning (continuous functions), there is no concern, however it is possible to create examples where it fails (for example the function $f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3$ over the rectangle $[0,2]\\times[0,1]$)."
    },
    {
      "chunk_id": "c9660f4898de_2",
      "chapter": "integral-calculus",
      "heading": "Multiple Integrals",
      "text": "Note that the choice to do the integral in $x$ first, and then the integral in $y$ was arbitrary. We could have equally well chosen to do $y$ first and then $x$ to see\n\n$$\n\\int _ {[a, b]\\times[c, d]} f(x, y)\\;dx\\;dy = \\int _ a^{b} \\left(\\int _ c^{d} f(x, y) \\;dy\\right) \\; dx. $$\n\nOften times, we will condense down to vector notation, and say that for $U = [a, b]\\times [c, d]$ this is\n\n$$\n\\int _ U f(\\mathbf{x})\\;d\\mathbf{x}. $$"
    },
    {
      "chunk_id": "29e5e7ccfaa4_0",
      "chapter": "integral-calculus",
      "heading": "Change of Variables in Multiple Integrals",
      "text": "As with single variables in :eqref:`eq_change_var`, the ability to change variables inside a higher dimensional integral is a key tool. Let's summarize the result without derivation. We need a function that reparametrizes our domain of integration. We can take this to be $\\phi : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, that is any function which takes in $n$ real variables and returns another $n$. To keep the expressions clean, we will assume that $\\phi$ is *injective* which is to say it never folds over itself ($\\phi(\\mathbf{x}) = \\phi(\\mathbf{y}) \\implies \\mathbf{x} = \\mathbf{y}$). In this case, we can say that\n\n$$\n\\int _ {\\phi(U)} f(\\mathbf{x})\\;d\\mathbf{x} = \\int _ {U} f(\\phi(\\mathbf{x})) \\left|\\det(D\\phi(\\mathbf{x}))\\right|\\;d\\mathbf{x}. $$\n\nwhere $D\\phi$ is the *Jacobian* of $\\phi$, which is the matrix of partial derivatives of $\\boldsymbol{\\phi} = (\\phi_1(x_1, \\ldots, x_n), \\ldots, \\phi_n(x_1, \\ldots, x_n))$,\n\n$$\nD\\boldsymbol{\\phi} = \\begin{bmatrix}\n\\frac{\\partial \\phi _ 1}{\\partial x _ 1} & \\cdots & \\frac{\\partial \\phi _ 1}{\\partial x _ n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial \\phi _ n}{\\partial x _ 1} & \\cdots & \\frac{\\partial \\phi _ n}{\\partial x _ n}\n\\end{bmatrix}. $$\n\nLooking closely, we see that this is similar to the single variable chain rule :eqref:`eq_change_var`, except we have replaced the term $\\frac{du}{dx}(x)$ with $\\left|\\det(D\\phi(\\mathbf{x}))\\right|$. Let's see how we can to interpret this term. Recall that the $\\frac{du}{dx}(x)$ term existed to say how much we stretched our $x$-axis by applying $u$. The same process in higher dimensions is to determine how much we stretch the area (or volume, or hyper-volume) of a little square (or little *hyper-cube*) by applying $\\boldsymbol{\\phi}$. If $\\boldsymbol{\\phi}$ was the multiplication by a matrix, then we know how the determinant already gives the answer."
    },
    {
      "chunk_id": "29e5e7ccfaa4_1",
      "chapter": "integral-calculus",
      "heading": "Change of Variables in Multiple Integrals",
      "text": "If $\\boldsymbol{\\phi}$ was the multiplication by a matrix, then we know how the determinant already gives the answer. With some work, one can show that the *Jacobian* provides the best approximation to a multivariable function $\\boldsymbol{\\phi}$ at a point by a matrix in the same way we could approximate by lines or planes with derivatives and gradients. Thus the determinant of the Jacobian exactly mirrors the scaling factor we identified in one dimension. It takes some work to fill in the details to this, so do not worry if they are not clear now. Let's see at least one example we will make use of later on. Consider the integral\n\n$$\n\\int _ {-\\infty}^{\\infty} \\int _ {-\\infty}^{\\infty} e^{-x^{2}-y^{2}} \\;dx\\;dy. $$\n\nPlaying with this integral directly will get us no-where, but if we change variables, we can make significant progress. If we let $\\boldsymbol{\\phi}(r, \\theta) = (r \\cos(\\theta),  r\\sin(\\theta))$ (which is to say that $x = r \\cos(\\theta)$, $y = r \\sin(\\theta)$), then we can apply the change of variable formula to see that this is the same thing as\n\n$$\n\\int _ 0^\\infty \\int_0 ^ {2\\pi} e^{-r^{2}} \\left|\\det(D\\mathbf{\\phi}(\\mathbf{x}))\\right|\\;d\\theta\\;dr,\n$$\n\nwhere\n\n$$\n\\left|\\det(D\\mathbf{\\phi}(\\mathbf{x}))\\right| = \\left|\\det\\begin{bmatrix}\n\\cos(\\theta) & -r\\sin(\\theta) \\\\\n\\sin(\\theta) & r\\cos(\\theta)\n\\end{bmatrix}\\right| = r(\\cos^{2}(\\theta) + \\sin^{2}(\\theta)) = r. $$\n\nThus, the integral is\n\n$$\n\\int _ 0^\\infty \\int _ 0 ^ {2\\pi} re^{-r^{2}} \\;d\\theta\\;dr = 2\\pi\\int _ 0^\\infty re^{-r^{2}} \\;dr = \\pi,\n$$\n\nwhere the final equality follows by the same computation that we used in section :numref:`subsec_integral_example`. We will meet this integral again when we study continuous random variables in :numref:`sec_random_variables`."
    },
    {
      "chunk_id": "6a9099d6b8ee_0",
      "chapter": "integral-calculus",
      "heading": "Summary",
      "text": "* The theory of integration allows us to answer questions about areas or volumes.\n* The fundamental theorem of calculus allows us to leverage knowledge about derivatives to compute areas via the observation that the derivative of the area up to some point is given by the value of the function being integrated.\n* Integrals in higher dimensions can be computed by iterating single variable integrals."
    },
    {
      "chunk_id": "e9ed651161a0_0",
      "chapter": "integral-calculus",
      "heading": "Exercises",
      "text": "1. What is $\\int_1^2 \\frac{1}{x} \\;dx$?\n2. Use the change of variables formula to integrate $\\int_0^{\\sqrt{\\pi}}x\\sin(x^2)\\;dx$.\n3. What is $\\int_{[0,1]^2} xy \\;dx\\;dy$?\n4. Use the change of variables formula to compute $\\int_0^2\\int_0^1xy(x^2-y^2)/(x^2+y^2)^3\\;dy\\;dx$ and $\\int_0^1\\int_0^2f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3\\;dx\\;dy$ to see they are different.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/414)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1092)\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1093)\n:end_tab:"
    },
    {
      "chunk_id": "fa0b79b0614c_0",
      "chapter": "maximum-likelihood",
      "heading": "maximum-likelihood",
      "text": "# Maximum Likelihood\n:label:`sec_maximum_likelihood`\n\nOne of the most commonly encountered way of thinking in machine learning is the maximum likelihood point of view.  This is the concept that when working with a probabilistic model with unknown parameters, the parameters which make the data have the highest probability are the most likely ones."
    },
    {
      "chunk_id": "0d1673badb6d_0",
      "chapter": "maximum-likelihood",
      "heading": "The Maximum Likelihood Principle",
      "text": "This has a Bayesian interpretation which can be helpful to think about.  Suppose that we have a model with parameters $\\boldsymbol{\\theta}$ and a collection of data examples $X$.  For concreteness, we can imagine that $\\boldsymbol{\\theta}$ is a single value representing the probability that a coin comes up heads when flipped, and $X$ is a sequence of independent coin flips.  We will look at this example in depth later.\n\nIf we want to find the most likely value for the parameters of our model, that means we want to find\n\n$$\\mathop{\\mathrm{argmax}} P(\\boldsymbol{\\theta}\\mid X).$$\n:eqlabel:`eq_max_like`\n\nBy Bayes' rule, this is the same thing as\n\n$$\n\\mathop{\\mathrm{argmax}} \\frac{P(X \\mid \\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(X)}.\n$$\n\nThe expression $P(X)$, a parameter agnostic probability of generating the data, does not depend on $\\boldsymbol{\\theta}$ at all, and so can be dropped without changing the best choice of $\\boldsymbol{\\theta}$.  Similarly, we may now posit that we have no prior assumption on which set of parameters are better than any others, so we may declare that $P(\\boldsymbol{\\theta})$ does not depend on theta either!  This, for instance, makes sense in our coin flipping example where the probability it comes up heads could be any value in $[0,1]$ without any prior belief it is fair or not (often referred to as an *uninformative prior*).  Thus we see that our application of Bayes' rule shows that our best choice of $\\boldsymbol{\\theta}$ is the maximum likelihood estimate for $\\boldsymbol{\\theta}$:\n\n$$\n\\hat{\\boldsymbol{\\theta}} = \\mathop{\\mathrm{argmax}} _ {\\boldsymbol{\\theta}} P(X \\mid \\boldsymbol{\\theta}).\n$$\n\nAs a matter of common terminology, the probability of the data given the parameters ($P(X \\mid \\boldsymbol{\\theta})$) is referred to as the *likelihood*."
    },
    {
      "chunk_id": "f0658ca6585b_0",
      "chapter": "maximum-likelihood",
      "heading": "A Concrete Example",
      "text": "Let's see how this works in a concrete example. Suppose that we have a single parameter $\\theta$ representing the probability that a coin flip is heads. Then the probability of getting a tails is $1-\\theta$, and so if our observed data $X$ is a sequence with $n_H$ heads and $n_T$ tails, we can use the fact that independent probabilities multiply to see that \n\n$$\nP(X \\mid \\theta) = \\theta^{n_H}(1-\\theta)^{n_T}. $$\n\nIf we flip $13$ coins and get the sequence \"HHHTHTTHHHHHT\", which has $n_H = 9$ and $n_T = 4$, we see that this is\n\n$$\nP(X \\mid \\theta) = \\theta^9(1-\\theta)^4. $$\n\nOne nice thing about this example will be that we know the answer going in. Indeed, if we said verbally, \"I flipped 13 coins, and 9 came up heads, what is our best guess for the probability that the coin comes us heads?, \" everyone would correctly guess $9/13$. What this maximum likelihood method will give us is a way to get that number from first principals in a way that will generalize to vastly more complex situations. For our example, the plot of $P(X \\mid \\theta)$ is as follows:\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx\nnpx.set_np()\n\ntheta = np.arange(0, 1, 0.001)\np = theta**9 * (1 - theta)**4. d2l.plot(theta, p, 'theta', 'likelihood')\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n\ntheta = torch.arange(0, 1, 0.001)\np = theta**9 * (1 - theta)**4. d2l.plot(theta, p, 'theta', 'likelihood')\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n\ntheta = tf.range(0, 1, 0.001)\np = theta**9 * (1 - theta)**4. d2l.plot(theta, p, 'theta', 'likelihood')\n```\n\nThis has its maximum value somewhere near our expected $9/13 \\approx 0.7\\ldots$. To see if it is exactly there, we can turn to calculus. Notice that at the maximum, the gradient of the function is flat."
    },
    {
      "chunk_id": "f0658ca6585b_1",
      "chapter": "maximum-likelihood",
      "heading": "A Concrete Example",
      "text": "d2l.plot(theta, p, 'theta', 'likelihood')\n```\n\nThis has its maximum value somewhere near our expected $9/13 \\approx 0.7\\ldots$. To see if it is exactly there, we can turn to calculus. Notice that at the maximum, the gradient of the function is flat. Thus, we could find the maximum likelihood estimate :eqref:`eq_max_like` by finding the values of $\\theta$ where the derivative is zero, and finding the one that gives the highest probability. We compute:\n\n$$\n\\begin{aligned}\n0 & = \\frac{d}{d\\theta} P(X \\mid \\theta) \\\\\n& = \\frac{d}{d\\theta} \\theta^9(1-\\theta)^4 \\\\\n& = 9\\theta^8(1-\\theta)^4 - 4\\theta^9(1-\\theta)^3 \\\\\n& = \\theta^8(1-\\theta)^3(9-13\\theta). \\end{aligned}\n$$\n\nThis has three solutions: $0$, $1$ and $9/13$. The first two are clearly minima, not maxima as they assign probability $0$ to our sequence. The final value does *not* assign zero probability to our sequence, and thus must be the maximum likelihood estimate $\\hat \\theta = 9/13$."
    },
    {
      "chunk_id": "5f30b8f9cb18_0",
      "chapter": "maximum-likelihood",
      "heading": "Numerical Optimization and the Negative Log-Likelihood",
      "text": "The previous example is nice, but what if we have billions of parameters and data examples? First, notice that if we make the assumption that all the data examples are independent, we can no longer practically consider the likelihood itself as it is a product of many probabilities. Indeed, each probability is in $[0,1]$, say typically of value about $1/2$, and the product of $(1/2)^{1000000000}$ is far below machine precision. We cannot work with that directly. However, recall that the logarithm turns products to sums, in which case \n\n$$\n\\log((1/2)^{1000000000}) = 1000000000\\cdot\\log(1/2) \\approx -301029995.6\\ldots\n$$\n\nThis number fits perfectly within even a single precision $32$-bit float. Thus, we should consider the *log-likelihood*, which is\n\n$$\n\\log(P(X \\mid \\boldsymbol{\\theta})). $$\n\nSince the function $x \\mapsto \\log(x)$ is increasing, maximizing the likelihood is the same thing as maximizing the log-likelihood. Indeed in :numref:`sec_naive_bayes` we will see this reasoning applied when working with the specific example of the naive Bayes classifier. We often work with loss functions, where we wish to minimize the loss. We may turn maximum likelihood into the minimization of a loss by taking $-\\log(P(X \\mid \\boldsymbol{\\theta}))$, which is the *negative log-likelihood*. To illustrate this, consider the coin flipping problem from before, and pretend that we do not know the closed form solution. We may compute that\n\n$$\n-\\log(P(X \\mid \\boldsymbol{\\theta})) = -\\log(\\theta^{n_H}(1-\\theta)^{n_T}) = -(n_H\\log(\\theta) + n_T\\log(1-\\theta)). $$\n\nThis can be written into code, and freely optimized even for billions of coin flips."
    },
    {
      "chunk_id": "5f30b8f9cb18_1",
      "chapter": "maximum-likelihood",
      "heading": "Numerical Optimization and the Negative Log-Likelihood",
      "text": "We may compute that\n\n$$\n-\\log(P(X \\mid \\boldsymbol{\\theta})) = -\\log(\\theta^{n_H}(1-\\theta)^{n_T}) = -(n_H\\log(\\theta) + n_T\\log(1-\\theta)). $$\n\nThis can be written into code, and freely optimized even for billions of coin flips. ```{.python .input}\n#@tab mxnet\n# Set up our data\nn_H = 8675309\nn_T = 256245\n\n# Initialize our paramteres\ntheta = np.array(0.5)\ntheta.attach_grad()\n\n# Perform gradient descent\nlr = 1e-9\nfor iter in range(100):\n    with autograd.record():\n        loss = -(n_H * np.log(theta) + n_T * np.log(1 - theta))\n    loss.backward()\n    theta -= lr * theta.grad\n\n# Check output\ntheta, n_H / (n_H + n_T)\n```\n\n```{.python .input}\n#@tab pytorch\n# Set up our data\nn_H = 8675309\nn_T = 256245\n\n# Initialize our paramteres\ntheta = torch.tensor(0.5, requires_grad=True)\n\n# Perform gradient descent\nlr = 1e-9\nfor iter in range(100):\n    loss = -(n_H * torch.log(theta) + n_T * torch.log(1 - theta))\n    loss.backward()\n    with torch.no_grad():\n        theta -= lr * theta.grad\n    theta.grad.zero_()\n\n# Check output\ntheta, n_H / (n_H + n_T)\n```\n\n```{.python .input}\n#@tab tensorflow\n# Set up our data\nn_H = 8675309\nn_T = 256245\n\n# Initialize our paramteres\ntheta = tf.Variable(tf.constant(0.5))\n\n# Perform gradient descent\nlr = 1e-9\nfor iter in range(100):\n    with tf.GradientTape() as t:\n        loss = -(n_H * tf.math.log(theta) + n_T * tf.math.log(1 - theta))\n    theta.assign_sub(lr * t.gradient(loss, theta))\n\n# Check output\ntheta, n_H / (n_H + n_T)\n```\n\nNumerical convenience is not the only reason why people like to use negative log-likelihoods. There are several other reasons why it is preferable. The second reason we consider the log-likelihood is the simplified application of calculus rules. As discussed above, due to independence assumptions, most probabilities we encounter in machine learning are products of individual probabilities. $$\nP(X\\mid\\boldsymbol{\\theta}) = p(x_1\\mid\\boldsymbol{\\theta})\\cdot p(x_2\\mid\\boldsymbol{\\theta})\\cdots p(x_n\\mid\\boldsymbol{\\theta})."
    },
    {
      "chunk_id": "5f30b8f9cb18_2",
      "chapter": "maximum-likelihood",
      "heading": "Numerical Optimization and the Negative Log-Likelihood",
      "text": "$$\nP(X\\mid\\boldsymbol{\\theta}) = p(x_1\\mid\\boldsymbol{\\theta})\\cdot p(x_2\\mid\\boldsymbol{\\theta})\\cdots p(x_n\\mid\\boldsymbol{\\theta}). $$\n\nThis means that if we directly apply the product rule to compute a derivative we get\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} P(X\\mid\\boldsymbol{\\theta}) & = \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}P(x_1\\mid\\boldsymbol{\\theta})\\right)\\cdot P(x_2\\mid\\boldsymbol{\\theta})\\cdots P(x_n\\mid\\boldsymbol{\\theta}) \\\\\n& \\quad + P(x_1\\mid\\boldsymbol{\\theta})\\cdot \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}P(x_2\\mid\\boldsymbol{\\theta})\\right)\\cdots P(x_n\\mid\\boldsymbol{\\theta}) \\\\\n& \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n& \\quad + P(x_1\\mid\\boldsymbol{\\theta})\\cdot P(x_2\\mid\\boldsymbol{\\theta}) \\cdots \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}P(x_n\\mid\\boldsymbol{\\theta})\\right). \\end{aligned}\n$$\n\nThis requires $n(n-1)$ multiplications, along with $(n-1)$ additions, so it is proportional to quadratic time in the inputs! Sufficient cleverness in grouping terms will reduce this to linear time, but it requires some thought. For the negative log-likelihood we have instead\n\n$$\n-\\log\\left(P(X\\mid\\boldsymbol{\\theta})\\right) = -\\log(P(x_1\\mid\\boldsymbol{\\theta})) - \\log(P(x_2\\mid\\boldsymbol{\\theta})) \\cdots - \\log(P(x_n\\mid\\boldsymbol{\\theta})),\n$$\n\nwhich then gives\n\n$$\n- \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log\\left(P(X\\mid\\boldsymbol{\\theta})\\right) = \\frac{1}{P(x_1\\mid\\boldsymbol{\\theta})}\\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}P(x_1\\mid\\boldsymbol{\\theta})\\right) + \\cdots + \\frac{1}{P(x_n\\mid\\boldsymbol{\\theta})}\\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}P(x_n\\mid\\boldsymbol{\\theta})\\right). $$\n\nThis requires only $n$ divides and $n-1$ sums, and thus is linear time in the inputs. The third and final reason to consider the negative log-likelihood is the relationship to information theory, which we will discuss in detail in :numref:`sec_information_theory`."
    },
    {
      "chunk_id": "5f30b8f9cb18_3",
      "chapter": "maximum-likelihood",
      "heading": "Numerical Optimization and the Negative Log-Likelihood",
      "text": "The third and final reason to consider the negative log-likelihood is the relationship to information theory, which we will discuss in detail in :numref:`sec_information_theory`. This is a rigorous mathematical theory which gives a way to measure the degree of information or randomness in a random variable. The key object of study in that field is the entropy which is \n\n$$\nH(p) = -\\sum_{i} p_i \\log_2(p_i),\n$$\n\nwhich measures the randomness of a source. Notice that this is nothing more than the average $-\\log$ probability, and thus if we take our negative log-likelihood and divide by the number of data examples, we get a relative of entropy known as cross-entropy. This theoretical interpretation alone would be sufficiently compelling to motivate reporting the average negative log-likelihood over the dataset as a way of measuring model performance."
    },
    {
      "chunk_id": "46bdbaae7d89_0",
      "chapter": "maximum-likelihood",
      "heading": "Maximum Likelihood for Continuous Variables",
      "text": "Everything that we have done so far assumes we are working with discrete random variables, but what if we want to work with continuous ones? The short summary is that nothing at all changes, except we replace all the instances of the probability with the probability density. Recalling that we write densities with lower case $p$, this means that for example we now say\n\n$$\n-\\log\\left(p(X\\mid\\boldsymbol{\\theta})\\right) = -\\log(p(x_1\\mid\\boldsymbol{\\theta})) - \\log(p(x_2\\mid\\boldsymbol{\\theta})) \\cdots - \\log(p(x_n\\mid\\boldsymbol{\\theta})) = -\\sum_i \\log(p(x_i \\mid \\theta)). $$\n\nThe question becomes, \"Why is this OK?\"  After all, the reason we introduced densities was because probabilities of getting specific outcomes themselves was zero, and thus is not the probability of generating our data for any set of parameters zero? Indeed, this is the case, and understanding why we can shift to densities is an exercise in tracing what happens to the epsilons. Let's first re-define our goal. Suppose that for continuous random variables we no longer want to compute the probability of getting exactly the right value, but instead matching to within some range $\\epsilon$. For simplicity, we assume our data is repeated observations $x_1, \\ldots, x_N$ of identically distributed random variables $X_1, \\ldots, X_N$. As we have seen previously, this can be written as\n\n$$\n\\begin{aligned}\n&P(X_1 \\in [x_1, x_1+\\epsilon], X_2 \\in [x_2, x_2+\\epsilon], \\ldots, X_N \\in [x_N, x_N+\\epsilon]\\mid\\boldsymbol{\\theta}) \\\\\n\\approx &\\epsilon^Np(x_1\\mid\\boldsymbol{\\theta})\\cdot p(x_2\\mid\\boldsymbol{\\theta}) \\cdots p(x_n\\mid\\boldsymbol{\\theta}). \\end{aligned}\n$$\n\nThus, if we take negative logarithms of this we obtain\n\n$$\n\\begin{aligned}\n&-\\log(P(X_1 \\in [x_1, x_1+\\epsilon], X_2 \\in [x_2, x_2+\\epsilon], \\ldots, X_N \\in [x_N, x_N+\\epsilon]\\mid\\boldsymbol{\\theta})) \\\\\n\\approx & -N\\log(\\epsilon) - \\sum_{i} \\log(p(x_i\\mid\\boldsymbol{\\theta}))."
    },
    {
      "chunk_id": "46bdbaae7d89_1",
      "chapter": "maximum-likelihood",
      "heading": "Maximum Likelihood for Continuous Variables",
      "text": "\\end{aligned}\n$$\n\nIf we examine this expression, the only place that the $\\epsilon$ occurs is in the additive constant $-N\\log(\\epsilon)$. This does not depend on the parameters $\\boldsymbol{\\theta}$ at all, so the optimal choice of $\\boldsymbol{\\theta}$ does not depend on our choice of $\\epsilon$! If we demand four digits or four-hundred, the best choice of $\\boldsymbol{\\theta}$ remains the same, thus we may freely drop the epsilon to see that what we want to optimize is\n\n$$\n- \\sum_{i} \\log(p(x_i\\mid\\boldsymbol{\\theta})). $$\n\nThus, we see that the maximum likelihood point of view can operate with continuous random variables as easily as with discrete ones by replacing the probabilities with probability densities."
    },
    {
      "chunk_id": "adaa8e00f948_0",
      "chapter": "maximum-likelihood",
      "heading": "Summary",
      "text": "* The maximum likelihood principle tells us that the best fit model for a given dataset is the one that generates the data with the highest probability.\n* Often people work with the negative log-likelihood instead for a variety of reasons: numerical stability, conversion of products to sums (and the resulting simplification of gradient computations), and theoretical ties to information theory.\n* While simplest to motivate in the discrete setting, it may be freely generalized to the continuous setting as well by maximizing the probability density assigned to the datapoints."
    },
    {
      "chunk_id": "726faf75d1b4_0",
      "chapter": "maximum-likelihood",
      "heading": "Exercises",
      "text": "1. Suppose that you know that a non-negative random variable has density $\\alpha e^{-\\alpha x}$ for some value $\\alpha>0$.  You obtain a single observation from the random variable which is the number $3$.  What is the maximum likelihood estimate for $\\alpha$?\n2. Suppose that you have a dataset of samples $\\{x_i\\}_{i=1}^N$ drawn from a Gaussian with unknown mean, but variance $1$.  What is the maximum likelihood estimate for the mean?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/416)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1096)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1097)\n:end_tab:"
    },
    {
      "chunk_id": "777c99ccbb2f_0",
      "chapter": "multivariable-calculus",
      "heading": "multivariable-calculus",
      "text": "# Multivariable Calculus\n:label:`sec_multivariable_calculus`\n\nNow that we have a fairly strong understanding of derivatives of a function of a single variable, let's return to our original question where we were considering a loss function of potentially billions of weights."
    },
    {
      "chunk_id": "3c59326f3309_0",
      "chapter": "multivariable-calculus",
      "heading": "Higher-Dimensional Differentiation",
      "text": "What :numref:`sec_single_variable_calculus` tells us is that if we change a single one of these billions of weights leaving every other one fixed, we know what will happen! This is nothing more than a function of a single variable, so we can write\n\n$$L(w_1+\\epsilon_1, w_2, \\ldots, w_N) \\approx L(w_1, w_2, \\ldots, w_N) + \\epsilon_1 \\frac{d}{dw_1} L(w_1, w_2, \\ldots, w_N).$$\n:eqlabel:`eq_part_der`\n\nWe will call the derivative in one variable while fixing the other variables the *partial derivative*, and we will use the notation $\\frac{\\partial}{\\partial w_1}$ for the derivative in :eqref:`eq_part_der`. Now, let's take this and change $w_2$ a little bit to $w_2 + \\epsilon_2$:\n\n$$\n\\begin{aligned}\nL(w_1+\\epsilon_1, w_2+\\epsilon_2, \\ldots, w_N) & \\approx L(w_1, w_2+\\epsilon_2, \\ldots, w_N) + \\epsilon_1 \\frac{\\partial}{\\partial w_1} L(w_1, w_2+\\epsilon_2, \\ldots, w_N+\\epsilon_N) \\\\\n& \\approx L(w_1, w_2, \\ldots, w_N) \\\\\n& \\quad + \\epsilon_2\\frac{\\partial}{\\partial w_2} L(w_1, w_2, \\ldots, w_N) \\\\\n& \\quad + \\epsilon_1 \\frac{\\partial}{\\partial w_1} L(w_1, w_2, \\ldots, w_N) \\\\\n& \\quad + \\epsilon_1\\epsilon_2\\frac{\\partial}{\\partial w_2}\\frac{\\partial}{\\partial w_1} L(w_1, w_2, \\ldots, w_N) \\\\\n& \\approx L(w_1, w_2, \\ldots, w_N) \\\\\n& \\quad + \\epsilon_2\\frac{\\partial}{\\partial w_2} L(w_1, w_2, \\ldots, w_N) \\\\\n& \\quad + \\epsilon_1 \\frac{\\partial}{\\partial w_1} L(w_1, w_2, \\ldots, w_N). \\end{aligned}\n$$\n\nWe have again used the idea that $\\epsilon_1\\epsilon_2$ is a higher order term that we can discard in the same way we could discard $\\epsilon^{2}$ in the previous section, along with what we saw in :eqref:`eq_part_der`. By continuing in this manner, we may write that\n\n$$\nL(w_1+\\epsilon_1, w_2+\\epsilon_2, \\ldots, w_N+\\epsilon_N) \\approx L(w_1, w_2, \\ldots, w_N) + \\sum_i \\epsilon_i \\frac{\\partial}{\\partial w_i} L(w_1, w_2, \\ldots, w_N)."
    },
    {
      "chunk_id": "3c59326f3309_1",
      "chapter": "multivariable-calculus",
      "heading": "Higher-Dimensional Differentiation",
      "text": "By continuing in this manner, we may write that\n\n$$\nL(w_1+\\epsilon_1, w_2+\\epsilon_2, \\ldots, w_N+\\epsilon_N) \\approx L(w_1, w_2, \\ldots, w_N) + \\sum_i \\epsilon_i \\frac{\\partial}{\\partial w_i} L(w_1, w_2, \\ldots, w_N). $$\n\nThis may look like a mess, but we can make this more familiar by noting that the sum on the right looks exactly like a dot product, so if we let\n\n$$\n\\boldsymbol{\\epsilon} = [\\epsilon_1, \\ldots, \\epsilon_N]^\\top \\; \\textrm{and} \\;\n\\nabla_{\\mathbf{x}} L = \\left[\\frac{\\partial L}{\\partial x_1}, \\ldots, \\frac{\\partial L}{\\partial x_N}\\right]^\\top,\n$$\n\nthen\n\n$$L(\\mathbf{w} + \\boldsymbol{\\epsilon}) \\approx L(\\mathbf{w}) + \\boldsymbol{\\epsilon}\\cdot \\nabla_{\\mathbf{w}} L(\\mathbf{w}).$$\n:eqlabel:`eq_nabla_use`\n\nWe will call the vector $\\nabla_{\\mathbf{w}} L$ the *gradient* of $L$. Equation :eqref:`eq_nabla_use` is worth pondering for a moment. It has exactly the format that we encountered in one dimension, just we have converted everything to vectors and dot products. It allows us to tell approximately how the function $L$ will change given any perturbation to the input. As we will see in the next section, this will provide us with an important tool in understanding geometrically how we can learn using information contained in the gradient. But first, let's see this approximation at work with an example. Suppose that we are working with the function\n\n$$\nf(x, y) = \\log(e^x + e^y) \\textrm{ with gradient } \\nabla f (x, y) = \\left[\\frac{e^x}{e^x+e^y}, \\frac{e^y}{e^x+e^y}\\right]. $$\n\nIf we look at a point like $(0, \\log(2))$, we see that\n\n$$\nf(x, y) = \\log(3) \\textrm{ with gradient } \\nabla f (x, y) = \\left[\\frac{1}{3}, \\frac{2}{3}\\right]. $$\n\nThus, if we want to approximate $f$ at $(\\epsilon_1, \\log(2) + \\epsilon_2)$,  we see that we should have the specific instance of :eqref:`eq_nabla_use`:\n\n$$\nf(\\epsilon_1, \\log(2) + \\epsilon_2) \\approx \\log(3) + \\frac{1}{3}\\epsilon_1 + \\frac{2}{3}\\epsilon_2. $$\n\nWe can test this in code to see how good the approximation is."
    },
    {
      "chunk_id": "3c59326f3309_2",
      "chapter": "multivariable-calculus",
      "heading": "Higher-Dimensional Differentiation",
      "text": "$$\n\nWe can test this in code to see how good the approximation is. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom mxnet import autograd, np, npx\nnpx.set_np()\n\ndef f(x, y):\n    return np.log(np.exp(x) + np.exp(y))\ndef grad_f(x, y):\n    return np.array([np.exp(x) / (np.exp(x) + np.exp(y)),\n                     np.exp(y) / (np.exp(x) + np.exp(y))])\n\nepsilon = np.array([0.01, -0.03])\ngrad_approx = f(0, np.log(2)) + epsilon.dot(grad_f(0, np.log(2)))\ntrue_value = f(0 + epsilon[0], np.log(2) + epsilon[1])\nf'approximation: {grad_approx}, true Value: {true_value}'\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nimport torch\nimport numpy as np\n\ndef f(x, y):\n    return torch.log(torch.exp(x) + torch.exp(y))\ndef grad_f(x, y):\n    return torch.tensor([torch.exp(x) / (torch.exp(x) + torch.exp(y)),\n                     torch.exp(y) / (torch.exp(x) + torch.exp(y))])\n\nepsilon = torch.tensor([0.01, -0.03])\ngrad_approx = f(torch.tensor([0.]), torch.log(\n    torch.tensor([2.]))) + epsilon.dot(\n    grad_f(torch.tensor([0.]), torch.log(torch.tensor(2.))))\ntrue_value = f(torch.tensor([0.]) + epsilon[0], torch.log(\n    torch.tensor([2.])) + epsilon[1])\nf'approximation: {grad_approx}, true Value: {true_value}'\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nimport tensorflow as tf\nimport numpy as np\n\ndef f(x, y):\n    return tf.math.log(tf.exp(x) + tf.exp(y))\ndef grad_f(x, y):\n    return tf.constant([(tf.exp(x) / (tf.exp(x) + tf.exp(y))).numpy(),\n                        (tf.exp(y) / (tf.exp(x) + tf.exp(y))).numpy()])\n\nepsilon = tf.constant([0.01, -0.03])\ngrad_approx = f(tf.constant([0.]), tf.math.log(\n    tf.constant([2.]))) + tf.tensordot(\n    epsilon, grad_f(tf.constant([0.]), tf.math.log(tf.constant(2.))), axes=1)\ntrue_value = f(tf.constant([0.]) + epsilon[0], tf.math.log(\n    tf.constant([2.])) + epsilon[1])\nf'approximation: {grad_approx}, true Value: {true_value}'\n```"
    },
    {
      "chunk_id": "39b80cc8244b_0",
      "chapter": "multivariable-calculus",
      "heading": "Geometry of Gradients and Gradient Descent",
      "text": "Consider the expression from :eqref:`eq_nabla_use` again:\n\n$$\nL(\\mathbf{w} + \\boldsymbol{\\epsilon}) \\approx L(\\mathbf{w}) + \\boldsymbol{\\epsilon}\\cdot \\nabla_{\\mathbf{w}} L(\\mathbf{w}). $$\n\nLet's suppose that I want to use this to help minimize our loss $L$. Let's understand geometrically the algorithm of gradient descent first described in  :numref:`sec_autograd`. What we will do is the following:\n\n1. Start with a random choice for the initial parameters $\\mathbf{w}$. 2. Find the direction $\\mathbf{v}$ that makes $L$ decrease the most rapidly at $\\mathbf{w}$. 3. Take a small step in that direction: $\\mathbf{w} \\rightarrow \\mathbf{w} + \\epsilon\\mathbf{v}$. 4. Repeat. The only thing we do not know exactly how to do is to compute the vector $\\mathbf{v}$ in the second step. We will call such a direction the *direction of steepest descent*. Using the geometric understanding of dot products from :numref:`sec_geometry-linear-algebraic-ops`, we see that we can rewrite :eqref:`eq_nabla_use` as\n\n$$\nL(\\mathbf{w} + \\mathbf{v}) \\approx L(\\mathbf{w}) + \\mathbf{v}\\cdot \\nabla_{\\mathbf{w}} L(\\mathbf{w}) = L(\\mathbf{w}) + \\|\\nabla_{\\mathbf{w}} L(\\mathbf{w})\\|\\cos(\\theta). $$\n\nNote that we have taken our direction to have length one for convenience, and used $\\theta$ for the angle between $\\mathbf{v}$ and $\\nabla_{\\mathbf{w}} L(\\mathbf{w})$. If we want to find the direction that decreases $L$ as rapidly as possible, we want to make this expression as negative as possible. The only way the direction we pick enters into this equation is through $\\cos(\\theta)$, and thus we wish to make this cosine as negative as possible. Now, recalling the shape of cosine, we can make this as negative as possible by making $\\cos(\\theta) = -1$ or equivalently making the angle between the gradient and our chosen direction to be $\\pi$ radians, or equivalently $180$ degrees. The only way to achieve this is to head in the exact opposite direction:  pick $\\mathbf{v}$ to point in the exact opposite direction to $\\nabla_{\\mathbf{w}} L(\\mathbf{w})$!"
    },
    {
      "chunk_id": "39b80cc8244b_1",
      "chapter": "multivariable-calculus",
      "heading": "Geometry of Gradients and Gradient Descent",
      "text": "The only way to achieve this is to head in the exact opposite direction:  pick $\\mathbf{v}$ to point in the exact opposite direction to $\\nabla_{\\mathbf{w}} L(\\mathbf{w})$! This brings us to one of the most important mathematical concepts in machine learning: the direction of steepest decent points in the direction of $-\\nabla_{\\mathbf{w}}L(\\mathbf{w})$. Thus our informal algorithm can be rewritten as follows. 1. Start with a random choice for the initial parameters $\\mathbf{w}$. 2. Compute $\\nabla_{\\mathbf{w}} L(\\mathbf{w})$. 3. Take a small step in the opposite of that direction: $\\mathbf{w} \\leftarrow \\mathbf{w} - \\epsilon\\nabla_{\\mathbf{w}} L(\\mathbf{w})$. 4. Repeat. This basic algorithm has been modified and adapted many ways by many researchers, but the core concept remains the same in all of them. Use the gradient to find the direction that decreases the loss as rapidly as possible, and update the parameters to take a step in that direction."
    },
    {
      "chunk_id": "e4d7646338d4_0",
      "chapter": "multivariable-calculus",
      "heading": "A Note on Mathematical Optimization",
      "text": "Throughout this book, we focus squarely on numerical optimization techniques for the practical reason that all functions we encounter in the deep learning setting are too complex to minimize explicitly. However, it is a useful exercise to consider what the geometric understanding we obtained above tells us about optimizing functions directly. Suppose that we wish to find the value of $\\mathbf{x}_0$ which minimizes some function $L(\\mathbf{x})$. Let's suppose that moreover someone gives us a value and tells us that it is the value that minimizes $L$. Is there anything we can check to see if their answer is even plausible? Again consider :eqref:`eq_nabla_use`:\n$$\nL(\\mathbf{x}_0 + \\boldsymbol{\\epsilon}) \\approx L(\\mathbf{x}_0) + \\boldsymbol{\\epsilon}\\cdot \\nabla_{\\mathbf{x}} L(\\mathbf{x}_0). $$\n\nIf the gradient is not zero, we know that we can take a step in the direction $-\\epsilon \\nabla_{\\mathbf{x}} L(\\mathbf{x}_0)$ to find a value of $L$ that is smaller. Thus, if we truly are at a minimum, this cannot be the case! We can conclude that if $\\mathbf{x}_0$ is a minimum, then $\\nabla_{\\mathbf{x}} L(\\mathbf{x}_0) = 0$. We call points with $\\nabla_{\\mathbf{x}} L(\\mathbf{x}_0) = 0$ *critical points*. This is nice, because in some rare settings, we *can* explicitly find all the points where the gradient is zero, and find the one with the smallest value. For a concrete example, consider the function\n$$\nf(x) = 3x^4 - 4x^3 -12x^2. $$\n\nThis function has derivative\n$$\n\\frac{df}{dx} = 12x^3 - 12x^2 -24x = 12x(x-2)(x+1). $$\n\nThe only possible location of minima are at $x = -1, 0, 2$, where the function takes the values $-5,0, -32$ respectively, and thus we can conclude that we minimize our function when $x = 2$. A quick plot confirms this."
    },
    {
      "chunk_id": "e4d7646338d4_1",
      "chapter": "multivariable-calculus",
      "heading": "A Note on Mathematical Optimization",
      "text": "$$\n\nThe only possible location of minima are at $x = -1, 0, 2$, where the function takes the values $-5,0, -32$ respectively, and thus we can conclude that we minimize our function when $x = 2$. A quick plot confirms this. ```{.python .input}\n#@tab mxnet\nx = np.arange(-2, 3, 0.01)\nf = (3 * x**4) - (4 * x**3) - (12 * x**2)\n\nd2l.plot(x, f, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.arange(-2, 3, 0.01)\nf = (3 * x**4) - (4 * x**3) - (12 * x**2)\n\nd2l.plot(x, f, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.range(-2, 3, 0.01)\nf = (3 * x**4) - (4 * x**3) - (12 * x**2)\n\nd2l.plot(x, f, 'x', 'f(x)')\n```\n\nThis highlights an important fact to know when working either theoretically or numerically: the only possible points where we can minimize (or maximize) a function will have gradient equal to zero, however, not every point with gradient zero is the true *global* minimum (or maximum)."
    },
    {
      "chunk_id": "6267802261e4_0",
      "chapter": "multivariable-calculus",
      "heading": "Multivariate Chain Rule",
      "text": "Let's suppose that we have a function of four variables ($w, x, y$, and $z$) which we can make by composing many terms:\n\n$$\\begin{aligned}f(u, v) & = (u+v)^{2} \\\\u(a, b) & = (a+b)^{2}, \\qquad v(a, b) = (a-b)^{2}, \\\\a(w, x, y, z) & = (w+x+y+z)^{2}, \\qquad b(w, x, y, z) = (w+x-y-z)^2.\\end{aligned}$$\n:eqlabel:`eq_multi_func_def`\n\nSuch chains of equations are common when working with neural networks, so trying to understand how to compute gradients of such functions is key. We can start to see visual hints of this connection in :numref:`fig_chain-1` if we take a look at what variables directly relate to one another. ![The function relations above where nodes represent values and edges show functional dependence.](../img/chain-net1.svg)\n:label:`fig_chain-1`\n\nNothing stops us from just composing everything from :eqref:`eq_multi_func_def` and writing out that\n\n$$\nf(w, x, y, z) = \\left(\\left((w+x+y+z)^2+(w+x-y-z)^2\\right)^2+\\left((w+x+y+z)^2-(w+x-y-z)^2\\right)^2\\right)^2. $$\n\nWe may then take the derivative by just using single variable derivatives, but if we did that we would quickly find ourself swamped with terms, many of which are repeats! Indeed, one can see that, for instance:\n\n$$\n\\begin{aligned}\n\\frac{\\partial f}{\\partial w} & = 2 \\left(2 \\left(2 (w + x + y + z) - 2 (w + x - y - z)\\right) \\left((w + x + y + z)^{2}- (w + x - y - z)^{2}\\right) + \\right.\\\\\n& \\left. \\quad 2 \\left(2 (w + x - y - z) + 2 (w + x + y + z)\\right) \\left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\\right)\\right) \\times \\\\\n& \\quad \\left(\\left((w + x + y + z)^{2}- (w + x - y - z)^2\\right)^{2}+ \\left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\\right)^{2}\\right). \\end{aligned}\n$$\n\nIf we then also wanted to compute $\\frac{\\partial f}{\\partial x}$, we would end up with a similar equation again with many repeated terms, and many *shared* repeated terms between the two derivatives. This represents a massive quantity of wasted work, and if we needed to compute derivatives this way, the whole deep learning revolution would have stalled out before it began!"
    },
    {
      "chunk_id": "6267802261e4_1",
      "chapter": "multivariable-calculus",
      "heading": "Multivariate Chain Rule",
      "text": "This represents a massive quantity of wasted work, and if we needed to compute derivatives this way, the whole deep learning revolution would have stalled out before it began! Let's break up the problem. We will start by trying to understand how $f$ changes when we change $a$, essentially assuming that $w, x, y$, and $z$ all do not exist. We will reason as we did back when we worked with the gradient for the first time. Let's take $a$ and add a small amount $\\epsilon$ to it. $$\n\\begin{aligned}\n& f(u(a+\\epsilon, b), v(a+\\epsilon, b)) \\\\\n\\approx & f\\left(u(a, b) + \\epsilon\\frac{\\partial u}{\\partial a}(a, b), v(a, b) + \\epsilon\\frac{\\partial v}{\\partial a}(a, b)\\right) \\\\\n\\approx & f(u(a, b), v(a, b)) + \\epsilon\\left[\\frac{\\partial f}{\\partial u}(u(a, b), v(a, b))\\frac{\\partial u}{\\partial a}(a, b) + \\frac{\\partial f}{\\partial v}(u(a, b), v(a, b))\\frac{\\partial v}{\\partial a}(a, b)\\right]. \\end{aligned}\n$$\n\nThe first line follows from the definition of partial derivative, and the second follows from the definition of gradient. It is notationally burdensome to track exactly where we evaluate every derivative, as in the expression $\\frac{\\partial f}{\\partial u}(u(a, b), v(a, b))$, so we often abbreviate this to the much more memorable\n\n$$\n\\frac{\\partial f}{\\partial a} = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial a}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial a}. $$\n\nIt is useful to think about the meaning of the process. We are trying to understand how a function of the form $f(u(a, b), v(a, b))$ changes its value with a change in $a$. There are two pathways this can occur: there is the pathway where $a \\rightarrow u \\rightarrow f$ and where $a \\rightarrow v \\rightarrow f$. We can compute both of these contributions via the chain rule: $\\frac{\\partial w}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial w}{\\partial v} \\cdot \\frac{\\partial v}{\\partial x}$ respectively, and added up."
    },
    {
      "chunk_id": "6267802261e4_2",
      "chapter": "multivariable-calculus",
      "heading": "Multivariate Chain Rule",
      "text": "We can compute both of these contributions via the chain rule: $\\frac{\\partial w}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial w}{\\partial v} \\cdot \\frac{\\partial v}{\\partial x}$ respectively, and added up. Imagine we have a different network of functions where the functions on the right depend on those that are connected to on the left as is shown in :numref:`fig_chain-2`. ![Another more subtle example of the chain rule.](../img/chain-net2.svg)\n:label:`fig_chain-2`\n\nTo compute something like $\\frac{\\partial f}{\\partial y}$, we need to sum over all (in this case $3$) paths from $y$ to $f$ giving\n\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial a} \\frac{\\partial a}{\\partial u} \\frac{\\partial u}{\\partial y} + \\frac{\\partial f}{\\partial u} \\frac{\\partial u}{\\partial y} + \\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial v} \\frac{\\partial v}{\\partial y}. $$\n\nUnderstanding the chain rule in this way will pay great dividends when trying to understand how gradients flow through networks, and why various architectural choices like those in LSTMs (:numref:`sec_lstm`) or residual layers (:numref:`sec_resnet`) can help shape the learning process by controlling gradient flow."
    },
    {
      "chunk_id": "1a822ce8c84a_0",
      "chapter": "multivariable-calculus",
      "heading": "The Backpropagation Algorithm",
      "text": "Let's return to the example of :eqref:`eq_multi_func_def` the previous section where\n\n$$\n\\begin{aligned}\nf(u, v) & = (u+v)^{2} \\\\\nu(a, b) & = (a+b)^{2}, \\qquad v(a, b) = (a-b)^{2}, \\\\\na(w, x, y, z) & = (w+x+y+z)^{2}, \\qquad b(w, x, y, z) = (w+x-y-z)^2. \\end{aligned}\n$$\n\nIf we want to compute say $\\frac{\\partial f}{\\partial w}$ we may apply the multi-variate chain rule to see:\n\n$$\n\\begin{aligned}\n\\frac{\\partial f}{\\partial w} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial w} + \\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial w}, \\\\\n\\frac{\\partial u}{\\partial w} & = \\frac{\\partial u}{\\partial a}\\frac{\\partial a}{\\partial w}+\\frac{\\partial u}{\\partial b}\\frac{\\partial b}{\\partial w}, \\\\\n\\frac{\\partial v}{\\partial w} & = \\frac{\\partial v}{\\partial a}\\frac{\\partial a}{\\partial w}+\\frac{\\partial v}{\\partial b}\\frac{\\partial b}{\\partial w}. \\end{aligned}\n$$\n\nLet's try using this decomposition to compute $\\frac{\\partial f}{\\partial w}$. Notice that all we need here are the various single step partials:\n\n$$\n\\begin{aligned}\n\\frac{\\partial f}{\\partial u} = 2(u+v), & \\quad\\frac{\\partial f}{\\partial v} = 2(u+v), \\\\\n\\frac{\\partial u}{\\partial a} = 2(a+b), & \\quad\\frac{\\partial u}{\\partial b} = 2(a+b), \\\\\n\\frac{\\partial v}{\\partial a} = 2(a-b), & \\quad\\frac{\\partial v}{\\partial b} = -2(a-b), \\\\\n\\frac{\\partial a}{\\partial w} = 2(w+x+y+z), & \\quad\\frac{\\partial b}{\\partial w} = 2(w+x-y-z). \\end{aligned}\n$$\n\nIf we write this out into code this becomes a fairly manageable expression."
    },
    {
      "chunk_id": "1a822ce8c84a_1",
      "chapter": "multivariable-calculus",
      "heading": "The Backpropagation Algorithm",
      "text": "\\end{aligned}\n$$\n\nIf we write this out into code this becomes a fairly manageable expression. ```{.python .input}\n#@tab all\n# Compute the value of the function from inputs to outputs\nw, x, y, z = -1, 0, -2, 1\na, b = (w + x + y + z)**2, (w + x - y - z)**2\nu, v = (a + b)**2, (a - b)**2\nf = (u + v)**2\nprint(f'    f at {w}, {x}, {y}, {z} is {f}')\n\n# Compute the single step partials\ndf_du, df_dv = 2*(u + v), 2*(u + v)\ndu_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)\nda_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)\n\n# Compute the final result from inputs to outputs\ndu_dw, dv_dw = du_da*da_dw + du_db*db_dw, dv_da*da_dw + dv_db*db_dw\ndf_dw = df_du*du_dw + df_dv*dv_dw\nprint(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')\n```\n\nHowever, note that this still does not make it easy to compute something like $\\frac{\\partial f}{\\partial x}$. The reason for that is the *way* we chose to apply the chain rule. If we look at what we did above, we always kept $\\partial w$ in the denominator when we could. In this way, we chose to apply the chain rule seeing how $w$ changed every other variable. If that is what we wanted, this would be a good idea. However, think back to our motivation from deep learning: we want to see how every parameter changes the *loss*. In essence, we want to apply the chain rule keeping $\\partial f$ in the numerator whenever we can! To be more explicit, note that we can write\n\n$$\n\\begin{aligned}\n\\frac{\\partial f}{\\partial w} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial w} + \\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial w}, \\\\\n\\frac{\\partial f}{\\partial a} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial a}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial a}, \\\\\n\\frac{\\partial f}{\\partial b} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial b}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial b}."
    },
    {
      "chunk_id": "1a822ce8c84a_2",
      "chapter": "multivariable-calculus",
      "heading": "The Backpropagation Algorithm",
      "text": "\\end{aligned}\n$$\n\nNote that this application of the chain rule has us explicitly compute $\\frac{\\partial f}{\\partial u}, \\frac{\\partial f}{\\partial v}, \\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}, \\; \\textrm{and} \\; \\frac{\\partial f}{\\partial w}$. Nothing stops us from also including the equations:\n\n$$\n\\begin{aligned}\n\\frac{\\partial f}{\\partial x} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial x} + \\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial x}, \\\\\n\\frac{\\partial f}{\\partial y} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial y}+\\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial y}, \\\\\n\\frac{\\partial f}{\\partial z} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial z}+\\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial z}. \\end{aligned}\n$$\n\nand then keeping track of how $f$ changes when we change *any* node in the entire network. Let's implement it."
    },
    {
      "chunk_id": "1a822ce8c84a_3",
      "chapter": "multivariable-calculus",
      "heading": "The Backpropagation Algorithm",
      "text": "\\end{aligned}\n$$\n\nand then keeping track of how $f$ changes when we change *any* node in the entire network. Let's implement it. ```{.python .input}\n#@tab all\n# Compute the value of the function from inputs to outputs\nw, x, y, z = -1, 0, -2, 1\na, b = (w + x + y + z)**2, (w + x - y - z)**2\nu, v = (a + b)**2, (a - b)**2\nf = (u + v)**2\nprint(f'f at {w}, {x}, {y}, {z} is {f}')\n\n# Compute the derivative using the decomposition above\n# First compute the single step partials\ndf_du, df_dv = 2*(u + v), 2*(u + v)\ndu_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)\nda_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)\nda_dx, db_dx = 2*(w + x + y + z), 2*(w + x - y - z)\nda_dy, db_dy = 2*(w + x + y + z), -2*(w + x - y - z)\nda_dz, db_dz = 2*(w + x + y + z), -2*(w + x - y - z)\n\n# Now compute how f changes when we change any value from output to input\ndf_da, df_db = df_du*du_da + df_dv*dv_da, df_du*du_db + df_dv*dv_db\ndf_dw, df_dx = df_da*da_dw + df_db*db_dw, df_da*da_dx + df_db*db_dx\ndf_dy, df_dz = df_da*da_dy + df_db*db_dy, df_da*da_dz + df_db*db_dz\n\nprint(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')\nprint(f'df/dx at {w}, {x}, {y}, {z} is {df_dx}')\nprint(f'df/dy at {w}, {x}, {y}, {z} is {df_dy}')\nprint(f'df/dz at {w}, {x}, {y}, {z} is {df_dz}')\n```\n\nThe fact that we compute derivatives from $f$ back towards the inputs rather than from the inputs forward to the outputs (as we did in the first code snippet above) is what gives this algorithm its name: *backpropagation*. Note that there are two steps:\n1. Compute the value of the function, and the single step partials from front to back. While not done above, this can be combined into a single *forward pass*. 2. Compute the gradient of $f$ from back to front. We call this the *backwards pass*. This is precisely what every deep learning algorithm implements to allow the computation of the gradient of the loss with respect to every weight in the network at one pass. It is an astonishing fact that we have such a decomposition."
    },
    {
      "chunk_id": "1a822ce8c84a_4",
      "chapter": "multivariable-calculus",
      "heading": "The Backpropagation Algorithm",
      "text": "This is precisely what every deep learning algorithm implements to allow the computation of the gradient of the loss with respect to every weight in the network at one pass. It is an astonishing fact that we have such a decomposition. To see how to encapsulated this, let's take a quick look at this example."
    },
    {
      "chunk_id": "1a822ce8c84a_5",
      "chapter": "multivariable-calculus",
      "heading": "The Backpropagation Algorithm",
      "text": "It is an astonishing fact that we have such a decomposition. To see how to encapsulated this, let's take a quick look at this example. ```{.python .input}\n#@tab mxnet\n# Initialize as ndarrays, then attach gradients\nw, x, y, z = np.array(-1), np.array(0), np.array(-2), np.array(1)\n\nw.attach_grad()\nx.attach_grad()\ny.attach_grad()\nz.attach_grad()\n\n# Do the computation like usual, tracking gradients\nwith autograd.record():\n    a, b = (w + x + y + z)**2, (w + x - y - z)**2\n    u, v = (a + b)**2, (a - b)**2\n    f = (u + v)**2\n\n# Execute backward pass\nf.backward()\n\nprint(f'df/dw at {w}, {x}, {y}, {z} is {w.grad}')\nprint(f'df/dx at {w}, {x}, {y}, {z} is {x.grad}')\nprint(f'df/dy at {w}, {x}, {y}, {z} is {y.grad}')\nprint(f'df/dz at {w}, {x}, {y}, {z} is {z.grad}')\n```\n\n```{.python .input}\n#@tab pytorch\n# Initialize as ndarrays, then attach gradients\nw = torch.tensor([-1.], requires_grad=True)\nx = torch.tensor([0.], requires_grad=True)\ny = torch.tensor([-2.], requires_grad=True)\nz = torch.tensor([1.], requires_grad=True)\n# Do the computation like usual, tracking gradients\na, b = (w + x + y + z)**2, (w + x - y - z)**2\nu, v = (a + b)**2, (a - b)**2\nf = (u + v)**2\n\n# Execute backward pass\nf.backward()\n\nprint(f'df/dw at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n      f'{z.data.item()} is {w.grad.data.item()}')\nprint(f'df/dx at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n      f'{z.data.item()} is {x.grad.data.item()}')\nprint(f'df/dy at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n      f'{z.data.item()} is {y.grad.data.item()}')\nprint(f'df/dz at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n      f'{z.data.item()} is {z.grad.data.item()}')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Initialize as ndarrays, then attach gradients\nw = tf.Variable(tf.constant([-1.]))\nx = tf.Variable(tf.constant([0.]))\ny = tf.Variable(tf.constant([-2.]))\nz = tf.Variable(tf.constant([1.]))\n# Do the computation like usual, tracking gradients\nwith tf.GradientTape(persistent=True) as t:\n    a, b = (w + x + y + z)**2, (w + x - y - z)**2\n    u, v = (a + b)**2, (a - b)**2\n    f = (u + v)**2\n\n# Execute backward pass\nw_grad = t.gradient(f, w).numpy()\nx_grad = t.gradient(f, x).numpy()\ny_grad = t.gradient(f, y).numpy()\nz_grad = t.gradient(f, z).numpy()\n\nprint(f'df/dw at {w.numpy()}, {x.numpy()}, {y.numpy()}, '\n      f'{z.numpy()} is {w_grad}')\nprint(f'df/dx at {w.numpy()}, {x.numpy()}, {y.numpy()}, '\n      f'{z.numpy()} is {x_grad}')\nprint(f'df/dy at {w.numpy()}, {x.numpy()}, {y.numpy()}, '\n      f'{z.numpy()} is {y_grad}')\nprint(f'df/dz at {w.numpy()}, {x.numpy()}, {y.numpy()}, '\n      f'{z.numpy()} is {z_grad}')\n```\n\nAll of what we did above can be done automatically by calling `f.backwards()`."
    },
    {
      "chunk_id": "2c7924c56d0f_0",
      "chapter": "multivariable-calculus",
      "heading": "Hessians",
      "text": "As with single variable calculus, it is useful to consider higher-order derivatives in order to get a handle on how we can obtain a better approximation to a function than using the gradient alone. There is one immediate problem one encounters when working with higher order derivatives of functions of several variables, and that is there are a large number of them. If we have a function $f(x_1, \\ldots, x_n)$ of $n$ variables, then we can take $n^{2}$ many second derivatives, namely for any choice of $i$ and $j$:\n\n$$\n\\frac{d^2f}{dx_idx_j} = \\frac{d}{dx_i}\\left(\\frac{d}{dx_j}f\\right). $$\n\nThis is traditionally assembled into a matrix called the *Hessian*:\n\n$$\\mathbf{H}_f = \\begin{bmatrix} \\frac{d^2f}{dx_1dx_1} & \\cdots & \\frac{d^2f}{dx_1dx_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{d^2f}{dx_ndx_1} & \\cdots & \\frac{d^2f}{dx_ndx_n} \\\\ \\end{bmatrix}.$$\n:eqlabel:`eq_hess_def`\n\nNot every entry of this matrix is independent. Indeed, we can show that as long as both *mixed partials* (partial derivatives with respect to more than one variable) exist and are continuous, we can say that for any $i$, and $j$,\n\n$$\n\\frac{d^2f}{dx_idx_j} = \\frac{d^2f}{dx_jdx_i}. $$\n\nThis follows by considering first perturbing a function in the direction of $x_i$, and then perturbing it in $x_j$ and then comparing the result of that with what happens if we perturb first $x_j$ and then $x_i$, with the knowledge that both of these orders lead to the same final change in the output of $f$. As with single variables, we can use these derivatives to get a far better idea of how the function behaves near a point. In particular, we can use it to find the best fitting quadratic near a point $\\mathbf{x}_0$, as we saw in a single variable. Let's see an example. Suppose that $f(x_1, x_2) = a + b_1x_1 + b_2x_2 + c_{11}x_1^{2} + c_{12}x_1x_2 + c_{22}x_2^{2}$. This is the general form for a quadratic in two variables."
    },
    {
      "chunk_id": "2c7924c56d0f_1",
      "chapter": "multivariable-calculus",
      "heading": "Hessians",
      "text": "Let's see an example. Suppose that $f(x_1, x_2) = a + b_1x_1 + b_2x_2 + c_{11}x_1^{2} + c_{12}x_1x_2 + c_{22}x_2^{2}$. This is the general form for a quadratic in two variables. If we look at the value of the function, its gradient, and its Hessian :eqref:`eq_hess_def`, all at the point zero:\n\n$$\n\\begin{aligned}\nf(0,0) & = a, \\\\\n\\nabla f (0,0) & = \\begin{bmatrix}b_1 \\\\ b_2\\end{bmatrix}, \\\\\n\\mathbf{H} f (0,0) & = \\begin{bmatrix}2 c_{11} & c_{12} \\\\ c_{12} & 2c_{22}\\end{bmatrix},\n\\end{aligned}\n$$\n\nwe can get our original polynomial back by saying\n\n$$\nf(\\mathbf{x}) = f(0) + \\nabla f (0) \\cdot \\mathbf{x} + \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{H} f (0) \\mathbf{x}. $$\n\nIn general, if we computed this expansion any point $\\mathbf{x}_0$, we see that\n\n$$\nf(\\mathbf{x}) = f(\\mathbf{x}_0) + \\nabla f (\\mathbf{x}_0) \\cdot (\\mathbf{x}-\\mathbf{x}_0) + \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_0)^\\top \\mathbf{H} f (\\mathbf{x}_0) (\\mathbf{x}-\\mathbf{x}_0). $$\n\nThis works for any dimensional input, and provides the best approximating quadratic to any function at a point. To give an example, let's plot the function\n\n$$\nf(x, y) = xe^{-x^2-y^2}. $$\n\nOne can compute that the gradient and Hessian are\n$$\n\\nabla f(x, y) = e^{-x^2-y^2}\\begin{pmatrix}1-2x^2 \\\\ -2xy\\end{pmatrix} \\; \\textrm{and} \\; \\mathbf{H}f(x, y) = e^{-x^2-y^2}\\begin{pmatrix} 4x^3 - 6x & 4x^2y - 2y \\\\ 4x^2y-2y &4xy^2-2x\\end{pmatrix}. $$\n\nAnd thus, with a little algebra, see that the approximating quadratic at $[-1,0]^\\top$ is\n\n$$\nf(x, y) \\approx e^{-1}\\left(-1 - (x+1) +(x+1)^2+y^2\\right)."
    },
    {
      "chunk_id": "2c7924c56d0f_2",
      "chapter": "multivariable-calculus",
      "heading": "Hessians",
      "text": "$$\n\nAnd thus, with a little algebra, see that the approximating quadratic at $[-1,0]^\\top$ is\n\n$$\nf(x, y) \\approx e^{-1}\\left(-1 - (x+1) +(x+1)^2+y^2\\right). $$\n\n```{.python .input}\n#@tab mxnet\n# Construct grid and compute function\nx, y = np.meshgrid(np.linspace(-2, 2, 101),\n                   np.linspace(-2, 2, 101), indexing='ij')\nz = x*np.exp(- x**2 - y**2)\n\n# Compute approximating quadratic with gradient and Hessian at (1, 0)\nw = np.exp(-1)*(-1 - (x + 1) + (x + 1)**2 + y**2)\n\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x.asnumpy(), y.asnumpy(), z.asnumpy(),\n                  **{'rstride': 10, 'cstride': 10})\nax.plot_wireframe(x.asnumpy(), y.asnumpy(), w.asnumpy(),\n                  **{'rstride': 10, 'cstride': 10}, color='purple')\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(-1, 1)\nax.dist = 12\n```\n\n```{.python .input}\n#@tab pytorch\n# Construct grid and compute function\nx, y = torch.meshgrid(torch.linspace(-2, 2, 101),\n                   torch.linspace(-2, 2, 101))\n\nz = x*torch.exp(- x**2 - y**2)\n\n# Compute approximating quadratic with gradient and Hessian at (1, 0)\nw = torch.exp(torch.tensor([-1.]))*(-1 - (x + 1) + 2 * (x + 1)**2 + 2 * y**2)\n\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x.numpy(), y.numpy(), z.numpy(),\n                  **{'rstride': 10, 'cstride': 10})\nax.plot_wireframe(x.numpy(), y.numpy(), w.numpy(),\n                  **{'rstride': 10, 'cstride': 10}, color='purple')\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(-1, 1)\nax.dist = 12\n```\n\n```{.python .input}\n#@tab tensorflow\n# Construct grid and compute function\nx, y = tf.meshgrid(tf.linspace(-2., 2., 101),\n                   tf.linspace(-2., 2., 101))\n\nz = x*tf.exp(- x**2 - y**2)\n\n# Compute approximating quadratic with gradient and Hessian at (1, 0)\nw = tf.exp(tf.constant([-1.]))*(-1 - (x + 1) + 2 * (x + 1)**2 + 2 * y**2)\n\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x.numpy(), y.numpy(), z.numpy(),\n                  **{'rstride': 10, 'cstride': 10})\nax.plot_wireframe(x.numpy(), y.numpy(), w.numpy(),\n                  **{'rstride': 10, 'cstride': 10}, color='purple')\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(-1, 1)\nax.dist = 12\n```\n\nThis forms the basis for Newton's Algorithm discussed in :numref:`sec_gd`, where we perform numerical optimization iteratively finding the best fitting quadratic, and then exactly minimizing that quadratic."
    },
    {
      "chunk_id": "ed870ff7a43b_0",
      "chapter": "multivariable-calculus",
      "heading": "A Little Matrix Calculus",
      "text": "Derivatives of functions involving matrices turn out to\u00a0be particularly nice. This section can become notationally heavy, so may be skipped in a first reading, but it is useful to know how derivatives of functions involving common matrix operations are often much cleaner than one might initially anticipate, particularly given how central matrix operations are to deep learning applications. Let's begin with an example. Suppose that we have some fixed column vector $\\boldsymbol{\\beta}$, and we want to take the product function $f(\\mathbf{x}) = \\boldsymbol{\\beta}^\\top\\mathbf{x}$, and understand how the dot product changes when we change $\\mathbf{x}$. A bit of notation that will be useful when working with matrix derivatives in ML is called the *denominator layout matrix derivative* where we assemble our partial derivatives into the shape of whatever vector, matrix, or tensor is in the denominator of the differential. In this case, we will write\n\n$$\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix}\n\\frac{df}{dx_1} \\\\\n\\vdots \\\\\n\\frac{df}{dx_n}\n\\end{bmatrix},\n$$\n\nwhere we matched the shape of the column vector $\\mathbf{x}$. If we write out our function into components this is\n\n$$\nf(\\mathbf{x}) = \\sum_{i = 1}^{n} \\beta_ix_i = \\beta_1x_1 + \\cdots + \\beta_nx_n. $$\n\nIf we now take the partial derivative with respect to say $\\beta_1$, note that everything is zero but the first term, which is just $x_1$ multiplied by $\\beta_1$, so we obtain that\n\n$$\n\\frac{df}{dx_1} = \\beta_1,\n$$\n\nor more generally that\n\n$$\n\\frac{df}{dx_i} = \\beta_i. $$\n\nWe can now reassemble this into a matrix to see\n\n$$\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix}\n\\frac{df}{dx_1} \\\\\n\\vdots \\\\\n\\frac{df}{dx_n}\n\\end{bmatrix} = \\begin{bmatrix}\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_n\n\\end{bmatrix} = \\boldsymbol{\\beta}. $$\n\nThis illustrates a few factors about matrix calculus that we will often counter throughout this section:\n\n* First, The computations will get rather involved."
    },
    {
      "chunk_id": "ed870ff7a43b_1",
      "chapter": "multivariable-calculus",
      "heading": "A Little Matrix Calculus",
      "text": "$$\n\nThis illustrates a few factors about matrix calculus that we will often counter throughout this section:\n\n* First, The computations will get rather involved. * Second, The final results are much cleaner than the intermediate process, and will always look similar to the single variable case. In this case, note that $\\frac{d}{dx}(bx) = b$ and $\\frac{d}{d\\mathbf{x}} (\\boldsymbol{\\beta}^\\top\\mathbf{x}) = \\boldsymbol{\\beta}$ are both similar. * Third, transposes can often appear seemingly from nowhere. The core reason for this is the convention that we match the shape of the denominator, thus when we multiply matrices, we will need to take transposes to match back to the shape of the original term. To keep building intuition, let's try a computation that is a little harder. Suppose that we have a column vector $\\mathbf{x}$, and a square matrix $A$ and we want to compute\n\n$$\\frac{d}{d\\mathbf{x}}(\\mathbf{x}^\\top A \\mathbf{x}).$$\n:eqlabel:`eq_mat_goal_1`\n\nTo drive towards easier to manipulate notation, let's consider this problem using Einstein notation. In this case we can write the function as\n\n$$\n\\mathbf{x}^\\top A \\mathbf{x} = x_ia_{ij}x_j. $$\n\nTo compute our derivative, we need to understand for every $k$, what is the value of\n\n$$\n\\frac{d}{dx_k}(\\mathbf{x}^\\top A \\mathbf{x}) = \\frac{d}{dx_k}x_ia_{ij}x_j. $$\n\nBy the product rule, this is\n\n$$\n\\frac{d}{dx_k}x_ia_{ij}x_j = \\frac{dx_i}{dx_k}a_{ij}x_j + x_ia_{ij}\\frac{dx_j}{dx_k}. $$\n\nFor a term like $\\frac{dx_i}{dx_k}$, it is not hard to see that this is one when $i=k$ and zero otherwise. This means that every term where $i$ and $k$ are different vanish from this sum, so the only terms that remain in that first sum are the ones where $i=k$. The same reasoning holds for the second term where we need $j=k$. This gives\n\n$$\n\\frac{d}{dx_k}x_ia_{ij}x_j = a_{kj}x_j + x_ia_{ik}."
    },
    {
      "chunk_id": "ed870ff7a43b_2",
      "chapter": "multivariable-calculus",
      "heading": "A Little Matrix Calculus",
      "text": "The same reasoning holds for the second term where we need $j=k$. This gives\n\n$$\n\\frac{d}{dx_k}x_ia_{ij}x_j = a_{kj}x_j + x_ia_{ik}. $$\n\nNow, the names of the indices in Einstein notation are arbitrary---the fact that $i$ and $j$ are different is immaterial to this computation at this point, so we can re-index so that they both use $i$ to see that\n\n$$\n\\frac{d}{dx_k}x_ia_{ij}x_j = a_{ki}x_i + x_ia_{ik} = (a_{ki} + a_{ik})x_i. $$\n\nNow, here is where we start to need some practice to go further. Let's try and identify this outcome in terms of matrix operations. $a_{ki} + a_{ik}$ is the $k, i$-th component of $\\mathbf{A} + \\mathbf{A}^\\top$. This gives\n\n$$\n\\frac{d}{dx_k}x_ia_{ij}x_j = [\\mathbf{A} + \\mathbf{A}^\\top]_{ki}x_i. $$\n\nSimilarly, this term is now the product of the matrix $\\mathbf{A} + \\mathbf{A}^\\top$ by the vector $\\mathbf{x}$, so we see that\n\n$$\n\\left[\\frac{d}{d\\mathbf{x}}(\\mathbf{x}^\\top A \\mathbf{x})\\right]_k = \\frac{d}{dx_k}x_ia_{ij}x_j = [(\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}]_k. $$\n\nThus, we see that the $k$-th entry of the desired derivative from :eqref:`eq_mat_goal_1` is just the $k$-th entry of the vector on the right, and thus the two are the same. Thus yields\n\n$$\n\\frac{d}{d\\mathbf{x}}(\\mathbf{x}^\\top A \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}. $$\n\nThis required significantly more work than our last one, but the final result is small. More than that, consider the following computation for traditional single variable derivatives:\n\n$$\n\\frac{d}{dx}(xax) = \\frac{dx}{dx}ax + xa\\frac{dx}{dx} = (a+a)x. $$\n\nEquivalently $\\frac{d}{dx}(ax^2) = 2ax = (a+a)x$. Again, we get a result that looks rather like the single variable result but with a transpose tossed in. At this point, the pattern should be looking rather suspicious, so let's try to figure out why. When we take matrix derivatives like this, let's first assume that the expression we get will be another matrix expression: an expression we can write it in terms of products and sums of matrices and their transposes."
    },
    {
      "chunk_id": "ed870ff7a43b_3",
      "chapter": "multivariable-calculus",
      "heading": "A Little Matrix Calculus",
      "text": "When we take matrix derivatives like this, let's first assume that the expression we get will be another matrix expression: an expression we can write it in terms of products and sums of matrices and their transposes. If such an expression exists, it will need to be true for all matrices. In particular, it will need to be true of $1 \\times 1$ matrices, in which case the matrix product is just the product of the numbers, the matrix sum is just the sum, and the transpose does nothing at all! In other words, whatever expression we get *must* match the single variable expression. This means that, with some practice, one can often guess matrix derivatives just by knowing what the associated single variable expression must look like! Let's try this out. Suppose that $\\mathbf{X}$ is a $n \\times m$ matrix, $\\mathbf{U}$ is an $n \\times r$ and $\\mathbf{V}$ is an $r \\times m$. Let's try to compute\n\n$$\\frac{d}{d\\mathbf{V}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2} = \\;?$$\n:eqlabel:`eq_mat_goal_2`\n\nThis computation is important in an area called matrix factorization. For us, however, it is just a derivative to compute. Let's try to imagine what this would be for $1\\times1$ matrices. In that case, we get the expression\n\n$$\n\\frac{d}{dv} (x-uv)^{2}= -2(x-uv)u,\n$$\n\nwhere, the derivative is rather standard. If we try to convert this back into a matrix expression we get\n\n$$\n\\frac{d}{d\\mathbf{V}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= -2(\\mathbf{X} - \\mathbf{U}\\mathbf{V})\\mathbf{U}. $$\n\nHowever, if we look at this it does not quite work. Recall that $\\mathbf{X}$ is $n \\times m$, as is $\\mathbf{U}\\mathbf{V}$, so the matrix $2(\\mathbf{X} - \\mathbf{U}\\mathbf{V})$ is $n \\times m$. On the other hand $\\mathbf{U}$ is $n \\times r$, and we cannot multiply a $n \\times m$ and a $n \\times r$ matrix since the dimensions do not match! We want to get $\\frac{d}{d\\mathbf{V}}$, which is the same shape as $\\mathbf{V}$, which is $r \\times m$."
    },
    {
      "chunk_id": "ed870ff7a43b_4",
      "chapter": "multivariable-calculus",
      "heading": "A Little Matrix Calculus",
      "text": "On the other hand $\\mathbf{U}$ is $n \\times r$, and we cannot multiply a $n \\times m$ and a $n \\times r$ matrix since the dimensions do not match! We want to get $\\frac{d}{d\\mathbf{V}}$, which is the same shape as $\\mathbf{V}$, which is $r \\times m$. So somehow we need to take a $n \\times m$ matrix and a $n \\times r$ matrix, multiply them together (perhaps with some transposes) to get a $r \\times m$. We can do this by multiplying $U^\\top$ by $(\\mathbf{X} - \\mathbf{U}\\mathbf{V})$. Thus, we can guess the solution to :eqref:`eq_mat_goal_2` is\n\n$$\n\\frac{d}{d\\mathbf{V}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= -2\\mathbf{U}^\\top(\\mathbf{X} - \\mathbf{U}\\mathbf{V}). $$\n\nTo show that this works, we would be remiss to not provide a detailed computation. If we already believe that this rule-of-thumb works, feel free to skip past this derivation. To compute\n\n$$\n\\frac{d}{d\\mathbf{V}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^2,\n$$\n\nwe must find for every $a$, and $b$\n\n$$\n\\frac{d}{dv_{ab}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= \\frac{d}{dv_{ab}} \\sum_{i, j}\\left(x_{ij} - \\sum_k u_{ik}v_{kj}\\right)^2. $$\n\nRecalling that all entries of $\\mathbf{X}$ and $\\mathbf{U}$ are constants as far as $\\frac{d}{dv_{ab}}$ is concerned, we may push the derivative inside the sum, and apply the chain rule to the square to get\n\n$$\n\\frac{d}{dv_{ab}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= \\sum_{i, j}2\\left(x_{ij} - \\sum_k u_{ik}v_{kj}\\right)\\left(-\\sum_k u_{ik}\\frac{dv_{kj}}{dv_{ab}} \\right). $$\n\nAs in the previous derivation, we may note that $\\frac{dv_{kj}}{dv_{ab}}$ is only non-zero if the $k=a$ and $j=b$. If either of those conditions do not hold, the term in the sum is zero, and we may freely discard it. We see that\n\n$$\n\\frac{d}{dv_{ab}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= -2\\sum_{i}\\left(x_{ib} - \\sum_k u_{ik}v_{kb}\\right)u_{ia}. $$\n\nAn important subtlety here is that the requirement that $k=a$ does not occur inside the inner sum since that $k$ is a dummy variable which we are summing over inside the inner term."
    },
    {
      "chunk_id": "ed870ff7a43b_5",
      "chapter": "multivariable-calculus",
      "heading": "A Little Matrix Calculus",
      "text": "$$\n\nAn important subtlety here is that the requirement that $k=a$ does not occur inside the inner sum since that $k$ is a dummy variable which we are summing over inside the inner term. For a notationally cleaner example, consider why\n\n$$\n\\frac{d}{dx_1} \\left(\\sum_i x_i \\right)^{2}= 2\\left(\\sum_i x_i \\right). $$\n\nFrom this point, we may start identifying components of the sum. First,\n\n$$\n\\sum_k u_{ik}v_{kb} = [\\mathbf{U}\\mathbf{V}]_{ib}. $$\n\nSo the entire expression in the inside of the sum is\n\n$$\nx_{ib} - \\sum_k u_{ik}v_{kb} = [\\mathbf{X}-\\mathbf{U}\\mathbf{V}]_{ib}. $$\n\nThis means we may now write our derivative as\n\n$$\n\\frac{d}{dv_{ab}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= -2\\sum_{i}[\\mathbf{X}-\\mathbf{U}\\mathbf{V}]_{ib}u_{ia}. $$\n\nWe want this to look like the $a, b$ element of a matrix so we can use the technique as in the previous example to arrive at a matrix expression, which means that we need to exchange the order of the indices on $u_{ia}$. If we notice that $u_{ia} = [\\mathbf{U}^\\top]_{ai}$, we can then write\n\n$$\n\\frac{d}{dv_{ab}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= -2\\sum_{i} [\\mathbf{U}^\\top]_{ai}[\\mathbf{X}-\\mathbf{U}\\mathbf{V}]_{ib}. $$\n\nThis is a matrix product, and thus we can conclude that\n\n$$\n\\frac{d}{dv_{ab}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= -2[\\mathbf{U}^\\top(\\mathbf{X}-\\mathbf{U}\\mathbf{V})]_{ab}. $$\n\nand thus we may write the solution to :eqref:`eq_mat_goal_2`\n\n$$\n\\frac{d}{d\\mathbf{V}} \\|\\mathbf{X} - \\mathbf{U}\\mathbf{V}\\|_2^{2}= -2\\mathbf{U}^\\top(\\mathbf{X} - \\mathbf{U}\\mathbf{V}). $$\n\nThis matches the solution we guessed above! It is reasonable to ask at this point, \"Why can I not just write down matrix versions of all the calculus rules I have learned? It is clear this is still mechanical. Why do we not just get it over with!\"  And indeed there are such rules and :cite:`Petersen.Pedersen.ea.2008` provides an excellent summary."
    },
    {
      "chunk_id": "ed870ff7a43b_6",
      "chapter": "multivariable-calculus",
      "heading": "A Little Matrix Calculus",
      "text": "It is clear this is still mechanical. Why do we not just get it over with!\"  And indeed there are such rules and :cite:`Petersen.Pedersen.ea.2008` provides an excellent summary. However, due to the plethora of ways matrix operations can be combined compared to single values, there are many more matrix derivative rules than single variable ones. It is often the case that it is best to work with the indices, or leave it up to automatic differentiation when appropriate."
    },
    {
      "chunk_id": "2c55a0b3d936_0",
      "chapter": "multivariable-calculus",
      "heading": "Summary",
      "text": "* In higher dimensions, we can define gradients which serve the same purpose as derivatives in one dimension.  These allow us to see how a multi-variable function changes when we make an arbitrary small change to the inputs.\n* The backpropagation algorithm can be seen to be a method of organizing the multi-variable chain rule to allow for the efficient computation of many partial derivatives.\n* Matrix calculus allows us to write the derivatives of matrix expressions in concise ways."
    },
    {
      "chunk_id": "2660626af240_0",
      "chapter": "multivariable-calculus",
      "heading": "Exercises",
      "text": "1. Given a column vector $\\boldsymbol{\\beta}$, compute the derivatives of both $f(\\mathbf{x}) = \\boldsymbol{\\beta}^\\top\\mathbf{x}$ and $g(\\mathbf{x}) = \\mathbf{x}^\\top\\boldsymbol{\\beta}$.  Why do you get the same answer?\n2. Let $\\mathbf{v}$ be an $n$ dimension vector. What is $\\frac{\\partial}{\\partial\\mathbf{v}}\\|\\mathbf{v}\\|_2$?\n3. Let $L(x, y) = \\log(e^x + e^y)$.  Compute the gradient.  What is the sum of the components of the gradient?\n4. Let $f(x, y) = x^2y + xy^2$. Show that the only critical point is $(0,0)$. By considering $f(x, x)$, determine if $(0,0)$ is a maximum, minimum, or neither.\n5. Suppose that we are minimizing a function $f(\\mathbf{x}) = g(\\mathbf{x}) + h(\\mathbf{x})$.  How can we geometrically interpret the condition of $\\nabla f = 0$ in terms of $g$ and $h$?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/413)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1090)\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1091)\n:end_tab:"
    },
    {
      "chunk_id": "f360e7d5fb5e_0",
      "chapter": "naive-bayes",
      "heading": "naive-bayes",
      "text": "# Naive Bayes\n:label:`sec_naive_bayes`\n\nThroughout the previous sections, we learned about the theory of probability and random variables.  To put this theory to work, let's introduce the *naive Bayes* classifier.  This uses nothing but probabilistic fundamentals to allow us to perform classification of digits.\n\nLearning is all about making assumptions. If we want to classify a new data example that we have never seen before we have to make some assumptions about which data examples are similar to each other. The naive Bayes classifier, a popular and remarkably clear algorithm, assumes all features are independent from each other to simplify the computation. In this section, we will apply this model to recognize characters in images.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import gluon, np, npx\nnpx.set_np()\nd2l.use_svg_display()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport math\nimport torch\nimport torchvision\nd2l.use_svg_display()\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport math\nimport tensorflow as tf\nd2l.use_svg_display()\n```"
    },
    {
      "chunk_id": "5b96d1f82ef7_0",
      "chapter": "naive-bayes",
      "heading": "Optical Character Recognition",
      "text": "MNIST :cite:`LeCun.Bottou.Bengio.ea.1998` is one of widely used datasets. It contains 60,000 images for training and 10,000 images for validation. Each image contains a handwritten digit from 0 to 9. The task is classifying each image into the corresponding digit. Gluon provides a `MNIST` class in the `data.vision` module to\nautomatically retrieve the dataset from the Internet. Subsequently, Gluon will use the already-downloaded local copy. We specify whether we are requesting the training set or the test set\nby setting the value of the parameter `train` to `True` or `False`, respectively. Each image is a grayscale image with both width and height of $28$ with shape ($28$,$28$,$1$). We use a customized transformation to remove the last channel dimension. In addition, the dataset represents each pixel by an unsigned $8$-bit integer. We quantize them into binary features to simplify the problem. ```{.python .input}\n#@tab mxnet\ndef transform(data, label):\n    return np.floor(data.astype('float32') / 128).squeeze(axis=-1), label\n\nmnist_train = gluon.data.vision.MNIST(train=True, transform=transform)\nmnist_test = gluon.data.vision.MNIST(train=False, transform=transform)\n```\n\n```{.python .input}\n#@tab pytorch\ndata_transform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    lambda x: torch.floor(x * 255 / 128).squeeze(dim=0)\n])\n\nmnist_train = torchvision.datasets.MNIST(\n    root='./temp', train=True, transform=data_transform, download=True)\nmnist_test = torchvision.datasets.MNIST(\n    root='./temp', train=False, transform=data_transform, download=True)\n```\n\n```{.python .input}\n#@tab tensorflow\n((train_images, train_labels), (\n    test_images, test_labels)) = tf.keras.datasets.mnist.load_data()\n\n# Original pixel values of MNIST range from 0-255 (as the digits are stored as\n# uint8). For this section, pixel values that are greater than 128 (in the\n# original image) are converted to 1 and values that are less than 128 are\n# converted to 0."
    },
    {
      "chunk_id": "5b96d1f82ef7_1",
      "chapter": "naive-bayes",
      "heading": "Optical Character Recognition",
      "text": "For this section, pixel values that are greater than 128 (in the\n# original image) are converted to 1 and values that are less than 128 are\n# converted to 0. See section 18.9.2 and 18.9.3 for why\ntrain_images = tf.floor(tf.constant(train_images / 128, dtype = tf.float32))\ntest_images = tf.floor(tf.constant(test_images / 128, dtype = tf.float32))\n\ntrain_labels = tf.constant(train_labels, dtype = tf.int32)\ntest_labels = tf.constant(test_labels, dtype = tf.int32)\n```\n\nWe can access a particular example, which contains the image and the corresponding label. ```{.python .input}\n#@tab mxnet\nimage, label = mnist_train[2]\nimage.shape, label\n```\n\n```{.python .input}\n#@tab pytorch\nimage, label = mnist_train[2]\nimage.shape, label\n```\n\n```{.python .input}\n#@tab tensorflow\nimage, label = train_images[2], train_labels[2]\nimage.shape, label.numpy()\n```\n\nOur example, stored here in the variable `image`, corresponds to an image with a height and width of $28$ pixels. ```{.python .input}\n#@tab all\nimage.shape, image.dtype\n```\n\nOur code stores the label of each image as a scalar. Its type is a $32$-bit integer. ```{.python .input}\n#@tab mxnet\nlabel, type(label), label.dtype\n```\n\n```{.python .input}\n#@tab pytorch\nlabel, type(label)\n```\n\n```{.python .input}\n#@tab tensorflow\nlabel.numpy(), label.dtype\n```\n\nWe can also access multiple examples at the same time. ```{.python .input}\n#@tab mxnet\nimages, labels = mnist_train[10:38]\nimages.shape, labels.shape\n```\n\n```{.python .input}\n#@tab pytorch\nimages = torch.stack([mnist_train[i][0] for i in range(10, 38)], dim=0)\nlabels = torch.tensor([mnist_train[i][1] for i in range(10, 38)])\nimages.shape, labels.shape\n```\n\n```{.python .input}\n#@tab tensorflow\nimages = tf.stack([train_images[i] for i in range(10, 38)], axis=0)\nlabels = tf.constant([train_labels[i].numpy() for i in range(10, 38)])\nimages.shape, labels.shape\n```\n\nLet's visualize these examples. ```{.python .input}\n#@tab all\nd2l.show_images(images, 2, 9);\n```"
    },
    {
      "chunk_id": "93094712a04f_0",
      "chapter": "naive-bayes",
      "heading": "The Probabilistic Model for Classification",
      "text": "In a classification task, we map an example into a category. Here an example is a grayscale $28\\times 28$ image, and a category is a digit. (Refer to :numref:`sec_softmax` for a more detailed explanation.)\nOne natural way to express the classification task is via the probabilistic question: what is the most likely label given the features (i.e., image pixels)? Denote by $\\mathbf x\\in\\mathbb R^d$ the features of the example and $y\\in\\mathbb R$ the label. Here features are image pixels, where we can reshape a $2$-dimensional image to a vector so that $d=28^2=784$, and labels are digits.\nThe probability of the label given the features is $p(y  \\mid  \\mathbf{x})$. If we are able to compute these probabilities, which are $p(y  \\mid  \\mathbf{x})$ for $y=0, \\ldots,9$ in our example, then the classifier will output the prediction $\\hat{y}$ given by the expression:\n\n$$\\hat{y} = \\mathrm{argmax} \\> p(y  \\mid  \\mathbf{x}).$$\n\nUnfortunately, this requires that we estimate $p(y  \\mid  \\mathbf{x})$ for every value of $\\mathbf{x} = x_1, ..., x_d$. Imagine that each feature could take one of $2$ values. For example, the feature $x_1 = 1$ might signify that the word apple appears in a given document and $x_1 = 0$ would signify that it does not. If we had $30$ such binary features, that would mean that we need to be prepared to classify any of $2^{30}$ (over 1 billion!) possible values of the input vector $\\mathbf{x}$.\n\nMoreover, where is the learning? If we need to see every single possible example in order to predict the corresponding label then we are not really learning a pattern but just memorizing the dataset."
    },
    {
      "chunk_id": "36b5242ed76f_0",
      "chapter": "naive-bayes",
      "heading": "The Naive Bayes Classifier",
      "text": "Fortunately, by making some assumptions about conditional independence, we can introduce some inductive bias and build a model capable of generalizing from a comparatively modest selection of training examples. To begin, let's use Bayes theorem, to express the classifier as\n\n$$\\hat{y} = \\mathrm{argmax}_y \\> p(y  \\mid  \\mathbf{x}) = \\mathrm{argmax}_y \\> \\frac{p( \\mathbf{x}  \\mid  y) p(y)}{p(\\mathbf{x})}.$$\n\nNote that the denominator is the normalizing term $p(\\mathbf{x})$ which does not depend on the value of the label $y$. As a result, we only need to worry about comparing the numerator across different values of $y$. Even if calculating the denominator turned out to be intractable, we could get away with ignoring it, so long as we could evaluate the numerator. Fortunately, even if we wanted to recover the normalizing constant, we could. We can always recover the normalization term since $\\sum_y p(y  \\mid  \\mathbf{x}) = 1$. Now, let's focus on $p( \\mathbf{x}  \\mid  y)$. Using the chain rule of probability, we can express the term $p( \\mathbf{x}  \\mid  y)$ as\n\n$$p(x_1  \\mid y) \\cdot p(x_2  \\mid  x_1, y) \\cdot ... \\cdot p( x_d  \\mid  x_1, ..., x_{d-1}, y).$$\n\nBy itself, this expression does not get us any further. We still must estimate roughly $2^d$ parameters. However, if we assume that *the features are conditionally independent of each other, given the label*, then suddenly we are in much better shape, as this term simplifies to $\\prod_i p(x_i  \\mid  y)$, giving us the predictor\n\n$$\\hat{y} = \\mathrm{argmax}_y \\> \\prod_{i=1}^d p(x_i  \\mid  y) p(y).$$\n\nIf we can estimate $p(x_i=1  \\mid  y)$ for every $i$ and $y$, and save its value in $P_{xy}[i, y]$, here $P_{xy}$ is a $d\\times n$ matrix with $n$ being the number of classes and $y\\in\\{1, \\ldots, n\\}$, then we can also use this to estimate $p(x_i = 0 \\mid y)$, i.e.,\n\n$$\np(x_i = t_i \\mid y) =\n\\begin{cases}\n    P_{xy}[i, y] & \\textrm{for } t_i=1 ;\\\\\n    1 - P_{xy}[i, y] & \\textrm{for } t_i = 0 ."
    },
    {
      "chunk_id": "36b5242ed76f_1",
      "chapter": "naive-bayes",
      "heading": "The Naive Bayes Classifier",
      "text": "\\end{cases}\n$$\n\nIn addition, we estimate $p(y)$ for every $y$ and save it in $P_y[y]$, with $P_y$ a $n$-length vector. Then, for any new example $\\mathbf t = (t_1, t_2, \\ldots, t_d)$, we could compute\n\n$$\\begin{aligned}\\hat{y} &= \\mathrm{argmax}_ y \\ p(y)\\prod_{i=1}^d   p(x_t = t_i \\mid y) \\\\ &= \\mathrm{argmax}_y \\ P_y[y]\\prod_{i=1}^d \\ P_{xy}[i, y]^{t_i}\\, \\left(1 - P_{xy}[i, y]\\right)^{1-t_i}\\end{aligned}$$\n:eqlabel:`eq_naive_bayes_estimation`\n\nfor any $y$. So our assumption of conditional independence has taken the complexity of our model from an exponential dependence on the number of features $\\mathcal{O}(2^dn)$ to a linear dependence, which is $\\mathcal{O}(dn)$."
    },
    {
      "chunk_id": "3390b60d229f_0",
      "chapter": "naive-bayes",
      "heading": "Training",
      "text": "The problem now is that we do not know $P_{xy}$ and $P_y$. So we need to estimate their values given some training data first. This is *training* the model. Estimating $P_y$ is not too hard. Since we are only dealing with $10$ classes, we may count the number of occurrences $n_y$ for each of the digits and divide it by the total amount of data $n$. For instance, if digit 8 occurs $n_8 = 5,800$ times and we have a total of $n = 60,000$ images, the probability estimate is $p(y=8) = 0.0967$. ```{.python .input}\n#@tab mxnet\nX, Y = mnist_train[:]  # All training examples\n\nn_y = np.zeros((10))\nfor y in range(10):\n    n_y[y] = (Y == y).sum()\nP_y = n_y / n_y.sum()\nP_y\n```\n\n```{.python .input}\n#@tab pytorch\nX = torch.stack([mnist_train[i][0] for i in range(len(mnist_train))], dim=0)\nY = torch.tensor([mnist_train[i][1] for i in range(len(mnist_train))])\n\nn_y = torch.zeros(10)\nfor y in range(10):\n    n_y[y] = (Y == y).sum()\nP_y = n_y / n_y.sum()\nP_y\n```\n\n```{.python .input}\n#@tab tensorflow\nX = train_images\nY = train_labels\n\nn_y = tf.Variable(tf.zeros(10))\nfor y in range(10):\n    n_y[y].assign(tf.reduce_sum(tf.cast(Y == y, tf.float32)))\nP_y = n_y / tf.reduce_sum(n_y)\nP_y\n```\n\nNow on to slightly more difficult things $P_{xy}$. Since we picked black and white images, $p(x_i  \\mid  y)$ denotes the probability that pixel $i$ is switched on for class $y$. Just like before we can go and count the number of times $n_{iy}$ such that an event occurs and divide it by the total number of occurrences of $y$, i.e., $n_y$. But there is something slightly troubling: certain pixels may never be black (e.g., for well cropped images the corner pixels might always be white). A convenient way for statisticians to deal with this problem is to add pseudo counts to all occurrences. Hence, rather than $n_{iy}$ we use $n_{iy}+1$ and instead of $n_y$ we use $n_{y}+2$ (since there are two possible values pixel $i$ can take - it can either be black or white). This is also called *Laplace Smoothing*."
    },
    {
      "chunk_id": "3390b60d229f_1",
      "chapter": "naive-bayes",
      "heading": "Training",
      "text": "Hence, rather than $n_{iy}$ we use $n_{iy}+1$ and instead of $n_y$ we use $n_{y}+2$ (since there are two possible values pixel $i$ can take - it can either be black or white). This is also called *Laplace Smoothing*. It may seem ad-hoc, however it can be motivated from a Bayesian point-of-view by a Beta-binomial model. ```{.python .input}\n#@tab mxnet\nn_x = np.zeros((10, 28, 28))\nfor y in range(10):\n    n_x[y] = np.array(X.asnumpy()[Y.asnumpy() == y].sum(axis=0))\nP_xy = (n_x + 1) / (n_y + 2).reshape(10, 1, 1)\n\nd2l.show_images(P_xy, 2, 5);\n```\n\n```{.python .input}\n#@tab pytorch\nn_x = torch.zeros((10, 28, 28))\nfor y in range(10):\n    n_x[y] = torch.tensor(X.numpy()[Y.numpy() == y].sum(axis=0))\nP_xy = (n_x + 1) / (n_y + 2).reshape(10, 1, 1)\n\nd2l.show_images(P_xy, 2, 5);\n```\n\n```{.python .input}\n#@tab tensorflow\nn_x = tf.Variable(tf.zeros((10, 28, 28)))\nfor y in range(10):\n    n_x[y].assign(tf.cast(tf.reduce_sum(\n        X.numpy()[Y.numpy() == y], axis=0), tf.float32))\nP_xy = (n_x + 1) / tf.reshape((n_y + 2), (10, 1, 1))\n\nd2l.show_images(P_xy, 2, 5);\n```\n\nBy visualizing these $10\\times 28\\times 28$ probabilities (for each pixel for each class) we could get some mean looking digits. Now we can use :eqref:`eq_naive_bayes_estimation` to predict a new image. Given $\\mathbf x$, the following functions computes $p(\\mathbf x \\mid y)p(y)$ for every $y$."
    },
    {
      "chunk_id": "3390b60d229f_2",
      "chapter": "naive-bayes",
      "heading": "Training",
      "text": "Now we can use :eqref:`eq_naive_bayes_estimation` to predict a new image. Given $\\mathbf x$, the following functions computes $p(\\mathbf x \\mid y)p(y)$ for every $y$. ```{.python .input}\n#@tab mxnet\ndef bayes_pred(x):\n    x = np.expand_dims(x, axis=0)  # (28, 28) -> (1, 28, 28)\n    p_xy = P_xy * x + (1 - P_xy)*(1 - x)\n    p_xy = p_xy.reshape(10, -1).prod(axis=1)  # p(x|y)\n    return np.array(p_xy) * P_y\n\nimage, label = mnist_test[0]\nbayes_pred(image)\n```\n\n```{.python .input}\n#@tab pytorch\ndef bayes_pred(x):\n    x = x.unsqueeze(0)  # (28, 28) -> (1, 28, 28)\n    p_xy = P_xy * x + (1 - P_xy)*(1 - x)\n    p_xy = p_xy.reshape(10, -1).prod(dim=1)  # p(x|y)\n    return p_xy * P_y\n\nimage, label = mnist_test[0]\nbayes_pred(image)\n```\n\n```{.python .input}\n#@tab tensorflow\ndef bayes_pred(x):\n    x = tf.expand_dims(x, axis=0)  # (28, 28) -> (1, 28, 28)\n    p_xy = P_xy * x + (1 - P_xy)*(1 - x)\n    p_xy = tf.math.reduce_prod(tf.reshape(p_xy, (10, -1)), axis=1)  # p(x|y)\n    return p_xy * P_y\n\nimage, label = train_images[0], train_labels[0]\nbayes_pred(image)\n```\n\nThis went horribly wrong! To find out why, let's look at the per pixel probabilities. They are typically numbers between $0.001$ and $1$. We are multiplying $784$ of them. At this point it is worth mentioning that we are calculating these numbers on a computer, hence with a fixed range for the exponent. What happens is that we experience *numerical underflow*, i.e., multiplying all the small numbers leads to something even smaller until it is rounded down to zero. We discussed this as a theoretical issue in :numref:`sec_maximum_likelihood`, but we see the phenomena clearly here in practice. As discussed in that section, we fix this by use the fact that $\\log a b = \\log a + \\log b$, i.e., we switch to summing logarithms. Even if both $a$ and $b$ are small numbers, the logarithm values should be in a proper range."
    },
    {
      "chunk_id": "3390b60d229f_3",
      "chapter": "naive-bayes",
      "heading": "Training",
      "text": "As discussed in that section, we fix this by use the fact that $\\log a b = \\log a + \\log b$, i.e., we switch to summing logarithms. Even if both $a$ and $b$ are small numbers, the logarithm values should be in a proper range. ```{.python .input}\n#@tab mxnet\na = 0.1\nprint('underflow:', a**784)\nprint('logarithm is normal:', 784*math.log(a))\n```\n\n```{.python .input}\n#@tab pytorch\na = 0.1\nprint('underflow:', a**784)\nprint('logarithm is normal:', 784*math.log(a))\n```\n\n```{.python .input}\n#@tab tensorflow\na = 0.1\nprint('underflow:', a**784)\nprint('logarithm is normal:', 784*tf.math.log(a).numpy())\n```\n\nSince the logarithm is an increasing function, we can rewrite :eqref:`eq_naive_bayes_estimation` as\n\n$$ \\hat{y} = \\mathrm{argmax}_y \\ \\log P_y[y] + \\sum_{i=1}^d \\Big[t_i\\log P_{xy}[x_i, y] + (1-t_i) \\log (1 - P_{xy}[x_i, y]) \\Big].$$\n\nWe can implement the following stable version:\n\n```{.python .input}\n#@tab mxnet\nlog_P_xy = np.log(P_xy)\nlog_P_xy_neg = np.log(1 - P_xy)\nlog_P_y = np.log(P_y)\n\ndef bayes_pred_stable(x):\n    x = np.expand_dims(x, axis=0)  # (28, 28) -> (1, 28, 28)\n    p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)\n    p_xy = p_xy.reshape(10, -1).sum(axis=1)  # p(x|y)\n    return p_xy + log_P_y\n\npy = bayes_pred_stable(image)\npy\n```\n\n```{.python .input}\n#@tab pytorch\nlog_P_xy = torch.log(P_xy)\nlog_P_xy_neg = torch.log(1 - P_xy)\nlog_P_y = torch.log(P_y)\n\ndef bayes_pred_stable(x):\n    x = x.unsqueeze(0)  # (28, 28) -> (1, 28, 28)\n    p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)\n    p_xy = p_xy.reshape(10, -1).sum(axis=1)  # p(x|y)\n    return p_xy + log_P_y\n\npy = bayes_pred_stable(image)\npy\n```\n\n```{.python .input}\n#@tab tensorflow\nlog_P_xy = tf.math.log(P_xy)\nlog_P_xy_neg = tf.math.log(1 - P_xy)\nlog_P_y = tf.math.log(P_y)\n\ndef bayes_pred_stable(x):\n    x = tf.expand_dims(x, axis=0)  # (28, 28) -> (1, 28, 28)\n    p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)\n    p_xy = tf.math.reduce_sum(tf.reshape(p_xy, (10, -1)), axis=1)  # p(x|y)\n    return p_xy + log_P_y\n\npy = bayes_pred_stable(image)\npy\n```\n\nWe may now check if the prediction is correct."
    },
    {
      "chunk_id": "3390b60d229f_4",
      "chapter": "naive-bayes",
      "heading": "Training",
      "text": "```{.python .input}\n#@tab mxnet\n# Convert label which is a scalar tensor of int32 dtype to a Python scalar\n# integer for comparison\npy.argmax(axis=0) == int(label)\n```\n\n```{.python .input}\n#@tab pytorch\npy.argmax(dim=0) == label\n```\n\n```{.python .input}\n#@tab tensorflow\ntf.argmax(py, axis=0, output_type = tf.int32) == label\n```\n\nIf we now predict a few validation examples, we can see the Bayes\nclassifier works pretty well. ```{.python .input}\n#@tab mxnet\ndef predict(X):\n    return [bayes_pred_stable(x).argmax(axis=0).astype(np.int32) for x in X]\n\nX, y = mnist_test[:18]\npreds = predict(X)\nd2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);\n```\n\n```{.python .input}\n#@tab pytorch\ndef predict(X):\n    return [bayes_pred_stable(x).argmax(dim=0).type(torch.int32).item()\n            for x in X]\n\nX = torch.stack([mnist_test[i][0] for i in range(18)], dim=0)\ny = torch.tensor([mnist_test[i][1] for i in range(18)])\npreds = predict(X)\nd2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);\n```\n\n```{.python .input}\n#@tab tensorflow\ndef predict(X):\n    return [tf.argmax(\n        bayes_pred_stable(x), axis=0, output_type = tf.int32).numpy()\n            for x in X]\n\nX = tf.stack([train_images[i] for i in range(10, 38)], axis=0)\ny = tf.constant([train_labels[i].numpy() for i in range(10, 38)])\npreds = predict(X)\nd2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);\n```\n\nFinally, let's compute the overall accuracy of the classifier."
    },
    {
      "chunk_id": "3390b60d229f_5",
      "chapter": "naive-bayes",
      "heading": "Training",
      "text": "```{.python .input}\n#@tab mxnet\nX, y = mnist_test[:]\npreds = np.array(predict(X), dtype=np.int32)\nfloat((preds == y).sum()) / len(y)  # Validation accuracy\n```\n\n```{.python .input}\n#@tab pytorch\nX = torch.stack([mnist_test[i][0] for i in range(len(mnist_test))], dim=0)\ny = torch.tensor([mnist_test[i][1] for i in range(len(mnist_test))])\npreds = torch.tensor(predict(X), dtype=torch.int32)\nfloat((preds == y).sum()) / len(y)  # Validation accuracy\n```\n\n```{.python .input}\n#@tab tensorflow\nX = test_images\ny = test_labels\npreds = tf.constant(predict(X), dtype=tf.int32)\n# Validation accuracy\ntf.reduce_sum(tf.cast(preds == y, tf.float32)).numpy() / len(y)\n```\n\nModern deep networks achieve error rates of less than $0.01$. The relatively poor performance is due to the incorrect statistical assumptions that we made in our model: we assumed that each and every pixel are *independently* generated, depending only on the label. This is clearly not how humans write digits, and this wrong assumption led to the downfall of our overly naive (Bayes) classifier."
    },
    {
      "chunk_id": "8f7d56fe996a_0",
      "chapter": "naive-bayes",
      "heading": "Summary",
      "text": "* Using Bayes' rule, a classifier can be made by assuming all observed features are independent.\n* This classifier can be trained on a dataset by counting the number of occurrences of combinations of labels and pixel values.\n* This classifier was the gold standard for decades for tasks such as spam detection."
    },
    {
      "chunk_id": "d00665d4cec4_0",
      "chapter": "naive-bayes",
      "heading": "Exercises",
      "text": "1. Consider the dataset $[[0,0], [0,1], [1,0], [1,1]]$ with labels given by the XOR of the two elements $[0,1,1,0]$.  What are the probabilities for a Naive Bayes classifier built on this dataset.  Does it successfully classify our points?  If not, what assumptions are violated?\n1. Suppose that we did not use Laplace smoothing when estimating probabilities and a data example arrived at testing time which contained a value never observed in training.  What would the model output?\n1. The naive Bayes classifier is a specific example of a Bayesian network, where the dependence of random variables are encoded with a graph structure.  While the full theory is beyond the scope of this section (see :citet:`Koller.Friedman.2009` for full details), explain why allowing explicit dependence between the two input variables in the XOR model allows for the creation of a successful classifier.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/418)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1100)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1101)\n:end_tab:"
    },
    {
      "chunk_id": "1f0cfa60a978_0",
      "chapter": "random-variables",
      "heading": "random-variables",
      "text": "# Random Variables\n:label:`sec_random_variables`\n\nIn :numref:`sec_prob` we saw the basics of how to work with discrete random variables, which in our case refer to those random variables which take either a finite set of possible values, or the integers.  In this section, we develop the theory of *continuous random variables*, which are random variables which  can take on any real value."
    },
    {
      "chunk_id": "419c536490b9_0",
      "chapter": "random-variables",
      "heading": "Continuous Random Variables",
      "text": "Continuous random variables are a significantly more subtle topic than discrete random variables.  A fair analogy to make is that the technical jump is comparable to the jump between adding lists of numbers and integrating functions.  As such, we will need to take some time to develop the theory."
    },
    {
      "chunk_id": "3a69c08c9e32_0",
      "chapter": "random-variables",
      "heading": "From Discrete to Continuous",
      "text": "To understand the additional technical challenges encountered when working with continuous random variables, let's perform a thought experiment. Suppose that we are throwing a dart at the dart board, and we want to know the probability that it hits exactly $2 \\textrm{cm}$ from the center of the board. To start with, we imagine measuring a single digit of accuracy, that is to say with bins for $0 \\textrm{cm}$, $1 \\textrm{cm}$, $2 \\textrm{cm}$, and so on. We throw say $100$ darts at the dart board, and if $20$ of them fall into the bin for $2\\textrm{cm}$ we conclude that $20\\%$ of the darts we throw hit the board $2 \\textrm{cm}$ away from the center. However, when we look closer, this does not match our question! We wanted exact equality, whereas these bins hold all that fell between say $1.5\\textrm{cm}$ and $2.5\\textrm{cm}$. Undeterred, we continue further. We measure even more precisely, say $1.9\\textrm{cm}$, $2.0\\textrm{cm}$, $2.1\\textrm{cm}$, and now see that perhaps $3$ of the $100$ darts hit the board in the $2.0\\textrm{cm}$ bucket. Thus we conclude the probability is $3\\%$. However, this does not solve anything! We have just pushed the issue down one digit further. Let's abstract a bit. Imagine we know the probability that the first $k$ digits match with $2.00000\\ldots$ and we want to know the probability it matches for the first $k+1$ digits. It is fairly reasonable to assume that the ${k+1}^{\\textrm{th}}$ digit is essentially a random choice from the set $\\{0, 1, 2, \\ldots, 9\\}$. At least, we cannot conceive of a physically meaningful process which would force the number of micrometers away form the center to prefer to end in a $7$ vs a $3$. What this means is that in essence each additional digit of accuracy we require should decrease probability of matching by a factor of $10$. Or put another way, we would expect that\n\n$$\nP(\\textrm{distance is}\\; 2.00\\ldots, \\;\\textrm{to}\\; k \\;\\textrm{digits} ) \\approx p\\cdot10^{-k}."
    },
    {
      "chunk_id": "3a69c08c9e32_1",
      "chapter": "random-variables",
      "heading": "From Discrete to Continuous",
      "text": "Or put another way, we would expect that\n\n$$\nP(\\textrm{distance is}\\; 2.00\\ldots, \\;\\textrm{to}\\; k \\;\\textrm{digits} ) \\approx p\\cdot10^{-k}. $$\n\nThe value $p$ essentially encodes what happens with the first few digits, and the $10^{-k}$ handles the rest. Notice that if we know the position accurate to $k=4$ digits after the decimal, that means we know the value falls within the interval say $[1.99995,2.00005]$ which is an interval of length $2.00005-1.99995 = 10^{-4}$. Thus, if we call the length of this interval $\\epsilon$, we can say\n\n$$\nP(\\textrm{distance is in an}\\; \\epsilon\\textrm{-sized interval around}\\; 2 ) \\approx \\epsilon \\cdot p. $$\n\nLet's take this one final step further. We have been thinking about the point $2$ the entire time, but never thinking about other points. Nothing is different there fundamentally, but it is the case that the value $p$ will likely be different. We would at least hope that a dart thrower was more likely to hit a point near the center, like $2\\textrm{cm}$ rather than $20\\textrm{cm}$. Thus, the value $p$ is not fixed, but rather should depend on the point $x$. This tells us that we should expect\n\n$$P(\\textrm{distance is in an}\\; \\epsilon \\textrm{-sized interval around}\\; x ) \\approx \\epsilon \\cdot p(x).$$\n:eqlabel:`eq_pdf_deriv`\n\nIndeed, :eqref:`eq_pdf_deriv` precisely defines the *probability density function*. It is a function $p(x)$ which encodes the relative probability of hitting near one point vs. another. Let's visualize what such a function might look like."
    },
    {
      "chunk_id": "3a69c08c9e32_2",
      "chapter": "random-variables",
      "heading": "From Discrete to Continuous",
      "text": "It is a function $p(x)$ which encodes the relative probability of hitting near one point vs. another. Let's visualize what such a function might look like. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom IPython import display\nfrom mxnet import np, npx\nnpx.set_np()\n\n# Plot the probability density function for some random variable\nx = np.arange(-5, 5, 0.01)\np = 0.2*np.exp(-(x - 3)**2 / 2)/np.sqrt(2 * np.pi) + \\\n    0.8*np.exp(-(x + 1)**2 / 2)/np.sqrt(2 * np.pi)\n\nd2l.plot(x, p, 'x', 'Density')\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom IPython import display\nimport torch\ntorch.pi = torch.acos(torch.zeros(1)).item() * 2  # Define pi in torch\n\n# Plot the probability density function for some random variable\nx = torch.arange(-5, 5, 0.01)\np = 0.2*torch.exp(-(x - 3)**2 / 2)/torch.sqrt(2 * torch.tensor(torch.pi)) + \\\n    0.8*torch.exp(-(x + 1)**2 / 2)/torch.sqrt(2 * torch.tensor(torch.pi))\n\nd2l.plot(x, p, 'x', 'Density')\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom IPython import display\nimport tensorflow as tf\ntf.pi = tf.acos(tf.zeros(1)).numpy() * 2  # Define pi in TensorFlow\n\n# Plot the probability density function for some random variable\nx = tf.range(-5, 5, 0.01)\np = 0.2*tf.exp(-(x - 3)**2 / 2)/tf.sqrt(2 * tf.constant(tf.pi)) + \\\n    0.8*tf.exp(-(x + 1)**2 / 2)/tf.sqrt(2 * tf.constant(tf.pi))\n\nd2l.plot(x, p, 'x', 'Density')\n```\n\nThe locations where the function value is large indicates regions where we are more likely to find the random value. The low portions are areas where we are unlikely to find the random value."
    },
    {
      "chunk_id": "af5a1e21f602_0",
      "chapter": "random-variables",
      "heading": "Probability Density Functions",
      "text": "Let's now investigate this further. We have already seen what a probability density function is intuitively for a random variable $X$, namely the density function is a function $p(x)$ so that\n\n$$P(X \\; \\textrm{is in an}\\; \\epsilon \\textrm{-sized interval around}\\; x ) \\approx \\epsilon \\cdot p(x).$$\n:eqlabel:`eq_pdf_def`\n\nBut what does this imply for the properties of $p(x)$? First, probabilities are never negative, thus we should expect that $p(x) \\ge 0$ as well. Second, let's imagine that we slice up the $\\mathbb{R}$ into an infinite number of slices which are $\\epsilon$ wide, say with slices $(\\epsilon\\cdot i, \\epsilon \\cdot (i+1)]$. For each of these, we know from :eqref:`eq_pdf_def` the probability is approximately\n\n$$\nP(X \\; \\textrm{is in an}\\; \\epsilon\\textrm{-sized interval around}\\; x ) \\approx \\epsilon \\cdot p(\\epsilon \\cdot i),\n$$\n\nso summed over all of them it should be\n\n$$\nP(X\\in\\mathbb{R}) \\approx \\sum_i \\epsilon \\cdot p(\\epsilon\\cdot i). $$\n\nThis is nothing more than the approximation of an integral discussed in :numref:`sec_integral_calculus`, thus we can say that\n\n$$\nP(X\\in\\mathbb{R}) = \\int_{-\\infty}^{\\infty} p(x) \\; dx. $$\n\nWe know that $P(X\\in\\mathbb{R}) = 1$, since the random variable must take on *some* number, we can conclude that for any density\n\n$$\n\\int_{-\\infty}^{\\infty} p(x) \\; dx = 1. $$\n\nIndeed, digging into this further shows that for any $a$, and $b$, we see that\n\n$$\nP(X\\in(a, b]) = \\int _ {a}^{b} p(x) \\; dx. $$\n\nWe may approximate this in code by using the same discrete approximation methods as before. In this case we can approximate the probability of falling in the blue region."
    },
    {
      "chunk_id": "af5a1e21f602_1",
      "chapter": "random-variables",
      "heading": "Probability Density Functions",
      "text": "$$\n\nWe may approximate this in code by using the same discrete approximation methods as before. In this case we can approximate the probability of falling in the blue region. ```{.python .input}\n#@tab mxnet\n# Approximate probability using numerical integration\nepsilon = 0.01\nx = np.arange(-5, 5, 0.01)\np = 0.2*np.exp(-(x - 3)**2 / 2) / np.sqrt(2 * np.pi) + \\\n    0.8*np.exp(-(x + 1)**2 / 2) / np.sqrt(2 * np.pi)\n\nd2l.set_figsize()\nd2l.plt.plot(x, p, color='black')\nd2l.plt.fill_between(x.tolist()[300:800], p.tolist()[300:800])\nd2l.plt.show()\n\nf'approximate Probability: {np.sum(epsilon*p[300:800])}'\n```\n\n```{.python .input}\n#@tab pytorch\n# Approximate probability using numerical integration\nepsilon = 0.01\nx = torch.arange(-5, 5, 0.01)\np = 0.2*torch.exp(-(x - 3)**2 / 2) / torch.sqrt(2 * torch.tensor(torch.pi)) +\\\n    0.8*torch.exp(-(x + 1)**2 / 2) / torch.sqrt(2 * torch.tensor(torch.pi))\n\nd2l.set_figsize()\nd2l.plt.plot(x, p, color='black')\nd2l.plt.fill_between(x.tolist()[300:800], p.tolist()[300:800])\nd2l.plt.show()\n\nf'approximate Probability: {torch.sum(epsilon*p[300:800])}'\n```\n\n```{.python .input}\n#@tab tensorflow\n# Approximate probability using numerical integration\nepsilon = 0.01\nx = tf.range(-5, 5, 0.01)\np = 0.2*tf.exp(-(x - 3)**2 / 2) / tf.sqrt(2 * tf.constant(tf.pi)) +\\\n    0.8*tf.exp(-(x + 1)**2 / 2) / tf.sqrt(2 * tf.constant(tf.pi))\n\nd2l.set_figsize()\nd2l.plt.plot(x, p, color='black')\nd2l.plt.fill_between(x.numpy().tolist()[300:800], p.numpy().tolist()[300:800])\nd2l.plt.show()\n\nf'approximate Probability: {tf.reduce_sum(epsilon*p[300:800])}'\n```\n\nIt turns out that these two properties describe exactly the space of possible probability density functions (or *p.d.f.*'s for the commonly encountered abbreviation)."
    },
    {
      "chunk_id": "af5a1e21f602_2",
      "chapter": "random-variables",
      "heading": "Probability Density Functions",
      "text": "They are non-negative functions $p(x) \\ge 0$ such that\n\n$$\\int_{-\\infty}^{\\infty} p(x) \\; dx = 1.$$\n:eqlabel:`eq_pdf_int_one`\n\nWe interpret this function by using integration to obtain the probability our random variable is in a specific interval:\n\n$$P(X\\in(a, b]) = \\int _ {a}^{b} p(x) \\; dx.$$\n:eqlabel:`eq_pdf_int_int`\n\nIn :numref:`sec_distributions` we will see a number of common distributions, but let's continue working in the abstract."
    },
    {
      "chunk_id": "ea8468af974e_0",
      "chapter": "random-variables",
      "heading": "Cumulative Distribution Functions",
      "text": "In the previous section, we saw the notion of the p.d.f.  In practice, this is a commonly encountered method to discuss continuous random variables, but it has one significant pitfall: that the values of the p.d.f. are not themselves probabilities, but rather a function that we must integrate to yield probabilities.  There is nothing wrong with a density being larger than $10$, as long as it is not larger than $10$ for more than an interval of length $1/10$.  This can be counter-intuitive, so people often also think in terms of the *cumulative distribution function*, or c.d.f., which *is* a probability.\n\nIn particular, by using :eqref:`eq_pdf_int_int`, we define the c.d.f. for a random variable $X$ with density $p(x)$ by\n\n$$\nF(x) = \\int _ {-\\infty}^{x} p(x) \\; dx = P(X \\le x).\n$$\n\nLet's observe a few properties.\n\n* $F(x) \\rightarrow 0$ as $x\\rightarrow -\\infty$.\n* $F(x) \\rightarrow 1$ as $x\\rightarrow \\infty$.\n* $F(x)$ is non-decreasing ($y > x \\implies F(y) \\ge F(x)$).\n* $F(x)$ is continuous (has no jumps) if $X$ is a continuous random variable.\n\nWith the fourth bullet point, note that this would not be true if $X$ were discrete, say taking the values $0$ and $1$ both with probability $1/2$.  In that case\n\n$$\nF(x) = \\begin{cases}\n0 & x < 0, \\\\\n\\frac{1}{2} & x < 1, \\\\\n1 & x \\ge 1.\n\\end{cases}\n$$\n\nIn this example, we see one of the benefits of working with the c.d.f., the ability to deal with continuous or discrete random variables in the same framework, or indeed mixtures of the two (flip a coin: if heads return the roll of a die, if tails return the distance of a dart throw from the center of a dart board)."
    },
    {
      "chunk_id": "bb1f762855eb_0",
      "chapter": "random-variables",
      "heading": "Means",
      "text": "Suppose that we are dealing with a random variables $X$. The distribution itself can be hard to interpret. It is often useful to be able to summarize the behavior of a random variable concisely. Numbers that help us capture the behavior of a random variable are called *summary statistics*. The most commonly encountered ones are the *mean*, the *variance*, and the *standard deviation*. The *mean* encodes the average value of a random variable. If we have a discrete random variable $X$, which takes the values $x_i$ with probabilities $p_i$, then the mean is given by the weighted average: sum the values times the probability that the random variable takes on that value:\n\n$$\\mu_X = E[X] = \\sum_i x_i p_i.$$\n:eqlabel:`eq_exp_def`\n\nThe way we should interpret the mean (albeit with caution) is that it tells us essentially where the random variable tends to be located. As a minimalistic example that we will examine throughout this section, let's take $X$ to be the random variable which takes the value $a-2$ with probability $p$, $a+2$ with probability $p$ and $a$ with probability $1-2p$. We can compute using :eqref:`eq_exp_def` that, for any possible choice of $a$ and $p$, the mean is\n\n$$\n\\mu_X = E[X] = \\sum_i x_i p_i = (a-2)p + a(1-2p) + (a+2)p = a. $$\n\nThus we see that the mean is $a$. This matches the intuition since $a$ is the location around which we centered our random variable. Because they are helpful, let's summarize a few properties. * For any random variable $X$ and numbers $a$ and $b$, we have that $\\mu_{aX+b} = a\\mu_X + b$. * If we have two random variables $X$ and $Y$, we have $\\mu_{X+Y} = \\mu_X+\\mu_Y$. Means are useful for understanding the average behavior of a random variable, however the mean is not sufficient to even have a full intuitive understanding. Making a profit of $\\$10 \\pm \\$1$ per sale is very different from making $\\$10 \\pm \\$15$ per sale despite having the same average value. The second one has a much larger degree of fluctuation, and thus represents a much larger risk."
    },
    {
      "chunk_id": "bb1f762855eb_1",
      "chapter": "random-variables",
      "heading": "Means",
      "text": "Making a profit of $\\$10 \\pm \\$1$ per sale is very different from making $\\$10 \\pm \\$15$ per sale despite having the same average value. The second one has a much larger degree of fluctuation, and thus represents a much larger risk. Thus, to understand the behavior of a random variable, we will need at minimum one more measure: some measure of how widely a random variable fluctuates."
    },
    {
      "chunk_id": "c8a27b5f2ffc_0",
      "chapter": "random-variables",
      "heading": "Variances",
      "text": "This leads us to consider the *variance* of a random variable. This is a quantitative measure of how far a random variable deviates from the mean. Consider the expression $X - \\mu_X$. This is the deviation of the random variable from its mean. This value can be positive or negative, so we need to do something to make it positive so that we are measuring the magnitude of the deviation. A reasonable thing to try is to look at $\\left|X-\\mu_X\\right|$, and indeed this leads to a useful quantity called the *mean absolute deviation*, however due to connections with other areas of mathematics and statistics, people often use a different solution. In particular, they look at $(X-\\mu_X)^2.$  If we look at the typical size of this quantity by taking the mean, we arrive at the variance\n\n$$\\sigma_X^2 = \\textrm{Var}(X) = E\\left[(X-\\mu_X)^2\\right] = E[X^2] - \\mu_X^2.$$\n:eqlabel:`eq_var_def`\n\nThe last equality in :eqref:`eq_var_def` holds by expanding out the definition in the middle, and applying the properties of expectation. Let's look at our example where $X$ is the random variable which takes the value $a-2$ with probability $p$, $a+2$ with probability $p$ and $a$ with probability $1-2p$. In this case $\\mu_X = a$, so all we need to compute is $E\\left[X^2\\right]$. This can readily be done:\n\n$$\nE\\left[X^2\\right] = (a-2)^2p + a^2(1-2p) + (a+2)^2p = a^2 + 8p. $$\n\nThus, we see that by :eqref:`eq_var_def` our variance is\n\n$$\n\\sigma_X^2 = \\textrm{Var}(X) = E[X^2] - \\mu_X^2 = a^2 + 8p - a^2 = 8p. $$\n\nThis result again makes sense. The largest $p$ can be is $1/2$ which corresponds to picking $a-2$ or $a+2$ with a coin flip. The variance of this being $4$ corresponds to the fact that both $a-2$ and $a+2$ are $2$ units away from the mean, and $2^2 = 4$. On the other end of the spectrum, if $p=0$, this random variable always takes the value $0$ and so it has no variance at all."
    },
    {
      "chunk_id": "c8a27b5f2ffc_1",
      "chapter": "random-variables",
      "heading": "Variances",
      "text": "The variance of this being $4$ corresponds to the fact that both $a-2$ and $a+2$ are $2$ units away from the mean, and $2^2 = 4$. On the other end of the spectrum, if $p=0$, this random variable always takes the value $0$ and so it has no variance at all. We will list a few properties of variance below:\n\n* For any random variable $X$, $\\textrm{Var}(X) \\ge 0$, with $\\textrm{Var}(X) = 0$ if and only if $X$ is a constant. * For any random variable $X$ and numbers $a$ and $b$, we have that $\\textrm{Var}(aX+b) = a^2\\textrm{Var}(X)$. * If we have two *independent* random variables $X$ and $Y$, we have $\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y)$. When interpreting these values, there can be a bit of a hiccup. In particular, let's try imagining what happens if we keep track of units through this computation. Suppose that we are working with the star rating assigned to a product on the web page. Then $a$, $a-2$, and $a+2$ are all measured in units of stars. Similarly, the mean $\\mu_X$ is then also measured in stars (being a weighted average). However, if we get to the variance, we immediately encounter an issue, which is we want to look at $(X-\\mu_X)^2$, which is in units of *squared stars*. This means that the variance itself is not comparable to the original measurements. To make it interpretable, we will need to return to our original units."
    },
    {
      "chunk_id": "3236c3019686_0",
      "chapter": "random-variables",
      "heading": "Standard Deviations",
      "text": "This summary statistics can always be deduced from the variance by taking the square root! Thus we define the *standard deviation* to be\n\n$$\n\\sigma_X = \\sqrt{\\textrm{Var}(X)}. $$\n\nIn our example, this means we now have the standard deviation is $\\sigma_X = 2\\sqrt{2p}$. If we are dealing with units of stars for our review example, $\\sigma_X$ is again in units of stars. The properties we had for the variance can be restated for the standard deviation. * For any random variable $X$, $\\sigma_{X} \\ge 0$. * For any random variable $X$ and numbers $a$ and $b$, we have that $\\sigma_{aX+b} = |a|\\sigma_{X}$\n* If we have two *independent* random variables $X$ and $Y$, we have $\\sigma_{X+Y} = \\sqrt{\\sigma_{X}^2 + \\sigma_{Y}^2}$. It is natural at this moment to ask, \"If the standard deviation is in the units of our original random variable, does it represent something we can draw with regards to that random variable?\"  The answer is a resounding yes! Indeed much like the mean told us the typical location of our random variable, the standard deviation gives the typical range of variation of that random variable. We can make this rigorous with what is known as Chebyshev's inequality:\n\n$$P\\left(X \\not\\in [\\mu_X - \\alpha\\sigma_X, \\mu_X + \\alpha\\sigma_X]\\right) \\le \\frac{1}{\\alpha^2}.$$\n:eqlabel:`eq_chebyshev`\n\nOr to state it verbally in the case of $\\alpha=10$, $99\\%$ of the samples from any random variable fall within $10$ standard deviations of the mean. This gives an immediate interpretation to our standard summary statistics. To see how this statement is rather subtle, let's take a look at our running example again where  $X$ is the random variable which takes the value $a-2$ with probability $p$, $a+2$ with probability $p$ and $a$ with probability $1-2p$. We saw that the mean was $a$ and the standard deviation was $2\\sqrt{2p}$. This means, if we take Chebyshev's inequality :eqref:`eq_chebyshev` with $\\alpha = 2$, we see that the expression is\n\n$$\nP\\left(X \\not\\in [a - 4\\sqrt{2p}, a + 4\\sqrt{2p}]\\right) \\le \\frac{1}{4}."
    },
    {
      "chunk_id": "3236c3019686_1",
      "chapter": "random-variables",
      "heading": "Standard Deviations",
      "text": "This means, if we take Chebyshev's inequality :eqref:`eq_chebyshev` with $\\alpha = 2$, we see that the expression is\n\n$$\nP\\left(X \\not\\in [a - 4\\sqrt{2p}, a + 4\\sqrt{2p}]\\right) \\le \\frac{1}{4}. $$\n\nThis means that $75\\%$ of the time, this random variable will fall within this interval for any value of $p$. Now, notice that as $p \\rightarrow 0$, this interval also converges to the single point $a$. But we know that our random variable takes the values $a-2, a$, and $a+2$ only so eventually we can be certain $a-2$ and $a+2$ will fall outside the interval! The question is, at what $p$ does that happen. So we want to solve: for what $p$ does $a+4\\sqrt{2p} = a+2$, which is solved when $p=1/8$, which is *exactly* the first $p$ where it could possibly happen without violating our claim that no more than $1/4$ of samples from the distribution would fall outside the interval ($1/8$ to the left, and $1/8$ to the right). Let's visualize this. We will show the probability of getting the three values as three vertical bars with height proportional to the probability. The interval will be drawn as a horizontal line in the middle. The first plot shows what happens for $p > 1/8$ where the interval safely contains all points."
    },
    {
      "chunk_id": "3236c3019686_2",
      "chapter": "random-variables",
      "heading": "Standard Deviations",
      "text": "The interval will be drawn as a horizontal line in the middle. The first plot shows what happens for $p > 1/8$ where the interval safely contains all points. ```{.python .input}\n#@tab mxnet\n# Define a helper to plot these figures\ndef plot_chebyshev(a, p):\n    d2l.set_figsize()\n    d2l.plt.stem([a-2, a, a+2], [p, 1-2*p, p], use_line_collection=True)\n    d2l.plt.xlim([-4, 4])\n    d2l.plt.xlabel('x')\n    d2l.plt.ylabel('p.m.f.')\n\n    d2l.plt.hlines(0.5, a - 4 * np.sqrt(2 * p),\n                   a + 4 * np.sqrt(2 * p), 'black', lw=4)\n    d2l.plt.vlines(a - 4 * np.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\n    d2l.plt.vlines(a + 4 * np.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\n    d2l.plt.title(f'p = {p:.3f}')\n\n    d2l.plt.show()\n\n# Plot interval when p > 1/8\nplot_chebyshev(0.0, 0.2)\n```\n\n```{.python .input}\n#@tab pytorch\n# Define a helper to plot these figures\ndef plot_chebyshev(a, p):\n    d2l.set_figsize()\n    d2l.plt.stem([a-2, a, a+2], [p, 1-2*p, p], use_line_collection=True)\n    d2l.plt.xlim([-4, 4])\n    d2l.plt.xlabel('x')\n    d2l.plt.ylabel('p.m.f.')\n\n    d2l.plt.hlines(0.5, a - 4 * torch.sqrt(2 * p),\n                   a + 4 * torch.sqrt(2 * p), 'black', lw=4)\n    d2l.plt.vlines(a - 4 * torch.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\n    d2l.plt.vlines(a + 4 * torch.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\n    d2l.plt.title(f'p = {p:.3f}')\n\n    d2l.plt.show()\n\n# Plot interval when p > 1/8\nplot_chebyshev(0.0, torch.tensor(0.2))\n```\n\n```{.python .input}\n#@tab tensorflow\n# Define a helper to plot these figures\ndef plot_chebyshev(a, p):\n    d2l.set_figsize()\n    d2l.plt.stem([a-2, a, a+2], [p, 1-2*p, p], use_line_collection=True)\n    d2l.plt.xlim([-4, 4])\n    d2l.plt.xlabel('x')\n    d2l.plt.ylabel('p.m.f.')\n\n    d2l.plt.hlines(0.5, a - 4 * tf.sqrt(2 * p),\n                   a + 4 * tf.sqrt(2 * p), 'black', lw=4)\n    d2l.plt.vlines(a - 4 * tf.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\n    d2l.plt.vlines(a + 4 * tf.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\n    d2l.plt.title(f'p = {p:.3f}')\n\n    d2l.plt.show()\n\n# Plot interval when p > 1/8\nplot_chebyshev(0.0, tf.constant(0.2))\n```\n\nThe second shows that at $p = 1/8$, the interval exactly touches the two points."
    },
    {
      "chunk_id": "3236c3019686_3",
      "chapter": "random-variables",
      "heading": "Standard Deviations",
      "text": "This shows that the inequality is *sharp*, since no smaller interval could be taken while keeping the inequality true. ```{.python .input}\n#@tab mxnet\n# Plot interval when p = 1/8\nplot_chebyshev(0.0, 0.125)\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot interval when p = 1/8\nplot_chebyshev(0.0, torch.tensor(0.125))\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot interval when p = 1/8\nplot_chebyshev(0.0, tf.constant(0.125))\n```\n\nThe third shows that for $p < 1/8$ the interval only contains the center. This does not invalidate the inequality since we only needed to ensure that no more than $1/4$ of the probability falls outside the interval, which means that once $p < 1/8$, the two points at $a-2$ and $a+2$ can be discarded. ```{.python .input}\n#@tab mxnet\n# Plot interval when p < 1/8\nplot_chebyshev(0.0, 0.05)\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot interval when p < 1/8\nplot_chebyshev(0.0, torch.tensor(0.05))\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot interval when p < 1/8\nplot_chebyshev(0.0, tf.constant(0.05))\n```"
    },
    {
      "chunk_id": "8d46d44ede05_0",
      "chapter": "random-variables",
      "heading": "Means and Variances in the Continuum",
      "text": "This has all been in terms of discrete random variables, but the case of continuous random variables is similar. To intuitively understand how this works, imagine that we split the real number line into intervals of length $\\epsilon$ given by $(\\epsilon i, \\epsilon (i+1)]$. Once we do this, our continuous random variable has been made discrete and we can use :eqref:`eq_exp_def` say that\n\n$$\n\\begin{aligned}\n\\mu_X & \\approx \\sum_{i} (\\epsilon i)P(X \\in (\\epsilon i, \\epsilon (i+1)]) \\\\\n& \\approx \\sum_{i} (\\epsilon i)p_X(\\epsilon i)\\epsilon, \\\\\n\\end{aligned}\n$$\n\nwhere $p_X$ is the density of $X$. This is an approximation to the integral of $xp_X(x)$, so we can conclude that\n\n$$\n\\mu_X = \\int_{-\\infty}^\\infty xp_X(x) \\; dx. $$\n\nSimilarly, using :eqref:`eq_var_def` the variance can be written as\n\n$$\n\\sigma^2_X = E[X^2] - \\mu_X^2 = \\int_{-\\infty}^\\infty x^2p_X(x) \\; dx - \\left(\\int_{-\\infty}^\\infty xp_X(x) \\; dx\\right)^2. $$\n\nEverything stated above about the mean, the variance, and the standard deviation still applies in this case. For instance, if we consider the random variable with density\n\n$$\np(x) = \\begin{cases}\n1 & x \\in [0,1], \\\\\n0 & \\textrm{otherwise}. \\end{cases}\n$$\n\nwe can compute\n\n$$\n\\mu_X = \\int_{-\\infty}^\\infty xp(x) \\; dx = \\int_0^1 x \\; dx = \\frac{1}{2}. $$\n\nand\n\n$$\n\\sigma_X^2 = \\int_{-\\infty}^\\infty x^2p(x) \\; dx - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{1}{12}. $$\n\nAs a warning, let's examine one more example, known as the *Cauchy distribution*. This is the distribution with p.d.f. given by\n\n$$\np(x) = \\frac{1}{1+x^2}. $$\n\n```{.python .input}\n#@tab mxnet\n# Plot the Cauchy distribution p.d.f. x = np.arange(-5, 5, 0.01)\np = 1 / (1 + x**2)\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot the Cauchy distribution p.d.f. x = torch.arange(-5, 5, 0.01)\np = 1 / (1 + x**2)\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot the Cauchy distribution p.d.f."
    },
    {
      "chunk_id": "8d46d44ede05_1",
      "chapter": "random-variables",
      "heading": "Means and Variances in the Continuum",
      "text": "x = torch.arange(-5, 5, 0.01)\np = 1 / (1 + x**2)\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot the Cauchy distribution p.d.f. x = tf.range(-5, 5, 0.01)\np = 1 / (1 + x**2)\n\nd2l.plot(x, p, 'x', 'p.d.f.')\n```\n\nThis function looks innocent, and indeed consulting a table of integrals will show it has area one under it, and thus it defines a continuous random variable. To see what goes astray, let's try to compute the variance of this. This would involve using :eqref:`eq_var_def` computing\n\n$$\n\\int_{-\\infty}^\\infty \\frac{x^2}{1+x^2}\\; dx. $$\n\nThe function on the inside looks like this:\n\n```{.python .input}\n#@tab mxnet\n# Plot the integrand needed to compute the variance\nx = np.arange(-20, 20, 0.01)\np = x**2 / (1 + x**2)\n\nd2l.plot(x, p, 'x', 'integrand')\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot the integrand needed to compute the variance\nx = torch.arange(-20, 20, 0.01)\np = x**2 / (1 + x**2)\n\nd2l.plot(x, p, 'x', 'integrand')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot the integrand needed to compute the variance\nx = tf.range(-20, 20, 0.01)\np = x**2 / (1 + x**2)\n\nd2l.plot(x, p, 'x', 'integrand')\n```\n\nThis function clearly has infinite area under it since it is essentially the constant one with a small dip near zero, and indeed we could show that\n\n$$\n\\int_{-\\infty}^\\infty \\frac{x^2}{1+x^2}\\; dx = \\infty. $$\n\nThis means it does not have a well-defined finite variance. However, looking deeper shows an even more disturbing result. Let's try to compute the mean using :eqref:`eq_exp_def`. Using the change of variables formula, we see\n\n$$\n\\mu_X = \\int_{-\\infty}^{\\infty} \\frac{x}{1+x^2} \\; dx = \\frac{1}{2}\\int_1^\\infty \\frac{1}{u} \\; du. $$\n\nThe integral inside is the definition of the logarithm, so this is in essence $\\log(\\infty) = \\infty$, so there is no well-defined average value either!"
    },
    {
      "chunk_id": "8d46d44ede05_2",
      "chapter": "random-variables",
      "heading": "Means and Variances in the Continuum",
      "text": "$$\n\nThe integral inside is the definition of the logarithm, so this is in essence $\\log(\\infty) = \\infty$, so there is no well-defined average value either! Machine learning scientists define their models so that we most often do not need to deal with these issues, and will in the vast majority of cases deal with random variables with well-defined means and variances. However, every so often random variables with *heavy tails* (that is those random variables where the probabilities of getting large values are large enough to make things like the mean or variance undefined) are helpful in modeling physical systems, thus it is worth knowing that they exist."
    },
    {
      "chunk_id": "bfd6444a4688_0",
      "chapter": "random-variables",
      "heading": "Joint Density Functions",
      "text": "The above work all assumes we are working with a single real valued random variable. But what if we are dealing with two or more potentially highly correlated random variables? This circumstance is the norm in machine learning: imagine random variables like $R_{i, j}$ which encode the red value of the pixel at the $(i, j)$ coordinate in an image, or $P_t$ which is a random variable given by a stock price at time $t$. Nearby pixels tend to have similar color, and nearby times tend to have similar prices. We cannot treat them as separate random variables, and expect to create a successful model (we will see in :numref:`sec_naive_bayes` a model that under-performs due to such an assumption). We need to develop the mathematical language to handle these correlated continuous random variables. Thankfully, with the multiple integrals in :numref:`sec_integral_calculus` we can develop such a language. Suppose that we have, for simplicity, two random variables $X, Y$ which can be correlated. Then, similar to the case of a single variable, we can ask the question:\n\n$$\nP(X \\;\\textrm{is in an}\\; \\epsilon \\textrm{-sized interval around}\\; x \\; \\textrm{and} \\;Y \\;\\textrm{is in an}\\; \\epsilon \\textrm{-sized interval around}\\; y ). $$\n\nSimilar reasoning to the single variable case shows that this should be approximately\n\n$$\nP(X \\;\\textrm{is in an}\\; \\epsilon \\textrm{-sized interval around}\\; x \\; \\textrm{and} \\;Y \\;\\textrm{is in an}\\; \\epsilon \\textrm{-sized interval around}\\; y ) \\approx \\epsilon^{2}p(x, y),\n$$\n\nfor some function $p(x, y)$. This is referred to as the joint density of $X$ and $Y$. Similar properties are true for this as we saw in the single variable case. Namely:\n\n* $p(x, y) \\ge 0$;\n* $\\int _ {\\mathbb{R}^2} p(x, y) \\;dx \\;dy = 1$;\n* $P((X, Y) \\in \\mathcal{D}) = \\int _ {\\mathcal{D}} p(x, y) \\;dx \\;dy$. In this way, we can deal with multiple, potentially correlated random variables."
    },
    {
      "chunk_id": "bfd6444a4688_1",
      "chapter": "random-variables",
      "heading": "Joint Density Functions",
      "text": "Namely:\n\n* $p(x, y) \\ge 0$;\n* $\\int _ {\\mathbb{R}^2} p(x, y) \\;dx \\;dy = 1$;\n* $P((X, Y) \\in \\mathcal{D}) = \\int _ {\\mathcal{D}} p(x, y) \\;dx \\;dy$. In this way, we can deal with multiple, potentially correlated random variables. If we wish to work with more than two random variables, we can extend the multivariate density to as many coordinates as desired by considering $p(\\mathbf{x}) = p(x_1, \\ldots, x_n)$. The same properties of being non-negative, and having total integral of one still hold."
    },
    {
      "chunk_id": "130f5fd52cba_0",
      "chapter": "random-variables",
      "heading": "Marginal Distributions",
      "text": "When dealing with multiple variables, we oftentimes want to be able to ignore the relationships and ask, \"how is this one variable distributed?\"  Such a distribution is called a *marginal distribution*. To be concrete, let's suppose that we have two random variables $X, Y$ with joint density given by $p _ {X, Y}(x, y)$. We will be using the subscript to indicate what random variables the density is for. The question of finding the marginal distribution is taking this function, and using it to find $p _ X(x)$. As with most things, it is best to return to the intuitive picture to figure out what should be true. Recall that the density is the function $p _ X$ so that\n\n$$\nP(X \\in [x, x+\\epsilon]) \\approx \\epsilon \\cdot p _ X(x). $$\n\nThere is no mention of $Y$, but if all we are given is $p _{X, Y}$, we need to include $Y$ somehow. We can first observe that this is the same as\n\n$$\nP(X \\in [x, x+\\epsilon] \\textrm{, and } Y \\in \\mathbb{R}) \\approx \\epsilon \\cdot p _ X(x). $$\n\nOur density does not directly tell us about what happens in this case, we need to split into small intervals in $y$ as well, so we can write this as\n\n$$\n\\begin{aligned}\n\\epsilon \\cdot p _ X(x) & \\approx \\sum _ {i} P(X \\in [x, x+\\epsilon] \\textrm{, and } Y \\in [\\epsilon \\cdot i, \\epsilon \\cdot (i+1)]) \\\\\n& \\approx \\sum _ {i} \\epsilon^{2} p _ {X, Y}(x, \\epsilon\\cdot i). \\end{aligned}\n$$\n\n![By summing along the columns of our array of probabilities, we are able to obtain the marginal distribution for just the random variable represented along the $\\mathit{x}$-axis.](../img/marginal.svg)\n:label:`fig_marginal`\n\nThis tells us to add up the value of the density along a series of squares in a line as is shown in :numref:`fig_marginal`. Indeed, after canceling one factor of epsilon from both sides, and recognizing the sum on the right is the integral over $y$, we can conclude that\n\n$$\n\\begin{aligned}\n p _ X(x) &  \\approx \\sum _ {i} \\epsilon p _ {X, Y}(x, \\epsilon\\cdot i) \\\\\n & \\approx \\int_{-\\infty}^\\infty p_{X, Y}(x, y) \\; dy."
    },
    {
      "chunk_id": "130f5fd52cba_1",
      "chapter": "random-variables",
      "heading": "Marginal Distributions",
      "text": "\\end{aligned}\n$$\n\nThus we see\n\n$$\np _ X(x) = \\int_{-\\infty}^\\infty p_{X, Y}(x, y) \\; dy. $$\n\nThis tells us that to get a marginal distribution, we integrate over the variables we do not care about. This process is often referred to as *integrating out* or *marginalized out* the unneeded variables."
    },
    {
      "chunk_id": "31032ff501d6_0",
      "chapter": "random-variables",
      "heading": "Covariance",
      "text": "When dealing with multiple random variables, there is one additional summary statistic which is helpful to know: the *covariance*. This measures the degree that two random variable fluctuate together. Suppose that we have two random variables $X$ and $Y$, to begin with, let's suppose they are discrete, taking on values $(x_i, y_j)$ with probability $p_{ij}$. In this case, the covariance is defined as\n\n$$\\sigma_{XY} = \\textrm{Cov}(X, Y) = \\sum_{i, j} (x_i - \\mu_X) (y_j-\\mu_Y) p_{ij}. = E[XY] - E[X]E[Y].$$\n:eqlabel:`eq_cov_def`\n\nTo think about this intuitively: consider the following pair of random variables. Suppose that $X$ takes the values $1$ and $3$, and $Y$ takes the values $-1$ and $3$. Suppose that we have the following probabilities\n\n$$\n\\begin{aligned}\nP(X = 1 \\; \\textrm{and} \\; Y = -1) & = \\frac{p}{2}, \\\\\nP(X = 1 \\; \\textrm{and} \\; Y = 3) & = \\frac{1-p}{2}, \\\\\nP(X = 3 \\; \\textrm{and} \\; Y = -1) & = \\frac{1-p}{2}, \\\\\nP(X = 3 \\; \\textrm{and} \\; Y = 3) & = \\frac{p}{2},\n\\end{aligned}\n$$\n\nwhere $p$ is a parameter in $[0,1]$ we get to pick. Notice that if $p=1$ then they are both always their minimum or maximum values simultaneously, and if $p=0$ they are guaranteed to take their flipped values simultaneously (one is large when the other is small and vice versa). If $p=1/2$, then the four possibilities are all equally likely, and neither should be related. Let's compute the covariance. First, note $\\mu_X = 2$ and $\\mu_Y = 1$, so we may compute using :eqref:`eq_cov_def`:\n\n$$\n\\begin{aligned}\n\\textrm{Cov}(X, Y) & = \\sum_{i, j} (x_i - \\mu_X) (y_j-\\mu_Y) p_{ij} \\\\\n& = (1-2)(-1-1)\\frac{p}{2} + (1-2)(3-1)\\frac{1-p}{2} + (3-2)(-1-1)\\frac{1-p}{2} + (3-2)(3-1)\\frac{p}{2} \\\\\n& = 4p-2. \\end{aligned}\n$$\n\nWhen $p=1$ (the case where they are both maximally positive or negative at the same time) has a covariance of $2$. When $p=0$ (the case where they are flipped) the covariance is $-2$. Finally, when $p=1/2$ (the case where they are unrelated), the covariance is $0$."
    },
    {
      "chunk_id": "31032ff501d6_1",
      "chapter": "random-variables",
      "heading": "Covariance",
      "text": "When $p=0$ (the case where they are flipped) the covariance is $-2$. Finally, when $p=1/2$ (the case where they are unrelated), the covariance is $0$. Thus we see that the covariance measures how these two random variables are related. A quick note on the covariance is that it only measures these linear relationships. More complex relationships like $X = Y^2$ where $Y$ is randomly chosen from $\\{-2, -1, 0, 1, 2\\}$ with equal probability can be missed. Indeed a quick computation shows that these random variables have covariance zero, despite one being a deterministic function of the other. For continuous random variables, much the same story holds. At this point, we are pretty comfortable with doing the transition between discrete and continuous, so we will provide the continuous analogue of :eqref:`eq_cov_def` without any derivation. $$\n\\sigma_{XY} = \\int_{\\mathbb{R}^2} (x-\\mu_X)(y-\\mu_Y)p(x, y) \\;dx \\;dy. $$\n\nFor visualization, let's take a look at a collection of random variables with tunable covariance."
    },
    {
      "chunk_id": "31032ff501d6_2",
      "chapter": "random-variables",
      "heading": "Covariance",
      "text": "$$\n\\sigma_{XY} = \\int_{\\mathbb{R}^2} (x-\\mu_X)(y-\\mu_Y)p(x, y) \\;dx \\;dy. $$\n\nFor visualization, let's take a look at a collection of random variables with tunable covariance. ```{.python .input}\n#@tab mxnet\n# Plot a few random variables adjustable covariance\ncovs = [-0.9, 0.0, 1.2]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\n    X = np.random.normal(0, 1, 500)\n    Y = covs[i]*X + np.random.normal(0, 1, (500))\n\n    d2l.plt.subplot(1, 4, i+1)\n    d2l.plt.scatter(X.asnumpy(), Y.asnumpy())\n    d2l.plt.xlabel('X')\n    d2l.plt.ylabel('Y')\n    d2l.plt.title(f'cov = {covs[i]}')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot a few random variables adjustable covariance\ncovs = [-0.9, 0.0, 1.2]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\n    X = torch.randn(500)\n    Y = covs[i]*X + torch.randn(500)\n\n    d2l.plt.subplot(1, 4, i+1)\n    d2l.plt.scatter(X.numpy(), Y.numpy())\n    d2l.plt.xlabel('X')\n    d2l.plt.ylabel('Y')\n    d2l.plt.title(f'cov = {covs[i]}')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot a few random variables adjustable covariance\ncovs = [-0.9, 0.0, 1.2]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\n    X = tf.random.normal((500, ))\n    Y = covs[i]*X + tf.random.normal((500, ))\n\n    d2l.plt.subplot(1, 4, i+1)\n    d2l.plt.scatter(X.numpy(), Y.numpy())\n    d2l.plt.xlabel('X')\n    d2l.plt.ylabel('Y')\n    d2l.plt.title(f'cov = {covs[i]}')\nd2l.plt.show()\n```\n\nLet's see some properties of covariances:\n\n* For any random variable $X$, $\\textrm{Cov}(X, X) = \\textrm{Var}(X)$. * For any random variables $X, Y$ and numbers $a$ and $b$, $\\textrm{Cov}(aX+b, Y) = \\textrm{Cov}(X, aY+b) = a\\textrm{Cov}(X, Y)$. * If $X$ and $Y$ are independent then $\\textrm{Cov}(X, Y) = 0$. In addition, we can use the covariance to expand a relationship we saw before. Recall that is $X$ and $Y$ are two independent random variables then\n\n$$\n\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y). $$\n\nWith knowledge of covariances, we can expand this relationship."
    },
    {
      "chunk_id": "31032ff501d6_3",
      "chapter": "random-variables",
      "heading": "Covariance",
      "text": "Recall that is $X$ and $Y$ are two independent random variables then\n\n$$\n\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y). $$\n\nWith knowledge of covariances, we can expand this relationship. Indeed, some algebra can show that in general,\n\n$$\n\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y) + 2\\textrm{Cov}(X, Y). $$\n\nThis allows us to generalize the variance summation rule for correlated random variables."
    },
    {
      "chunk_id": "0ba6238c4a6f_0",
      "chapter": "random-variables",
      "heading": "Correlation",
      "text": "As we did in the case of means and variances, let's now consider units. If $X$ is measured in one unit (say inches), and $Y$ is measured in another (say dollars), the covariance is measured in the product of these two units $\\textrm{inches} \\times \\textrm{dollars}$. These units can be hard to interpret. What we will often want in this case is a unit-less measurement of relatedness. Indeed, often we do not care about exact quantitative correlation, but rather ask if the correlation is in the same direction, and how strong the relationship is. To see what makes sense, let's perform a thought experiment. Suppose that we convert our random variables in inches and dollars to be in inches and cents. In this case the random variable $Y$ is multiplied by $100$. If we work through the definition, this means that $\\textrm{Cov}(X, Y)$ will be multiplied by $100$. Thus we see that in this case a change of units change the covariance by a factor of $100$. Thus, to find our unit-invariant measure of correlation, we will need to divide by something else that also gets scaled by $100$. Indeed we have a clear candidate, the standard deviation! Indeed if we define the *correlation coefficient* to be\n\n$$\\rho(X, Y) = \\frac{\\textrm{Cov}(X, Y)}{\\sigma_{X}\\sigma_{Y}},$$\n:eqlabel:`eq_cor_def`\n\nwe see that this is a unit-less value. A little mathematics can show that this number is between $-1$ and $1$ with $1$ meaning maximally positively correlated, whereas $-1$ means maximally negatively correlated. Returning to our explicit discrete example above, we can see that $\\sigma_X = 1$ and $\\sigma_Y = 2$, so we can compute the correlation between the two random variables using :eqref:`eq_cor_def` to see that\n\n$$\n\\rho(X, Y) = \\frac{4p-2}{1\\cdot 2} = 2p-1. $$\n\nThis now ranges between $-1$ and $1$ with the expected behavior of $1$ meaning most correlated, and $-1$ meaning minimally correlated. As another example, consider $X$ as any random variable, and $Y=aX+b$ as any linear deterministic function of $X$."
    },
    {
      "chunk_id": "0ba6238c4a6f_1",
      "chapter": "random-variables",
      "heading": "Correlation",
      "text": "$$\n\nThis now ranges between $-1$ and $1$ with the expected behavior of $1$ meaning most correlated, and $-1$ meaning minimally correlated. As another example, consider $X$ as any random variable, and $Y=aX+b$ as any linear deterministic function of $X$. Then, one can compute that\n\n$$\\sigma_{Y} = \\sigma_{aX+b} = |a|\\sigma_{X},$$\n\n$$\\textrm{Cov}(X, Y) = \\textrm{Cov}(X, aX+b) = a\\textrm{Cov}(X, X) = a\\textrm{Var}(X),$$\n\nand thus by :eqref:`eq_cor_def` that\n\n$$\n\\rho(X, Y) = \\frac{a\\textrm{Var}(X)}{|a|\\sigma_{X}^2} = \\frac{a}{|a|} = \\textrm{sign}(a). $$\n\nThus we see that the correlation is $+1$ for any $a > 0$, and $-1$ for any $a < 0$ illustrating that correlation measures the degree and directionality the two random variables are related, not the scale that the variation takes. Let's again plot a collection of random variables with tunable correlation."
    },
    {
      "chunk_id": "0ba6238c4a6f_2",
      "chapter": "random-variables",
      "heading": "Correlation",
      "text": "Let's again plot a collection of random variables with tunable correlation. ```{.python .input}\n#@tab mxnet\n# Plot a few random variables adjustable correlations\ncors = [-0.9, 0.0, 1.0]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\n    X = np.random.normal(0, 1, 500)\n    Y = cors[i] * X + np.sqrt(1 - cors[i]**2) * np.random.normal(0, 1, 500)\n\n    d2l.plt.subplot(1, 4, i + 1)\n    d2l.plt.scatter(X.asnumpy(), Y.asnumpy())\n    d2l.plt.xlabel('X')\n    d2l.plt.ylabel('Y')\n    d2l.plt.title(f'cor = {cors[i]}')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot a few random variables adjustable correlations\ncors = [-0.9, 0.0, 1.0]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\n    X = torch.randn(500)\n    Y = cors[i] * X + torch.sqrt(torch.tensor(1) -\n                                 cors[i]**2) * torch.randn(500)\n\n    d2l.plt.subplot(1, 4, i + 1)\n    d2l.plt.scatter(X.numpy(), Y.numpy())\n    d2l.plt.xlabel('X')\n    d2l.plt.ylabel('Y')\n    d2l.plt.title(f'cor = {cors[i]}')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot a few random variables adjustable correlations\ncors = [-0.9, 0.0, 1.0]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\n    X = tf.random.normal((500, ))\n    Y = cors[i] * X + tf.sqrt(tf.constant(1.) -\n                                 cors[i]**2) * tf.random.normal((500, ))\n\n    d2l.plt.subplot(1, 4, i + 1)\n    d2l.plt.scatter(X.numpy(), Y.numpy())\n    d2l.plt.xlabel('X')\n    d2l.plt.ylabel('Y')\n    d2l.plt.title(f'cor = {cors[i]}')\nd2l.plt.show()\n```\n\nLet's list a few properties of the correlation below. * For any random variable $X$, $\\rho(X, X) = 1$. * For any random variables $X, Y$ and numbers $a$ and $b$, $\\rho(aX+b, Y) = \\rho(X, aY+b) = \\rho(X, Y)$. * If $X$ and $Y$ are independent with non-zero variance then $\\rho(X, Y) = 0$. As a final note, you may feel like some of these formulae are familiar."
    },
    {
      "chunk_id": "0ba6238c4a6f_3",
      "chapter": "random-variables",
      "heading": "Correlation",
      "text": "* For any random variables $X, Y$ and numbers $a$ and $b$, $\\rho(aX+b, Y) = \\rho(X, aY+b) = \\rho(X, Y)$. * If $X$ and $Y$ are independent with non-zero variance then $\\rho(X, Y) = 0$. As a final note, you may feel like some of these formulae are familiar. Indeed, if we expand everything out assuming that $\\mu_X = \\mu_Y = 0$, we see that this is\n\n$$\n\\rho(X, Y) = \\frac{\\sum_{i, j} x_iy_ip_{ij}}{\\sqrt{\\sum_{i, j}x_i^2 p_{ij}}\\sqrt{\\sum_{i, j}y_j^2 p_{ij}}}. $$\n\nThis looks like a sum of a product of terms divided by the square root of sums of terms. This is exactly the formula for the cosine of the angle between two vectors $\\mathbf{v}, \\mathbf{w}$ with the different coordinates weighted by $p_{ij}$:\n\n$$\n\\cos(\\theta) = \\frac{\\mathbf{v}\\cdot \\mathbf{w}}{\\|\\mathbf{v}\\|\\|\\mathbf{w}\\|} = \\frac{\\sum_{i} v_iw_i}{\\sqrt{\\sum_{i}v_i^2}\\sqrt{\\sum_{i}w_i^2}}. $$\n\nIndeed if we think of norms as being related to standard deviations, and correlations as being cosines of angles, much of the intuition we have from geometry can be applied to thinking about random variables."
    },
    {
      "chunk_id": "332d792700a1_0",
      "chapter": "random-variables",
      "heading": "Summary",
      "text": "* Continuous random variables are random variables that can take on a continuum of values.  They have some technical difficulties that make them more challenging to work with compared to discrete random variables.\n* The probability density function allows us to work with continuous random variables by giving a function where the area under the curve on some interval gives the probability of finding a sample point in that interval.\n* The cumulative distribution function is the probability of observing the random variable to be less than a given threshold.  It can provide a useful alternate viewpoint which unifies discrete and continuous variables.\n* The mean is the average value of a random variable.\n* The variance is the expected square of the difference between the random variable and its mean.\n* The standard deviation is the square root of the variance.  It can be thought of as measuring the range of values the random variable may take.\n* Chebyshev's inequality allows us to make this intuition rigorous by giving an explicit interval that contains the random variable most of the time.\n* Joint densities allow us to work with correlated random variables.  We may marginalize joint densities by integrating over unwanted random variables to get the distribution of the desired random variable.\n* The covariance and correlation coefficient provide a way to measure any linear relationship between two correlated random variables."
    },
    {
      "chunk_id": "6f279146f39d_0",
      "chapter": "random-variables",
      "heading": "Exercises",
      "text": "1. Suppose that we have the random variable with density given by $p(x) = \\frac{1}{x^2}$ for $x \\ge 1$ and $p(x) = 0$ otherwise.  What is $P(X > 2)$?\n2. The Laplace distribution is a random variable whose density is given by $p(x = \\frac{1}{2}e^{-|x|}$.  What is the mean and the standard deviation of this function?  As a hint, $\\int_0^\\infty xe^{-x} \\; dx = 1$ and $\\int_0^\\infty x^2e^{-x} \\; dx = 2$.\n3. I walk up to you on the street and say \"I have a random variable with mean $1$, standard deviation $2$, and I observed $25\\%$ of my samples taking a value larger than $9$.\"  Do you believe me?  Why or why not?\n4. Suppose that you have two random variables $X, Y$, with joint density given by $p_{XY}(x, y) = 4xy$ for $x, y \\in [0,1]$ and $p_{XY}(x, y) = 0$ otherwise.  What is the covariance of $X$ and $Y$?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/415)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1094)\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1095)\n:end_tab:"
    },
    {
      "chunk_id": "ba95eb97c030_0",
      "chapter": "single-variable-calculus",
      "heading": "single-variable-calculus",
      "text": "# Single Variable Calculus\n:label:`sec_single_variable_calculus`\n\nIn :numref:`sec_calculus`, we saw the basic elements of differential calculus.  This section takes a deeper dive into the fundamentals of calculus and how we can understand and apply it in the context of machine learning."
    },
    {
      "chunk_id": "ff55455e18ff_0",
      "chapter": "single-variable-calculus",
      "heading": "Differential Calculus",
      "text": "Differential calculus is fundamentally the study of how functions behave under small changes. To see why this is so core to deep learning, let's consider an example. Suppose that we have a deep neural network where the weights are, for convenience, concatenated into a single vector $\\mathbf{w} = (w_1, \\ldots, w_n)$. Given a training dataset, we consider the loss of our neural network on this dataset, which we will write as $\\mathcal{L}(\\mathbf{w})$. This function is extraordinarily complex, encoding the performance of all possible models of the given architecture on this dataset, so it is nearly impossible to tell what set of weights $\\mathbf{w}$ will minimize the loss. Thus, in practice, we often start by initializing our weights *randomly*, and then iteratively take small steps in the direction which makes the loss decrease as rapidly as possible. The question then becomes something that on the surface is no easier: how do we find the direction which makes the weights decrease as quickly as possible? To dig into this, let's first examine the case with only a single weight: $L(\\mathbf{w}) = L(x)$ for a single real value $x$. Let's take $x$ and try to understand what happens when we change it by a small amount to $x + \\epsilon$. If you wish to be concrete, think a number like $\\epsilon = 0.0000001$. To help us visualize what happens, let's graph an example function, $f(x) = \\sin(x^x)$, over the $[0, 3]$."
    },
    {
      "chunk_id": "ff55455e18ff_1",
      "chapter": "single-variable-calculus",
      "heading": "Differential Calculus",
      "text": "If you wish to be concrete, think a number like $\\epsilon = 0.0000001$. To help us visualize what happens, let's graph an example function, $f(x) = \\sin(x^x)$, over the $[0, 3]$. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom IPython import display\nfrom mxnet import np, npx\nnpx.set_np()\n\n# Plot a function in a normal range\nx_big = np.arange(0.01, 3.01, 0.01)\nys = np.sin(x_big**x_big)\nd2l.plot(x_big, ys, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom IPython import display\nimport torch\ntorch.pi = torch.acos(torch.zeros(1)).item() * 2  # Define pi in torch\n\n# Plot a function in a normal range\nx_big = torch.arange(0.01, 3.01, 0.01)\nys = torch.sin(x_big**x_big)\nd2l.plot(x_big, ys, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom IPython import display\nimport tensorflow as tf\ntf.pi = tf.acos(tf.zeros(1)).numpy() * 2  # Define pi in TensorFlow\n\n# Plot a function in a normal range\nx_big = tf.range(0.01, 3.01, 0.01)\nys = tf.sin(x_big**x_big)\nd2l.plot(x_big, ys, 'x', 'f(x)')\n```\n\nAt this large scale, the function's behavior is not simple. However, if we reduce our range to something smaller like $[1.75,2.25]$, we see that the graph becomes much simpler. ```{.python .input}\n#@tab mxnet\n# Plot a the same function in a tiny range\nx_med = np.arange(1.75, 2.25, 0.001)\nys = np.sin(x_med**x_med)\nd2l.plot(x_med, ys, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot a the same function in a tiny range\nx_med = torch.arange(1.75, 2.25, 0.001)\nys = torch.sin(x_med**x_med)\nd2l.plot(x_med, ys, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot a the same function in a tiny range\nx_med = tf.range(1.75, 2.25, 0.001)\nys = tf.sin(x_med**x_med)\nd2l.plot(x_med, ys, 'x', 'f(x)')\n```\n\nTaking this to an extreme, if we zoom into a tiny segment, the behavior becomes far simpler: it is just a straight line."
    },
    {
      "chunk_id": "ff55455e18ff_2",
      "chapter": "single-variable-calculus",
      "heading": "Differential Calculus",
      "text": "```{.python .input}\n#@tab mxnet\n# Plot a the same function in a tiny range\nx_small = np.arange(2.0, 2.01, 0.0001)\nys = np.sin(x_small**x_small)\nd2l.plot(x_small, ys, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab pytorch\n# Plot a the same function in a tiny range\nx_small = torch.arange(2.0, 2.01, 0.0001)\nys = torch.sin(x_small**x_small)\nd2l.plot(x_small, ys, 'x', 'f(x)')\n```\n\n```{.python .input}\n#@tab tensorflow\n# Plot a the same function in a tiny range\nx_small = tf.range(2.0, 2.01, 0.0001)\nys = tf.sin(x_small**x_small)\nd2l.plot(x_small, ys, 'x', 'f(x)')\n```\n\nThis is the key observation of single variable calculus: the behavior of familiar functions can be modeled by a line in a small enough range. This means that for most functions, it is reasonable to expect that as we shift the $x$ value of the function by a little bit, the output $f(x)$ will also be shifted by a little bit. The only question we need to answer is, \"How large is the change in the output compared to the change in the input? Is it half as large? Twice as large?\"\n\nThus, we can consider the ratio of the change in the output of a function for a small change in the input of the function. We can write this formally as\n\n$$\n\\frac{L(x+\\epsilon) - L(x)}{(x+\\epsilon) - x} = \\frac{L(x+\\epsilon) - L(x)}{\\epsilon}. $$\n\nThis is already enough to start to play around with in code. For instance, suppose that we know that $L(x) = x^{2} + 1701(x-4)^3$, then we can see how large this value is at the point $x = 4$ as follows. ```{.python .input}\n#@tab all\n# Define our function\ndef L(x):\n    return x**2 + 1701*(x-4)**3\n\n# Print the difference divided by epsilon for several epsilon\nfor epsilon in [0.1, 0.001, 0.0001, 0.00001]:\n    print(f'epsilon = {epsilon:.5f} -> {(L(4+epsilon) - L(4)) / epsilon:.5f}')\n```\n\nNow, if we are observant, we will notice that the output of this number is suspiciously close to $8$. Indeed, if we decrease $\\epsilon$, we will see value becomes progressively closer to $8$."
    },
    {
      "chunk_id": "ff55455e18ff_3",
      "chapter": "single-variable-calculus",
      "heading": "Differential Calculus",
      "text": "Indeed, if we decrease $\\epsilon$, we will see value becomes progressively closer to $8$. Thus we may conclude, correctly, that the value we seek (the degree a change in the input changes the output) should be $8$ at the point $x=4$. The way that a mathematician encodes this fact is\n\n$$\n\\lim_{\\epsilon \\rightarrow 0}\\frac{L(4+\\epsilon) - L(4)}{\\epsilon} = 8. $$\n\nAs a bit of a historical digression: in the first few decades of neural network research, scientists used this algorithm (the *method of finite differences*) to evaluate how a loss function changed under small perturbation: just change the weights and see how the loss changed. This is computationally inefficient, requiring two evaluations of the loss function to see how a single change of one variable influenced the loss. If we tried to do this with even a paltry few thousand parameters, it would require several thousand evaluations of the network over the entire dataset! It was not solved until 1986 that the *backpropagation algorithm* introduced in :citet:`Rumelhart.Hinton.Williams.ea.1988` provided a way to calculate how *any* change of the weights together would change the loss in the same computation time as a single prediction of the network over the dataset. Back in our example, this value $8$ is different for different values of $x$, so it makes sense to define it as a function of $x$. More formally, this value dependent rate of change is referred to as the *derivative* which is written as\n\n$$\\frac{df}{dx}(x) = \\lim_{\\epsilon \\rightarrow 0}\\frac{f(x+\\epsilon) - f(x)}{\\epsilon}.$$\n:eqlabel:`eq_der_def`\n\nDifferent texts will use different notations for the derivative. For instance, all of the below notations indicate the same thing:\n\n$$\n\\frac{df}{dx} = \\frac{d}{dx}f = f' = \\nabla_xf = D_xf = f_x. $$\n\nMost authors will pick a single notation and stick with it, however even that is not guaranteed. It is best to be familiar with all of these."
    },
    {
      "chunk_id": "ff55455e18ff_4",
      "chapter": "single-variable-calculus",
      "heading": "Differential Calculus",
      "text": "$$\n\nMost authors will pick a single notation and stick with it, however even that is not guaranteed. It is best to be familiar with all of these. We will use the notation $\\frac{df}{dx}$ throughout this text, unless we want to take the derivative of a complex expression, in which case we will use $\\frac{d}{dx}f$ to write expressions like\n$$\n\\frac{d}{dx}\\left[x^4+\\cos\\left(\\frac{x^2+1}{2x-1}\\right)\\right]. $$\n\nOftentimes, it is intuitively useful to unravel the definition of derivative :eqref:`eq_der_def` again to see how a function changes when we make a small change of $x$:\n\n$$\\begin{aligned} \\frac{df}{dx}(x) = \\lim_{\\epsilon \\rightarrow 0}\\frac{f(x+\\epsilon) - f(x)}{\\epsilon} & \\implies \\frac{df}{dx}(x) \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon} \\\\ & \\implies \\epsilon \\frac{df}{dx}(x) \\approx f(x+\\epsilon) - f(x) \\\\ & \\implies f(x+\\epsilon) \\approx f(x) + \\epsilon \\frac{df}{dx}(x). \\end{aligned}$$\n:eqlabel:`eq_small_change`\n\nThe last equation is worth explicitly calling out. It tells us that if you take any function and change the input by a small amount, the output would change by that small amount scaled by the derivative. In this way, we can understand the derivative as the scaling factor that tells us how large of change we get in the output from a change in the input."
    },
    {
      "chunk_id": "3e125b4d89d3_0",
      "chapter": "single-variable-calculus",
      "heading": "Rules of Calculus",
      "text": ":label:`sec_derivative_table`\n\nWe now turn to the task of understanding how to compute the derivative of an explicit function.  A full formal treatment of calculus would derive everything from first principles.  We will not indulge in this temptation here, but rather provide an understanding of the common rules encountered."
    },
    {
      "chunk_id": "67b71db0c68c_0",
      "chapter": "single-variable-calculus",
      "heading": "Common Derivatives",
      "text": "As was seen in :numref:`sec_calculus`, when computing derivatives one can oftentimes use a series of rules to reduce the computation to a few core functions.  We repeat them here for ease of reference.\n\n* **Derivative of constants.** $\\frac{d}{dx}c = 0$.\n* **Derivative of linear functions.** $\\frac{d}{dx}(ax) = a$.\n* **Power rule.** $\\frac{d}{dx}x^n = nx^{n-1}$.\n* **Derivative of exponentials.** $\\frac{d}{dx}e^x = e^x$.\n* **Derivative of the logarithm.** $\\frac{d}{dx}\\log(x) = \\frac{1}{x}$."
    },
    {
      "chunk_id": "4059856c7cb9_0",
      "chapter": "single-variable-calculus",
      "heading": "Derivative Rules",
      "text": "If every derivative needed to be separately computed and stored in a table, differential calculus would be near impossible. It is a gift of mathematics that we can generalize the above derivatives and compute more complex derivatives like finding the derivative of $f(x) = \\log\\left(1+(x-1)^{10}\\right)$. As was mentioned in :numref:`sec_calculus`, the key to doing so is to codify what happens when we take functions and combine them in various ways, most importantly: sums, products, and compositions. * **Sum rule.** $\\frac{d}{dx}\\left(g(x) + h(x)\\right) = \\frac{dg}{dx}(x) + \\frac{dh}{dx}(x)$. * **Product rule.** $\\frac{d}{dx}\\left(g(x)\\cdot h(x)\\right) = g(x)\\frac{dh}{dx}(x) + \\frac{dg}{dx}(x)h(x)$. * **Chain rule.** $\\frac{d}{dx}g(h(x)) = \\frac{dg}{dh}(h(x))\\cdot \\frac{dh}{dx}(x)$. Let's see how we may use :eqref:`eq_small_change` to understand these rules. For the sum rule, consider following chain of reasoning:\n\n$$\n\\begin{aligned}\nf(x+\\epsilon) & = g(x+\\epsilon) + h(x+\\epsilon) \\\\\n& \\approx g(x) + \\epsilon \\frac{dg}{dx}(x) + h(x) + \\epsilon \\frac{dh}{dx}(x) \\\\\n& = g(x) + h(x) + \\epsilon\\left(\\frac{dg}{dx}(x) + \\frac{dh}{dx}(x)\\right) \\\\\n& = f(x) + \\epsilon\\left(\\frac{dg}{dx}(x) + \\frac{dh}{dx}(x)\\right). \\end{aligned}\n$$\n\nBy comparing this result with the fact that $f(x+\\epsilon) \\approx f(x) + \\epsilon \\frac{df}{dx}(x)$, we see that $\\frac{df}{dx}(x) = \\frac{dg}{dx}(x) + \\frac{dh}{dx}(x)$ as desired. The intuition here is: when we change the input $x$, $g$ and $h$ jointly contribute to the change of the output by $\\frac{dg}{dx}(x)$ and $\\frac{dh}{dx}(x)$. The product is more subtle, and will require a new observation about how to work with these expressions."
    },
    {
      "chunk_id": "4059856c7cb9_1",
      "chapter": "single-variable-calculus",
      "heading": "Derivative Rules",
      "text": "The product is more subtle, and will require a new observation about how to work with these expressions. We will begin as before using :eqref:`eq_small_change`:\n\n$$\n\\begin{aligned}\nf(x+\\epsilon) & = g(x+\\epsilon)\\cdot h(x+\\epsilon) \\\\\n& \\approx \\left(g(x) + \\epsilon \\frac{dg}{dx}(x)\\right)\\cdot\\left(h(x) + \\epsilon \\frac{dh}{dx}(x)\\right) \\\\\n& = g(x)\\cdot h(x) + \\epsilon\\left(g(x)\\frac{dh}{dx}(x) + \\frac{dg}{dx}(x)h(x)\\right) + \\epsilon^2\\frac{dg}{dx}(x)\\frac{dh}{dx}(x) \\\\\n& = f(x) + \\epsilon\\left(g(x)\\frac{dh}{dx}(x) + \\frac{dg}{dx}(x)h(x)\\right) + \\epsilon^2\\frac{dg}{dx}(x)\\frac{dh}{dx}(x). \\\\\n\\end{aligned}\n$$\n\n\nThis resembles the computation done above, and indeed we see our answer ($\\frac{df}{dx}(x) = g(x)\\frac{dh}{dx}(x) + \\frac{dg}{dx}(x)h(x)$) sitting next to $\\epsilon$, but there is the issue of that term of size $\\epsilon^{2}$. We will refer to this as a *higher-order term*, since the power of $\\epsilon^2$ is higher than the power of $\\epsilon^1$. We will see in a later section that we will sometimes want to keep track of these, however for now observe that if $\\epsilon = 0.0000001$, then $\\epsilon^{2}= 0.0000000000001$, which is vastly smaller. As we send $\\epsilon \\rightarrow 0$, we may safely ignore the higher order terms. As a general convention in this appendix, we will use \"$\\approx$\" to denote that the two terms are equal up to higher order terms. However, if we wish to be more formal we may examine the difference quotient\n\n$$\n\\frac{f(x+\\epsilon) - f(x)}{\\epsilon} = g(x)\\frac{dh}{dx}(x) + \\frac{dg}{dx}(x)h(x) + \\epsilon \\frac{dg}{dx}(x)\\frac{dh}{dx}(x),\n$$\n\nand see that as we send $\\epsilon \\rightarrow 0$, the right hand term goes to zero as well."
    },
    {
      "chunk_id": "4059856c7cb9_2",
      "chapter": "single-variable-calculus",
      "heading": "Derivative Rules",
      "text": "Finally, with the chain rule, we can again progress as before using :eqref:`eq_small_change` and see that\n\n$$\n\\begin{aligned}\nf(x+\\epsilon) & = g(h(x+\\epsilon)) \\\\\n& \\approx g\\left(h(x) + \\epsilon \\frac{dh}{dx}(x)\\right) \\\\\n& \\approx g(h(x)) + \\epsilon \\frac{dh}{dx}(x) \\frac{dg}{dh}(h(x))\\\\\n& = f(x) + \\epsilon \\frac{dg}{dh}(h(x))\\frac{dh}{dx}(x),\n\\end{aligned}\n$$\n\nwhere in the second line we view the function $g$ as having its input ($h(x)$) shifted by the tiny quantity $\\epsilon \\frac{dh}{dx}(x)$. These rule provide us with a flexible set of tools to compute essentially any expression desired. For instance,\n\n$$\n\\begin{aligned}\n\\frac{d}{dx}\\left[\\log\\left(1+(x-1)^{10}\\right)\\right] & = \\left(1+(x-1)^{10}\\right)^{-1}\\frac{d}{dx}\\left[1+(x-1)^{10}\\right]\\\\\n& = \\left(1+(x-1)^{10}\\right)^{-1}\\left(\\frac{d}{dx}[1] + \\frac{d}{dx}[(x-1)^{10}]\\right) \\\\\n& = \\left(1+(x-1)^{10}\\right)^{-1}\\left(0 + 10(x-1)^9\\frac{d}{dx}[x-1]\\right) \\\\\n& = 10\\left(1+(x-1)^{10}\\right)^{-1}(x-1)^9 \\\\\n& = \\frac{10(x-1)^9}{1+(x-1)^{10}}. \\end{aligned}\n$$\n\nWhere each line has used the following rules:\n\n1. The chain rule and derivative of logarithm. 2. The sum rule. 3. The derivative of constants, chain rule, and power rule. 4. The sum rule, derivative of linear functions, derivative of constants. Two things should be clear after doing this example:\n\n1. Any function we can write down using sums, products, constants, powers, exponentials, and logarithms can have its derivative computed mechanically by following these rules. 2. Having a human follow these rules can be tedious and error prone! Thankfully, these two facts together hint towards a way forward: this is a perfect candidate for mechanization! Indeed backpropagation, which we will revisit later in this section, is exactly that."
    },
    {
      "chunk_id": "cb97ee2b90d7_0",
      "chapter": "single-variable-calculus",
      "heading": "Linear Approximation",
      "text": "When working with derivatives, it is often useful to geometrically interpret the approximation used above.  In particular, note that the equation\n\n$$\nf(x+\\epsilon) \\approx f(x) + \\epsilon \\frac{df}{dx}(x),\n$$\n\napproximates the value of $f$ by a line which passes through the point $(x, f(x))$ and has slope $\\frac{df}{dx}(x)$.  In this way we say that the derivative gives a linear approximation to the function $f$, as illustrated below:\n\n```{.python .input}\n#@tab mxnet\n# Compute sin\nxs = np.arange(-np.pi, np.pi, 0.01)\nplots = [np.sin(xs)]\n\n# Compute some linear approximations. Use d(sin(x)) / dx = cos(x)\nfor x0 in [-1.5, 0, 2]:\n    plots.append(np.sin(x0) + (xs - x0) * np.cos(x0))\n\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute sin\nxs = torch.arange(-torch.pi, torch.pi, 0.01)\nplots = [torch.sin(xs)]\n\n# Compute some linear approximations. Use d(sin(x))/dx = cos(x)\nfor x0 in [-1.5, 0.0, 2.0]:\n    plots.append(torch.sin(torch.tensor(x0)) + (xs - x0) *\n                 torch.cos(torch.tensor(x0)))\n\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\n```\n\n```{.python .input}\n#@tab tensorflow\n# Compute sin\nxs = tf.range(-tf.pi, tf.pi, 0.01)\nplots = [tf.sin(xs)]\n\n# Compute some linear approximations. Use d(sin(x))/dx = cos(x)\nfor x0 in [-1.5, 0.0, 2.0]:\n    plots.append(tf.sin(tf.constant(x0)) + (xs - x0) *\n                 tf.cos(tf.constant(x0)))\n\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\n```"
    },
    {
      "chunk_id": "8292cff73530_0",
      "chapter": "single-variable-calculus",
      "heading": "Higher Order Derivatives",
      "text": "Let's now do something that may on the surface seem strange. Take a function $f$ and compute the derivative $\\frac{df}{dx}$. This gives us the rate of change of $f$ at any point. However, the derivative, $\\frac{df}{dx}$, can be viewed as a function itself, so nothing stops us from computing the derivative of $\\frac{df}{dx}$ to get $\\frac{d^2f}{dx^2} = \\frac{df}{dx}\\left(\\frac{df}{dx}\\right)$. We will call this the second derivative of $f$. This function is the rate of change of the rate of change of $f$, or in other words, how the rate of change is changing. We may apply the derivative any number of times to obtain what is called the $n$-th derivative. To keep the notation clean, we will denote the $n$-th derivative as\n\n$$\nf^{(n)}(x) = \\frac{d^{n}f}{dx^{n}} = \\left(\\frac{d}{dx}\\right)^{n} f. $$\n\nLet's try to understand *why* this is a useful notion. Below, we visualize $f^{(2)}(x)$, $f^{(1)}(x)$, and $f(x)$. First, consider the case that the second derivative $f^{(2)}(x)$ is a positive constant. This means that the slope of the first derivative is positive. As a result, the first derivative $f^{(1)}(x)$ may start out negative, becomes zero at a point, and then becomes positive in the end. This tells us the slope of our original function $f$ and therefore, the function $f$ itself decreases, flattens out, then increases. In other words, the function $f$ curves up, and has a single minimum as is shown in :numref:`fig_positive-second`. ![If we assume the second derivative is a positive constant, then the fist derivative in increasing, which implies the function itself has a minimum.](../img/posSecDer.svg)\n:label:`fig_positive-second`\n\n\nSecond, if the second derivative is a negative constant, that means that the first derivative is decreasing. This implies the first derivative may start out positive, becomes zero at a point, and then becomes negative. Hence, the function $f$ itself increases, flattens out, then decreases."
    },
    {
      "chunk_id": "8292cff73530_1",
      "chapter": "single-variable-calculus",
      "heading": "Higher Order Derivatives",
      "text": "This implies the first derivative may start out positive, becomes zero at a point, and then becomes negative. Hence, the function $f$ itself increases, flattens out, then decreases. In other words, the function $f$ curves down, and has a single maximum as is shown in :numref:`fig_negative-second`. ![If we assume the second derivative is a negative constant, then the fist derivative in decreasing, which implies the function itself has a maximum.](../img/negSecDer.svg)\n:label:`fig_negative-second`\n\n\nThird, if the second derivative is a always zero, then the first derivative will never change---it is constant! This means that $f$ increases (or decreases) at a fixed rate, and $f$ is itself a straight line  as is shown in :numref:`fig_zero-second`. ![If we assume the second derivative is zero, then the fist derivative is constant, which implies the function itself is a straight line.](../img/zeroSecDer.svg)\n:label:`fig_zero-second`\n\nTo summarize, the second derivative can be interpreted as describing the way that the function $f$ curves. A positive second derivative leads to a upwards curve, while a negative second derivative means that $f$ curves downwards, and a zero second derivative means that $f$ does not curve at all. Let's take this one step further. Consider the function $g(x) = ax^{2}+ bx + c$. We can then compute that\n\n$$\n\\begin{aligned}\n\\frac{dg}{dx}(x) & = 2ax + b \\\\\n\\frac{d^2g}{dx^2}(x) & = 2a. \\end{aligned}\n$$\n\nIf we have some original function $f(x)$ in mind, we may compute the first two derivatives and find the values for $a, b$, and $c$ that make them match this computation. Similarly to the previous section where we saw that the first derivative gave the best approximation with a straight line, this construction provides the best approximation by a quadratic. Let's visualize this for $f(x) = \\sin(x)$. ```{.python .input}\n#@tab mxnet\n# Compute sin\nxs = np.arange(-np.pi, np.pi, 0.01)\nplots = [np.sin(xs)]\n\n# Compute some quadratic approximations."
    },
    {
      "chunk_id": "8292cff73530_2",
      "chapter": "single-variable-calculus",
      "heading": "Higher Order Derivatives",
      "text": "Let's visualize this for $f(x) = \\sin(x)$. ```{.python .input}\n#@tab mxnet\n# Compute sin\nxs = np.arange(-np.pi, np.pi, 0.01)\nplots = [np.sin(xs)]\n\n# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)\nfor x0 in [-1.5, 0, 2]:\n    plots.append(np.sin(x0) + (xs - x0) * np.cos(x0) -\n                              (xs - x0)**2 * np.sin(x0) / 2)\n\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute sin\nxs = torch.arange(-torch.pi, torch.pi, 0.01)\nplots = [torch.sin(xs)]\n\n# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)\nfor x0 in [-1.5, 0.0, 2.0]:\n    plots.append(torch.sin(torch.tensor(x0)) + (xs - x0) *\n                 torch.cos(torch.tensor(x0)) - (xs - x0)**2 *\n                 torch.sin(torch.tensor(x0)) / 2)\n\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\n```\n\n```{.python .input}\n#@tab tensorflow\n# Compute sin\nxs = tf.range(-tf.pi, tf.pi, 0.01)\nplots = [tf.sin(xs)]\n\n# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)\nfor x0 in [-1.5, 0.0, 2.0]:\n    plots.append(tf.sin(tf.constant(x0)) + (xs - x0) *\n                 tf.cos(tf.constant(x0)) - (xs - x0)**2 *\n                 tf.sin(tf.constant(x0)) / 2)\n\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\n```\n\nWe will extend this idea to the idea of a *Taylor series* in the next section."
    },
    {
      "chunk_id": "0edf74e49434_0",
      "chapter": "single-variable-calculus",
      "heading": "Taylor Series",
      "text": "The *Taylor series* provides a method to approximate the function $f(x)$ if we are given values for the first $n$ derivatives at a point $x_0$, i.e., $\\left\\{ f(x_0), f^{(1)}(x_0), f^{(2)}(x_0), \\ldots, f^{(n)}(x_0) \\right\\}$. The idea will be to find a degree $n$ polynomial that matches all the given derivatives at $x_0$. We saw the case of $n=2$ in the previous section and a little algebra shows this is\n\n$$\nf(x) \\approx \\frac{1}{2}\\frac{d^2f}{dx^2}(x_0)(x-x_0)^{2}+ \\frac{df}{dx}(x_0)(x-x_0) + f(x_0). $$\n\nAs we can see above, the denominator of $2$ is there to cancel out the $2$ we get when we take two derivatives of $x^2$, while the other terms are all zero. Same logic applies for the first derivative and the value itself. If we push the logic further to $n=3$, we will conclude that\n\n$$\nf(x) \\approx \\frac{\\frac{d^3f}{dx^3}(x_0)}{6}(x-x_0)^3 + \\frac{\\frac{d^2f}{dx^2}(x_0)}{2}(x-x_0)^{2}+ \\frac{df}{dx}(x_0)(x-x_0) + f(x_0). $$\n\nwhere the $6 = 3 \\times 2 = 3!$ comes from the constant we get in front if we take three derivatives of $x^3$. Furthermore, we can get a degree $n$ polynomial by\n\n$$\nP_n(x) = \\sum_{i = 0}^{n} \\frac{f^{(i)}(x_0)}{i!}(x-x_0)^{i}. $$\n\nwhere the notation\n\n$$\nf^{(n)}(x) = \\frac{d^{n}f}{dx^{n}} = \\left(\\frac{d}{dx}\\right)^{n} f. $$\n\n\nIndeed, $P_n(x)$ can be viewed as the best $n$-th degree polynomial approximation to our function $f(x)$. While we are not going to dive all the way into the error of the above approximations, it is worth mentioning the infinite limit. In this case, for well behaved functions (known as real analytic functions) like $\\cos(x)$ or $e^{x}$, we can write out the infinite number of terms and approximate the exactly same function\n\n$$\nf(x) = \\sum_{n = 0}^\\infty \\frac{f^{(n)}(x_0)}{n!}(x-x_0)^{n}. $$\n\nTake $f(x) = e^{x}$ as am example. Since $e^{x}$ is its own derivative, we know that $f^{(n)}(x) = e^{x}$."
    },
    {
      "chunk_id": "0edf74e49434_1",
      "chapter": "single-variable-calculus",
      "heading": "Taylor Series",
      "text": "$$\n\nTake $f(x) = e^{x}$ as am example. Since $e^{x}$ is its own derivative, we know that $f^{(n)}(x) = e^{x}$. Therefore, $e^{x}$ can be reconstructed by taking the Taylor series at $x_0 = 0$, i.e.,\n\n$$\ne^{x} = \\sum_{n = 0}^\\infty \\frac{x^{n}}{n!} = 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6} + \\cdots. $$\n\nLet's see how this works in code and observe how increasing the degree of the Taylor approximation brings us closer to the desired function $e^x$. ```{.python .input}\n#@tab mxnet\n# Compute the exponential function\nxs = np.arange(0, 3, 0.01)\nys = np.exp(xs)\n\n# Compute a few Taylor series approximations\nP1 = 1 + xs\nP2 = 1 + xs + xs**2 / 2\nP5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120\n\nd2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[\n    \"Exponential\", \"Degree 1 Taylor Series\", \"Degree 2 Taylor Series\",\n    \"Degree 5 Taylor Series\"])\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute the exponential function\nxs = torch.arange(0, 3, 0.01)\nys = torch.exp(xs)\n\n# Compute a few Taylor series approximations\nP1 = 1 + xs\nP2 = 1 + xs + xs**2 / 2\nP5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120\n\nd2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[\n    \"Exponential\", \"Degree 1 Taylor Series\", \"Degree 2 Taylor Series\",\n    \"Degree 5 Taylor Series\"])\n```\n\n```{.python .input}\n#@tab tensorflow\n# Compute the exponential function\nxs = tf.range(0, 3, 0.01)\nys = tf.exp(xs)\n\n# Compute a few Taylor series approximations\nP1 = 1 + xs\nP2 = 1 + xs + xs**2 / 2\nP5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120\n\nd2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[\n    \"Exponential\", \"Degree 1 Taylor Series\", \"Degree 2 Taylor Series\",\n    \"Degree 5 Taylor Series\"])\n```\n\nTaylor series have two primary applications:\n\n1. *Theoretical applications*: Often when we try to understand a too complex function, using Taylor series enables us to turn it into a polynomial that we can work with directly. 2. *Numerical applications*: Some functions like $e^{x}$ or $\\cos(x)$ are  difficult for machines to compute."
    },
    {
      "chunk_id": "0edf74e49434_2",
      "chapter": "single-variable-calculus",
      "heading": "Taylor Series",
      "text": "2. *Numerical applications*: Some functions like $e^{x}$ or $\\cos(x)$ are  difficult for machines to compute. They can store tables of values at a fixed precision (and this is often done), but it still leaves open questions like \"What is the 1000-th digit of $\\cos(1)$?\"  Taylor series are often helpful to answer such questions."
    },
    {
      "chunk_id": "c4a71ec350d4_0",
      "chapter": "single-variable-calculus",
      "heading": "Summary",
      "text": "* Derivatives can be used to express how functions change when we change the input by a small amount.\n* Elementary derivatives can be combined using derivative rules to create arbitrarily complex derivatives.\n* Derivatives can be iterated to get second or higher order derivatives.  Each increase in order provides more fine grained information on the behavior of the function.\n* Using information in the derivatives of a single data example, we can approximate well behaved functions by polynomials obtained from the Taylor series."
    },
    {
      "chunk_id": "cc802a9a71a3_0",
      "chapter": "single-variable-calculus",
      "heading": "Exercises",
      "text": "1. What is the derivative of $x^3-4x+1$?\n2. What is the derivative of $\\log(\\frac{1}{x})$?\n3. True or False: If $f'(x) = 0$ then $f$ has a maximum or minimum at $x$?\n4. Where is the minimum of $f(x) = x\\log(x)$ for $x\\ge0$ (where we assume that $f$ takes the limiting value of $0$ at $f(0)$)?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/412)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1088)\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1089)\n:end_tab:"
    },
    {
      "chunk_id": "4ba0a1fd8b08_0",
      "chapter": "statistics",
      "heading": "statistics",
      "text": "# Statistics\n:label:`sec_statistics`\n\nUndoubtedly, to be a top deep learning practitioner, the ability to train the state-of-the-art and high accurate models is crucial. However, it is often unclear when improvements are significant, or only the result of random fluctuations in the training process. To be able to discuss uncertainty in estimated values, we must learn some statistics. The earliest reference of *statistics* can be traced back to an Arab scholar Al-Kindi in the $9^{\\textrm{th}}$-century, who gave a detailed description of how to use statistics and frequency analysis to decipher encrypted messages. After 800 years, the modern statistics arose from Germany in 1700s, when the researchers focused on the demographic and economic data collection and analysis. Today, statistics is the science subject that concerns the collection, processing, analysis, interpretation and visualization of data. What is more, the core theory of statistics has been widely used in the research within academia, industry, and government. More specifically, statistics can be divided to *descriptive statistics* and *statistical inference*. The former focus on summarizing and illustrating the features of a collection of observed data, which is referred to as a *sample*. The sample is drawn from a *population*, denotes the total set of similar individuals, items, or events of our experiment interests. Contrary to descriptive statistics, *statistical inference* further deduces the characteristics of a population from the given *samples*, based on the assumptions that the sample distribution can replicate the population distribution at some degree. You may wonder: \u201cWhat is the essential difference between machine learning and statistics?\u201d Fundamentally speaking, statistics focuses on the inference problem. This type of problems includes modeling the relationship between the variables, such as causal inference, and testing the statistically significance of model parameters, such as A/B testing."
    },
    {
      "chunk_id": "4ba0a1fd8b08_1",
      "chapter": "statistics",
      "heading": "statistics",
      "text": "This type of problems includes modeling the relationship between the variables, such as causal inference, and testing the statistically significance of model parameters, such as A/B testing. In contrast, machine learning emphasizes on making accurate predictions, without explicitly programming and understanding each parameter's functionality. In this section, we will introduce three types of statistics inference methods: evaluating and comparing estimators, conducting hypothesis tests, and constructing confidence intervals. These methods can help us infer the characteristics of a given population, i.e., the true parameter $\\theta$. For brevity, we assume that the true parameter $\\theta$ of a given population is a scalar value. It is straightforward to extend to the case where $\\theta$ is a vector or a tensor, thus we omit it in our discussion."
    },
    {
      "chunk_id": "b85659c25510_0",
      "chapter": "statistics",
      "heading": "Evaluating and Comparing Estimators",
      "text": "In statistics, an *estimator* is a function of given samples used to estimate the true parameter $\\theta$. We will write $\\hat{\\theta}_n = \\hat{f}(x_1, \\ldots, x_n)$ for the estimate of $\\theta$ after observing the samples {$x_1, x_2, \\ldots, x_n$}. We have seen simple examples of estimators before in section :numref:`sec_maximum_likelihood`. If you have a number of samples from a Bernoulli random variable, then the maximum likelihood estimate for the probability the random variable is one can be obtained by counting the number of ones observed and dividing by the total number of samples. Similarly, an exercise asked you to show that the maximum likelihood estimate of the mean of a Gaussian given a number of samples is given by the average value of all the samples. These estimators will almost never give the true value of the parameter, but ideally for a large number of samples the estimate will be close. As an example, we show below the true density of a Gaussian random variable with mean zero and variance one, along with a collection samples from that Gaussian. We constructed the $y$ coordinate so every point is visible and the relationship to the original density is clearer."
    },
    {
      "chunk_id": "b85659c25510_1",
      "chapter": "statistics",
      "heading": "Evaluating and Comparing Estimators",
      "text": "We constructed the $y$ coordinate so every point is visible and the relationship to the original density is clearer. ```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nimport random\nnpx.set_np()\n\n# Sample datapoints and create y coordinate\nepsilon = 0.1\nrandom.seed(8675309)\nxs = np.random.normal(loc=0, scale=1, size=(300,))\n\nys = [np.sum(np.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2))\n             / np.sqrt(2*np.pi*epsilon**2)) / len(xs) for i in range(len(xs))]\n\n# Compute true density\nxd = np.arange(np.min(xs), np.max(xs), 0.01)\nyd = np.exp(-xd**2/2) / np.sqrt(2 * np.pi)\n\n# Plot the results\nd2l.plot(xd, yd, 'x', 'density')\nd2l.plt.scatter(xs, ys)\nd2l.plt.axvline(x=0)\nd2l.plt.axvline(x=np.mean(xs), linestyle='--', color='purple')\nd2l.plt.title(f'sample mean: {float(np.mean(xs)):.2f}')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\n\ntorch.pi = torch.acos(torch.zeros(1)) * 2  #define pi in torch\n\n# Sample datapoints and create y coordinate\nepsilon = 0.1\ntorch.manual_seed(8675309)\nxs = torch.randn(size=(300,))\n\nys = torch.tensor(\n    [torch.sum(torch.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2))\\\n               / torch.sqrt(2*torch.pi*epsilon**2)) / len(xs)\\\n     for i in range(len(xs))])\n\n# Compute true density\nxd = torch.arange(torch.min(xs), torch.max(xs), 0.01)\nyd = torch.exp(-xd**2/2) / torch.sqrt(2 * torch.pi)\n\n# Plot the results\nd2l.plot(xd, yd, 'x', 'density')\nd2l.plt.scatter(xs, ys)\nd2l.plt.axvline(x=0)\nd2l.plt.axvline(x=torch.mean(xs), linestyle='--', color='purple')\nd2l.plt.title(f'sample mean: {float(torch.mean(xs).item()):.2f}')\nd2l.plt.show()\n```\n\n```{.python .input}\n#@tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n\ntf.pi = tf.acos(tf.zeros(1)) * 2  # define pi in TensorFlow\n\n# Sample datapoints and create y coordinate\nepsilon = 0.1\nxs = tf.random.normal((300,))\n\nys = tf.constant(\n    [(tf.reduce_sum(tf.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2)) \\\n               / tf.sqrt(2*tf.pi*epsilon**2)) / tf.cast(\n        tf.size(xs), dtype=tf.float32)).numpy() \\\n     for i in range(tf.size(xs))])\n\n# Compute true density\nxd = tf.range(tf.reduce_min(xs), tf.reduce_max(xs), 0.01)\nyd = tf.exp(-xd**2/2) / tf.sqrt(2 * tf.pi)\n\n# Plot the results\nd2l.plot(xd, yd, 'x', 'density')\nd2l.plt.scatter(xs, ys)\nd2l.plt.axvline(x=0)\nd2l.plt.axvline(x=tf.reduce_mean(xs), linestyle='--', color='purple')\nd2l.plt.title(f'sample mean: {float(tf.reduce_mean(xs).numpy()):.2f}')\nd2l.plt.show()\n```\n\nThere can be many ways to compute an estimator of a parameter $\\hat{\\theta}_n$."
    },
    {
      "chunk_id": "b85659c25510_2",
      "chapter": "statistics",
      "heading": "Evaluating and Comparing Estimators",
      "text": "In this section, we introduce three common methods to evaluate and compare estimators: the mean squared error, the standard deviation, and statistical bias."
    },
    {
      "chunk_id": "40f32cb705a2_0",
      "chapter": "statistics",
      "heading": "Mean Squared Error",
      "text": "Perhaps the simplest metric used to evaluate estimators is the *mean squared error (MSE)* (or $l_2$ loss) estimator which can be defined as\n\n$$\\textrm{MSE} (\\hat{\\theta}_n, \\theta) = E[(\\hat{\\theta}_n - \\theta)^2].$$\n:eqlabel:`eq_mse_est`\n\nThis allows us to quantify the average squared deviation from the true value.  MSE is always non-negative. If you have read :numref:`sec_linear_regression`, you will recognize it as the most commonly used regression loss function. As a measure to evaluate an estimator, the closer its value is to zero, the closer the estimator is to the true parameter $\\theta$."
    },
    {
      "chunk_id": "3571c79089f6_0",
      "chapter": "statistics",
      "heading": "Statistical Bias",
      "text": "The MSE provides a natural metric, but we can easily imagine multiple different phenomena that might make it large.  Two fundamentally important are fluctuation in the estimator due to randomness in the dataset, and systematic error in the estimator due to the estimation procedure.\n\n\nFirst, let's measure the systematic error. For an estimator $\\hat{\\theta}_n$, the mathematical illustration of *statistical bias* can be defined as\n\n$$\\textrm{bias}(\\hat{\\theta}_n) = E(\\hat{\\theta}_n - \\theta) = E(\\hat{\\theta}_n) - \\theta.$$\n:eqlabel:`eq_bias`\n\nNote that when $\\textrm{bias}(\\hat{\\theta}_n) = 0$, the expectation of the estimator $\\hat{\\theta}_n$ is equal to the true value of parameter.  In this case, we say $\\hat{\\theta}_n$ is an unbiased estimator.  In general, an unbiased estimator is better than a biased estimator since its expectation is the same as the true parameter.\n\n\nIt is worth being aware, however, that biased estimators are frequently used in practice.  There are cases where unbiased estimators do not exist without further assumptions, or are intractable to compute.  This may seem like a significant flaw in an estimator, however the majority of estimators encountered in practice are at least asymptotically unbiased in the sense that the bias tends to zero as the number of available samples tends to infinity: $\\lim_{n \\rightarrow \\infty} \\textrm{bias}(\\hat{\\theta}_n) = 0$."
    },
    {
      "chunk_id": "35dd01d01d0a_0",
      "chapter": "statistics",
      "heading": "Variance and Standard Deviation",
      "text": "Second, let's measure the randomness in the estimator.  Recall from :numref:`sec_random_variables`, the *standard deviation* (or *standard error*) is defined as the squared root of the variance.  We may measure the degree of fluctuation of an estimator by measuring the standard deviation or variance of that estimator.\n\n$$\\sigma_{\\hat{\\theta}_n} = \\sqrt{\\textrm{Var} (\\hat{\\theta}_n )} = \\sqrt{E[(\\hat{\\theta}_n - E(\\hat{\\theta}_n))^2]}.$$\n:eqlabel:`eq_var_est`\n\nIt is important to compare :eqref:`eq_var_est` to :eqref:`eq_mse_est`.  In this equation we do not compare to the true population value $\\theta$, but instead to $E(\\hat{\\theta}_n)$, the expected sample mean.  Thus we are not measuring how far the estimator tends to be from the true value, but instead we are measuring the fluctuation of the estimator itself."
    },
    {
      "chunk_id": "a67434264d66_0",
      "chapter": "statistics",
      "heading": "The Bias-Variance Trade-off",
      "text": "It is intuitively clear that these two main components contribute to the mean squared error.  What is somewhat shocking is that we can show that this is actually a *decomposition* of the mean squared error into these two contributions plus a third one. That is to say that we can write the mean squared error as the sum of the square of the bias, the variance and the irreducible error.\n\n$$\n\\begin{aligned}\n\\textrm{MSE} (\\hat{\\theta}_n, \\theta) &= E[(\\hat{\\theta}_n - \\theta)^2] \\\\\n &= E[(\\hat{\\theta}_n)^2] + E[\\theta^2] - 2E[\\hat{\\theta}_n\\theta] \\\\\n &= \\textrm{Var} [\\hat{\\theta}_n] + E[\\hat{\\theta}_n]^2 + \\textrm{Var} [\\theta] + E[\\theta]^2 - 2E[\\hat{\\theta}_n]E[\\theta] \\\\\n &= (E[\\hat{\\theta}_n] - E[\\theta])^2 + \\textrm{Var} [\\hat{\\theta}_n] + \\textrm{Var} [\\theta] \\\\\n &= (E[\\hat{\\theta}_n - \\theta])^2 + \\textrm{Var} [\\hat{\\theta}_n] + \\textrm{Var} [\\theta] \\\\\n &= (\\textrm{bias} [\\hat{\\theta}_n])^2 + \\textrm{Var} (\\hat{\\theta}_n) + \\textrm{Var} [\\theta].\\\\\n\\end{aligned}\n$$\n\nWe refer the above formula as *bias-variance trade-off*. The mean squared error can be divided into three sources of error: the error from high bias, the error from high variance and the irreducible error. The bias error is commonly seen in a simple model (such as a linear regression model), which cannot extract high dimensional relations between the features and the outputs. If a model suffers from high bias error, we often say it is *underfitting* or lack of *flexibility* as introduced in (:numref:`sec_generalization_basics`). The high variance usually results from a too complex model, which overfits the training data. As a result, an *overfitting* model is sensitive to small fluctuations in the data. If a model suffers from high variance, we often say it is *overfitting* and lack of *generalization* as introduced in (:numref:`sec_generalization_basics`). The irreducible error is the result from noise in the $\\theta$ itself."
    },
    {
      "chunk_id": "2df8c7b5723d_0",
      "chapter": "statistics",
      "heading": "Evaluating Estimators in Code",
      "text": "Since the standard deviation of an estimator has been implementing by simply calling `a.std()` for a tensor `a`, we will skip it but implement the statistical bias and the mean squared error. ```{.python .input}\n#@tab mxnet\n# Statistical bias\ndef stat_bias(true_theta, est_theta):\n    return(np.mean(est_theta) - true_theta)\n\n# Mean squared error\ndef mse(data, true_theta):\n    return(np.mean(np.square(data - true_theta)))\n```\n\n```{.python .input}\n#@tab pytorch\n# Statistical bias\ndef stat_bias(true_theta, est_theta):\n    return(torch.mean(est_theta) - true_theta)\n\n# Mean squared error\ndef mse(data, true_theta):\n    return(torch.mean(torch.square(data - true_theta)))\n```\n\n```{.python .input}\n#@tab tensorflow\n# Statistical bias\ndef stat_bias(true_theta, est_theta):\n    return(tf.reduce_mean(est_theta) - true_theta)\n\n# Mean squared error\ndef mse(data, true_theta):\n    return(tf.reduce_mean(tf.square(data - true_theta)))\n```\n\nTo illustrate the equation of the bias-variance trade-off, let's simulate of normal distribution $\\mathcal{N}(\\theta, \\sigma^2)$ with $10,000$ samples. Here, we use a $\\theta = 1$ and $\\sigma = 4$. As the estimator is a function of the given samples, here we use the mean of the samples as an estimator for true $\\theta$ in this normal distribution $\\mathcal{N}(\\theta, \\sigma^2)$ . ```{.python .input}\n#@tab mxnet\ntheta_true = 1\nsigma = 4\nsample_len = 10000\nsamples = np.random.normal(theta_true, sigma, sample_len)\ntheta_est = np.mean(samples)\ntheta_est\n```\n\n```{.python .input}\n#@tab pytorch\ntheta_true = 1\nsigma = 4\nsample_len = 10000\nsamples = torch.normal(theta_true, sigma, size=(sample_len, 1))\ntheta_est = torch.mean(samples)\ntheta_est\n```\n\n```{.python .input}\n#@tab tensorflow\ntheta_true = 1\nsigma = 4\nsample_len = 10000\nsamples = tf.random.normal((sample_len, 1), theta_true, sigma)\ntheta_est = tf.reduce_mean(samples)\ntheta_est\n```\n\nLet's validate the trade-off equation by calculating the summation of the squared bias and the variance of our estimator. First, calculate the MSE of our estimator."
    },
    {
      "chunk_id": "2df8c7b5723d_1",
      "chapter": "statistics",
      "heading": "Evaluating Estimators in Code",
      "text": "First, calculate the MSE of our estimator. ```{.python .input}\n#@tab all\nmse(samples, theta_true)\n```\n\nNext, we calculate $\\textrm{Var} (\\hat{\\theta}_n) + [\\textrm{bias} (\\hat{\\theta}_n)]^2$ as below. As you can see, the two values agree to numerical precision. ```{.python .input}\n#@tab mxnet\nbias = stat_bias(theta_true, theta_est)\nnp.square(samples.std()) + np.square(bias)\n```\n\n```{.python .input}\n#@tab pytorch\nbias = stat_bias(theta_true, theta_est)\ntorch.square(samples.std(unbiased=False)) + torch.square(bias)\n```\n\n```{.python .input}\n#@tab tensorflow\nbias = stat_bias(theta_true, theta_est)\ntf.square(tf.math.reduce_std(samples)) + tf.square(bias)\n```"
    },
    {
      "chunk_id": "d8350b91f441_0",
      "chapter": "statistics",
      "heading": "Conducting Hypothesis Tests",
      "text": "The most commonly encountered topic in statistical inference is hypothesis testing. While hypothesis testing was popularized in the early $20^{th}$ century, the first use can be traced back to John Arbuthnot in the 1700s. John tracked 80-year birth records in London and concluded that more men were born than women each year. Following that, the modern significance testing is the intelligence heritage by Karl Pearson who invented $p$-value and Pearson's chi-squared test, William Gosset who is the father of Student's t-distribution, and Ronald Fisher who initialed the null hypothesis and the significance test. A *hypothesis test* is a way of evaluating some evidence against the default statement about a population. We refer the default statement as the *null hypothesis* $H_0$, which we try to reject using the observed data. Here, we use $H_0$ as a starting point for the statistical significance testing. The *alternative hypothesis* $H_A$ (or $H_1$) is a statement that is contrary to the null hypothesis. A null hypothesis is often stated in a declarative form which posits a relationship between variables. It should reflect the brief as explicit as possible, and be testable by statistics theory. Imagine you are a chemist. After spending thousands of hours in the lab, you develop a new medicine which can dramatically improve one's ability to understand math. To show its magic power, you need to test it. Naturally, you may need some volunteers to take the medicine and see whether it can help them learn mathematics better. How do you get started? First, you will need carefully random selected two groups of volunteers, so that there is no difference between their mathematical understanding ability measured by some metrics. The two groups are commonly referred to as the test group and the control group."
    },
    {
      "chunk_id": "d8350b91f441_1",
      "chapter": "statistics",
      "heading": "Conducting Hypothesis Tests",
      "text": "The two groups are commonly referred to as the test group and the control group. The *test group* (or *treatment group*) is a group of individuals who will experience the medicine, while the *control group* represents the group of users who are set aside as a benchmark, i.e., identical environment setups except taking this medicine. In this way, the influence of all the variables are minimized, except the impact of the independent variable in the treatment. Second, after a period of taking the medicine, you will need to measure the two groups' mathematical understanding by the same metrics, such as letting the volunteers do the same tests after learning a new mathematical formula. Then, you can collect their performance and compare the results. In this case, our null hypothesis will be that there is no difference between the two groups, and our alternate will be that there is. This is still not fully formal. There are many details you have to think of carefully. For example, what is the suitable metrics to test their mathematical understanding ability? How many volunteers for your test so you can be confident to claim the effectiveness of your medicine? How long should you run the test? How do you decide if there is a difference between the two groups? Do you care about the average performance only, or also the range of variation of the scores? And so on. In this way, hypothesis testing provides a framework for experimental design and reasoning about certainty in observed results. If we can now show that the null hypothesis is very unlikely to be true, we may reject it with confidence. To complete the story of how to work with hypothesis testing, we need to now introduce some additional terminology and make some of our concepts above formal."
    },
    {
      "chunk_id": "ce1daf8dc5b0_0",
      "chapter": "statistics",
      "heading": "Statistical Significance",
      "text": "The *statistical significance* measures the probability of erroneously rejecting the null hypothesis, $H_0$, when it should not be rejected, i.e.,\n\n$$ \\textrm{statistical significance }= 1 - \\alpha = 1 - P(\\textrm{reject } H_0 \\mid H_0 \\textrm{ is true} ).$$\n\nIt is also referred to as the *type I error* or *false positive*. The $\\alpha$, is called as the *significance level* and its commonly used value is $5\\%$, i.e., $1-\\alpha = 95\\%$. The significance level can be explained as the level of risk that we are willing to take, when we reject a true null hypothesis.\n\n:numref:`fig_statistical_significance` shows the observations' values and probability of a given normal distribution in a two-sample hypothesis test. If the observation data example is located outsides the $95\\%$ threshold, it will be a very unlikely observation under the null hypothesis assumption. Hence, there might be something wrong with the null hypothesis and we will reject it.\n\n![Statistical significance.](../img/statistical-significance.svg)\n:label:`fig_statistical_significance`"
    },
    {
      "chunk_id": "82b84956e209_0",
      "chapter": "statistics",
      "heading": "Statistical Power",
      "text": "The *statistical power* (or *sensitivity*) measures the probability of reject the null hypothesis, $H_0$, when it should be rejected, i.e.,\n\n$$ \\textrm{statistical power }= 1 - \\beta = 1 - P(\\textrm{ fail to reject } H_0  \\mid H_0 \\textrm{ is false} ).$$\n\nRecall that a *type I error* is error caused by rejecting the null hypothesis when it is true, whereas a *type II error* is resulted from failing to reject the null hypothesis when it is false. A type II error is usually denoted as $\\beta$, and hence the corresponding statistical power is $1-\\beta$. Intuitively, statistical power can be interpreted as how likely our test will detect a real discrepancy of some minimum magnitude at a desired statistical significance level. $80\\%$ is a commonly used statistical power threshold. The higher the statistical power, the more likely we are to detect true differences. One of the most common uses of statistical power is in determining the number of samples needed. The probability you reject the null hypothesis when it is false depends on the degree to which it is false (known as the *effect size*) and the number of samples you have. As you might expect, small effect sizes will require a very large number of samples to be detectable with high probability. While beyond the scope of this brief appendix to derive in detail, as an example, want to be able to reject a null hypothesis that our sample came from a mean zero variance one Gaussian, and we believe that our sample's mean is actually close to one, we can do so with acceptable error rates with a sample size of only $8$. However, if we think our sample population true mean is close to $0.01$, then we'd need a sample size of nearly $80000$ to detect the difference. We can imagine the power as a water filter. In this analogy, a high power hypothesis test is like a high quality water filtration system that will reduce harmful substances in the water as much as possible."
    },
    {
      "chunk_id": "82b84956e209_1",
      "chapter": "statistics",
      "heading": "Statistical Power",
      "text": "We can imagine the power as a water filter. In this analogy, a high power hypothesis test is like a high quality water filtration system that will reduce harmful substances in the water as much as possible. On the other hand, a smaller discrepancy is like a low quality water filter, where some relative small substances may easily escape from the gaps. Similarly, if the statistical power is not of enough high power, then the test may not catch the smaller discrepancy."
    },
    {
      "chunk_id": "29be82f0d493_0",
      "chapter": "statistics",
      "heading": "Test Statistic",
      "text": "A *test statistic* $T(x)$ is a scalar which summarizes some characteristic of the sample data.  The goal of defining such a statistic is that it should allow us to distinguish between different distributions and conduct our hypothesis test.  Thinking back to our chemist example, if we wish to show that one population performs better than the other, it could be reasonable to take the mean as the test statistic.  Different choices of test statistic can lead to statistical test with drastically different statistical power.\n\nOften, $T(X)$ (the distribution of the test statistic under our null hypothesis) will follow, at least approximately, a common probability distribution such as a normal distribution when considered under the null hypothesis. If we can derive explicitly such a distribution, and then measure our test statistic on our dataset, we can safely reject the null hypothesis if our statistic is far outside the range that we would expect.  Making this quantitative leads us to the notion of $p$-values."
    },
    {
      "chunk_id": "11c9e71e91e5_0",
      "chapter": "statistics",
      "heading": "$p$-value",
      "text": "The $p$-value (or the *probability value*) is the probability that $T(X)$ is at least as extreme as the observed test statistic $T(x)$ assuming that the null hypothesis is *true*, i.e.,\n\n$$ p\\textrm{-value} = P_{H_0}(T(X) \\geq T(x)).$$\n\nIf the $p$-value is smaller than or equal to a predefined and fixed statistical significance level $\\alpha$, we may reject the null hypothesis. Otherwise, we will conclude that we are lack of evidence to reject the null hypothesis. For a given population distribution, the *region of rejection* will be the interval contained of all the points which has a $p$-value smaller than the statistical significance level $\\alpha$."
    },
    {
      "chunk_id": "e6b98ee43b16_0",
      "chapter": "statistics",
      "heading": "One-side Test and Two-sided Test",
      "text": "Normally there are two kinds of significance test: the one-sided test and the two-sided test. The *one-sided test* (or *one-tailed test*) is applicable when the null hypothesis and the alternative hypothesis only have one direction. For example, the null hypothesis may state that the true parameter $\\theta$ is less than or equal to a value $c$. The alternative hypothesis would be that $\\theta$ is greater than $c$. That is, the region of rejection is on only one side of the sampling distribution.  Contrary to the one-sided test, the *two-sided test* (or *two-tailed test*) is applicable when the region of rejection is on both sides of the sampling distribution. An example in this case may have a null hypothesis state that the true parameter $\\theta$ is equal to a value $c$. The alternative hypothesis would be that $\\theta$ is not equal to $c$."
    },
    {
      "chunk_id": "4eb598002658_0",
      "chapter": "statistics",
      "heading": "General Steps of Hypothesis Testing",
      "text": "After getting familiar with the above concepts, let's go through the general steps of hypothesis testing.\n\n1. State the question and establish a null hypotheses $H_0$.\n2. Set the statistical significance level $\\alpha$ and a statistical power ($1 - \\beta$).\n3. Obtain samples through experiments.  The number of samples needed will depend on the statistical power, and the expected effect size.\n4. Calculate the test statistic and the $p$-value.\n5. Make the decision to keep or reject the null hypothesis based on the $p$-value and the statistical significance level $\\alpha$.\n\nTo conduct a hypothesis test, we start by defining a null hypothesis and a level of risk that we are willing to take. Then we calculate the test statistic of the sample, taking an extreme value of the test statistic as evidence against the null hypothesis. If the test statistic falls within the reject region, we may reject the null hypothesis in favor of the alternative.\n\nHypothesis testing is applicable in a variety of scenarios such as the clinical trails and A/B testing."
    },
    {
      "chunk_id": "67c714b193dd_0",
      "chapter": "statistics",
      "heading": "Constructing Confidence Intervals",
      "text": "When estimating the value of a parameter $\\theta$, point estimators like $\\hat \\theta$ are of limited utility since they contain no notion of uncertainty. Rather, it would be far better if we could produce an interval that would contain the true parameter $\\theta$ with high probability.  If you were interested in such ideas a century ago, then you would have been excited to read \"Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability\" by Jerzy Neyman :cite:`Neyman.1937`, who first introduced the concept of confidence interval in 1937.\n\nTo be useful, a confidence interval should be as small as possible for a given degree of certainty. Let's see how to derive it."
    },
    {
      "chunk_id": "b4f551e01f58_0",
      "chapter": "statistics",
      "heading": "Definition",
      "text": "Mathematically, a *confidence interval* for the true parameter $\\theta$ is an interval $C_n$ that computed from the sample data such that\n\n$$P_{\\theta} (C_n \\ni \\theta) \\geq 1 - \\alpha, \\forall \\theta.$$\n:eqlabel:`eq_confidence`\n\nHere $\\alpha \\in (0, 1)$, and $1 - \\alpha$ is called the *confidence level* or *coverage* of the interval. This is the same $\\alpha$ as the significance level as we discussed about above.\n\nNote that :eqref:`eq_confidence` is about variable $C_n$, not about the fixed $\\theta$. To emphasize this, we write $P_{\\theta} (C_n \\ni \\theta)$ rather than $P_{\\theta} (\\theta \\in C_n)$."
    },
    {
      "chunk_id": "566543a00ea3_0",
      "chapter": "statistics",
      "heading": "Interpretation",
      "text": "It is very tempting to interpret a $95\\%$ confidence interval as an interval where you can be $95\\%$ sure the true parameter lies, however this is sadly not true.  The true parameter is fixed, and it is the interval that is random.  Thus a better interpretation would be to say that if you generated a large number of confidence intervals by this procedure, $95\\%$ of the generated intervals would contain the true parameter.\n\nThis may seem pedantic, but it can have real implications for the interpretation of the results.  In particular, we may satisfy :eqref:`eq_confidence` by constructing intervals that we are *almost certain* do not contain the true value, as long as we only do so rarely enough.  We close this section by providing three tempting but false statements.  An in-depth discussion of these points can be found in :citet:`Morey.Hoekstra.Rouder.ea.2016`.\n\n* **Fallacy 1**. Narrow confidence intervals mean we can estimate the parameter precisely.\n* **Fallacy 2**. The values inside the confidence interval are more likely to be the true value than those outside the interval.\n* **Fallacy 3**. The probability that a particular observed $95\\%$ confidence interval contains the true value is $95\\%$.\n\nSufficed to say, confidence intervals are subtle objects.  However, if you keep the interpretation clear, they can be powerful tools."
    },
    {
      "chunk_id": "79872c841b2b_0",
      "chapter": "statistics",
      "heading": "A Gaussian Example",
      "text": "Let's discuss the most classical example, the confidence interval for the mean of a Gaussian of unknown mean and variance. Suppose we collect $n$ samples $\\{x_i\\}_{i=1}^n$ from our Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$. We can compute estimators for the mean and variance by taking\n\n$$\\hat\\mu_n = \\frac{1}{n}\\sum_{i=1}^n x_i \\;\\textrm{and}\\; \\hat\\sigma^2_n = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\hat\\mu)^2.$$\n\nIf we now consider the random variable\n\n$$\nT = \\frac{\\hat\\mu_n - \\mu}{\\hat\\sigma_n/\\sqrt{n}},\n$$\n\nwe obtain a random variable following a well-known distribution called the *Student's t-distribution on* $n-1$ *degrees of freedom*. This distribution is very well studied, and it is known, for instance, that as $n\\rightarrow \\infty$, it is approximately a standard Gaussian, and thus by looking up values of the Gaussian c.d.f. in a table, we may conclude that the value of $T$ is in the interval $[-1.96, 1.96]$ at least $95\\%$ of the time. For finite values of $n$, the interval needs to be somewhat larger, but are well known and precomputed in tables. Thus, we may conclude that for large $n$,\n\n$$\nP\\left(\\frac{\\hat\\mu_n - \\mu}{\\hat\\sigma_n/\\sqrt{n}} \\in [-1.96, 1.96]\\right) \\ge 0.95. $$\n\nRearranging this by multiplying both sides by $\\hat\\sigma_n/\\sqrt{n}$ and then adding $\\hat\\mu_n$, we obtain\n\n$$\nP\\left(\\mu \\in \\left[\\hat\\mu_n - 1.96\\frac{\\hat\\sigma_n}{\\sqrt{n}}, \\hat\\mu_n + 1.96\\frac{\\hat\\sigma_n}{\\sqrt{n}}\\right]\\right) \\ge 0.95. $$\n\nThus we know that we have found our $95\\%$ confidence interval:\n$$\\left[\\hat\\mu_n - 1.96\\frac{\\hat\\sigma_n}{\\sqrt{n}}, \\hat\\mu_n + 1.96\\frac{\\hat\\sigma_n}{\\sqrt{n}}\\right].$$\n:eqlabel:`eq_gauss_confidence`\n\nIt is safe to say that :eqref:`eq_gauss_confidence` is one of the most used formula in statistics. Let's close our discussion of statistics by implementing it. For simplicity, we assume we are in the asymptotic regime. Small values of $N$ should include the correct value of `t_star` obtained either programmatically or from a $t$-table."
    },
    {
      "chunk_id": "79872c841b2b_1",
      "chapter": "statistics",
      "heading": "A Gaussian Example",
      "text": "Let's close our discussion of statistics by implementing it. For simplicity, we assume we are in the asymptotic regime. Small values of $N$ should include the correct value of `t_star` obtained either programmatically or from a $t$-table. ```{.python .input}\n#@tab mxnet\n# Number of samples\nN = 1000\n\n# Sample dataset\nsamples = np.random.normal(loc=0, scale=1, size=(N,))\n\n# Lookup Students's t-distribution c.d.f. t_star = 1.96\n\n# Construct interval\nmu_hat = np.mean(samples)\nsigma_hat = samples.std(ddof=1)\n(mu_hat - t_star*sigma_hat/np.sqrt(N), mu_hat + t_star*sigma_hat/np.sqrt(N))\n```\n\n```{.python .input}\n#@tab pytorch\n# PyTorch uses Bessel's correction by default, which means the use of ddof=1\n# instead of default ddof=0 in numpy. We can use unbiased=False to imitate\n# ddof=0. # Number of samples\nN = 1000\n\n# Sample dataset\nsamples = torch.normal(0, 1, size=(N,))\n\n# Lookup Students's t-distribution c.d.f. t_star = 1.96\n\n# Construct interval\nmu_hat = torch.mean(samples)\nsigma_hat = samples.std(unbiased=True)\n(mu_hat - t_star*sigma_hat/torch.sqrt(torch.tensor(N, dtype=torch.float32)),\\\n mu_hat + t_star*sigma_hat/torch.sqrt(torch.tensor(N, dtype=torch.float32)))\n```\n\n```{.python .input}\n#@tab tensorflow\n# Number of samples\nN = 1000\n\n# Sample dataset\nsamples = tf.random.normal((N,), 0, 1)\n\n# Lookup Students's t-distribution c.d.f. t_star = 1.96\n\n# Construct interval\nmu_hat = tf.reduce_mean(samples)\nsigma_hat = tf.math.reduce_std(samples)\n(mu_hat - t_star*sigma_hat/tf.sqrt(tf.constant(N, dtype=tf.float32)), \\\n mu_hat + t_star*sigma_hat/tf.sqrt(tf.constant(N, dtype=tf.float32)))\n```"
    },
    {
      "chunk_id": "ce4a801d9d40_0",
      "chapter": "statistics",
      "heading": "Summary",
      "text": "* Statistics focuses on inference problems, whereas deep learning emphasizes on making accurate predictions without explicitly programming and understanding.\n* There are three common statistics inference methods: evaluating and comparing estimators, conducting hypothesis tests, and constructing confidence intervals.\n* There are three most common estimators: statistical bias, standard deviation, and mean square error.\n* A confidence interval is an estimated range of a true population parameter that we can construct by given the samples.\n* Hypothesis testing is a way of evaluating some evidence against the default statement about a population."
    },
    {
      "chunk_id": "499f88ef688a_0",
      "chapter": "statistics",
      "heading": "Exercises",
      "text": "1. Let $X_1, X_2, \\ldots, X_n \\overset{\\textrm{iid}}{\\sim} \\textrm{Unif}(0, \\theta)$, where \"iid\" stands for *independent and identically distributed*. Consider the following estimators of $\\theta$:\n$$\\hat{\\theta} = \\max \\{X_1, X_2, \\ldots, X_n \\};$$\n$$\\tilde{\\theta} = 2 \\bar{X_n} = \\frac{2}{n} \\sum_{i=1}^n X_i.$$\n    * Find the statistical bias, standard deviation, and mean square error of $\\hat{\\theta}.$\n    * Find the statistical bias, standard deviation, and mean square error of $\\tilde{\\theta}.$\n    * Which estimator is better?\n1. For our chemist example in introduction, can you derive the 5 steps to conduct a two-sided hypothesis testing? Given the statistical significance level $\\alpha = 0.05$ and the statistical power $1 - \\beta = 0.8$.\n1. Run the confidence interval code with $N=2$ and $\\alpha = 0.5$ for $100$ independently generated dataset, and plot the resulting intervals (in this case `t_star = 1.0`).  You will see several very short intervals which are very far from containing the true mean $0$.  Does this contradict the interpretation of the confidence interval?  Do you feel comfortable using short intervals to indicate high precision estimates?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/419)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1102)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1103)\n:end_tab:"
    },
    {
      "chunk_id": "ea06c34ad86f_0",
      "chapter": "aws",
      "heading": "aws",
      "text": "# Using AWS EC2 Instances\n:label:`sec_aws`\n\nIn this section, we will show you how to install all libraries on a raw Linux machine. Recall that in :numref:`sec_sagemaker` we discussed how to use Amazon SageMaker, while building an instance by yourself costs less on AWS. The walkthrough includes three steps:\n\n1. Request for a GPU Linux instance from AWS EC2.\n1. Install CUDA (or use an Amazon Machine Image with preinstalled CUDA).\n1. Install the deep learning framework and other libraries for running the code of the book.\n\nThis process applies to other instances (and other clouds), too, albeit with some minor modifications. Before going forward, you need to create an AWS account, see :numref:`sec_sagemaker` for more details."
    },
    {
      "chunk_id": "b8fd5b859acf_0",
      "chapter": "aws",
      "heading": "Creating and Running an EC2 Instance",
      "text": "After logging into your AWS account, click \"EC2\" (:numref:`fig_aws`) to go to the EC2 panel.\n\n![Open the EC2 console.](../img/aws.png)\n:width:`400px`\n:label:`fig_aws`\n\n:numref:`fig_ec2` shows the EC2 panel.\n\n![The EC2 panel.](../img/ec2.png)\n:width:`700px`\n:label:`fig_ec2`"
    },
    {
      "chunk_id": "bf73f0ca5988_0",
      "chapter": "aws",
      "heading": "Presetting Location",
      "text": "Select a nearby data center to reduce latency, e.g., \"Oregon\" (marked by the red box in the top-right of :numref:`fig_ec2`). If you are located in China,\nyou can select a nearby Asia Pacific region, such as Seoul or Tokyo. Please note\nthat some data centers may not have GPU instances."
    },
    {
      "chunk_id": "6a36dde285d5_0",
      "chapter": "aws",
      "heading": "Increasing Limits",
      "text": "Before choosing an instance, check if there are quantity\nrestrictions by clicking the \"Limits\" label in the bar on the left as shown in\n:numref:`fig_ec2`. \n:numref:`fig_limits` shows an example of such a\nlimitation. The account currently cannot open \"p2.xlarge\" instances according to the region. If\nyou need to open one or more instances, click on the \"Request limit increase\" link to\napply for a higher instance quota.\nGenerally, it takes one business day to\nprocess an application.\n\n![Instance quantity restrictions.](../img/limits.png)\n:width:`700px`\n:label:`fig_limits`"
    },
    {
      "chunk_id": "c3c02a1b8b2b_0",
      "chapter": "aws",
      "heading": "Launching an Instance",
      "text": "Next, click the \"Launch Instance\" button marked by the red box in :numref:`fig_ec2` to launch your instance. We begin by selecting a suitable Amazon Machine Image (AMI). Select an Ubuntu instance (:numref:`fig_ubuntu`). ![Choose an AMI.](../img/ubuntu-new.png)\n:width:`700px`\n:label:`fig_ubuntu`\n\nEC2 provides many different instance configurations to choose from. This can sometimes feel overwhelming to a beginner. :numref:`tab_ec2` lists different suitable machines. :Different EC2 instance types\n:label:`tab_ec2`\n\n| Name | GPU         | Notes                         |\n|------|-------------|-------------------------------|\n| g2   | Grid K520   | ancient                       |\n| p2   | Kepler K80  | old but often cheap as spot   |\n| g3   | Maxwell M60 | good trade-off                |\n| p3   | Volta V100  | high performance for FP16     |\n| p4   | Ampere A100 | high performance for large-scale training |\n| g4   | Turing T4   | inference optimized FP16/INT8 |\n\n\nAll these servers come in multiple flavors indicating the number of GPUs used. For example, a p2.xlarge has 1 GPU and a p2.16xlarge has 16 GPUs and more memory. For more details, see the [AWS EC2 documentation](https://aws.amazon.com/ec2/instance-types/) or a [summary page](https://www.ec2instances.info). For the purpose of illustration, a p2.xlarge will suffice (marked in the red box of :numref:`fig_p2x`). ![Choose an instance.](../img/p2x.png)\n:width:`700px`\n:label:`fig_p2x`\n\nNote that you should use a GPU-enabled instance with suitable drivers and a GPU-enabled deep learning framework. Otherwise you will not see any benefit from using GPUs. We go on to select the key pair used to access\nthe instance. If you do not have a key pair, click \"Create new key pair\" in :numref:`fig_keypair` to generate a key pair. Subsequently,\nyou can select the\npreviously generated key pair. Make sure that you download the key pair and store it in a safe location if you\ngenerated a new one. This is your only way to SSH into the server."
    },
    {
      "chunk_id": "c3c02a1b8b2b_1",
      "chapter": "aws",
      "heading": "Launching an Instance",
      "text": "Subsequently,\nyou can select the\npreviously generated key pair. Make sure that you download the key pair and store it in a safe location if you\ngenerated a new one. This is your only way to SSH into the server. ![Select a key pair.](../img/keypair.png)\n:width:`500px`\n:label:`fig_keypair`\n\nIn this example, we will keep the default configurations for \"Network settings\" (click the \"Edit\" button to configure items such as the subnet and security groups). We just increase the default hard disk size to 64 GB (:numref:`fig_disk`). Note that CUDA by itself already takes up 4 GB. ![Modify the hard disk size.](../img/disk.png)\n:width:`700px`\n:label:`fig_disk`\n\n\nClick \"Launch Instance\" to launch the created\ninstance. Click the\ninstance ID shown in :numref:`fig_launching` to view the status of this instance. ![Click the instance ID.](../img/launching.png)\n:width:`700px`\n:label:`fig_launching`"
    },
    {
      "chunk_id": "21dbcc276ae0_0",
      "chapter": "aws",
      "heading": "Connecting to the Instance",
      "text": "As shown in :numref:`fig_connect`, after the instance state turns green, right-click the instance and select `Connect` to view the instance access method.\n\n![View the instance access method.](../img/connect.png)\n:width:`700px`\n:label:`fig_connect`\n\nIf this is a new key, it must not be publicly viewable for SSH to work. Go to the folder where you store `D2L_key.pem` and \nexecute the following command \nto make the key not publicly viewable:\n\n```bash\nchmod 400 D2L_key.pem\n```\n\n\n![View instance access and startup method.](../img/chmod.png)\n:width:`400px`\n:label:`fig_chmod`\n\n\nNow, copy the SSH command in the lower red box of :numref:`fig_chmod` and paste onto the command line:\n\n```bash\nssh -i \"D2L_key.pem\" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com\n```\n\n\nWhen the command line prompts \"Are you sure you want to continue connecting (yes/no)\", enter \"yes\" and press Enter to log into the instance.\n\nYour server is ready now."
    },
    {
      "chunk_id": "a15da65b3abf_0",
      "chapter": "aws",
      "heading": "Installing CUDA",
      "text": "Before installing CUDA, be sure to update the instance with the latest drivers.\n\n```bash\nsudo apt-get update && sudo apt-get install -y build-essential git libgfortran3\n```\n\n\nHere we download CUDA 12.1. Visit NVIDIA's [official repository](https://developer.nvidia.com/cuda-toolkit-archive) to find the download link as shown in :numref:`fig_cuda`.\n\n![Find the CUDA 12.1 download address.](../img/cuda121.png)\n:width:`500px`\n:label:`fig_cuda`\n\nCopy the instructions and paste them onto the terminal to install CUDA 12.1.\n\n```bash\n# The link and file name are subject to changes\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\nsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\n```\n\n\nAfter installing the program, run the following command to view the GPUs:\n\n```bash\nnvidia-smi\n```\n\n\nFinally, add CUDA to the library path to help other libraries find it, such as appending the following lines to the end of `~/.bashrc`.\n\n```bash\nexport PATH=\"/usr/local/cuda-12.1/bin:$PATH\"\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-12.1/lib64\n```"
    },
    {
      "chunk_id": "7f3984f51f4c_0",
      "chapter": "aws",
      "heading": "Installing Libraries for Running the Code",
      "text": "To run the code of this book,\njust follow steps in :ref:`chap_installation`\nfor Linux users on the EC2 instance\nand use the following tips \nfor working on a remote Linux server:\n\n* To download the bash script on the Miniconda installation page, right click the download link and select \"Copy Link Address\", then execute `wget [copied link address]`.\n* After running `~/miniconda3/bin/conda init`, you may execute `source ~/.bashrc` instead of closing and reopening your current shell."
    },
    {
      "chunk_id": "0bb5c3f7cac8_0",
      "chapter": "aws",
      "heading": "Running the Jupyter Notebook remotely",
      "text": "To run the Jupyter Notebook remotely you need to use SSH port forwarding. After all, the server in the cloud does not have a monitor or keyboard. For this, log into your server from your desktop (or laptop) as follows:\n\n```\n# This command must be run in the local command line\nssh -i \"/path/to/key.pem\" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com -L 8889:localhost:8888\n```\n\n\nNext, go to the location \nof the downloaded code of this book\non the EC2 instance,\nthen run:\n\n```\nconda activate d2l\njupyter notebook\n```\n\n\n:numref:`fig_jupyter` shows the possible output after you run the Jupyter Notebook. The last row is the URL for port 8888.\n\n![Output after running the Jupyter Notebook. The last row is the URL for port 8888.](../img/jupyter.png)\n:width:`700px`\n:label:`fig_jupyter`\n\nSince you used port forwarding to port 8889,\ncopy the last row in the red box of :numref:`fig_jupyter`,\nreplace \"8888\" with \"8889\" in the URL,\nand open it in your local browser."
    },
    {
      "chunk_id": "1983202f8ae7_0",
      "chapter": "aws",
      "heading": "Closing Unused Instances",
      "text": "As cloud services are billed by the time of use, you should close instances that are not being used. Note that there are alternatives:\n\n* \"Stopping\" an instance means that you will be able to start it again. This is akin to switching off the power for your regular server. However, stopped instances will still be billed a small amount for the hard disk space retained. \n* \"Terminating\" an instance will delete all data associated with it. This includes the disk, hence you cannot start it again. Only do this if you know that you will not need it in the future.\n\nIf you want to use the instance as a template for many more instances,\nright-click on the example in :numref:`fig_connect` and select \"Image\" $\\rightarrow$\n\"Create\" to create an image of the instance. Once this is complete, select\n\"Instance State\" $\\rightarrow$ \"Terminate\" to terminate the instance. The next\ntime you want to use this instance, you can follow the steps in this section \nto create an instance based on\nthe saved image. The only difference is that, in \"1. Choose AMI\" shown in\n:numref:`fig_ubuntu`, you must use the \"My AMIs\" option on the left to select your saved\nimage. The created instance will retain the information stored on the image hard\ndisk. For example, you will not have to reinstall CUDA and other runtime\nenvironments."
    },
    {
      "chunk_id": "b9bdb43b16a7_0",
      "chapter": "aws",
      "heading": "Summary",
      "text": "* We can launch and stop instances on demand without having to buy and build our own computer.\n* We need to install CUDA before using the GPU-enabled deep learning framework.\n* We can use port forwarding to run the Jupyter Notebook on a remote server."
    },
    {
      "chunk_id": "c3c53200a16b_0",
      "chapter": "aws",
      "heading": "Exercises",
      "text": "1. The cloud offers convenience, but it does not come cheap. Find out how to launch [spot instances](https://aws.amazon.com/ec2/spot/) to see how to reduce costs.\n1. Experiment with different GPU servers. How fast are they?\n1. Experiment with multi-GPU servers. How well can you scale things up?\n\n\n[Discussions](https://discuss.d2l.ai/t/423)"
    },
    {
      "chunk_id": "7de71844d466_0",
      "chapter": "colab",
      "heading": "colab",
      "text": "# Using Google Colab\n:label:`sec_colab`\n\nWe introduced how to run this book on AWS in :numref:`sec_sagemaker` and :numref:`sec_aws`. Another option is running this book on [Google Colab](https://colab.research.google.com/)\nif you have a Google account.\n\nTo run the code of a section on Colab, simply click the `Colab` button as shown in :numref:`fig_colab`. \n\n![Run the code of a section on Colab.](../img/colab.png)\n:width:`300px`\n:label:`fig_colab`\n\n\nIf it is your first time to run a code cell,\nyou will receive a warning message as shown in :numref:`fig_colab2`.\nJust click \"RUN ANYWAY\" to ignore it.\n\n![Ignore the warning message by clicking \"RUN ANYWAY\".](../img/colab-2.png)\n:width:`300px`\n:label:`fig_colab2`\n\nNext, Colab will connect you to an instance to run the code of this section.\nSpecifically, if a GPU is needed, \nColab will be automatically requested \nfor connecting to a GPU instance."
    },
    {
      "chunk_id": "1bb78c5c5032_0",
      "chapter": "colab",
      "heading": "Summary",
      "text": "* You can use Google Colab to run each section's code in this book.\n* Colab will be requested to connect to a GPU instance if a GPU is needed in any section of this book."
    },
    {
      "chunk_id": "8b534b339ba7_0",
      "chapter": "colab",
      "heading": "Exercises",
      "text": "1. Open any section of this book using Google Colab.\n1. Edit and run any section that requires a GPU using Google Colab.\n\n\n[Discussions](https://discuss.d2l.ai/t/424)"
    },
    {
      "chunk_id": "cff1146d6b6c_0",
      "chapter": "contributing",
      "heading": "contributing",
      "text": "# Contributing to This Book\n:label:`sec_how_to_contribute`\n\nContributions by [readers](https://github.com/d2l-ai/d2l-en/graphs/contributors) help us improve this book. If you find a typo, an outdated link, something where you think we missed a citation, where the code does not look elegant or where an explanation is unclear, please contribute back and help us help our readers. While in regular books the delay between print runs (and thus between typo corrections) can be measured in years, it typically takes hours to days to incorporate an improvement in this book. This is all possible due to version control and continuous integration (CI) testing. To do so you need to submit a [pull request](https://github.com/d2l-ai/d2l-en/pulls) to the GitHub repository. When your pull request is merged into the code repository by the authors, you will become a contributor."
    },
    {
      "chunk_id": "8446eb5c21e7_0",
      "chapter": "contributing",
      "heading": "Submitting Minor Changes",
      "text": "The most common contributions are editing one sentence or fixing typos. We recommend that you find the source file in the [GitHub repository](https://github.com/d2l-ai/d2l-en) and edit the file directly. For example, you can search the file through the [Find file](https://github.com/d2l-ai/d2l-en/find/master) button (:numref:`fig_edit_file`) to locate the source file (a markdown file). Then you click the \"Edit this file\" button on the upper-right corner to make your changes in the markdown file.\n\n![Edit the file on Github.](../img/edit-file.png)\n:width:`300px`\n:label:`fig_edit_file`\n\nAfter you are done, fill in your change descriptions in the \"Propose file change\" panel on the page bottom and then click the \"Propose file change\" button. It will redirect you to a new page to review your changes (:numref:`fig_git_createpr`). If everything is good, you can submit a pull request by clicking the \"Create pull request\" button."
    },
    {
      "chunk_id": "a4330d8d139f_0",
      "chapter": "contributing",
      "heading": "Proposing Major Changes",
      "text": "If you plan to update a large portion of text or code, then you need to know a little bit more about the format this book is using. The source file is based on the [markdown format](https://daringfireball.net/projects/markdown/syntax) with a set of extensions through the [D2L-Book](http://book.d2l.ai/user/markdown.html) package such as referring to equations, images, chapters, and citations. You can use any markdown editors to open these files and make your changes.\n\nIf you would like to change the code, we recommend that you use the Jupyter Notebook to open these markdown files as described in :numref:`sec_jupyter`, so that you can run and test your changes. Please remember to clear all outputs before submitting your changes since our CI system will execute the sections you updated to generate outputs.\n\nSome sections may support multiple framework implementations.\nIf you add a new code block, please use `%%tab` to mark this block on the beginning line. For example,\n`%%tab pytorch` for a PyTorch code block, `%%tab tensorflow` for a TensorFlow code block, or `%%tab all` a shared code block for all implementations. You may refer to the `d2lbook` package for more information."
    },
    {
      "chunk_id": "055ef8f7b9c9_0",
      "chapter": "contributing",
      "heading": "Submitting Major Changes",
      "text": "We suggest you to use the standard Git process to submit a major change. In a nutshell the process works as described in :numref:`fig_contribute`.\n\n![Contributing to the book.](../img/contribute.svg)\n:label:`fig_contribute`\n\nWe will walk you through the steps in detail. If you are already familiar with Git you can skip this section. For concreteness we assume that the contributor's user name is \"astonzhang\"."
    },
    {
      "chunk_id": "0b54a6fdee07_0",
      "chapter": "contributing",
      "heading": "Installing Git",
      "text": "The Git open-source book describes [how to install Git](https://git-scm.com/book/en/v2). This typically works via `apt install git` on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by using GitHub's [desktop client](https://desktop.github.com). If you do not have a GitHub account, you need to sign up for one."
    },
    {
      "chunk_id": "093c09d68c7d_0",
      "chapter": "contributing",
      "heading": "Logging in to GitHub",
      "text": "Enter the [address](https://github.com/d2l-ai/d2l-en/) of the book's code repository in your browser. Click on the `Fork` button in the red box at the upper-right of :numref:`fig_git_fork`, to make a copy of the repository of this book. This is now *your copy* and you can change it any way you want.\n\n![The code repository page.](../img/git-fork.png)\n:width:`700px`\n:label:`fig_git_fork`\n\n\nNow, the code repository of this book will be forked (i.e., copied) to your username, such as `astonzhang/d2l-en` shown at the upper-left of :numref:`fig_git_forked`.\n\n![The forked code repository.](../img/git-forked.png)\n:width:`700px`\n:label:`fig_git_forked`"
    },
    {
      "chunk_id": "b3e0c0de8cec_0",
      "chapter": "contributing",
      "heading": "Cloning the Repository",
      "text": "To clone the repository (i.e., to make a local copy) we need to get its repository address. The green button in :numref:`fig_git_clone` displays this. Make sure that your local copy is up to date with the main repository if you decide to keep this fork around for longer. For now simply follow the instructions in :ref:`chap_installation` to get started. The main difference is that you are now downloading *your own fork* of the repository.\n\n![Cloning the repository.](../img/git-clone.png)\n:width:`700px`\n:label:`fig_git_clone`\n\n```\n# Replace your_github_username with your GitHub username\ngit clone https://github.com/your_github_username/d2l-en.git\n```"
    },
    {
      "chunk_id": "d7da26c31f7b_0",
      "chapter": "contributing",
      "heading": "Editing and Pushing",
      "text": "Now it is time to edit the book. It is best to edit it in the Jupyter Notebook following instructions in :numref:`sec_jupyter`. Make the changes and check that they are OK. Assume that we have modified a typo in the file `~/d2l-en/chapter_appendix-tools-for-deep-learning/contributing.md`.\nYou can then check which files you have changed.\n\nAt this point Git will prompt that the `chapter_appendix-tools-for-deep-learning/contributing.md` file has been modified.\n\n```\nmylaptop:d2l-en me$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\n\n\tmodified:   chapter_appendix-tools-for-deep-learning/contributing.md\n```\n\n\nAfter confirming that this is what you want, execute the following command:\n\n```\ngit add chapter_appendix-tools-for-deep-learning/contributing.md\ngit commit -m 'Fix a typo in git documentation'\ngit push\n```\n\n\nThe changed code will then be in your personal fork of the repository. To request the addition of your change, you have to create a pull request for the official repository of the book."
    },
    {
      "chunk_id": "69dca9c64fe6_0",
      "chapter": "contributing",
      "heading": "Submitting Pull Requests",
      "text": "As shown in :numref:`fig_git_newpr`, go to your fork of the repository on GitHub and select \"New pull request\". This will open up a screen that shows you the changes between your edits and what is current in the main repository of the book.\n\n![New pull request.](../img/git-newpr.png)\n:width:`700px`\n:label:`fig_git_newpr`\n\n\nFinally, submit a pull request by clicking the button as shown in :numref:`fig_git_createpr`. Make sure to describe the changes you have made in the pull request.\nThis will make it easier for the authors to review it and to merge it with the book. Depending on the changes, this might get accepted right away, rejected, or more likely, you will get some feedback on the changes. Once you have incorporated them, you are good to go.\n\n![Create pull request.](../img/git-createpr.png)\n:width:`700px`\n:label:`fig_git_createpr`"
    },
    {
      "chunk_id": "ae6c6836f35c_0",
      "chapter": "contributing",
      "heading": "Summary",
      "text": "* You can use GitHub to contribute to this book.\n* You can edit the file on GitHub directly for minor changes.\n* For a major change, please fork the repository, edit things locally, and only contribute back once you are ready.\n* Pull requests are how contributions are being bundled up. Try not to submit huge pull requests since this makes them hard to understand and incorporate. Better send several smaller ones."
    },
    {
      "chunk_id": "2924c03a55da_0",
      "chapter": "contributing",
      "heading": "Exercises",
      "text": "1. Star and fork the `d2l-ai/d2l-en` repository.\n1. If you spot anything that needs improvement (e.g., missing a reference), submit a pull request. \n1. It is usually a better practice to create a pull request using a new branch. Learn how to do it with [Git branching](https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell).\n\n[Discussions](https://discuss.d2l.ai/t/426)"
    },
    {
      "chunk_id": "8c95bd9be1ec_0",
      "chapter": "d2l",
      "heading": "d2l",
      "text": "# The `d2l` API Document\n:label:`sec_d2l`\n\nThis section displays classes and functions (sorted alphabetically) in the `d2l` package, showing where they are defined in the book so you can find more detailed implementations and explanations. \nSee also the source code on the [GitHub repository](https://github.com/d2l-ai/d2l-en/tree/master/d2l).\n\n:begin_tab:`pytorch`\n\n```eval_rst\n\n.. currentmodule:: d2l.torch\n\n```\n\n:begin_tab:`mxnet`\n\n```eval_rst\n\n.. currentmodule:: d2l.mxnet\n\n```\n\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n\n```eval_rst\n\n.. currentmodule:: d2l.torch\n\n```\n\n\n:end_tab:"
    },
    {
      "chunk_id": "a1d13c870b30_0",
      "chapter": "d2l",
      "heading": "Classes",
      "text": "```eval_rst \n\n.. autoclass:: AdditiveAttention\n   :members:\n   \n.. autoclass:: AddNorm\n   :members:\n\n.. autoclass:: AttentionDecoder\n   :members: \n\n.. autoclass:: Classifier\n   :members: \n   \n.. autoclass:: DataModule\n   :members: \n   \n.. autoclass:: Decoder\n   :members: \n   \n.. autoclass:: DotProductAttention\n   :members:\n   \n.. autoclass:: Encoder\n   :members:\n   \n.. autoclass:: EncoderDecoder\n   :members:\n   \n.. autoclass:: FashionMNIST\n   :members: \n   \n.. autoclass:: GRU\n   :members: \n   \n.. autoclass:: HyperParameters\n   :members: \n   \n.. autoclass:: LeNet\n   :members: \n   \n.. autoclass:: LinearRegression\n   :members: \n   \n.. autoclass:: LinearRegressionScratch\n   :members: \n   \n.. autoclass:: Module\n   :members: \n   \n.. autoclass:: MTFraEng\n   :members: \n   \n.. autoclass:: MultiHeadAttention\n   :members:\n   \n.. autoclass:: PositionalEncoding\n   :members:\n   \n.. autoclass:: PositionWiseFFN\n   :members:\n   \n.. autoclass:: ProgressBoard\n   :members: \n   \n.. autoclass:: Residual\n   :members: \n   \n.. autoclass:: ResNeXtBlock\n   :members:\n   \n.. autoclass:: RNN\n   :members: \n   \n.. autoclass:: RNNLM\n   :members:\n   \n.. autoclass:: RNNLMScratch\n   :members:\n   \n.. autoclass:: RNNScratch\n   :members: \n   \n.. autoclass:: Seq2Seq\n   :members:  \n   \n.. autoclass:: Seq2SeqEncoder\n   :members:\n   \n.. autoclass:: SGD\n   :members: \n   \n.. autoclass:: SoftmaxRegression\n   :members: \n\n.. autoclass:: SyntheticRegressionData\n   :members: \n\n.. autoclass:: TimeMachine\n   :members: \n\n.. autoclass:: Trainer\n   :members: \n\n.. autoclass:: TransformerEncoder \n   :members:\n\n.. autoclass:: TransformerEncoderBlock\n   :members:\n\n.. autoclass:: Vocab\n   :members: \n```"
    },
    {
      "chunk_id": "6596cef945f1_0",
      "chapter": "d2l",
      "heading": "Functions",
      "text": "```eval_rst \n\n.. autofunction:: add_to_class\n\n.. autofunction:: bleu\n\n.. autofunction:: check_len\n\n.. autofunction:: check_shape\n\n.. autofunction:: corr2d\n\n.. autofunction:: cpu\n\n.. autofunction:: gpu\n\n.. autofunction:: init_cnn\n\n.. autofunction:: init_seq2seq\n\n.. autofunction:: masked_softmax\n\n.. autofunction:: num_gpus\n\n.. autofunction:: plot\n\n.. autofunction:: set_axes\n\n.. autofunction:: set_figsize\n\n.. autofunction:: show_heatmaps\n\n.. autofunction:: show_list_len_pair_hist\n\n.. autofunction:: try_all_gpus\n\n.. autofunction:: try_gpu\n\n.. autofunction:: use_svg_display\n\n```"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Appendix: Tools for Deep Learning\n:label:`chap_appendix_tools`\n\n\nTo get the most out of *Dive into Deep Learning*,\nwe will talk you through different tools\nin this appendix,\nsuch as\nfor running and contributing to this\ninteractive open-source book.\n\n```toc\n:maxdepth: 2\n\njupyter\nsagemaker\naws\ncolab\nselecting-servers-gpus\ncontributing\nutils\nd2l\n```"
    },
    {
      "chunk_id": "25dabd5f0e3e_0",
      "chapter": "jupyter",
      "heading": "jupyter",
      "text": "# Using Jupyter Notebooks\n:label:`sec_jupyter`\n\n\nThis section describes how to edit and run the code\nin each section of this book\nusing the Jupyter Notebook. Make sure you have\ninstalled Jupyter and downloaded the\ncode as described in\n:ref:`chap_installation`.\nIf you want to know more about Jupyter see the excellent tutorial in\ntheir [documentation](https://jupyter.readthedocs.io/en/latest/)."
    },
    {
      "chunk_id": "01cecc903058_0",
      "chapter": "jupyter",
      "heading": "Editing and Running the Code Locally",
      "text": "Suppose that the local path of the book's code is `xx/yy/d2l-en/`. Use the shell to change the directory to this path (`cd xx/yy/d2l-en`) and run the command `jupyter notebook`. If your browser does not do this automatically, open http://localhost:8888 and you will see the interface of Jupyter and all the folders containing the code of the book, as shown in :numref:`fig_jupyter00`. ![The folders containing the code of this book.](../img/jupyter00.png)\n:width:`600px`\n:label:`fig_jupyter00`\n\n\nYou can access the notebook files by clicking on the folder displayed on the webpage. They usually have the suffix \".ipynb\". For the sake of brevity, we create a temporary \"test.ipynb\" file. The content displayed after you click it is\nshown in :numref:`fig_jupyter01`. This notebook includes a markdown cell and a code cell. The content in the markdown cell includes \"This Is a Title\" and \"This is text.\". The code cell contains two lines of Python code. ![Markdown and code cells in the \"text.ipynb\" file.](../img/jupyter01.png)\n:width:`600px`\n:label:`fig_jupyter01`\n\n\nDouble click on the markdown cell to enter edit mode. Add a new text string \"Hello world.\" at the end of the cell, as shown in :numref:`fig_jupyter02`. ![Edit the markdown cell.](../img/jupyter02.png)\n:width:`600px`\n:label:`fig_jupyter02`\n\n\nAs demonstrated in :numref:`fig_jupyter03`,\nclick \"Cell\" $\\rightarrow$ \"Run Cells\" in the menu bar to run the edited cell. ![Run the cell.](../img/jupyter03.png)\n:width:`600px`\n:label:`fig_jupyter03`\n\nAfter running, the markdown cell is shown in :numref:`fig_jupyter04`. ![The markdown cell after running.](../img/jupyter04.png)\n:width:`600px`\n:label:`fig_jupyter04`\n\n\nNext, click on the code cell. Multiply the elements by 2 after the last line of code, as shown in :numref:`fig_jupyter05`. ![Edit the code cell.](../img/jupyter05.png)\n:width:`600px`\n:label:`fig_jupyter05`\n\n\nYou can also run the cell with a shortcut (\"Ctrl + Enter\" by default) and obtain the output result from :numref:`fig_jupyter06`."
    },
    {
      "chunk_id": "01cecc903058_1",
      "chapter": "jupyter",
      "heading": "Editing and Running the Code Locally",
      "text": "![Edit the code cell.](../img/jupyter05.png)\n:width:`600px`\n:label:`fig_jupyter05`\n\n\nYou can also run the cell with a shortcut (\"Ctrl + Enter\" by default) and obtain the output result from :numref:`fig_jupyter06`. ![Run the code cell to obtain the output.](../img/jupyter06.png)\n:width:`600px`\n:label:`fig_jupyter06`\n\n\nWhen a notebook contains more cells, we can click \"Kernel\" $\\rightarrow$ \"Restart & Run All\" in the menu bar to run all the cells in the entire notebook. By clicking \"Help\" $\\rightarrow$ \"Edit Keyboard Shortcuts\" in the menu bar, you can edit the shortcuts according to your preferences."
    },
    {
      "chunk_id": "bae9e2cb718a_0",
      "chapter": "jupyter",
      "heading": "Advanced Options",
      "text": "Beyond local editing two things are quite important: editing the notebooks in the markdown format and running Jupyter remotely.\nThe latter matters when we want to run the code on a faster server.\nThe former matters since Jupyter's native ipynb format stores a lot of auxiliary data that is\nirrelevant to the content,\nmostly related to how and where the code is run.\nThis is confusing for Git, making\nreviewing contributions very difficult.\nFortunately there is an alternative---native editing in the markdown format."
    },
    {
      "chunk_id": "e7e387465c6c_0",
      "chapter": "jupyter",
      "heading": "Markdown Files in Jupyter",
      "text": "If you wish to contribute to the content of this book, you need to modify the\nsource file (md file, not ipynb file) on GitHub.\nUsing the notedown plugin we\ncan modify notebooks in the md format directly in Jupyter.\n\n\nFirst, install the notedown plugin, run the Jupyter Notebook, and load the plugin:\n\n```\npip install d2l-notedown  # You may need to uninstall the original notedown.\njupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'\n```\n\n\nYou may also turn on the notedown plugin by default whenever you run the Jupyter Notebook.\nFirst, generate a Jupyter Notebook configuration file (if it has already been generated, you can skip this step).\n\n```\njupyter notebook --generate-config\n```\n\n\nThen, add the following line to the end of the Jupyter Notebook configuration file (for Linux or macOS, usually in the path `~/.jupyter/jupyter_notebook_config.py`):\n\n```\nc.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\n```\n\n\nAfter that, you only need to run the `jupyter notebook` command to turn on the notedown plugin by default."
    },
    {
      "chunk_id": "21349acda1f2_0",
      "chapter": "jupyter",
      "heading": "Running Jupyter Notebooks on a Remote Server",
      "text": "Sometimes, you may want to run Jupyter notebooks on a remote server and access it through a browser on your local computer. If Linux or macOS is installed on your local machine (Windows can also support this function through third-party software such as PuTTY), you can use port forwarding:\n\n```\nssh myserver -L 8888:localhost:8888\n```\n\n\nThe above string `myserver` is the address of the remote server.\nThen we can use http://localhost:8888 to access the remote server `myserver` that runs Jupyter notebooks. We will detail on how to run Jupyter notebooks on AWS instances\nlater in this appendix."
    },
    {
      "chunk_id": "493d8521c86c_0",
      "chapter": "jupyter",
      "heading": "Timing",
      "text": "We can use the `ExecuteTime` plugin to time the execution of each code cell in Jupyter notebooks.\nUse the following commands to install the plugin:\n\n```\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable execute_time/ExecuteTime\n```"
    },
    {
      "chunk_id": "80c844070192_0",
      "chapter": "jupyter",
      "heading": "Summary",
      "text": "* Using the Jupyter Notebook tool, we can edit, run, and contribute to each section of the book.\n* We can run Jupyter notebooks on remote servers using port forwarding."
    },
    {
      "chunk_id": "60f7622e1b39_0",
      "chapter": "jupyter",
      "heading": "Exercises",
      "text": "1. Edit and run the code in this book with the Jupyter Notebook on your local machine.\n1. Edit and run the code in this book with the Jupyter Notebook *remotely* via port forwarding.\n1. Compare the running time of the operations $\\mathbf{A}^\\top \\mathbf{B}$ and $\\mathbf{A} \\mathbf{B}$ for two square matrices in $\\mathbb{R}^{1024 \\times 1024}$. Which one is faster?\n\n\n[Discussions](https://discuss.d2l.ai/t/421)"
    },
    {
      "chunk_id": "2fd43095c42c_0",
      "chapter": "sagemaker",
      "heading": "sagemaker",
      "text": "# Using Amazon SageMaker\n:label:`sec_sagemaker`\n\nDeep learning applications\nmay demand so much computational resource\nthat easily goes beyond\nwhat your local machine can offer.\nCloud computing services\nallow you to \nrun GPU-intensive code of this book\nmore easily\nusing more powerful computers.\nThis section will introduce \nhow to use Amazon SageMaker\nto run the code of this book."
    },
    {
      "chunk_id": "e00eb4dd3f9a_0",
      "chapter": "sagemaker",
      "heading": "Signing Up",
      "text": "First, we need to sign up an account at https://aws.amazon.com/.\nFor additional security,\nusing two-factor authentication \nis encouraged.\nIt is also a good idea to\nset up detailed billing and spending alerts to\navoid any surprise,\ne.g., \nwhen forgetting to stop running instances.\nAfter logging into your AWS account, \ngo to your [console](http://console.aws.amazon.com/) and search for \"Amazon SageMaker\" (see :numref:`fig_sagemaker`), \nthen click it to open the SageMaker panel.\n\n![Search for and open the SageMaker panel.](../img/sagemaker.png)\n:width:`300px`\n:label:`fig_sagemaker`"
    },
    {
      "chunk_id": "fcc89a2dd88d_0",
      "chapter": "sagemaker",
      "heading": "Creating a SageMaker Instance",
      "text": "Next, let's create a notebook instance as described in :numref:`fig_sagemaker-create`.\n\n![Create a SageMaker instance.](../img/sagemaker-create.png)\n:width:`400px`\n:label:`fig_sagemaker-create`\n\nSageMaker provides multiple [instance types](https://aws.amazon.com/sagemaker/pricing/instance-types/) with varying computational power and prices.\nWhen creating a notebook instance,\nwe can specify its name and type.\nIn :numref:`fig_sagemaker-create-2`, we choose `ml.p3.2xlarge`: with one Tesla V100 GPU and an 8-core CPU, this instance is powerful enough for most of the book.\n\n![Choose the instance type.](../img/sagemaker-create-2.png)\n:width:`400px`\n:label:`fig_sagemaker-create-2`\n\n:begin_tab:`mxnet`\nThe entire book in the ipynb format for running with SageMaker is available at https://github.com/d2l-ai/d2l-en-sagemaker. We can specify this GitHub repository URL (:numref:`fig_sagemaker-create-3`) to allow SageMaker to clone it when creating the instance.\n:end_tab:\n\n:begin_tab:`pytorch`\nThe entire book in the ipynb format for running with SageMaker is available at https://github.com/d2l-ai/d2l-pytorch-sagemaker. We can specify this GitHub repository URL (:numref:`fig_sagemaker-create-3`) to allow SageMaker to clone it when creating the instance.\n:end_tab:\n\n:begin_tab:`tensorflow`\nThe entire book in the ipynb format for running with SageMaker is available at https://github.com/d2l-ai/d2l-tensorflow-sagemaker. We can specify this GitHub repository URL (:numref:`fig_sagemaker-create-3`) to allow SageMaker to clone it when creating the instance.\n:end_tab:\n\n![Specify the GitHub repository.](../img/sagemaker-create-3.png)\n:width:`400px`\n:label:`fig_sagemaker-create-3`"
    },
    {
      "chunk_id": "fe956fe573e8_0",
      "chapter": "sagemaker",
      "heading": "Running and Stopping an Instance",
      "text": "Creating an instance\nmay take a few minutes.\nWhen it is ready,\nclick on the \"Open Jupyter\" link next to it (:numref:`fig_sagemaker-open`) so you can\nedit and run all the Jupyter notebooks\nof this book on this instance\n(similar to steps in :numref:`sec_jupyter`).\n\n![Open Jupyter on the created SageMaker instance.](../img/sagemaker-open.png)\n:width:`400px`\n:label:`fig_sagemaker-open`\n\n\nAfter finishing your work,\ndo not forget to stop the instance to avoid \nbeing charged further (:numref:`fig_sagemaker-stop`).\n\n![Stop a SageMaker instance.](../img/sagemaker-stop.png)\n:width:`300px`\n:label:`fig_sagemaker-stop`"
    },
    {
      "chunk_id": "f02c449de26c_0",
      "chapter": "sagemaker",
      "heading": "Updating Notebooks",
      "text": ":begin_tab:`mxnet`\nNotebooks of this open-source book will be regularly updated in the [d2l-ai/d2l-en-sagemaker](https://github.com/d2l-ai/d2l-en-sagemaker) repository\non GitHub.\nTo update to the latest version,\nyou may open a terminal on the SageMaker instance (:numref:`fig_sagemaker-terminal`).\n:end_tab:\n\n:begin_tab:`pytorch`\nNotebooks of this open-source book will be regularly updated in the [d2l-ai/d2l-pytorch-sagemaker](https://github.com/d2l-ai/d2l-pytorch-sagemaker) repository\non GitHub.\nTo update to the latest version,\nyou may open a terminal on the SageMaker instance (:numref:`fig_sagemaker-terminal`).\n:end_tab:\n\n\n:begin_tab:`tensorflow`\nNotebooks of this open-source book will be regularly updated in the [d2l-ai/d2l-tensorflow-sagemaker](https://github.com/d2l-ai/d2l-tensorflow-sagemaker) repository\non GitHub.\nTo update to the latest version,\nyou may open a terminal on the SageMaker instance (:numref:`fig_sagemaker-terminal`).\n:end_tab:\n\n\n![Open a terminal on the SageMaker instance.](../img/sagemaker-terminal.png)\n:width:`300px`\n:label:`fig_sagemaker-terminal`\n\nYou may wish to commit your local changes before pulling updates from the remote repository. \nOtherwise, simply discard all your local changes\nwith the following commands in the terminal:\n\n:begin_tab:`mxnet`\n\n```bash\ncd SageMaker/d2l-en-sagemaker/\ngit reset --hard\ngit pull\n```\n\n\n:end_tab:\n\n:begin_tab:`pytorch`\n\n```bash\ncd SageMaker/d2l-pytorch-sagemaker/\ngit reset --hard\ngit pull\n```\n\n\n:end_tab:\n\n:begin_tab:`tensorflow`\n\n```bash\ncd SageMaker/d2l-tensorflow-sagemaker/\ngit reset --hard\ngit pull\n```\n\n\n:end_tab:"
    },
    {
      "chunk_id": "9544b361519f_0",
      "chapter": "sagemaker",
      "heading": "Summary",
      "text": "* We can create a notebook instance using Amazon SageMaker to run GPU-intensive code of this book.\n* We can update notebooks via the terminal on the Amazon SageMaker instance."
    },
    {
      "chunk_id": "d1ec53272b45_0",
      "chapter": "sagemaker",
      "heading": "Exercises",
      "text": "1. Edit and run any section that requires a GPU using Amazon SageMaker.\n1. Open a terminal to access the local directory that hosts all the notebooks of this book.\n\n\n[Discussions](https://discuss.d2l.ai/t/422)"
    },
    {
      "chunk_id": "f08d1b686af9_0",
      "chapter": "selecting-servers-gpus",
      "heading": "selecting-servers-gpus",
      "text": "# Selecting Servers and GPUs\n:label:`sec_buy_gpu`\n\nDeep learning training generally requires large amounts of computation. At present GPUs are the most cost-effective hardware accelerators for deep learning. In particular, compared with CPUs, GPUs are cheaper and offer higher performance, often by over an order of magnitude. Furthermore, a single server can support multiple GPUs, up to 8 for high end servers. More typical numbers are up to 4 GPUs for an engineering workstation, since heat, cooling, and power requirements escalate quickly beyond what an office building can support. For larger deployments, cloud computing (e.g., Amazon's [P3](https://aws.amazon.com/ec2/instance-types/p3/) and [G4](https://aws.amazon.com/blogs/aws/in-the-works-ec2-instances-g4-with-nvidia-t4-gpus/) instances) is a much more practical solution."
    },
    {
      "chunk_id": "ba99032aafa2_0",
      "chapter": "selecting-servers-gpus",
      "heading": "Selecting Servers",
      "text": "There is typically no need to purchase high-end CPUs with many threads since much of the computation occurs on the GPUs. That said, due to the global interpreter lock (GIL) in Python single-thread performance of a CPU can matter in situations where we have 4--8 GPUs. All things equal this suggests that CPUs with a smaller number of cores but a higher clock frequency might be a more economical choice. For example, when choosing between a 6-core 4 GHz and an 8-core 3.5 GHz CPU, the former is much preferable, even though its aggregate speed is less. An important consideration is that GPUs use lots of power and thus dissipate lots of heat. This requires very good cooling and a large enough chassis to use the GPUs. Follow the guidelines below if possible:\n\n1. **Power Supply**. GPUs use significant amounts of power. Budget with up to 350W per device (check for the *peak demand* of the graphics card rather than typical demand, since efficient code can use lots of energy). If your power supply is not up to the demand you will find that your system becomes unstable. 1. **Chassis Size**. GPUs are large and the auxiliary power connectors often need extra space. Also, large chassis are easier to cool. 1. **GPU Cooling**. If you have a large number of GPUs you might want to invest in water cooling. Also, aim for *reference designs* even if they have fewer fans, since they are thin enough to allow for air intake between the devices. If you buy a multi-fan GPU it might be too thick to get enough air when installing multiple GPUs and you will run into thermal throttling. 1. **PCIe Slots**. Moving data to and from the GPU (and exchanging it between GPUs) requires lots of bandwidth. We recommend PCIe 3.0 slots with 16 lanes. If you mount multiple GPUs, be sure to carefully read the motherboard description to ensure that 16$\\times$ bandwidth is still available when multiple GPUs are used at the same time and that you are getting PCIe 3.0 as opposed to PCIe 2.0 for the additional slots."
    },
    {
      "chunk_id": "ba99032aafa2_1",
      "chapter": "selecting-servers-gpus",
      "heading": "Selecting Servers",
      "text": "Some motherboards downgrade to 8$\\times$ or even 4$\\times$ bandwidth with multiple GPUs installed. This is partly due to the number of PCIe lanes that the CPU offers. In short, here are some recommendations for building a deep learning server:\n\n* **Beginner**. Buy a low end GPU with low power consumption (cheap gaming GPUs suitable for deep learning use 150--200W). If you are lucky your current computer supports it. * **1 GPU**. A low-end CPU with 4 cores will be sufficient and most motherboards suffice. Aim for at least 32 GB DRAM and invest into an SSD for local data access. A power supply with 600W should be sufficient. Buy a GPU with lots of fans. * **2 GPUs**. A low-end CPU with 4-6 cores will suffice. Aim for 64 GB DRAM and invest into an SSD. You will need in the order of 1000W for two high-end GPUs. In terms of mainboards, make sure that they have *two* PCIe 3.0 x16 slots. If you can, get a mainboard that has two free spaces (60mm spacing) between the PCIe 3.0 x16 slots for extra air. In this case, buy two GPUs with lots of fans. * **4 GPUs**. Make sure that you buy a CPU with relatively fast single-thread speed (i.e., high clock frequency). You will probably need a CPU with a larger number of PCIe lanes, such as an AMD Threadripper. You will likely need relatively expensive mainboards to get 4 PCIe 3.0 x16 slots since they probably need a PLX to multiplex the PCIe lanes. Buy GPUs with reference design that are narrow and let air in between the GPUs. You need a 1600--2000W power supply and the outlet in your office might not support that. This server will probably run *loud and hot*. You do not want it under your desk. 128 GB of DRAM is recommended. Get an SSD (1--2 TB NVMe) for local storage and a bunch of hard disks in RAID configuration to store your data. * **8 GPUs**. You need to buy a dedicated multi-GPU server chassis with multiple redundant power supplies (e.g., 2+1 for 1600W per power supply)."
    },
    {
      "chunk_id": "ba99032aafa2_2",
      "chapter": "selecting-servers-gpus",
      "heading": "Selecting Servers",
      "text": "Get an SSD (1--2 TB NVMe) for local storage and a bunch of hard disks in RAID configuration to store your data. * **8 GPUs**. You need to buy a dedicated multi-GPU server chassis with multiple redundant power supplies (e.g., 2+1 for 1600W per power supply). This will require dual socket server CPUs, 256 GB ECC DRAM, a fast network card (10 GBE recommended), and you will need to check whether the servers support the *physical form factor* of the GPUs. Airflow and wiring placement differ significantly between consumer and server GPUs (e.g., RTX 2080 vs. Tesla V100). This means that you might not be able to install the consumer GPU in a server due to insufficient clearance for the power cable or lack of a suitable wiring harness (as one of the coauthors painfully discovered)."
    },
    {
      "chunk_id": "8ede3449a124_0",
      "chapter": "selecting-servers-gpus",
      "heading": "Selecting GPUs",
      "text": "At present, AMD and NVIDIA are the two main manufacturers of dedicated GPUs. NVIDIA was the first to enter the deep learning field and provides better support for deep learning frameworks via CUDA. Therefore, most buyers choose NVIDIA GPUs. NVIDIA provides two types of GPUs, targeting individual users (e.g., via the GTX and RTX series) and enterprise users (via its Tesla series). The two types of GPUs provide comparable compute power. However, the enterprise user GPUs generally use (passive) forced cooling, more memory, and ECC (error correcting) memory. These GPUs are more suitable for data centers and usually cost ten times more than consumer GPUs. If you are a large company with 100+ servers you should consider the NVIDIA Tesla series or alternatively use GPU servers in the cloud. For a lab or a small to medium company with 10+ servers the NVIDIA RTX series is likely most cost effective. You can buy preconfigured servers with Supermicro or Asus chassis that hold 4--8 GPUs efficiently. GPU vendors typically release a new generation every one to two years, such as the GTX 1000 (Pascal) series released in 2017 and the RTX 2000 (Turing) series released in 2019. Each series offers several different models that provide different performance levels. GPU performance is primarily a combination of the following three parameters:\n\n1. **Compute Power**. Generally we look for 32-bit floating-point compute power. 16-bit floating point training (FP16) is also entering the mainstream. If you are only interested in prediction, you can also use 8-bit integer. The latest generation of Turing GPUs offers 4-bit acceleration. Unfortunately at the time of writing the algorithms for training low-precision networks are not yet widespread. 1. **Memory Size**. As your models become larger or the batches used during training grow bigger, you will need more GPU memory. Check for HBM2 (High Bandwidth Memory) vs. GDDR6 (Graphics DDR) memory. HBM2 is faster but much more expensive. 1. **Memory Bandwidth**."
    },
    {
      "chunk_id": "8ede3449a124_1",
      "chapter": "selecting-servers-gpus",
      "heading": "Selecting GPUs",
      "text": "As your models become larger or the batches used during training grow bigger, you will need more GPU memory. Check for HBM2 (High Bandwidth Memory) vs. GDDR6 (Graphics DDR) memory. HBM2 is faster but much more expensive. 1. **Memory Bandwidth**. You can only get the most out of your compute power when you have sufficient memory bandwidth. Look for wide memory buses if using GDDR6. For most users, it is enough to look at compute power. Note that many GPUs offer different types of acceleration. For example, NVIDIA's TensorCores accelerate a subset of operators by 5$\\times$. Ensure that your libraries support this. The GPU memory should be no less than 4 GB (8 GB is much better). Try to avoid using the GPU also for displaying a GUI (use the built-in graphics instead). If you cannot avoid it, add an extra 2 GB of RAM for safety. :numref:`fig_flopsvsprice` compares the 32-bit floating-point compute power and price of the various GTX 900, GTX 1000 and RTX 2000 series models. The prices suggested are those found on Wikipedia at the time of writing. ![Floating-point compute power and price comparison. ](../img/flopsvsprice.svg)\n:label:`fig_flopsvsprice`\n\n\nWe can see a number of things:\n\n1. Within each series, price and performance are roughly proportional. Titan models command a significant premium for the benefit of larger amounts of GPU memory. However, the newer models offer better cost effectiveness, as can be seen by comparing the 980 Ti and 1080 Ti. The price does not appear to improve much for the RTX 2000 series. However, this is due to the fact that they offer far superior low precision performance (FP16, INT8, and INT4). 2. The performance-to-cost ratio of the GTX 1000 series is about two times greater than the 900 series. 3. For the RTX 2000 series the performance (in GFLOPs) is an *affine* function of the price. ![Floating-point compute power and energy consumption."
    },
    {
      "chunk_id": "8ede3449a124_2",
      "chapter": "selecting-servers-gpus",
      "heading": "Selecting GPUs",
      "text": "2. The performance-to-cost ratio of the GTX 1000 series is about two times greater than the 900 series. 3. For the RTX 2000 series the performance (in GFLOPs) is an *affine* function of the price. ![Floating-point compute power and energy consumption. ](../img/wattvsprice.svg)\n:label:`fig_wattvsprice`\n\n\n:numref:`fig_wattvsprice` shows how energy consumption scales mostly linearly with the amount of computation. Second, later generations are more efficient. This seems to be contradicted by the graph corresponding to the RTX 2000 series. However, this is a consequence of the TensorCores that draw disproportionately much energy."
    },
    {
      "chunk_id": "9e50fdccfa13_0",
      "chapter": "selecting-servers-gpus",
      "heading": "Summary",
      "text": "* Watch out for power, PCIe bus lanes, CPU single thread speed, and cooling when building a server.\n* You should purchase the latest GPU generation if possible.\n* Use the cloud for large deployments.\n* High density servers may not be compatible with all GPUs. Check the mechanical and cooling specifications before you buy.\n* Use FP16 or lower precision for high efficiency.\n\n\n[Discussions](https://discuss.d2l.ai/t/425)"
    },
    {
      "chunk_id": "cd327ed47fcc_0",
      "chapter": "utils",
      "heading": "utils",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Utility Functions and Classes\n:label:`sec_utils`\n\n\nThis section contains the implementations of utility functions and classes used in this book. ```{.python .input}\n%%tab mxnet\nimport inspect\nimport collections\nfrom d2l import mxnet as d2l\nfrom IPython import display\nfrom mxnet import autograd, gluon, np, npx\nfrom mxnet.gluon import nn\nimport random\nnpx.set_np()\n```\n\n```{.python .input  n=1}\n%%tab pytorch\nimport inspect\nimport collections\nfrom d2l import torch as d2l\nfrom IPython import display\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport inspect\nfrom IPython import display\nimport collections\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nimport inspect\nfrom IPython import display\nimport collections\nfrom d2l import jax as d2l\nimport jax\n```\n\nHyperparameters. ```{.python .input}\n%%tab all\n@d2l.add_to_class(d2l.HyperParameters)  #@save\ndef save_hyperparameters(self, ignore=[]):\n    \"\"\"Save function arguments into class attributes.\"\"\"\n    frame = inspect.currentframe().f_back\n    _, _, _, local_vars = inspect.getargvalues(frame)\n    self.hparams = {k:v for k, v in local_vars.items()\n                    if k not in set(ignore+['self']) and not k.startswith('_')}\n    for k, v in self.hparams.items():\n        setattr(self, k, v)\n```\n\nProgress bar."
    },
    {
      "chunk_id": "cd327ed47fcc_1",
      "chapter": "utils",
      "heading": "utils",
      "text": "```{.python .input  n=22}\n%%tab all\n@d2l.add_to_class(d2l.ProgressBoard)  #@save\ndef draw(self, x, y, label, every_n=1):\n    Point = collections.namedtuple('Point', ['x', 'y'])\n    if not hasattr(self, 'raw_points'):\n        self.raw_points = collections.OrderedDict()\n        self.data = collections.OrderedDict()\n    if label not in self.raw_points:\n        self.raw_points[label] = []\n        self.data[label] = []    \n    points = self.raw_points[label]\n    line = self.data[label]\n    points.append(Point(x, y))\n    if len(points) != every_n:\n        return    \n    mean = lambda x: sum(x) / len(x)\n    line.append(Point(mean([p.x for p in points]), \n                      mean([p.y for p in points])))\n    points.clear()\n    if not self.display: \n        return\n    d2l.use_svg_display()\n    if self.fig is None:\n        self.fig = d2l.plt.figure(figsize=self.figsize)\n    plt_lines, labels = [], []\n    for (k, v), ls, color in zip(self.data.items(), self.ls, self.colors):        \n        plt_lines.append(d2l.plt.plot([p.x for p in v], [p.y for p in v], \n                                      linestyle=ls, color=color)[0])\n        labels.append(k)        \n    axes = self.axes if self.axes else d2l.plt.gca()\n    if self.xlim: axes.set_xlim(self.xlim)\n    if self.ylim: axes.set_ylim(self.ylim)\n    if not self.xlabel: self.xlabel = self.x    \n    axes.set_xlabel(self.xlabel)\n    axes.set_ylabel(self.ylabel)\n    axes.set_xscale(self.xscale)\n    axes.set_yscale(self.yscale)\n    axes.legend(plt_lines, labels)    \n    display.display(self.fig)\n    display.clear_output(wait=True)\n```\n\nAdd FrozenLake enviroment\n```{.python .input}\n%%tab pytorch\n\ndef frozen_lake(seed): #@save\n    # See https://www.gymlibrary.dev/environments/toy_text/frozen_lake/ to learn more about this env\n    # How to process env.P.items is adpated from https://sites.google.com/view/deep-rl-bootcamp/labs\n    import gym\n\n    env = gym.make('FrozenLake-v1', is_slippery=False)\n    env.seed(seed)\n    env.action_space.np_random.seed(seed)\n    env.action_space.seed(seed)\n    env_info = {}\n    env_info['desc'] = env.desc  # 2D array specifying what each grid item means\n    env_info['num_states'] = env.nS  # Number of observations/states or obs/state dim\n    env_info['num_actions'] = env.nA  # Number of actions or action dim\n    # Define indices for (transition probability, nextstate, reward, done) tuple\n    env_info['trans_prob_idx'] = 0  # Index of transition probability entry\n    env_info['nextstate_idx'] = 1  # Index of next state entry\n    env_info['reward_idx'] = 2  # Index of reward entry\n    env_info['done_idx'] = 3  # Index of done entry\n    env_info['mdp'] = {}\n    env_info['env'] = env\n\n    for (s, others) in env.P.items():\n        # others(s) = {a0: [ (p(s'|s,a0), s', reward, done),...], a1:[...], ...}\n\n        for (a, pxrds) in others.items():\n            # pxrds is [(p1,next1,r1,d1),(p2,next2,r2,d2),..]."
    },
    {
      "chunk_id": "cd327ed47fcc_2",
      "chapter": "utils",
      "heading": "utils",
      "text": "# e.g. [(0.3, 0, 0, False), (0.3, 0, 0, False), (0.3, 4, 1, False)]\n            env_info['mdp'][(s,a)] = pxrds\n\n    return env_info\n\n```\n\nCreate enviroment\n```{.python .input}\n%%tab pytorch\n\ndef make_env(name ='', seed=0): #@save\n    # Input parameters:\n    # name: specifies a gym environment. # For Value iteration, only FrozenLake-v1 is supported. if name == 'FrozenLake-v1':\n        return frozen_lake(seed)\n\n    else:\n        raise ValueError(\"%s env is not supported in this Notebook\")\n\n```\n\nShow value function\n```{.python .input}\n%%tab pytorch\n\ndef show_value_function_progress(env_desc, V, pi): #@save\n    # This function visualizes how value and policy changes over time."
    },
    {
      "chunk_id": "cd327ed47fcc_3",
      "chapter": "utils",
      "heading": "utils",
      "text": "# V: [num_iters, num_states]\n    # pi: [num_iters, num_states]\n    # How to visualize value function is adapted (but changed) from: https://sites.google.com/view/deep-rl-bootcamp/labs\n\n    num_iters = V.shape[0]\n    fig, ax  = plt.subplots(figsize=(15, 15))\n\n    for k in range(V.shape[0]):\n        plt.subplot(4, 4, k + 1)\n        plt.imshow(V[k].reshape(4,4), cmap=\"bone\")\n        ax = plt.gca()\n        ax.set_xticks(np.arange(0, 5)-.5, minor=True)\n        ax.set_yticks(np.arange(0, 5)-.5, minor=True)\n        ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n        ax.tick_params(which=\"minor\", bottom=False, left=False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # LEFT action: 0, DOWN action: 1\n        # RIGHT action: 2, UP action: 3\n        action2dxdy = {0:(-.25, 0),1: (0, .25),\n                       2:(0.25, 0),3: (-.25, 0)}\n\n        for y in range(4):\n            for x in range(4):\n                action = pi[k].reshape(4,4)[y, x]\n                dx, dy = action2dxdy[action]\n\n                if env_desc[y,x].decode() == 'H':\n                    ax.text(x, y, str(env_desc[y,x].decode()),\n                       ha=\"center\", va=\"center\", color=\"y\",\n                         size=20, fontweight='bold')\n\n                elif env_desc[y,x].decode() == 'G':\n                    ax.text(x, y, str(env_desc[y,x].decode()),\n                       ha=\"center\", va=\"center\", color=\"w\",\n                         size=20, fontweight='bold')\n\n                else:\n                    ax.text(x, y, str(env_desc[y,x].decode()),\n                       ha=\"center\", va=\"center\", color=\"g\",\n                         size=15, fontweight='bold')\n\n                # No arrow for cells with G and H labels\n                if env_desc[y,x].decode() != 'G' and env_desc[y,x].decode() != 'H':\n                    ax.arrow(x, y, dx, dy, color='r', head_width=0.2, head_length=0.15)\n\n        ax.set_title(\"Step = \"  + str(k + 1), fontsize=20)\n\n    fig.tight_layout()\n    plt.show()\n\n```\nShow Q function\n```{.python .input}\n%%tab pytorch\n\ndef show_Q_function_progress(env_desc, V_all, pi_all): #@save\n    # This function visualizes how value and policy changes over time."
    },
    {
      "chunk_id": "cd327ed47fcc_4",
      "chapter": "utils",
      "heading": "utils",
      "text": "# V: [num_iters, num_states]\n    # pi: [num_iters, num_states]\n\n    # We want to only shows few values\n    num_iters_all = V_all.shape[0]\n    num_iters = num_iters_all // 10\n\n    vis_indx = np.arange(0, num_iters_all, num_iters).tolist()\n    vis_indx.append(num_iters_all - 1)\n    V = np.zeros((len(vis_indx), V_all.shape[1]))\n    pi = np.zeros((len(vis_indx), V_all.shape[1]))\n\n    for c, i in enumerate(vis_indx):\n        V[c]  = V_all[i]\n        pi[c] = pi_all[i]\n\n    num_iters = V.shape[0]\n    fig, ax = plt.subplots(figsize=(15, 15))\n\n    for k in range(V.shape[0]):\n        plt.subplot(4, 4, k + 1)\n        plt.imshow(V[k].reshape(4,4), cmap=\"bone\")\n        ax = plt.gca()\n        ax.set_xticks(np.arange(0, 5)-.5, minor=True)\n        ax.set_yticks(np.arange(0, 5)-.5, minor=True)\n        ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n        ax.tick_params(which=\"minor\", bottom=False, left=False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # LEFT action: 0, DOWN action: 1\n        # RIGHT action: 2, UP action: 3\n        action2dxdy = {0:(-.25, 0),1:(0, .25),\n                       2:(0.25, 0),3:(-.25, 0)}\n\n        for y in range(4):\n            for x in range(4):\n                action = pi[k].reshape(4,4)[y, x]\n                dx, dy = action2dxdy[action]\n\n                if env_desc[y,x].decode() == 'H':\n                    ax.text(x, y, str(env_desc[y,x].decode()),\n                       ha=\"center\", va=\"center\", color=\"y\",\n                         size=20, fontweight='bold')\n\n                elif env_desc[y,x].decode() == 'G':\n                    ax.text(x, y, str(env_desc[y,x].decode()),\n                       ha=\"center\", va=\"center\", color=\"w\",\n                         size=20, fontweight='bold')\n\n                else:\n                    ax.text(x, y, str(env_desc[y,x].decode()),\n                       ha=\"center\", va=\"center\", color=\"g\",\n                         size=15, fontweight='bold')\n\n                # No arrow for cells with G and H labels\n                if env_desc[y,x].decode() != 'G' and env_desc[y,x].decode() != 'H':\n                    ax.arrow(x, y, dx, dy, color='r', head_width=0.2, head_length=0.15)\n\n        ax.set_title(\"Step = \"  + str(vis_indx[k] + 1), fontsize=20)\n\n    fig.tight_layout()\n    plt.show()\n\n```\n\nTrainer\n\nA bunch of functions that will be deprecated:\n\n```{.python .input}\n%%tab mxnet\ndef load_array(data_arrays, batch_size, is_train=True):  #@save\n    \"\"\"Construct a Gluon data iterator.\"\"\"\n    dataset = gluon.data.ArrayDataset(*data_arrays)\n    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"Generate y = Xw + b + noise.\"\"\"\n    X = d2l.normal(0, 1, (num_examples, len(w)))\n    y = d2l.matmul(X, w) + b\n    y += d2l.normal(0, 0.01, y.shape)\n    return X, d2l.reshape(y, (-1, 1))\n\ndef sgd(params, lr, batch_size):  #@save\n    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n    for param in params:\n        param[:] = param - lr * param.grad / batch_size\n\ndef get_dataloader_workers():  #@save\n    \"\"\"Use 4 processes to read the data except for Windows.\"\"\"\n    return 0 if sys.platform.startswith('win') else 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):  #@save\n    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n    dataset = gluon.data.vision\n    trans = [dataset.transforms.ToTensor()]\n    if resize:\n        trans.insert(0, dataset.transforms.Resize(resize))\n    trans = dataset.transforms.Compose(trans)\n    mnist_train = dataset.FashionMNIST(train=True).transform_first(trans)\n    mnist_test = dataset.FashionMNIST(train=False).transform_first(trans)\n    return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True,\n                                  num_workers=get_dataloader_workers()),\n            gluon.data.DataLoader(mnist_test, batch_size, shuffle=False,\n                                  num_workers=get_dataloader_workers()))\n\ndef evaluate_accuracy_gpu(net, data_iter, device=None):  #@save\n    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n    if not device:  # Query the first device where the first parameter is on\n        device = list(net.collect_params().values())[0].list_ctx()[0]\n    # No."
    },
    {
      "chunk_id": "cd327ed47fcc_5",
      "chapter": "utils",
      "heading": "utils",
      "text": "of correct predictions, no. of predictions\n    metric = d2l.Accumulator(2)\n    for X, y in data_iter:\n        X, y = X.as_in_ctx(device), y.as_in_ctx(device)\n        metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n    return metric[0] / metric[1]\n\n#@save\ndef train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\n    net.initialize(force_reinit=True, ctx=device, init=init.Xavier())\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n    trainer = gluon.Trainer(net.collect_params(),\n                            'sgd', {'learning_rate': lr})\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=['train loss', 'train acc', 'test acc'])\n    timer, num_batches = d2l.Timer(), len(train_iter)\n    for epoch in range(num_epochs):\n        # Sum of training loss, sum of training accuracy, no."
    },
    {
      "chunk_id": "cd327ed47fcc_6",
      "chapter": "utils",
      "heading": "utils",
      "text": "of examples\n        metric = d2l.Accumulator(3)\n        for i, (X, y) in enumerate(train_iter):\n            timer.start()\n            # Here is the major difference from `d2l.train_epoch_ch3`\n            X, y = X.as_in_ctx(device), y.as_in_ctx(device)\n            with autograd.record():\n                y_hat = net(X)\n                l = loss(y_hat, y)\n            l.backward()\n            trainer.step(X.shape[0])\n            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])\n            timer.stop()\n            train_l = metric[0] / metric[2]\n            train_acc = metric[1] / metric[2]\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (train_l, train_acc, None))\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n          f'test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n          f'on {str(device)}')\n    \ndef grad_clipping(net, theta):  #@save\n    \"\"\"Clip the gradient.\"\"\"\n    if isinstance(net, gluon.Block):\n        params = [p.data() for p in net.collect_params().values()]\n    else:\n        params = net.params\n    norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))\n    if norm > theta:\n        for param in params:\n            param.grad[:] *= theta / norm\n```\n\n```{.python .input}\n%%tab pytorch\n\ndef load_array(data_arrays, batch_size, is_train=True):  #@save\n    \"\"\"Construct a PyTorch data iterator.\"\"\"\n    dataset = torch.utils.data.TensorDataset(*data_arrays)\n    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"Generate y = Xw + b + noise.\"\"\"\n    X = d2l.normal(0, 1, (num_examples, len(w)))\n    y = d2l.matmul(X, w) + b\n    y += d2l.normal(0, 0.01, y.shape)\n    return X, d2l.reshape(y, (-1, 1))\n\ndef sgd(params, lr, batch_size): #@save\n    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n    with torch.no_grad():\n        for param in params:\n            param -= lr * param.grad / batch_size\n            param.grad.zero_()\n\ndef get_dataloader_workers():  #@save\n    \"\"\"Use 4 processes to read the data.\"\"\"\n    return 4\n\ndef load_data_fashion_mnist(batch_size, resize=None):  #@save\n    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n    trans = [transforms.ToTensor()]\n    if resize:\n        trans.insert(0, transforms.Resize(resize))\n    trans = transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(\n        root=\"../data\", train=True, transform=trans, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(\n        root=\"../data\", train=False, transform=trans, download=True)\n    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n                                        num_workers=get_dataloader_workers()),\n            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n                                        num_workers=get_dataloader_workers()))\n\ndef evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n    if isinstance(net, nn.Module):\n        net.eval()  # Set the model to evaluation mode\n        if not device:\n            device = next(iter(net.parameters())).device\n    # No."
    },
    {
      "chunk_id": "cd327ed47fcc_7",
      "chapter": "utils",
      "heading": "utils",
      "text": "of correct predictions, no. of predictions\n    metric = d2l.Accumulator(2)\n\n    with torch.no_grad():\n        for X, y in data_iter:\n            if isinstance(X, list):\n                # Required for BERT Fine-tuning (to be covered later)\n                X = [x.to(device) for x in X]\n            else:\n                X = X.to(device)\n            y = y.to(device)\n            metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n    return metric[0] / metric[1]\n\n\n#@save\ndef train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\n    def init_weights(m):\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n            nn.init.xavier_uniform_(m.weight)\n    net.apply(init_weights)\n    print('training on', device)\n    net.to(device)\n    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n    loss = nn.CrossEntropyLoss()\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=['train loss', 'train acc', 'test acc'])\n    timer, num_batches = d2l.Timer(), len(train_iter)\n    for epoch in range(num_epochs):\n        # Sum of training loss, sum of training accuracy, no."
    },
    {
      "chunk_id": "cd327ed47fcc_8",
      "chapter": "utils",
      "heading": "utils",
      "text": "of examples\n        metric = d2l.Accumulator(3)\n        net.train()\n        for i, (X, y) in enumerate(train_iter):\n            timer.start()\n            optimizer.zero_grad()\n            X, y = X.to(device), y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            optimizer.step()\n            with torch.no_grad():\n                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n            timer.stop()\n            train_l = metric[0] / metric[2]\n            train_acc = metric[1] / metric[2]\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (train_l, train_acc, None))\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n          f'test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n          f'on {str(device)}')\n```\n\n```{.python .input}\n%%tab tensorflow\n\ndef load_array(data_arrays, batch_size, is_train=True):  #@save\n    \"\"\"Construct a TensorFlow data iterator.\"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n    if is_train:\n        dataset = dataset.shuffle(buffer_size=1000)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\ndef synthetic_data(w, b, num_examples):  #@save\n    \"\"\"Generate y = Xw + b + noise.\"\"\"\n    X = tf.zeros((num_examples, w.shape[0]))\n    X += tf.random.normal(shape=X.shape)\n    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b\n    y += tf.random.normal(shape=y.shape, stddev=0.01)\n    y = tf.reshape(y, (-1, 1))\n    return X, y\n\n\ndef sgd(params, grads, lr, batch_size):  #@save\n    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n    for param, grad in zip(params, grads):\n        param.assign_sub(lr * grad / batch_size)\n\ndef load_data_fashion_mnist(batch_size, resize=None):   #@save\n    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()\n    # Divide all numbers by 255 so that all pixel values are between\n    # 0 and 1, add a batch dimension at the last."
    },
    {
      "chunk_id": "cd327ed47fcc_9",
      "chapter": "utils",
      "heading": "utils",
      "text": "And cast label to int32\n    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n                            tf.cast(y, dtype='int32'))\n    resize_fn = lambda X, y: (\n        tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n    return (\n        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(\n            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),\n        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(\n            batch_size).map(resize_fn))\n\nclass TrainCallback(tf.keras.callbacks.Callback):  #@save\n    \"\"\"A callback to visiualize the training progress.\"\"\"\n    def __init__(self, net, train_iter, test_iter, num_epochs, device_name):\n        self.timer = d2l.Timer()\n        self.animator = d2l.Animator(\n            xlabel='epoch', xlim=[1, num_epochs], legend=[\n                'train loss', 'train acc', 'test acc'])\n        self.net = net\n        self.train_iter = train_iter\n        self.test_iter = test_iter\n        self.num_epochs = num_epochs\n        self.device_name = device_name\n    def on_epoch_begin(self, epoch, logs=None):\n        self.timer.start()\n    def on_epoch_end(self, epoch, logs):\n        self.timer.stop()\n        test_acc = self.net.evaluate(\n            self.test_iter, verbose=0, return_dict=True)['accuracy']\n        metrics = (logs['loss'], logs['accuracy'], test_acc)\n        self.animator.add(epoch + 1, metrics)\n        if epoch == self.num_epochs - 1:\n            batch_size = next(iter(self.train_iter))[0].shape[0]\n            num_examples = batch_size * tf.data.experimental.cardinality(\n                self.train_iter).numpy()\n            print(f'loss {metrics[0]:.3f}, train acc {metrics[1]:.3f}, '\n                  f'test acc {metrics[2]:.3f}')\n            print(f'{num_examples / self.timer.avg():.1f} examples/sec on '\n                  f'{str(self.device_name)}')\n\n#@save\ndef train_ch6(net_fn, train_iter, test_iter, num_epochs, lr, device):\n    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\n    device_name = device._device_name\n    strategy = tf.distribute.OneDeviceStrategy(device_name)\n    with strategy.scope():\n        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        net = net_fn()\n        net.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    callback = TrainCallback(net, train_iter, test_iter, num_epochs,\n                             device_name)\n    net.fit(train_iter, epochs=num_epochs, verbose=0, callbacks=[callback])\n    return net\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow\ndef evaluate_accuracy(net, data_iter):  #@save\n    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n    metric = Accumulator(2)  # No."
    },
    {
      "chunk_id": "cd327ed47fcc_10",
      "chapter": "utils",
      "heading": "utils",
      "text": "of correct predictions, no."
    },
    {
      "chunk_id": "cd327ed47fcc_11",
      "chapter": "utils",
      "heading": "utils",
      "text": "of correct predictions, no. of predictions\n    for X, y in data_iter:\n        metric.add(accuracy(net(X), y), d2l.size(y))\n    return metric[0] / metric[1]\n```\n\n```{.python .input}\n%%tab all\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n    \"\"\"Plot a list of images.\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten()\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        try:\n            img = d2l.numpy(img)\n        except:\n            pass\n        ax.imshow(img)\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n```\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n\ndef linreg(X, w, b):  #@save\n    \"\"\"The linear regression model.\"\"\"\n    return d2l.matmul(X, w) + b\n\ndef squared_loss(y_hat, y):  #@save\n    \"\"\"Squared loss.\"\"\"\n    return (y_hat - d2l.reshape(y, y_hat.shape)) ** 2 / 2\n\ndef get_fashion_mnist_labels(labels):  #@save\n    \"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n\n#@tab pytorch, mxnet, tensorflow\nclass Animator:  #@save\n    \"\"\"For plotting data in animation.\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        # Incrementally plot multiple lines\n        if legend is None:\n            legend = []\n        d2l.use_svg_display()\n        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes, ]\n        # Use a lambda function to capture arguments\n        self.config_axes = lambda: d2l.set_axes(\n            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # Add multiple data points into the figure\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n#@tab pytorch, mxnet, tensorflow\nclass Accumulator:  #@save\n    \"\"\"For accumulating sums over `n` variables.\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\n#@tab pytorch, mxnet, tensorflow\ndef accuracy(y_hat, y):  #@save\n    \"\"\"Compute the number of correct predictions.\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n        y_hat = d2l.argmax(y_hat, axis=1)\n    cmp = d2l.astype(y_hat, y.dtype) == y\n    return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))\n```\n\n```{.python .input}\n%%tab all\n\nimport os\nimport requests\nimport zipfile\nimport tarfile\nimport hashlib\n\ndef download(url, folder='../data', sha1_hash=None):  #@save\n    \"\"\"Download a file to folder and return the local filepath.\"\"\"\n    if not url.startswith('http'):\n        # For back compatability\n        url, sha1_hash = DATA_HUB[url]\n    os.makedirs(folder, exist_ok=True)\n    fname = os.path.join(folder, url.split('/')[-1])\n    # Check if hit cache\n    if os.path.exists(fname) and sha1_hash:\n        sha1 = hashlib.sha1()\n        with open(fname, 'rb') as f:\n            while True:\n                data = f.read(1048576)\n                if not data:\n                    break\n                sha1.update(data)\n        if sha1.hexdigest() == sha1_hash:\n            return fname\n    # Download\n    print(f'Downloading {fname} from {url}...')\n    r = requests.get(url, stream=True, verify=True)\n    with open(fname, 'wb') as f:\n        f.write(r.content)\n    return fname\n\ndef extract(filename, folder=None):  #@save\n    \"\"\"Extract a zip/tar file into folder.\"\"\"\n    base_dir = os.path.dirname(filename)\n    _, ext = os.path.splitext(filename)\n    assert ext in ('.zip', '.tar', '.gz'), 'Only support zip/tar files.'\n    if ext == '.zip':\n        fp = zipfile.ZipFile(filename, 'r')\n    else:\n        fp = tarfile.open(filename, 'r')\n    if folder is None:\n        folder = base_dir\n    fp.extractall(folder)\n```\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n\ndef download_extract(name, folder=None):  #@save\n    \"\"\"Download and extract a zip/tar file.\"\"\"\n    fname = download(name)\n    base_dir = os.path.dirname(fname)\n    data_dir, ext = os.path.splitext(fname)\n    if ext == '.zip':\n        fp = zipfile.ZipFile(fname, 'r')\n    elif ext in ('.tar', '.gz'):\n        fp = tarfile.open(fname, 'r')\n    else:\n        assert False, 'Only zip/tar files can be extracted.'\n    fp.extractall(base_dir)\n    return os.path.join(base_dir, folder) if folder else data_dir\n\n\ndef tokenize(lines, token='word'):  #@save\n    \"\"\"Split text lines into word or character tokens.\"\"\"\n    assert token in ('word', 'char'), 'Unknown token type: ' + token\n    return [line.split() if token == 'word' else list(line) for line in lines]\n\n```\n\n```{.python .input}\n%%tab pytorch\n\ndef evaluate_loss(net, data_iter, loss):  #@save\n    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n    metric = d2l.Accumulator(2)  # Sum of losses, no."
    },
    {
      "chunk_id": "cd327ed47fcc_12",
      "chapter": "utils",
      "heading": "utils",
      "text": "of examples\n    for X, y in data_iter:\n        out = net(X)\n        y = d2l.reshape(y, out.shape)\n        l = loss(out, y)\n        metric.add(d2l.reduce_sum(l), d2l.size(l))\n    return metric[0] / metric[1]\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow\ndef evaluate_loss(net, data_iter, loss):  #@save\n    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n    metric = d2l.Accumulator(2)  # Sum of losses, no. of examples\n    for X, y in data_iter:\n        l = loss(net(X), y)\n        metric.add(d2l.reduce_sum(l), d2l.size(l))\n    return metric[0] / metric[1]\n```\n\n```{.python .input}\n%%tab pytorch\ndef grad_clipping(net, theta):  #@save\n    \"\"\"Clip the gradient.\"\"\"\n    if isinstance(net, nn.Module):\n        params = [p for p in net.parameters() if p.requires_grad]\n    else:\n        params = net.params\n    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n    if norm > theta:\n        for param in params:\n            param.grad[:] *= theta / norm\n```\n\n```{.python .input}\n%%tab tensorflow\ndef grad_clipping(grads, theta):  #@save\n    \"\"\"Clip the gradient.\"\"\"\n    theta = tf.constant(theta, dtype=tf.float32)\n    new_grad = []\n    for grad in grads:\n        if isinstance(grad, tf.IndexedSlices):\n            new_grad.append(tf.convert_to_tensor(grad))\n        else:\n            new_grad.append(grad)\n    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)).numpy()\n                        for grad in new_grad))\n    norm = tf.cast(norm, tf.float32)\n    if tf.greater(norm, theta):\n        for i, grad in enumerate(new_grad):\n            new_grad[i] = grad * theta / norm\n    else:\n        new_grad = new_grad\n    return new_grad\n```\n\nMore for the attention chapter."
    },
    {
      "chunk_id": "cd327ed47fcc_13",
      "chapter": "utils",
      "heading": "utils",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n#@save\nd2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',\n                           '94646ad1522d915e7b0f9296181140edcf86a4f5')\n\n#@save\ndef read_data_nmt():\n    \"\"\"Load the English-French dataset.\"\"\"\n    data_dir = d2l.download_extract('fra-eng')\n    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\n        return f.read()\n\n#@save\ndef preprocess_nmt(text):\n    \"\"\"Preprocess the English-French dataset.\"\"\"\n    def no_space(char, prev_char):\n        return char in set(',.!?') and prev_char != ' '\n\n    # Replace non-breaking space with space, and convert uppercase letters to\n    # lowercase ones\n    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n    # Insert space between words and punctuation marks\n    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n           for i, char in enumerate(text)]\n    return ''.join(out)\n\n#@save\ndef tokenize_nmt(text, num_examples=None):\n    \"\"\"Tokenize the English-French dataset.\"\"\"\n    source, target = [], []\n    for i, line in enumerate(text.split('\\n')):\n        if num_examples and i > num_examples:\n            break\n        parts = line.split('\\t')\n        if len(parts) == 2:\n            source.append(parts[0].split(' '))\n            target.append(parts[1].split(' '))\n    return source, target\n\n    \n#@save\ndef truncate_pad(line, num_steps, padding_token):\n    \"\"\"Truncate or pad sequences.\"\"\"\n    if len(line) > num_steps:\n        return line[:num_steps]  # Truncate\n    return line + [padding_token] * (num_steps - len(line))  # Pad\n\n\n#@save\ndef build_array_nmt(lines, vocab, num_steps):\n    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n    lines = [vocab[l] for l in lines]\n    lines = [l + [vocab['<eos>']] for l in lines]\n    array = d2l.tensor([truncate_pad(\n        l, num_steps, vocab['<pad>']) for l in lines])\n    valid_len = d2l.reduce_sum(\n        d2l.astype(array != vocab['<pad>'], d2l.int32), 1)\n    return array, valid_len\n\n\n#@save\ndef load_data_nmt(batch_size, num_steps, num_examples=600):\n    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n    text = preprocess_nmt(read_data_nmt())\n    source, target = tokenize_nmt(text, num_examples)\n    src_vocab = d2l.Vocab(source, min_freq=2,\n                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n    tgt_vocab = d2l.Vocab(target, min_freq=2,\n                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n    data_iter = d2l.load_array(data_arrays, batch_size)\n    return data_iter, src_vocab, tgt_vocab\n```\n\n```{.python .input}\n%%tab mxnet\n    \n#@save\nclass MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n    # `label` shape: (`batch_size`, `num_steps`)\n    # `valid_len` shape: (`batch_size`,)\n    def forward(self, pred, label, valid_len):\n        # `weights` shape: (`batch_size`, `num_steps`, 1)\n        weights = np.expand_dims(np.ones_like(label), axis=-1)\n        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)\n\n#@save\ndef train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n    \"\"\"Train a model for sequence to sequence.\"\"\"\n    net.initialize(init.Xavier(), force_reinit=True, ctx=device)\n    trainer = gluon.Trainer(net.collect_params(), 'adam',\n                            {'learning_rate': lr})\n    loss = MaskedSoftmaxCELoss()\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[10, num_epochs])\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(2)  # Sum of training loss, no."
    },
    {
      "chunk_id": "cd327ed47fcc_14",
      "chapter": "utils",
      "heading": "utils",
      "text": "of tokens\n        for batch in data_iter:\n            X, X_valid_len, Y, Y_valid_len = [\n                x.as_in_ctx(device) for x in batch]\n            bos = np.array(\n                [tgt_vocab['<bos>']] * Y.shape[0], ctx=device).reshape(-1, 1)\n            dec_input = d2l.concat([bos, Y[:, :-1]], 1)  # Teacher forcing\n            with autograd.record():\n                Y_hat, _ = net(X, dec_input, X_valid_len)\n                l = loss(Y_hat, Y, Y_valid_len)\n            l.backward()\n            d2l.grad_clipping(net, 1)\n            num_tokens = Y_valid_len.sum()\n            trainer.step(num_tokens)\n            metric.add(l.sum(), num_tokens)\n        if (epoch + 1) % 10 == 0:\n            animator.add(epoch + 1, (metric[0] / metric[1],))\n    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n          f'tokens/sec on {str(device)}')\n\n#@save\ndef predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n                    device, save_attention_weights=False):\n    \"\"\"Predict for sequence to sequence.\"\"\"\n    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n        src_vocab['<eos>']]\n    enc_valid_len = np.array([len(src_tokens)], ctx=device)\n    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n    # Add the batch axis\n    enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\n    enc_outputs = net.encoder(enc_X, enc_valid_len)\n    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n    # Add the batch axis\n    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), axis=0)\n    output_seq, attention_weight_seq = [], []\n    for _ in range(num_steps):\n        Y, dec_state = net.decoder(dec_X, dec_state)\n        # We use the token with the highest prediction likelihood as input\n        # of the decoder at the next time step\n        dec_X = Y.argmax(axis=2)\n        pred = dec_X.squeeze(axis=0).astype('int32').item()\n        # Save attention weights (to be covered later)\n        if save_attention_weights:\n            attention_weight_seq.append(net.decoder.attention_weights)\n        # Once the end-of-sequence token is predicted, the generation of the\n        # output sequence is complete\n        if pred == tgt_vocab['<eos>']:\n            break\n        output_seq.append(pred)\n    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq\n```\n\n```{.python .input}\n%%tab pytorch\n#@save\ndef sequence_mask(X, valid_len, value=0):\n    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n    maxlen = X.size(1)\n    mask = torch.arange((maxlen), dtype=torch.float32,\n                        device=X.device)[None, :] < valid_len[:, None]\n    X[~mask] = value\n    return X\n\n    \n#@save\nclass MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n    # `label` shape: (`batch_size`, `num_steps`)\n    # `valid_len` shape: (`batch_size`,)\n    def forward(self, pred, label, valid_len):\n        weights = torch.ones_like(label)\n        weights = sequence_mask(weights, valid_len)\n        self.reduction='none'\n        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n            pred.permute(0, 2, 1), label)\n        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n        return weighted_loss\n    \n#@save\ndef train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n    \"\"\"Train a model for sequence to sequence.\"\"\"\n    def xavier_init_weights(m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n        if type(m) == nn.GRU:\n            for param in m._flat_weights_names:\n                if \"weight\" in param:\n                    nn.init.xavier_uniform_(m._parameters[param])\n    net.apply(xavier_init_weights)\n    net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    loss = MaskedSoftmaxCELoss()\n    net.train()\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[10, num_epochs])\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(2)  # Sum of training loss, no."
    },
    {
      "chunk_id": "cd327ed47fcc_15",
      "chapter": "utils",
      "heading": "utils",
      "text": "of tokens\n        for batch in data_iter:\n            optimizer.zero_grad()\n            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n                               device=device).reshape(-1, 1)\n            dec_input = d2l.concat([bos, Y[:, :-1]], 1)  # Teacher forcing\n            Y_hat, _ = net(X, dec_input, X_valid_len)\n            l = loss(Y_hat, Y, Y_valid_len)\n            l.sum().backward()  # Make the loss scalar for `backward`\n            d2l.grad_clipping(net, 1)\n            num_tokens = Y_valid_len.sum()\n            optimizer.step()\n            with torch.no_grad():\n                metric.add(l.sum(), num_tokens)\n        if (epoch + 1) % 10 == 0:\n            animator.add(epoch + 1, (metric[0] / metric[1],))\n    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n          f'tokens/sec on {str(device)}')\n    \n\n#@save\ndef predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n                    device, save_attention_weights=False):\n    \"\"\"Predict for sequence to sequence.\"\"\"\n    # Set `net` to eval mode for inference\n    net.eval()\n    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n        src_vocab['<eos>']]\n    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n    # Add the batch axis\n    enc_X = torch.unsqueeze(\n        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n    enc_outputs = net.encoder(enc_X, enc_valid_len)\n    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n    # Add the batch axis\n    dec_X = torch.unsqueeze(torch.tensor(\n        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n    output_seq, attention_weight_seq = [], []\n    for _ in range(num_steps):\n        Y, dec_state = net.decoder(dec_X, dec_state)\n        # We use the token with the highest prediction likelihood as input\n        # of the decoder at the next time step\n        dec_X = Y.argmax(dim=2)\n        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n        # Save attention weights (to be covered later)\n        if save_attention_weights:\n            attention_weight_seq.append(net.decoder.attention_weights)\n        # Once the end-of-sequence token is predicted, the generation of the\n        # output sequence is complete\n        if pred == tgt_vocab['<eos>']:\n            break\n        output_seq.append(pred)\n    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq\n```\n\n```{.python .input}\n%%tab tensorflow\n#@save\ndef sequence_mask(X, valid_len, value=0):\n    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n    maxlen = X.shape[1]\n    mask = tf.range(start=0, limit=maxlen, dtype=tf.float32)[\n        None, :] < tf.cast(valid_len[:, None], dtype=tf.float32)\n    \n    if len(X.shape) == 3:\n        return tf.where(tf.expand_dims(mask, axis=-1), X, value)\n    else:\n        return tf.where(mask, X, value)\n\n    \n#@save\nclass MaskedSoftmaxCELoss(tf.keras.losses.Loss):\n    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n    def __init__(self, valid_len):\n        super().__init__(reduction='none')\n        self.valid_len = valid_len\n    \n    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n    # `label` shape: (`batch_size`, `num_steps`)\n    # `valid_len` shape: (`batch_size`,)\n    def call(self, label, pred):\n        weights = tf.ones_like(label, dtype=tf.float32)\n        weights = sequence_mask(weights, self.valid_len)\n        label_one_hot = tf.one_hot(label, depth=pred.shape[-1])\n        unweighted_loss = tf.keras.losses.CategoricalCrossentropy(\n            from_logits=True, reduction='none')(label_one_hot, pred)\n        weighted_loss = tf.reduce_mean((unweighted_loss*weights), axis=1)\n        return weighted_loss\n    \n#@save\ndef train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n    \"\"\"Train a model for sequence to sequence.\"\"\"\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    animator = d2l.Animator(xlabel=\"epoch\", ylabel=\"loss\",\n                            xlim=[10, num_epochs])\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(2)  # Sum of training loss, no."
    },
    {
      "chunk_id": "cd327ed47fcc_16",
      "chapter": "utils",
      "heading": "utils",
      "text": "of tokens\n        for batch in data_iter:\n            X, X_valid_len, Y, Y_valid_len = [x for x in batch]\n            bos = tf.reshape(tf.constant([tgt_vocab['<bos>']] * Y.shape[0]),\n                             shape=(-1, 1))\n            dec_input = tf.concat([bos, Y[:, :-1]], 1)  # Teacher forcing\n            with tf.GradientTape() as tape:\n                Y_hat, _ = net(X, dec_input, X_valid_len, training=True)\n                l = MaskedSoftmaxCELoss(Y_valid_len)(Y, Y_hat)\n            gradients = tape.gradient(l, net.trainable_variables)\n            gradients = d2l.grad_clipping(gradients, 1)\n            optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n            num_tokens = tf.reduce_sum(Y_valid_len).numpy()\n            metric.add(tf.reduce_sum(l), num_tokens)\n        if (epoch + 1) % 10 == 0:\n            animator.add(epoch + 1, (metric[0] / metric[1],))\n    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n          f'tokens/sec on {str(device._device_name)}')\n    \n#@save\ndef predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n                    save_attention_weights=False):\n    \"\"\"Predict for sequence to sequence.\"\"\"\n    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n        src_vocab['<eos>']]\n    enc_valid_len = tf.constant([len(src_tokens)])\n    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n    # Add the batch axis\n    enc_X = tf.expand_dims(src_tokens, axis=0)\n    enc_outputs = net.encoder(enc_X, enc_valid_len, training=False)\n    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n    # Add the batch axis\n    dec_X = tf.expand_dims(tf.constant([tgt_vocab['<bos>']]), axis=0)\n    output_seq, attention_weight_seq = [], []\n    for _ in range(num_steps):\n        Y, dec_state = net.decoder(dec_X, dec_state, training=False)\n        # We use the token with the highest prediction likelihood as input\n        # of the decoder at the next time step\n        dec_X = tf.argmax(Y, axis=2)\n        pred = tf.squeeze(dec_X, axis=0)\n        # Save attention weights\n        if save_attention_weights:\n            attention_weight_seq.append(net.decoder.attention_weights)\n        # Once the end-of-sequence token is predicted, the generation of the\n        # output sequence is complete\n        if pred == tgt_vocab['<eos>']:\n            break\n        output_seq.append(pred.numpy())\n    return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, shape = -1).numpy().tolist())), attention_weight_seq\n```"
    },
    {
      "chunk_id": "27c30c9df920_0",
      "chapter": "attention-pooling",
      "heading": "attention-pooling",
      "text": "# Attention Pooling by Similarity\n\n:label:`sec_attention-pooling`\n\nNow that we have introduced the primary components of the attention mechanism, let's use them in a rather classical setting, namely regression and classification via kernel density estimation :cite:`Nadaraya.1964,Watson.1964`. This detour simply provides additional background: it is entirely optional and can be skipped if needed. At their core, Nadaraya--Watson estimators rely on some similarity kernel $\\alpha(\\mathbf{q}, \\mathbf{k})$ relating queries $\\mathbf{q}$ to keys $\\mathbf{k}$. Some common kernels are\n\n$$\\begin{aligned}\n\\alpha(\\mathbf{q}, \\mathbf{k}) & = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{k}\\|^2 \\right) && \\textrm{Gaussian;} \\\\\n\\alpha(\\mathbf{q}, \\mathbf{k}) & = 1 \\textrm{ if } \\|\\mathbf{q} - \\mathbf{k}\\| \\leq 1 && \\textrm{Boxcar;} \\\\\n\\alpha(\\mathbf{q}, \\mathbf{k}) & = \\mathop{\\mathrm{max}}\\left(0, 1 - \\|\\mathbf{q} - \\mathbf{k}\\|\\right) && \\textrm{Epanechikov.}\n\\end{aligned}\n$$\n\nThere are many more choices that we could pick. See a [Wikipedia article](https://en.wikipedia.org/wiki/Kernel_(statistics)) for a more extensive review and how the choice of kernels is related to kernel density estimation, sometimes also called *Parzen Windows* :cite:`parzen1957consistent`. All of the kernels are heuristic and can be tuned. For instance, we can adjust the width, not only on a global basis but even on a per-coordinate basis. Regardless, all of them lead to the following equation for regression and classification alike:\n\n$$f(\\mathbf{q}) = \\sum_i \\mathbf{v}_i \\frac{\\alpha(\\mathbf{q}, \\mathbf{k}_i)}{\\sum_j \\alpha(\\mathbf{q}, \\mathbf{k}_j)}.$$\n\nIn the case of a (scalar) regression with observations $(\\mathbf{x}_i, y_i)$ for features and labels respectively, $\\mathbf{v}_i = y_i$ are scalars, $\\mathbf{k}_i = \\mathbf{x}_i$ are vectors, and the query $\\mathbf{q}$ denotes the new location where $f$ should be evaluated. In the case of (multiclass) classification, we use one-hot-encoding of $y_i$ to obtain $\\mathbf{v}_i$."
    },
    {
      "chunk_id": "27c30c9df920_1",
      "chapter": "attention-pooling",
      "heading": "attention-pooling",
      "text": "In the case of (multiclass) classification, we use one-hot-encoding of $y_i$ to obtain $\\mathbf{v}_i$. One of the convenient properties of this estimator is that it requires no training. Even more so, if we suitably narrow the kernel with increasing amounts of data, the approach is consistent :cite:`mack1982weak`, i.e., it will converge to some statistically optimal solution. Let's start by inspecting some kernels. ```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\nd2l.use_svg_display()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport numpy as np\n\nd2l.use_svg_display()\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport numpy as np\n\nd2l.use_svg_display()\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nfrom flax import linen as nn\n```"
    },
    {
      "chunk_id": "1568798338b9_0",
      "chapter": "attention-pooling",
      "heading": "[**Kernels and Data**]",
      "text": "All the kernels $\\alpha(\\mathbf{k}, \\mathbf{q})$ defined in this section are *translation and rotation invariant*; that is, if we shift and rotate $\\mathbf{k}$ and $\\mathbf{q}$ in the same manner, the value of $\\alpha$ remains unchanged. For simplicity we thus pick scalar arguments $k, q \\in \\mathbb{R}$ and pick the key $k = 0$ as the origin. This yields:\n\n```{.python .input}\n%%tab all\n# Define some kernels\ndef gaussian(x):\n    return d2l.exp(-x**2 / 2)\n\ndef boxcar(x):\n    return d2l.abs(x) < 1.0\n\ndef constant(x):\n    return 1.0 + 0 * x\n \nif tab.selected('pytorch'):\n    def epanechikov(x):\n        return torch.max(1 - d2l.abs(x), torch.zeros_like(x))\nif tab.selected('mxnet'):\n    def epanechikov(x):\n        return np.maximum(1 - d2l.abs(x), 0)\nif tab.selected('tensorflow'):\n    def epanechikov(x):\n        return tf.maximum(1 - d2l.abs(x), 0)\nif tab.selected('jax'):\n    def epanechikov(x):\n        return jnp.maximum(1 - d2l.abs(x), 0)\n```\n\n```{.python .input}\n%%tab all\nfig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))\n\nkernels = (gaussian, boxcar, constant, epanechikov)\nnames = ('Gaussian', 'Boxcar', 'Constant', 'Epanechikov')\nx = d2l.arange(-2.5, 2.5, 0.1)\nfor kernel, name, ax in zip(kernels, names, axes):\n    if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n        ax.plot(d2l.numpy(x), d2l.numpy(kernel(x)))\n    if tab.selected('jax'):\n        ax.plot(x, kernel(x))\n    ax.set_xlabel(name)\n\nd2l.plt.show()\n```\n\nDifferent kernels correspond to different notions of range and smoothness. For instance, the boxcar kernel only attends to observations within a distance of $1$ (or some otherwise defined hyperparameter) and does so indiscriminately. To see Nadaraya--Watson estimation in action, let's define some training data. In the following we use the dependency\n\n$$y_i = 2\\sin(x_i) + x_i + \\epsilon,$$\n\nwhere $\\epsilon$ is drawn from a normal distribution with zero mean and unit variance. We draw 40 training examples."
    },
    {
      "chunk_id": "1568798338b9_1",
      "chapter": "attention-pooling",
      "heading": "[**Kernels and Data**]",
      "text": "In the following we use the dependency\n\n$$y_i = 2\\sin(x_i) + x_i + \\epsilon,$$\n\nwhere $\\epsilon$ is drawn from a normal distribution with zero mean and unit variance. We draw 40 training examples. ```{.python .input}\n%%tab all\ndef f(x):\n    return 2 * d2l.sin(x) + x\n\nn = 40\nif tab.selected('pytorch'):\n    x_train, _ = torch.sort(d2l.rand(n) * 5)\n    y_train = f(x_train) + d2l.randn(n)\nif tab.selected('mxnet'):\n    x_train = np.sort(d2l.rand(n) * 5, axis=None)\n    y_train = f(x_train) + d2l.randn(n)\nif tab.selected('tensorflow'):\n    x_train = tf.sort(d2l.rand((n,1)) * 5, 0)\n    y_train = f(x_train) + d2l.normal((n, 1))\nif tab.selected('jax'):\n    x_train = jnp.sort(jax.random.uniform(d2l.get_key(), (n,)) * 5)\n    y_train = f(x_train) + jax.random.normal(d2l.get_key(), (n,))\nx_val = d2l.arange(0, 5, 0.1)\ny_val = f(x_val)\n```"
    },
    {
      "chunk_id": "b6be0ef66e8b_0",
      "chapter": "attention-pooling",
      "heading": "[**Attention Pooling via Nadaraya--Watson Regression**]",
      "text": "Now that we have data and kernels, all we need is a function that computes the kernel regression estimates. Note that we also want to obtain the relative kernel weights in order to perform some minor diagnostics. Hence we first compute the kernel between all training features (covariates) `x_train` and all validation features `x_val`. This yields a matrix, which we subsequently normalize. When multiplied with the training labels `y_train` we obtain the estimates. Recall attention pooling in :eqref:`eq_attention_pooling`. Let each validation feature be a query, and each training feature--label pair be a key--value pair. As a result, the  normalized relative kernel weights (`attention_w` below) are the *attention weights*. ```{.python .input}\n%%tab all\ndef nadaraya_watson(x_train, y_train, x_val, kernel):\n    dists = d2l.reshape(x_train, (-1, 1)) - d2l.reshape(x_val, (1, -1))\n    # Each column/row corresponds to each query/key\n    k = d2l.astype(kernel(dists), d2l.float32)\n    # Normalization over keys for each query\n    attention_w = k / d2l.reduce_sum(k, 0)\n    if tab.selected('pytorch'):\n        y_hat = y_train@attention_w\n    if tab.selected('mxnet'):\n        y_hat = np.dot(y_train, attention_w)\n    if tab.selected('tensorflow'):\n        y_hat = d2l.transpose(d2l.transpose(y_train)@attention_w)\n    if tab.selected('jax'):\n        y_hat = y_train@attention_w\n    return y_hat, attention_w\n```\n\nLet's have a look at the kind of estimates that the different kernels produce."
    },
    {
      "chunk_id": "b6be0ef66e8b_1",
      "chapter": "attention-pooling",
      "heading": "[**Attention Pooling via Nadaraya--Watson Regression**]",
      "text": "```{.python .input}\n%%tab all\ndef plot(x_train, y_train, x_val, y_val, kernels, names, attention=False):\n    fig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))\n    for kernel, name, ax in zip(kernels, names, axes):\n        y_hat, attention_w = nadaraya_watson(x_train, y_train, x_val, kernel)\n        if attention:\n            if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n                pcm = ax.imshow(d2l.numpy(attention_w), cmap='Reds')\n            if tab.selected('jax'):\n                pcm = ax.imshow(attention_w, cmap='Reds')\n        else:\n            ax.plot(x_val, y_hat)\n            ax.plot(x_val, y_val, 'm--')\n            ax.plot(x_train, y_train, 'o', alpha=0.5);\n        ax.set_xlabel(name)\n        if not attention:\n            ax.legend(['y_hat', 'y'])\n    if attention:\n        fig.colorbar(pcm, ax=axes, shrink=0.7)\n```\n\n```{.python .input}\n%%tab all\nplot(x_train, y_train, x_val, y_val, kernels, names)\n```\n\nThe first thing that stands out is that all three nontrivial kernels (Gaussian, Boxcar, and Epanechikov) produce fairly workable estimates that are not too far from the true function. Only the constant kernel that leads to the trivial estimate $f(x) = \\frac{1}{n} \\sum_i y_i$ produces a rather unrealistic result. Let's inspect the attention weighting a bit more closely:\n\n```{.python .input}\n%%tab all\nplot(x_train, y_train, x_val, y_val, kernels, names, attention=True)\n```\n\nThe visualization clearly shows why the estimates for Gaussian, Boxcar, and Epanechikov are very similar: after all, they are derived from very similar attention weights, despite the different functional form of the kernel. This raises the question as to whether this is always the case."
    },
    {
      "chunk_id": "8450c9933b90_0",
      "chapter": "attention-pooling",
      "heading": "[**Adapting Attention Pooling**]",
      "text": "We could replace the Gaussian kernel with one of a different width. That is, we could use \n$\\alpha(\\mathbf{q}, \\mathbf{k}) = \\exp\\left(-\\frac{1}{2 \\sigma^2} \\|\\mathbf{q} - \\mathbf{k}\\|^2 \\right)$ where $\\sigma^2$ determines the width of the kernel. Let's see whether this affects the outcomes.\n\n```{.python .input}\n%%tab all\nsigmas = (0.1, 0.2, 0.5, 1)\nnames = ['Sigma ' + str(sigma) for sigma in sigmas]\n\ndef gaussian_with_width(sigma): \n    return (lambda x: d2l.exp(-x**2 / (2*sigma**2)))\n\nkernels = [gaussian_with_width(sigma) for sigma in sigmas]\nplot(x_train, y_train, x_val, y_val, kernels, names)\n```\n\nClearly, the narrower the kernel, the less smooth the estimate. At the same time, it adapts better to the local variations. Let's look at the corresponding attention weights.\n\n```{.python .input}\n%%tab all\nplot(x_train, y_train, x_val, y_val, kernels, names, attention=True)\n```\n\nAs we would expect, the narrower the kernel, the narrower the range of large attention weights. It is also clear that picking the same width might not be ideal. In fact, :citet:`Silverman86` proposed a heuristic that depends on the local density. Many more such \"tricks\" have been proposed. For instance, :citet:`norelli2022asif` used a similar nearest-neighbor interpolation technique for designing cross-modal image and text representations. \n\nThe astute reader might wonder why we are providing this deep dive for a method that is over half a century old. First, it is one of the earliest precursors of modern attention mechanisms. Second, it is great for visualization. Third, and just as importantly, it demonstrates the limits of hand-crafted attention mechanisms. A much better strategy is to *learn* the mechanism, by learning the representations for queries and keys. This is what we will embark on in the following sections."
    },
    {
      "chunk_id": "42c389c385b3_0",
      "chapter": "attention-pooling",
      "heading": "Summary",
      "text": "Nadaraya--Watson kernel regression is an early precursor of the current attention mechanisms. \nIt can be used directly with little to no training or tuning, either for classification or regression. \nThe attention weight is assigned according to the similarity (or distance) between query and key, and according to how many similar observations are available."
    },
    {
      "chunk_id": "7996f287da2e_0",
      "chapter": "attention-pooling",
      "heading": "Exercises",
      "text": "1. Parzen windows density estimates are given by $\\hat{p}(\\mathbf{x}) = \\frac{1}{n} \\sum_i k(\\mathbf{x}, \\mathbf{x}_i)$. Prove that for binary classification the function $\\hat{p}(\\mathbf{x}, y=1) - \\hat{p}(\\mathbf{x}, y=-1)$, as obtained by Parzen windows is equivalent to Nadaraya--Watson classification. \n1. Implement stochastic gradient descent to learn a good value for kernel widths in Nadaraya--Watson regression. \n    1. What happens if you just use the above estimates to minimize $(f(\\mathbf{x_i}) - y_i)^2$ directly? Hint: $y_i$ is part of the terms used to compute $f$.\n    1. Remove $(\\mathbf{x}_i, y_i)$ from the estimate for $f(\\mathbf{x}_i)$ and optimize over the kernel widths. Do you still observe overfitting?\n1. Assume that all $\\mathbf{x}$ lie on the unit sphere, i.e., all satisfy $\\|\\mathbf{x}\\| = 1$. Can you simplify the $\\|\\mathbf{x} - \\mathbf{x}_i\\|^2$ term in the exponential? Hint: we will later see that this is very closely related to dot product attention. \n1. Recall that :citet:`mack1982weak` proved that Nadaraya--Watson estimation is consistent. How quickly should you reduce the scale for the attention mechanism as you get more data? Provide some intuition for your answer. Does it depend on the dimensionality of the data? How?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/1598)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1599)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3866)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18026)\n:end_tab:"
    },
    {
      "chunk_id": "b7e4ca90a94b_0",
      "chapter": "attention-scoring-functions",
      "heading": "attention-scoring-functions",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n# Attention Scoring Functions\n:label:`sec_attention-scoring-functions`\n\n\nIn :numref:`sec_attention-pooling`,\nwe used a number of different distance-based kernels, including a Gaussian kernel to model\ninteractions between queries and keys. As it turns out, distance functions are slightly more expensive to compute than dot products. As such, \nwith the softmax operation to ensure nonnegative attention weights,\nmuch of the work has gone into *attention scoring functions* $a$ in :eqref:`eq_softmax_attention` and :numref:`fig_attention_output` that are simpler to compute. \n\n![Computing the output of attention pooling as a weighted average of values, where weights are computed with the attention scoring function $\\mathit{a}$ and the softmax operation.](../img/attention-output.svg)\n:label:`fig_attention_output`\n\n```{.python .input}\n%%tab mxnet\nimport math\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport math\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\nimport math\n```"
    },
    {
      "chunk_id": "41c00ad4c0ac_0",
      "chapter": "attention-scoring-functions",
      "heading": "[**Dot Product Attention**]",
      "text": "Let's review the attention function (without exponentiation) from the Gaussian kernel for a moment:\n\n$$\na(\\mathbf{q}, \\mathbf{k}_i) = -\\frac{1}{2} \\|\\mathbf{q} - \\mathbf{k}_i\\|^2  = \\mathbf{q}^\\top \\mathbf{k}_i -\\frac{1}{2} \\|\\mathbf{k}_i\\|^2  -\\frac{1}{2} \\|\\mathbf{q}\\|^2. $$\n\nFirst, note that the final term depends on $\\mathbf{q}$ only. As such it is identical for all $(\\mathbf{q}, \\mathbf{k}_i)$ pairs. Normalizing the attention weights to $1$, as is done in :eqref:`eq_softmax_attention`, ensures that this term disappears entirely. Second, note that both batch and layer normalization (to be discussed later) lead to activations that have well-bounded, and often constant, norms $\\|\\mathbf{k}_i\\|$. This is the case, for instance, whenever the keys $\\mathbf{k}_i$ were generated by a layer norm. As such, we can drop it from the definition of $a$ without any major change in the outcome. Last, we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query $\\mathbf{q} \\in \\mathbb{R}^d$ and the key $\\mathbf{k}_i \\in \\mathbb{R}^d$ are independent and identically drawn random variables with zero mean and unit variance. The dot product between both vectors has zero mean and a variance of $d$. To ensure that the variance of the dot product still remains $1$ regardless of vector length, we use the *scaled dot product attention* scoring function. That is, we rescale the dot product by $1/\\sqrt{d}$. We thus arrive at the first commonly used attention function that is used, e.g., in Transformers :cite:`Vaswani.Shazeer.Parmar.ea.2017`:\n\n$$ a(\\mathbf{q}, \\mathbf{k}_i) = \\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d}.$$\n:eqlabel:`eq_dot_product_attention`\n\nNote that attention weights $\\alpha$ still need normalizing."
    },
    {
      "chunk_id": "41c00ad4c0ac_1",
      "chapter": "attention-scoring-functions",
      "heading": "[**Dot Product Attention**]",
      "text": "We can simplify this further via :eqref:`eq_softmax_attention` by using the softmax operation:\n\n$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(\\mathbf{q}^\\top \\mathbf{k}_i / \\sqrt{d})}{\\sum_{j=1} \\exp(\\mathbf{q}^\\top \\mathbf{k}_j / \\sqrt{d})}.$$\n:eqlabel:`eq_attn-scoring-alpha`\n\nAs it turns out, all popular attention mechanisms use the softmax, hence we will limit ourselves to that in the remainder of this chapter."
    },
    {
      "chunk_id": "adf400e6486f_0",
      "chapter": "attention-scoring-functions",
      "heading": "Convenience Functions",
      "text": "We need a few functions to make the attention mechanism efficient to deploy. This includes tools for dealing with strings of variable lengths (common for natural language processing) and tools for efficient evaluation on minibatches (batch matrix multiplication)."
    },
    {
      "chunk_id": "8e5093a3b3c9_0",
      "chapter": "attention-scoring-functions",
      "heading": "[**Masked Softmax Operation**]",
      "text": "One of the most popular applications of the attention mechanism is to sequence models. Hence we need to be able to deal with sequences of different lengths. In some cases, such sequences may end up in the same minibatch, necessitating padding with dummy tokens for shorter sequences (see :numref:`sec_machine_translation` for an example). These special tokens do not carry meaning. For instance, assume that we have the following three sentences:\n\n```\nDive  into  Deep    Learning \nLearn to    code    <blank>\nHello world <blank> <blank>\n```\n\n\nSince we do not want blanks in our attention model we simply need to limit $\\sum_{i=1}^n \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i$ to $\\sum_{i=1}^l \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i$ for however long, $l \\leq n$, the actual sentence is. Since it is such a common problem, it has a name: the *masked softmax operation*. Let's implement it. Actually, the implementation cheats ever so slightly by setting the values of $\\mathbf{v}_i$, for $i > l$, to zero. Moreover, it sets the attention weights to a large negative number, such as $-10^{6}$, in order to make their contribution to gradients and values vanish in practice. This is done since linear algebra kernels and operators are heavily optimized for GPUs and it is faster to be slightly wasteful in computation rather than to have code with conditional (if then else) statements."
    },
    {
      "chunk_id": "8e5093a3b3c9_1",
      "chapter": "attention-scoring-functions",
      "heading": "[**Masked Softmax Operation**]",
      "text": "This is done since linear algebra kernels and operators are heavily optimized for GPUs and it is faster to be slightly wasteful in computation rather than to have code with conditional (if then else) statements. ```{.python .input}\n%%tab mxnet\ndef masked_softmax(X, valid_lens):  #@save\n    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n    # X: 3D tensor, valid_lens: 1D or 2D tensor\n    if valid_lens is None:\n        return npx.softmax(X)\n    else:\n        shape = X.shape\n        if valid_lens.ndim == 1:\n            valid_lens = valid_lens.repeat(shape[1])\n        else:\n            valid_lens = valid_lens.reshape(-1)\n        # On the last axis, replace masked elements with a very large negative\n        # value, whose exponentiation outputs 0\n        X = npx.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, True,\n                              value=-1e6, axis=1)\n        return npx.softmax(X).reshape(shape)\n```\n\n```{.python .input}\n%%tab pytorch\ndef masked_softmax(X, valid_lens):  #@save\n    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n    # X: 3D tensor, valid_lens: 1D or 2D tensor \n    def _sequence_mask(X, valid_len, value=0):\n        maxlen = X.size(1)\n        mask = torch.arange((maxlen), dtype=torch.float32,\n                            device=X.device)[None, :] < valid_len[:, None]\n        X[~mask] = value\n        return X\n    \n    if valid_lens is None:\n        return nn.functional.softmax(X, dim=-1)\n    else:\n        shape = X.shape\n        if valid_lens.dim() == 1:\n            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n        else:\n            valid_lens = valid_lens.reshape(-1)\n        # On the last axis, replace masked elements with a very large negative\n        # value, whose exponentiation outputs 0\n        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n        return nn.functional.softmax(X.reshape(shape), dim=-1)\n```\n\n```{.python .input}\n%%tab tensorflow\ndef masked_softmax(X, valid_lens):  #@save\n    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n    # X: 3D tensor, valid_lens: 1D or 2D tensor\n    def _sequence_mask(X, valid_len, value=0):\n        maxlen = X.shape[1]\n        mask = tf.range(start=0, limit=maxlen, dtype=tf.float32)[\n            None, :] < tf.cast(valid_len[:, None], dtype=tf.float32)\n\n        if len(X.shape) == 3:\n            return tf.where(tf.expand_dims(mask, axis=-1), X, value)\n        else:\n            return tf.where(mask, X, value)\n    \n    if valid_lens is None:\n        return tf.nn.softmax(X, axis=-1)\n    else:\n        shape = X.shape\n        if len(valid_lens.shape) == 1:\n            valid_lens = tf.repeat(valid_lens, repeats=shape[1])\n            \n        else:\n            valid_lens = tf.reshape(valid_lens, shape=-1)\n        # On the last axis, replace masked elements with a very large negative\n        # value, whose exponentiation outputs 0    \n        X = _sequence_mask(tf.reshape(X, shape=(-1, shape[-1])), valid_lens,\n                           value=-1e6)    \n        return tf.nn.softmax(tf.reshape(X, shape=shape), axis=-1)\n```\n\n```{.python .input}\n%%tab jax\ndef masked_softmax(X, valid_lens):  #@save\n    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n    # X: 3D tensor, valid_lens: 1D or 2D tensor\n    def _sequence_mask(X, valid_len, value=0):\n        maxlen = X.shape[1]\n        mask = jnp.arange((maxlen),\n                          dtype=jnp.float32)[None, :] < valid_len[:, None]\n        return jnp.where(mask, X, value)\n\n    if valid_lens is None:\n        return nn.softmax(X, axis=-1)\n    else:\n        shape = X.shape\n        if valid_lens.ndim == 1:\n            valid_lens = jnp.repeat(valid_lens, shape[1])\n        else:\n            valid_lens = valid_lens.reshape(-1)\n        # On the last axis, replace masked elements with a very large negative\n        # value, whose exponentiation outputs 0\n        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n        return nn.softmax(X.reshape(shape), axis=-1)\n```\n\nTo [**illustrate how this function works**],\nconsider a minibatch of two examples of size $2 \\times 4$,\nwhere their valid lengths are $2$ and $3$, respectively."
    },
    {
      "chunk_id": "8e5093a3b3c9_2",
      "chapter": "attention-scoring-functions",
      "heading": "[**Masked Softmax Operation**]",
      "text": "As a result of the masked softmax operation,\nvalues beyond the valid lengths for each pair of vectors are all masked as zero. ```{.python .input}\n%%tab mxnet\nmasked_softmax(np.random.uniform(size=(2, 2, 4)), d2l.tensor([2, 3]))\n```\n\n```{.python .input}\n%%tab pytorch\nmasked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))\n```\n\n```{.python .input}\n%%tab tensorflow\nmasked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([2, 3]))\n```\n\n```{.python .input}\n%%tab jax\nmasked_softmax(jax.random.uniform(d2l.get_key(), (2, 2, 4)), jnp.array([2, 3]))\n```\n\nIf we need more fine-grained control to specify the valid length for each of the two vectors of every example, we simply use a two-dimensional tensor of valid lengths. This yields:\n\n```{.python .input}\n%%tab mxnet\nmasked_softmax(np.random.uniform(size=(2, 2, 4)),\n               d2l.tensor([[1, 3], [2, 4]]))\n```\n\n```{.python .input}\n%%tab pytorch\nmasked_softmax(torch.rand(2, 2, 4), d2l.tensor([[1, 3], [2, 4]]))\n```\n\n```{.python .input}\n%%tab tensorflow\nmasked_softmax(tf.random.uniform((2, 2, 4)), tf.constant([[1, 3], [2, 4]]))\n```\n\n```{.python .input}\n%%tab jax\nmasked_softmax(jax.random.uniform(d2l.get_key(), (2, 2, 4)),\n               jnp.array([[1, 3], [2, 4]]))\n```"
    },
    {
      "chunk_id": "2bc5678ae6bd_0",
      "chapter": "attention-scoring-functions",
      "heading": "Batch Matrix Multiplication",
      "text": ":label:`subsec_batch_dot`\n\nAnother commonly used operation is to multiply batches of matrices by one another. This comes in handy when we have minibatches of queries, keys, and values. More specifically, assume that \n\n$$\\mathbf{Q} = [\\mathbf{Q}_1, \\mathbf{Q}_2, \\ldots, \\mathbf{Q}_n]  \\in \\mathbb{R}^{n \\times a \\times b}, \\\\\n    \\mathbf{K} = [\\mathbf{K}_1, \\mathbf{K}_2, \\ldots, \\mathbf{K}_n]  \\in \\mathbb{R}^{n \\times b \\times c}.\n$$\n\nThen the batch matrix multiplication (BMM) computes the elementwise product\n\n$$\\textrm{BMM}(\\mathbf{Q}, \\mathbf{K}) = [\\mathbf{Q}_1 \\mathbf{K}_1, \\mathbf{Q}_2 \\mathbf{K}_2, \\ldots, \\mathbf{Q}_n \\mathbf{K}_n] \\in \\mathbb{R}^{n \\times a \\times c}.$$\n:eqlabel:`eq_batch-matrix-mul`\n\nLet's see this in action in a deep learning framework.\n\n```{.python .input}\n%%tab mxnet\nQ = d2l.ones((2, 3, 4))\nK = d2l.ones((2, 4, 6))\nd2l.check_shape(npx.batch_dot(Q, K), (2, 3, 6))\n```\n\n```{.python .input}\n%%tab pytorch\nQ = d2l.ones((2, 3, 4))\nK = d2l.ones((2, 4, 6))\nd2l.check_shape(torch.bmm(Q, K), (2, 3, 6))\n```\n\n```{.python .input}\n%%tab tensorflow\nQ = d2l.ones((2, 3, 4))\nK = d2l.ones((2, 4, 6))\nd2l.check_shape(tf.matmul(Q, K).numpy(), (2, 3, 6))\n```\n\n```{.python .input}\n%%tab jax\nQ = d2l.ones((2, 3, 4))\nK = d2l.ones((2, 4, 6))\nd2l.check_shape(jax.lax.batch_matmul(Q, K), (2, 3, 6))\n```"
    },
    {
      "chunk_id": "fe3ce5590af4_0",
      "chapter": "attention-scoring-functions",
      "heading": "[**Scaled Dot Product Attention**]",
      "text": "Let's return to the dot product attention introduced in :eqref:`eq_dot_product_attention`. In general, it requires that both the query and the key\nhave the same vector length, say $d$, even though this can be addressed easily by replacing \n$\\mathbf{q}^\\top \\mathbf{k}$ with $\\mathbf{q}^\\top \\mathbf{M} \\mathbf{k}$ where $\\mathbf{M}$ is a matrix suitably chosen for translating between both spaces. For now assume that the dimensions match. In practice, we often think of minibatches for efficiency,\nsuch as computing attention for $n$ queries and $m$ key-value pairs,\nwhere queries and keys are of length $d$\nand values are of length $v$. The scaled dot product attention \nof queries $\\mathbf Q\\in\\mathbb R^{n\\times d}$,\nkeys $\\mathbf K\\in\\mathbb R^{m\\times d}$,\nand values $\\mathbf V\\in\\mathbb R^{m\\times v}$\nthus can be written as \n\n$$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$\n:eqlabel:`eq_softmax_QK_V`\n\nNote that when applying this to a minibatch, we need the batch matrix multiplication introduced in :eqref:`eq_batch-matrix-mul`. In the following implementation of the scaled dot product attention,\nwe use dropout for model regularization. ```{.python .input}\n%%tab mxnet\nclass DotProductAttention(nn.Block):  #@save\n    \"\"\"Scaled dot product attention.\"\"\"\n    def __init__(self, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n    # Shape of queries: (batch_size, no. of queries, d)\n    # Shape of keys: (batch_size, no. of key-value pairs, d)\n    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n    # Shape of valid_lens: (batch_size,) or (batch_size, no."
    },
    {
      "chunk_id": "fe3ce5590af4_1",
      "chapter": "attention-scoring-functions",
      "heading": "[**Scaled Dot Product Attention**]",
      "text": "of queries, d)\n    # Shape of keys: (batch_size, no. of key-value pairs, d)\n    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n    def forward(self, queries, keys, values, valid_lens=None):\n        d = queries.shape[-1]\n        # Set transpose_b=True to swap the last two dimensions of keys\n        scores = npx.batch_dot(queries, keys, transpose_b=True) / math.sqrt(d)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        return npx.batch_dot(self.dropout(self.attention_weights), values)\n```\n\n```{.python .input}\n%%tab pytorch\nclass DotProductAttention(nn.Module):  #@save\n    \"\"\"Scaled dot product attention.\"\"\"\n    def __init__(self, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n\n    # Shape of queries: (batch_size, no. of queries, d)\n    # Shape of keys: (batch_size, no. of key-value pairs, d)\n    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n    def forward(self, queries, keys, values, valid_lens=None):\n        d = queries.shape[-1]\n        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass DotProductAttention(tf.keras.layers.Layer):  #@save\n    \"\"\"Scaled dot product attention.\"\"\"\n    def __init__(self, dropout):\n        super().__init__()\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        \n    # Shape of queries: (batch_size, no. of queries, d)\n    # Shape of keys: (batch_size, no. of key-value pairs, d)\n    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n    # Shape of valid_lens: (batch_size,) or (batch_size, no."
    },
    {
      "chunk_id": "fe3ce5590af4_2",
      "chapter": "attention-scoring-functions",
      "heading": "[**Scaled Dot Product Attention**]",
      "text": "of queries, d)\n    # Shape of keys: (batch_size, no. of key-value pairs, d)\n    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n    def call(self, queries, keys, values, valid_lens=None, **kwargs):\n        d = queries.shape[-1]\n        scores = tf.matmul(queries, keys, transpose_b=True)/tf.math.sqrt(\n            tf.cast(d, dtype=tf.float32))\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        return tf.matmul(self.dropout(self.attention_weights, **kwargs), values)\n```\n\n```{.python .input}\n%%tab jax\nclass DotProductAttention(nn.Module):  #@save\n    \"\"\"Scaled dot product attention.\"\"\"\n    dropout: float\n\n    # Shape of queries: (batch_size, no. of queries, d)\n    # Shape of keys: (batch_size, no. of key-value pairs, d)\n    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n    @nn.compact\n    def __call__(self, queries, keys, values, valid_lens=None,\n                 training=False):\n        d = queries.shape[-1]\n        # Swap the last two dimensions of keys with keys.swapaxes(1, 2)\n        scores = queries@(keys.swapaxes(1, 2)) / math.sqrt(d)\n        attention_weights = masked_softmax(scores, valid_lens)\n        dropout_layer = nn.Dropout(self.dropout, deterministic=not training)\n        return dropout_layer(attention_weights)@values, attention_weights\n```\n\nTo [**illustrate how the `DotProductAttention` class works**],\nwe use the same keys, values, and valid lengths from the earlier toy example for additive attention. For the purpose of our example we assume that we have a minibatch size of $2$, a total of $10$ keys and values, and that the dimensionality of the values is $4$. Lastly, we assume that the valid length per observation is $2$ and $6$ respectively. Given that, we expect the output to be a $2 \\times 1 \\times 4$ tensor, i.e., one row per example of the minibatch."
    },
    {
      "chunk_id": "fe3ce5590af4_3",
      "chapter": "attention-scoring-functions",
      "heading": "[**Scaled Dot Product Attention**]",
      "text": "Lastly, we assume that the valid length per observation is $2$ and $6$ respectively. Given that, we expect the output to be a $2 \\times 1 \\times 4$ tensor, i.e., one row per example of the minibatch. ```{.python .input}\n%%tab mxnet\nqueries = d2l.normal(0, 1, (2, 1, 2))\nkeys = d2l.normal(0, 1, (2, 10, 2))\nvalues = d2l.normal(0, 1, (2, 10, 4))\nvalid_lens = d2l.tensor([2, 6])\n\nattention = DotProductAttention(dropout=0.5)\nattention.initialize()\nd2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))\n```\n\n```{.python .input}\n%%tab pytorch\nqueries = d2l.normal(0, 1, (2, 1, 2))\nkeys = d2l.normal(0, 1, (2, 10, 2))\nvalues = d2l.normal(0, 1, (2, 10, 4))\nvalid_lens = d2l.tensor([2, 6])\n\nattention = DotProductAttention(dropout=0.5)\nattention.eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))\n```\n\n```{.python .input}\n%%tab tensorflow\nqueries = tf.random.normal(shape=(2, 1, 2))\nkeys = tf.random.normal(shape=(2, 10, 2))\nvalues = tf.random.normal(shape=(2, 10, 4))\nvalid_lens = tf.constant([2, 6])\n\nattention = DotProductAttention(dropout=0.5)\nd2l.check_shape(attention(queries, keys, values, valid_lens, training=False),\n                (2, 1, 4))\n```\n\n```{.python .input}\n%%tab jax\nqueries = jax.random.normal(d2l.get_key(), (2, 1, 2))\nkeys = jax.random.normal(d2l.get_key(), (2, 10, 2))\nvalues = jax.random.normal(d2l.get_key(), (2, 10, 4))\nvalid_lens = d2l.tensor([2, 6])\n\nattention = DotProductAttention(dropout=0.5)\n(output, attention_weights), params = attention.init_with_output(\n    d2l.get_key(), queries, keys, values, valid_lens)\nprint(output)\n```\n\nLet's check whether the attention weights actually vanish for anything beyond the second and sixth column respectively (because of setting the valid length to $2$ and $6$)."
    },
    {
      "chunk_id": "fe3ce5590af4_4",
      "chapter": "attention-scoring-functions",
      "heading": "[**Scaled Dot Product Attention**]",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nd2l.show_heatmaps(d2l.reshape(attention.attention_weights, (1, 1, 2, 10)),\n                  xlabel='Keys', ylabel='Queries')\n```\n\n```{.python .input}\n%%tab jax\nd2l.show_heatmaps(d2l.reshape(attention_weights, (1, 1, 2, 10)),\n                  xlabel='Keys', ylabel='Queries')\n```"
    },
    {
      "chunk_id": "4293e5587ab5_0",
      "chapter": "attention-scoring-functions",
      "heading": "[**Additive Attention**]",
      "text": ":label:`subsec_additive-attention`\n\nWhen queries $\\mathbf{q}$ and keys $\\mathbf{k}$ are vectors of different dimension,\nwe can either use a matrix to address the mismatch via $\\mathbf{q}^\\top \\mathbf{M} \\mathbf{k}$, or we can use additive attention \nas the scoring function. Another benefit is that, as its name indicates, the attention is additive. This can lead to some minor computational savings. Given a query $\\mathbf{q} \\in \\mathbb{R}^q$\nand a key $\\mathbf{k} \\in \\mathbb{R}^k$,\nthe *additive attention* scoring function :cite:`Bahdanau.Cho.Bengio.2014` is given by \n\n$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\textrm{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$\n:eqlabel:`eq_additive-attn`\n\nwhere $\\mathbf W_q\\in\\mathbb R^{h\\times q}$, $\\mathbf W_k\\in\\mathbb R^{h\\times k}$, \nand $\\mathbf w_v\\in\\mathbb R^{h}$ are the learnable parameters. This term is then fed into a softmax to ensure both nonnegativity and normalization. An equivalent interpretation of :eqref:`eq_additive-attn` is that the query and key are concatenated\nand fed into an MLP with a single hidden layer. Using $\\tanh$ as the activation function and disabling bias terms, \nwe implement additive attention as follows:\n\n```{.python .input}\n%%tab mxnet\nclass AdditiveAttention(nn.Block):  #@save\n    \"\"\"Additive attention.\"\"\"\n    def __init__(self, num_hiddens, dropout, **kwargs):\n        super(AdditiveAttention, self).__init__(**kwargs)\n        # Use flatten=False to only transform the last axis so that the\n        # shapes for the other axes are kept the same\n        self.W_k = nn.Dense(num_hiddens, use_bias=False, flatten=False)\n        self.W_q = nn.Dense(num_hiddens, use_bias=False, flatten=False)\n        self.w_v = nn.Dense(1, use_bias=False, flatten=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, queries, keys, values, valid_lens):\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # After dimension expansion, shape of queries: (batch_size, no."
    },
    {
      "chunk_id": "4293e5587ab5_1",
      "chapter": "attention-scoring-functions",
      "heading": "[**Additive Attention**]",
      "text": "of\n        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1,\n        # no. of key-value pairs, num_hiddens). Sum them up with\n        # broadcasting\n        features = np.expand_dims(queries, axis=2) + np.expand_dims(\n            keys, axis=1)\n        features = np.tanh(features)\n        # There is only one output of self.w_v, so we remove the last\n        # one-dimensional entry from the shape. Shape of scores:\n        # (batch_size, no. of queries, no. of key-value pairs)\n        scores = np.squeeze(self.w_v(features), axis=-1)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # Shape of values: (batch_size, no. of key-value pairs, value\n        # dimension)\n        return npx.batch_dot(self.dropout(self.attention_weights), values)\n```\n\n```{.python .input}\n%%tab pytorch\nclass AdditiveAttention(nn.Module):  #@save\n    \"\"\"Additive attention.\"\"\"\n    def __init__(self, num_hiddens, dropout, **kwargs):\n        super(AdditiveAttention, self).__init__(**kwargs)\n        self.W_k = nn.LazyLinear(num_hiddens, bias=False)\n        self.W_q = nn.LazyLinear(num_hiddens, bias=False)\n        self.w_v = nn.LazyLinear(1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, queries, keys, values, valid_lens):\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # After dimension expansion, shape of queries: (batch_size, no. of\n        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n        # key-value pairs, num_hiddens). Sum them up with broadcasting\n        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n        features = torch.tanh(features)\n        # There is only one output of self.w_v, so we remove the last\n        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n        # no. of queries, no. of key-value pairs)\n        scores = self.w_v(features).squeeze(-1)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # Shape of values: (batch_size, no."
    },
    {
      "chunk_id": "4293e5587ab5_2",
      "chapter": "attention-scoring-functions",
      "heading": "[**Additive Attention**]",
      "text": "Shape of scores: (batch_size,\n        # no. of queries, no. of key-value pairs)\n        scores = self.w_v(features).squeeze(-1)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # Shape of values: (batch_size, no. of key-value pairs, value\n        # dimension)\n        return torch.bmm(self.dropout(self.attention_weights), values)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass AdditiveAttention(tf.keras.layers.Layer):  #@save\n    \"\"\"Additive attention.\"\"\"\n    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n        super().__init__(**kwargs)\n        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=False)\n        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=False)\n        self.w_v = tf.keras.layers.Dense(1, use_bias=False)\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        \n    def call(self, queries, keys, values, valid_lens, **kwargs):\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # After dimension expansion, shape of queries: (batch_size, no. of\n        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n        # key-value pairs, num_hiddens). Sum them up with broadcasting\n        features = tf.expand_dims(queries, axis=2) + tf.expand_dims(\n            keys, axis=1)\n        features = tf.nn.tanh(features)\n        # There is only one output of self.w_v, so we remove the last\n        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n        # no. of queries, no. of key-value pairs)\n        scores = tf.squeeze(self.w_v(features), axis=-1)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # Shape of values: (batch_size, no."
    },
    {
      "chunk_id": "4293e5587ab5_3",
      "chapter": "attention-scoring-functions",
      "heading": "[**Additive Attention**]",
      "text": "Shape of scores: (batch_size,\n        # no. of queries, no. of key-value pairs)\n        scores = tf.squeeze(self.w_v(features), axis=-1)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # Shape of values: (batch_size, no. of key-value pairs, value\n        # dimension)\n        return tf.matmul(self.dropout(\n            self.attention_weights, **kwargs), values)\n```\n\n```{.python .input}\n%%tab jax\nclass AdditiveAttention(nn.Module):  #@save\n    num_hiddens: int\n    dropout: float\n\n    def setup(self):\n        self.W_k = nn.Dense(self.num_hiddens, use_bias=False)\n        self.W_q = nn.Dense(self.num_hiddens, use_bias=False)\n        self.w_v = nn.Dense(1, use_bias=False)\n\n    @nn.compact\n    def __call__(self, queries, keys, values, valid_lens, training=False):\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # After dimension expansion, shape of queries: (batch_size, no. of\n        # queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n        # key-value pairs, num_hiddens). Sum them up with broadcasting\n        features = jnp.expand_dims(queries, axis=2) + jnp.expand_dims(keys, axis=1)\n        features = nn.tanh(features)\n        # There is only one output of self.w_v, so we remove the last\n        # one-dimensional entry from the shape. Shape of scores: (batch_size,\n        # no. of queries, no. of key-value pairs)\n        scores = self.w_v(features).squeeze(-1)\n        attention_weights = masked_softmax(scores, valid_lens)\n        dropout_layer = nn.Dropout(self.dropout, deterministic=not training)\n        # Shape of values: (batch_size, no. of key-value pairs, value\n        # dimension)\n        return dropout_layer(attention_weights)@values, attention_weights\n```\n\nLet's [**see how `AdditiveAttention` works**]. In our toy example we pick queries, keys and values of size \n$(2, 1, 20)$, $(2, 10, 2)$ and $(2, 10, 4)$, respectively. This is identical to our choice for `DotProductAttention`, except that now the queries are $20$-dimensional."
    },
    {
      "chunk_id": "4293e5587ab5_4",
      "chapter": "attention-scoring-functions",
      "heading": "[**Additive Attention**]",
      "text": "In our toy example we pick queries, keys and values of size \n$(2, 1, 20)$, $(2, 10, 2)$ and $(2, 10, 4)$, respectively. This is identical to our choice for `DotProductAttention`, except that now the queries are $20$-dimensional. Likewise, we pick $(2, 6)$ as the valid lengths for the sequences in the minibatch. ```{.python .input}\n%%tab mxnet\nqueries = d2l.normal(0, 1, (2, 1, 20))\n\nattention = AdditiveAttention(num_hiddens=8, dropout=0.1)\nattention.initialize()\nd2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))\n```\n\n```{.python .input}\n%%tab pytorch\nqueries = d2l.normal(0, 1, (2, 1, 20))\n\nattention = AdditiveAttention(num_hiddens=8, dropout=0.1)\nattention.eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))\n```\n\n```{.python .input}\n%%tab tensorflow\nqueries = tf.random.normal(shape=(2, 1, 20))\n\nattention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n                              dropout=0.1)\nd2l.check_shape(attention(queries, keys, values, valid_lens, training=False),\n                (2, 1, 4))\n```\n\n```{.python .input}\n%%tab jax\nqueries = jax.random.normal(d2l.get_key(), (2, 1, 20))\nattention = AdditiveAttention(num_hiddens=8, dropout=0.1)\n(output, attention_weights), params = attention.init_with_output(\n    d2l.get_key(), queries, keys, values, valid_lens)\nprint(output)\n```\n\nWhen reviewing the attention function we see a behavior that is qualitatively quite similar to that of `DotProductAttention`. That is, only terms within the chosen valid length $(2, 6)$ are nonzero. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nd2l.show_heatmaps(d2l.reshape(attention.attention_weights, (1, 1, 2, 10)),\n                  xlabel='Keys', ylabel='Queries')\n```\n\n```{.python .input}\n%%tab jax\nd2l.show_heatmaps(d2l.reshape(attention_weights, (1, 1, 2, 10)),\n                  xlabel='Keys', ylabel='Queries')\n```"
    },
    {
      "chunk_id": "0cd70db855b4_0",
      "chapter": "attention-scoring-functions",
      "heading": "Summary",
      "text": "In this section we introduced the two key attention scoring functions: dot product and additive attention. They are effective tools for aggregating across sequences of variable length. In particular, the dot product attention is the mainstay of modern Transformer architectures. When queries and keys are vectors of different lengths,\nwe can use the additive attention scoring function instead. Optimizing these layers is one of the key areas of advance in recent years. For instance, [NVIDIA's Transformer Library](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html) and Megatron :cite:`shoeybi2019megatron` crucially rely on efficient variants of the attention mechanism. We will dive into this in quite a bit more detail as we review Transformers in later sections."
    },
    {
      "chunk_id": "6dec529ef0d6_0",
      "chapter": "attention-scoring-functions",
      "heading": "Exercises",
      "text": "1. Implement distance-based attention by modifying the `DotProductAttention` code. Note that you only need the squared norms of the keys $\\|\\mathbf{k}_i\\|^2$ for an efficient implementation. \n1. Modify the dot product attention to allow for queries and keys of different dimensionalities by employing a matrix to adjust dimensions. \n1. How does the computational cost scale with the dimensionality of the keys, queries, values, and their number? What about the memory bandwidth requirements?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/346)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1064)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3867)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18027)\n:end_tab:"
    },
    {
      "chunk_id": "442815afdd0d_0",
      "chapter": "bahdanau-attention",
      "heading": "bahdanau-attention",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n# The Bahdanau Attention Mechanism\n:label:`sec_seq2seq_attention`\n\nWhen we encountered machine translation in :numref:`sec_seq2seq`,\nwe designed an encoder--decoder architecture for sequence-to-sequence learning\nbased on two RNNs :cite:`Sutskever.Vinyals.Le.2014`. Specifically, the RNN encoder transforms a variable-length sequence\ninto a *fixed-shape* context variable. Then, the RNN decoder generates the output (target) sequence token by token\nbased on the generated tokens and the context variable. Recall :numref:`fig_seq2seq_details` which we repeat (:numref:`fig_s2s_attention_state`) with some additional detail. Conventionally, in an RNN all relevant information about a source sequence is translated into some internal *fixed-dimensional* state representation by the encoder. It is this very state that is used by the decoder as the complete and exclusive source of information for generating the translated sequence. In other words, the sequence-to-sequence mechanism treats the intermediate state as a sufficient statistic of whatever string might have served as input. ![Sequence-to-sequence model. The state, as generated by the encoder, is the only piece of information shared between the encoder and the decoder.](../img/seq2seq-state.svg)\n:label:`fig_s2s_attention_state`\n\nWhile this is quite reasonable for short sequences, it is clear that it is infeasible for long ones, such as a book chapter or even just a very long sentence. After all, before too long there will simply not be enough \"space\" in the intermediate representation to store all that is important in the source sequence. Consequently the decoder will fail to translate long and complex sentences. One of the first to encounter this was :citet:`Graves.2013` who tried to design an RNN to generate handwritten text."
    },
    {
      "chunk_id": "442815afdd0d_1",
      "chapter": "bahdanau-attention",
      "heading": "bahdanau-attention",
      "text": "Consequently the decoder will fail to translate long and complex sentences. One of the first to encounter this was :citet:`Graves.2013` who tried to design an RNN to generate handwritten text. Since the source text has arbitrary length they designed a differentiable attention model\nto align text characters with the much longer pen trace,\nwhere the alignment moves only in one direction. This, in turn, draws on decoding algorithms in speech recognition, e.g., hidden Markov models :cite:`rabiner1993fundamentals`. Inspired by the idea of learning to align,\n:citet:`Bahdanau.Cho.Bengio.2014` proposed a differentiable attention model\n*without* the unidirectional alignment limitation. When predicting a token,\nif not all the input tokens are relevant,\nthe model aligns (or attends)\nonly to parts of the input sequence\nthat are deemed relevant to the current prediction. This is then used to update the current state before generating the next token. While quite innocuous in its description, this *Bahdanau attention mechanism* has arguably turned into one of the most influential ideas of the past decade in deep learning, giving rise to Transformers :cite:`Vaswani.Shazeer.Parmar.ea.2017` and many related new architectures. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import init, np, npx\nfrom mxnet.gluon import rnn, nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\n```"
    },
    {
      "chunk_id": "a9cb295476ad_0",
      "chapter": "bahdanau-attention",
      "heading": "Model",
      "text": "We follow the notation introduced by the sequence-to-sequence architecture of :numref:`sec_seq2seq`, in particular :eqref:`eq_seq2seq_s_t`.\nThe key idea is that instead of keeping the state,\ni.e., the context variable $\\mathbf{c}$ summarizing the source sentence, as fixed, we dynamically update it, as a function of both the original text (encoder hidden states $\\mathbf{h}_{t}$) and the text that was already generated (decoder hidden states $\\mathbf{s}_{t'-1}$). This yields $\\mathbf{c}_{t'}$, which is updated after any decoding time step $t'$. Suppose that the input sequence is of length $T$. In this case the context variable is the output of attention pooling:\n\n$$\\mathbf{c}_{t'} = \\sum_{t=1}^{T} \\alpha(\\mathbf{s}_{t' - 1}, \\mathbf{h}_{t}) \\mathbf{h}_{t}.$$\n\nWe used $\\mathbf{s}_{t' - 1}$ as the query, and\n$\\mathbf{h}_{t}$ as both the key and the value. Note that $\\mathbf{c}_{t'}$ is then used to generate the state $\\mathbf{s}_{t'}$ and to generate a new token: see :eqref:`eq_seq2seq_s_t`. In particular, the attention weight $\\alpha$ is computed as in :eqref:`eq_attn-scoring-alpha`\nusing the additive attention scoring function\ndefined by :eqref:`eq_additive-attn`.\nThis RNN encoder--decoder architecture\nusing attention is depicted in :numref:`fig_s2s_attention_details`. Note that later this model was modified so as to include the already generated tokens in the decoder as further context (i.e., the attention sum does not stop at $T$ but rather it proceeds up to $t'-1$). For instance, see :citet:`chan2015listen` for a description of this strategy, as applied to speech recognition.\n\n![Layers in an RNN encoder--decoder model with the Bahdanau attention mechanism.](../img/seq2seq-details-attention.svg)\n:label:`fig_s2s_attention_details`"
    },
    {
      "chunk_id": "2336a06f37e0_0",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "To implement the RNN encoder--decoder with attention,\nwe only need to redefine the decoder (omitting the generated symbols from the attention function simplifies the design). Let's begin with [**the base interface for decoders with attention**] by defining the quite unsurprisingly named `AttentionDecoder` class. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass AttentionDecoder(d2l.Decoder):  #@save\n    \"\"\"The base attention-based decoder interface.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def attention_weights(self):\n        raise NotImplementedError\n```\n\nWe need to [**implement the RNN decoder**]\nin the `Seq2SeqAttentionDecoder` class. The state of the decoder is initialized with\n(i) the hidden states of the last layer of the encoder at all time steps, used as keys and values for attention;\n(ii) the hidden state of the encoder at all layers at the final time step, which serves to initialize the hidden state of the decoder;\nand (iii) the valid length of the encoder, to exclude the padding tokens in attention pooling. At each decoding time step, the hidden state of the final layer of the decoder, obtained at the previous time step, is used as the query of the attention mechanism. Both the output of the attention mechanism and the input embedding are concatenated to serve as the input of the RNN decoder. ```{.python .input}\n%%tab mxnet\nclass Seq2SeqAttentionDecoder(AttentionDecoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Dense(vocab_size, flatten=False)\n        self.initialize(init.Xavier())\n\n    def init_state(self, enc_outputs, enc_valid_lens):\n        # Shape of outputs: (num_steps, batch_size, num_hiddens)."
    },
    {
      "chunk_id": "2336a06f37e0_1",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n        outputs, hidden_state = enc_outputs\n        return (outputs.swapaxes(0, 1), hidden_state, enc_valid_lens)\n\n    def forward(self, X, state):\n        # Shape of enc_outputs: (batch_size, num_steps, num_hiddens)."
    },
    {
      "chunk_id": "2336a06f37e0_2",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n        enc_outputs, hidden_state, enc_valid_lens = state\n        # Shape of the output X: (num_steps, batch_size, embed_size)\n        X = self.embedding(X).swapaxes(0, 1)\n        outputs, self._attention_weights = [], []\n        for x in X:\n            # Shape of query: (batch_size, 1, num_hiddens)\n            query = np.expand_dims(hidden_state[-1], axis=1)\n            # Shape of context: (batch_size, 1, num_hiddens)\n            context = self.attention(\n                query, enc_outputs, enc_outputs, enc_valid_lens)\n            # Concatenate on the feature dimension\n            x = np.concatenate((context, np.expand_dims(x, axis=1)), axis=-1)\n            # Reshape x as (1, batch_size, embed_size + num_hiddens)\n            out, hidden_state = self.rnn(x.swapaxes(0, 1), hidden_state)\n            hidden_state = hidden_state[0]\n            outputs.append(out)\n            self._attention_weights.append(self.attention.attention_weights)\n        # After fully connected layer transformation, shape of outputs:\n        # (num_steps, batch_size, vocab_size)\n        outputs = self.dense(np.concatenate(outputs, axis=0))\n        return outputs.swapaxes(0, 1), [enc_outputs, hidden_state,\n                                        enc_valid_lens]\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n```\n\n```{.python .input}\n%%tab pytorch\nclass Seq2SeqAttentionDecoder(AttentionDecoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.GRU(\n            embed_size + num_hiddens, num_hiddens, num_layers,\n            dropout=dropout)\n        self.dense = nn.LazyLinear(vocab_size)\n        self.apply(d2l.init_seq2seq)\n\n    def init_state(self, enc_outputs, enc_valid_lens):\n        # Shape of outputs: (num_steps, batch_size, num_hiddens)."
    },
    {
      "chunk_id": "2336a06f37e0_3",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n        outputs, hidden_state = enc_outputs\n        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n\n    def forward(self, X, state):\n        # Shape of enc_outputs: (batch_size, num_steps, num_hiddens)."
    },
    {
      "chunk_id": "2336a06f37e0_4",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n        enc_outputs, hidden_state, enc_valid_lens = state\n        # Shape of the output X: (num_steps, batch_size, embed_size)\n        X = self.embedding(X).permute(1, 0, 2)\n        outputs, self._attention_weights = [], []\n        for x in X:\n            # Shape of query: (batch_size, 1, num_hiddens)\n            query = torch.unsqueeze(hidden_state[-1], dim=1)\n            # Shape of context: (batch_size, 1, num_hiddens)\n            context = self.attention(\n                query, enc_outputs, enc_outputs, enc_valid_lens)\n            # Concatenate on the feature dimension\n            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n            # Reshape x as (1, batch_size, embed_size + num_hiddens)\n            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n            outputs.append(out)\n            self._attention_weights.append(self.attention.attention_weights)\n        # After fully connected layer transformation, shape of outputs:\n        # (num_steps, batch_size, vocab_size)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n                                          enc_valid_lens]\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Seq2SeqAttentionDecoder(AttentionDecoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.attention = d2l.AdditiveAttention(num_hiddens, num_hiddens,\n                                               num_hiddens, dropout)\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n        self.rnn = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(\n            [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)\n             for _ in range(num_layers)]), return_sequences=True,\n                                       return_state=True)\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens):\n        # Shape of outputs: (batch_size, num_steps, num_hiddens)."
    },
    {
      "chunk_id": "2336a06f37e0_5",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "# Length of list hidden_state is num_layers, where the shape of its\n        # element is (batch_size, num_hiddens)\n        outputs, hidden_state = enc_outputs\n        return (tf.transpose(outputs, (1, 0, 2)), hidden_state,\n                enc_valid_lens)\n\n    def call(self, X, state, **kwargs):\n        # Shape of output enc_outputs: # (batch_size, num_steps, num_hiddens)\n        # Length of list hidden_state is num_layers, where the shape of its\n        # element is (batch_size, num_hiddens)\n        enc_outputs, hidden_state, enc_valid_lens = state\n        # Shape of the output X: (num_steps, batch_size, embed_size)\n        X = self.embedding(X)  # Input X has shape: (batch_size, num_steps)\n        X = tf.transpose(X, perm=(1, 0, 2))\n        outputs, self._attention_weights = [], []\n        for x in X:\n            # Shape of query: (batch_size, 1, num_hiddens)\n            query = tf.expand_dims(hidden_state[-1], axis=1)\n            # Shape of context: (batch_size, 1, num_hiddens)\n            context = self.attention(query, enc_outputs, enc_outputs,\n                                     enc_valid_lens, **kwargs)\n            # Concatenate on the feature dimension\n            x = tf.concat((context, tf.expand_dims(x, axis=1)), axis=-1)\n            out = self.rnn(x, hidden_state, **kwargs)\n            hidden_state = out[1:]\n            outputs.append(out[0])\n            self._attention_weights.append(self.attention.attention_weights)\n        # After fully connected layer transformation, shape of outputs:\n        # (batch_size, num_steps, vocab_size)\n        outputs = self.dense(tf.concat(outputs, axis=1))\n        return outputs, [enc_outputs, hidden_state, enc_valid_lens]\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n```\n\n```{.python .input}\n%%tab jax\nclass Seq2SeqAttentionDecoder(nn.Module):\n    vocab_size: int\n    embed_size: int\n    num_hiddens: int\n    num_layers: int\n    dropout: float = 0\n\n    def setup(self):\n        self.attention = d2l.AdditiveAttention(self.num_hiddens, self.dropout)\n        self.embedding = nn.Embed(self.vocab_size, self.embed_size)\n        self.dense = nn.Dense(self.vocab_size)\n        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout=self.dropout)\n\n    def init_state(self, enc_outputs, enc_valid_lens, *args):\n        # Shape of outputs: (num_steps, batch_size, num_hiddens)."
    },
    {
      "chunk_id": "2336a06f37e0_6",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n        outputs, hidden_state = enc_outputs\n        # Attention Weights are returned as part of state; init with None\n        return (outputs.transpose(1, 0, 2), hidden_state, enc_valid_lens)\n\n    @nn.compact\n    def __call__(self, X, state, training=False):\n        # Shape of enc_outputs: (batch_size, num_steps, num_hiddens)."
    },
    {
      "chunk_id": "2336a06f37e0_7",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n        # Ignore Attention value in state\n        enc_outputs, hidden_state, enc_valid_lens = state\n        # Shape of the output X: (num_steps, batch_size, embed_size)\n        X = self.embedding(X).transpose(1, 0, 2)\n        outputs, attention_weights = [], []\n        for x in X:\n            # Shape of query: (batch_size, 1, num_hiddens)\n            query = jnp.expand_dims(hidden_state[-1], axis=1)\n            # Shape of context: (batch_size, 1, num_hiddens)\n            context, attention_w = self.attention(query, enc_outputs,\n                                                  enc_outputs, enc_valid_lens,\n                                                  training=training)\n            # Concatenate on the feature dimension\n            x = jnp.concatenate((context, jnp.expand_dims(x, axis=1)), axis=-1)\n            # Reshape x as (1, batch_size, embed_size + num_hiddens)\n            out, hidden_state = self.rnn(x.transpose(1, 0, 2), hidden_state,\n                                         training=training)\n            outputs.append(out)\n            attention_weights.append(attention_w)\n\n        # Flax sow API is used to capture intermediate variables\n        self.sow('intermediates', 'dec_attention_weights', attention_weights)\n\n        # After fully connected layer transformation, shape of outputs:\n        # (num_steps, batch_size, vocab_size)\n        outputs = self.dense(jnp.concatenate(outputs, axis=0))\n        return outputs.transpose(1, 0, 2), [enc_outputs, hidden_state,\n                                            enc_valid_lens]\n```\n\nIn the following, we [**test the implemented\ndecoder**] with attention\nusing a minibatch of four sequences, each of which are seven time steps long."
    },
    {
      "chunk_id": "2336a06f37e0_8",
      "chapter": "bahdanau-attention",
      "heading": "Defining the Decoder with Attention",
      "text": "```{.python .input}\n%%tab all\nvocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\nbatch_size, num_steps = 4, 7\nencoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\ndecoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\n                                  num_layers)\nif tab.selected('mxnet'):\n    X = d2l.zeros((batch_size, num_steps))\n    state = decoder.init_state(encoder(X), None)\n    output, state = decoder(X, state)\nif tab.selected('pytorch'):\n    X = d2l.zeros((batch_size, num_steps), dtype=torch.long)\n    state = decoder.init_state(encoder(X), None)\n    output, state = decoder(X, state)\nif tab.selected('tensorflow'):\n    X = tf.zeros((batch_size, num_steps))\n    state = decoder.init_state(encoder(X, training=False), None)\n    output, state = decoder(X, state, training=False)\nif tab.selected('jax'):\n    X = jnp.zeros((batch_size, num_steps), dtype=jnp.int32)\n    state = decoder.init_state(encoder.init_with_output(d2l.get_key(),\n                                                        X, training=False)[0],\n                               None)\n    (output, state), _ = decoder.init_with_output(d2l.get_key(), X,\n                                                  state, training=False)\nd2l.check_shape(output, (batch_size, num_steps, vocab_size))\nd2l.check_shape(state[0], (batch_size, num_steps, num_hiddens))\nd2l.check_shape(state[1][0], (batch_size, num_hiddens))\n```"
    },
    {
      "chunk_id": "2fa39665a063_0",
      "chapter": "bahdanau-attention",
      "heading": "[**Training**]",
      "text": "Now that we specified the new decoder we can proceed analogously to :numref:`sec_seq2seq_training`:\nspecify the hyperparameters, instantiate\na regular encoder and a decoder with attention,\nand train this model for machine translation. ```{.python .input}\n%%tab all\ndata = d2l.MTFraEng(batch_size=128)\nembed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    encoder = d2l.Seq2SeqEncoder(\n        len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n    decoder = Seq2SeqAttentionDecoder(\n        len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nif tab.selected('mxnet', 'pytorch'):\n    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                        lr=0.005)\nif tab.selected('jax'):\n    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                        lr=0.005, training=True)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        encoder = d2l.Seq2SeqEncoder(\n            len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n        decoder = Seq2SeqAttentionDecoder(\n            len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n        model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                            lr=0.005)\n    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)\ntrainer.fit(model, data)\n```\n\nAfter the model is trained,\nwe use it to [**translate a few English sentences**]\ninto French and compute their BLEU scores."
    },
    {
      "chunk_id": "2fa39665a063_1",
      "chapter": "bahdanau-attention",
      "heading": "[**Training**]",
      "text": "```{.python .input}\n%%tab all\nengs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\nfras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    preds, _ = model.predict_step(\n        data.build(engs, fras), d2l.try_gpu(), data.num_steps)\nif tab.selected('jax'):\n    preds, _ = model.predict_step(\n        trainer.state.params, data.build(engs, fras), data.num_steps)\nfor en, fr, p in zip(engs, fras, preds):\n    translation = []\n    for token in data.tgt_vocab.to_tokens(p):\n        if token == '<eos>':\n            break\n        translation.append(token)\n    print(f'{en} => {translation}, bleu,'\n          f'{d2l.bleu(\" \".join(translation), fr, k=2):.3f}')\n```\n\nLet's [**visualize the attention weights**]\nwhen translating the last English sentence. We see that each query assigns non-uniform weights\nover key--value pairs. It shows that at each decoding step,\ndifferent parts of the input sequences\nare selectively aggregated in the attention pooling."
    },
    {
      "chunk_id": "2fa39665a063_2",
      "chapter": "bahdanau-attention",
      "heading": "[**Training**]",
      "text": "We see that each query assigns non-uniform weights\nover key--value pairs. It shows that at each decoding step,\ndifferent parts of the input sequences\nare selectively aggregated in the attention pooling. ```{.python .input}\n%%tab all\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    _, dec_attention_weights = model.predict_step(\n        data.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)\nif tab.selected('jax'):\n    _, (dec_attention_weights, _) = model.predict_step(\n        trainer.state.params, data.build([engs[-1]], [fras[-1]]),\n        data.num_steps, True)\nattention_weights = d2l.concat(\n    [step[0][0][0] for step in dec_attention_weights], 0)\nattention_weights = d2l.reshape(attention_weights, (1, 1, -1, data.num_steps))\n```\n\n```{.python .input}\n%%tab mxnet\n# Plus one to include the end-of-sequence token\nd2l.show_heatmaps(\n    attention_weights[:, :, :, :len(engs[-1].split()) + 1],\n    xlabel='Key positions', ylabel='Query positions')\n```\n\n```{.python .input}\n%%tab pytorch\n# Plus one to include the end-of-sequence token\nd2l.show_heatmaps(\n    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),\n    xlabel='Key positions', ylabel='Query positions')\n```\n\n```{.python .input}\n%%tab tensorflow\n# Plus one to include the end-of-sequence token\nd2l.show_heatmaps(attention_weights[:, :, :, :len(engs[-1].split()) + 1],\n                  xlabel='Key positions', ylabel='Query positions')\n```\n\n```{.python .input}\n%%tab jax\n# Plus one to include the end-of-sequence token\nd2l.show_heatmaps(attention_weights[:, :, :, :len(engs[-1].split()) + 1],\n                  xlabel='Key positions', ylabel='Query positions')\n```"
    },
    {
      "chunk_id": "c9f7a7f460d0_0",
      "chapter": "bahdanau-attention",
      "heading": "Summary",
      "text": "When predicting a token, if not all the input tokens are relevant, the RNN encoder--decoder with the Bahdanau attention mechanism selectively aggregates different parts of the input sequence. This is achieved by treating the state (context variable) as an output of additive attention pooling.\nIn the RNN encoder--decoder, the Bahdanau attention mechanism treats the decoder hidden state at the previous time step as the query, and the encoder hidden states at all the time steps as both the keys and values."
    },
    {
      "chunk_id": "c56059d92102_0",
      "chapter": "bahdanau-attention",
      "heading": "Exercises",
      "text": "1. Replace GRU with LSTM in the experiment.\n1. Modify the experiment to replace the additive attention scoring function with the scaled dot-product. How does it influence the training efficiency?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/347)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1065)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3868)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18028)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Attention Mechanisms and Transformers\n:label:`chap_attention-and-transformers`\n\n\nThe earliest years of the deep learning boom were driven primarily\nby results produced using the multilayer perceptron,\nconvolutional network, and recurrent network architectures. Remarkably, the model architectures that underpinned\nmany of deep learning's breakthroughs in the 2010s\nhad changed remarkably little relative to their\nantecedents despite the lapse of nearly 30 years. While plenty of new methodological innovations\nmade their way into most practitioner's toolkits---ReLU\nactivations, residual layers, batch normalization, dropout,\nand adaptive learning rate schedules come to mind---the core\nunderlying architectures were clearly recognizable as\nscaled-up implementations of classic ideas. Despite thousands of papers proposing alternative ideas,\nmodels resembling classical convolutional neural networks (:numref:`chap_cnn`)\nretained *state-of-the-art* status in computer vision\nand models resembling Sepp Hochreiter's original design\nfor the LSTM recurrent neural network (:numref:`sec_lstm`),\ndominated most applications in natural language processing. Arguably, to that point, the rapid emergence of deep learning\nappeared to be primarily attributable to shifts\nin the available computational resources\n(thanks to innovations in parallel computing with GPUs)\nand the availability of massive data resources\n(thanks to cheap storage and Internet services). While these factors may indeed remain the primary drivers\nbehind this technology's increasing power\nwe are also witnessing, at long last,\na sea change in the landscape of dominant architectures. At the present moment, the dominant models\nfor nearly all natural language processing tasks\nare based on the Transformer architecture."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "At the present moment, the dominant models\nfor nearly all natural language processing tasks\nare based on the Transformer architecture. Given any new task in natural language processing, the default first-pass approach\nis to grab a large Transformer-based pretrained model,\n(e.g., BERT :cite:`Devlin.Chang.Lee.ea.2018`, ELECTRA :cite:`clark2019electra`, RoBERTa :cite:`Liu.Ott.Goyal.ea.2019`, or Longformer :cite:`beltagy2020longformer`)\nadapting the output layers as necessary,\nand fine-tuning the model on the available\ndata for the downstream task. If you have been paying attention to the last few years\nof breathless news coverage centered on OpenAI's\nlarge language models, then you have been tracking a conversation\ncentered on the GPT-2 and GPT-3 Transformer-based models :cite:`Radford.Wu.Child.ea.2019,brown2020language`. Meanwhile, the vision Transformer has emerged\nas a default model for diverse vision tasks,\nincluding image recognition, object detection,\nsemantic segmentation, and superresolution :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021,liu2021swin`. Transformers also showed up as competitive methods\nfor speech recognition :cite:`gulati2020conformer`,\nreinforcement learning :cite:`chen2021decision`,\nand graph neural networks :cite:`dwivedi2020generalization`. The core idea behind the Transformer model is the *attention mechanism*,\nan innovation that was originally envisioned as an enhancement\nfor encoder--decoder RNNs applied to sequence-to-sequence applications,\nsuch as machine translations :cite:`Bahdanau.Cho.Bengio.2014`. You might recall that in the first sequence-to-sequence models\nfor machine translation :cite:`Sutskever.Vinyals.Le.2014`,\nthe entire input was compressed by the encoder\ninto a single fixed-length vector to be fed into the decoder. The intuition behind attention is that rather than compressing the input,\nit might be better for the decoder to revisit the input sequence at every step."
    },
    {
      "chunk_id": "01f4e33118cb_2",
      "chapter": "index",
      "heading": "index",
      "text": "The intuition behind attention is that rather than compressing the input,\nit might be better for the decoder to revisit the input sequence at every step. Moreover, rather than always seeing the same representation of the input,\none might imagine that the decoder should selectively focus\non particular parts of the input sequence at particular decoding steps. Bahdanau's attention mechanism provided a simple means\nby which the decoder could dynamically *attend* to different\nparts of the input at each decoding step. The high-level idea is that the encoder could produce a representation\nof length equal to the original input sequence. Then, at decoding time, the decoder can (via some control mechanism)\nreceive as input a context vector consisting of a weighted sum\nof the representations on the input at each time step. Intuitively, the weights determine the extent\nto which each step's context \"focuses\" on each input token,\nand the key is to make this process\nfor assigning the weights differentiable\nso that it can be learned along with\nall of the other neural network parameters. Initially, the idea was a remarkably successful\nenhancement to the recurrent neural networks\nthat already dominated machine translation applications. The models performed better than the original\nencoder--decoder sequence-to-sequence architectures. Furthermore, researchers noted that some nice qualitative insights\nsometimes emerged from inspecting the pattern of attention weights. In translation tasks, attention models\noften assigned high attention weights to cross-lingual synonyms\nwhen generating the corresponding words in the target language. For example, when translating the sentence \"my feet hurt\"\nto \"j'ai mal au pieds\", the neural network might assign\nhigh attention weights to the representation of \"feet\"\nwhen generating the corresponding French word \"pieds\"."
    },
    {
      "chunk_id": "01f4e33118cb_3",
      "chapter": "index",
      "heading": "index",
      "text": "For example, when translating the sentence \"my feet hurt\"\nto \"j'ai mal au pieds\", the neural network might assign\nhigh attention weights to the representation of \"feet\"\nwhen generating the corresponding French word \"pieds\". These insights spurred claims that attention models confer \"interpretability\"\nalthough what precisely the attention weights mean---i.e.,\nhow, if at all, they should be *interpreted* remains a hazy research topic. However, attention mechanisms soon emerged as more significant concerns,\nbeyond their usefulness as an enhancement for encoder--decoder recurrent neural networks\nand their putative usefulness for picking out salient inputs. :citet:`Vaswani.Shazeer.Parmar.ea.2017` proposed\nthe Transformer architecture for machine translation,\ndispensing with recurrent connections altogether,\nand instead relying on cleverly arranged attention mechanisms\nto capture all relationships among input and output tokens. The architecture performed remarkably well,\nand by 2018 the Transformer began showing up\nin the majority of state-of-the-art natural language processing systems. Moreover, at the same time, the dominant practice in natural language processing\nbecame to pretrain large-scale models\non enormous generic background corpora\nto optimize some self-supervised pretraining objective,\nand then to fine-tune these models\nusing the available downstream data. The gap between Transformers and traditional architectures\ngrew especially wide when applied in this pretraining paradigm,\nand thus the ascendance of Transformers coincided\nwith the ascendence of such large-scale pretrained models,\nnow sometimes called *foundation models* :cite:`bommasani2021opportunities`. In this chapter, we introduce attention models,\nstarting with the most basic intuitions\nand the simplest instantiations of the idea. We then work our way up to the Transformer architecture,\nthe vision Transformer, and the landscape\nof modern Transformer-based pretrained models."
    },
    {
      "chunk_id": "01f4e33118cb_4",
      "chapter": "index",
      "heading": "index",
      "text": "We then work our way up to the Transformer architecture,\nthe vision Transformer, and the landscape\nof modern Transformer-based pretrained models. ```toc\n:maxdepth: 2\n\nqueries-keys-values\nattention-pooling\nattention-scoring-functions\nbahdanau-attention\nmultihead-attention\nself-attention-and-positional-encoding\ntransformer\nvision-transformer\nlarge-pretraining-transformers\n```"
    },
    {
      "chunk_id": "0fad9a14f46d_0",
      "chapter": "large-pretraining-transformers",
      "heading": "large-pretraining-transformers",
      "text": "# Large-Scale Pretraining with Transformers\n:label:`sec_large-pretraining-transformers`\n\nSo far in our image classification and machine translation experiments,\nmodels have been trained on datasets with input--output examples\n*from scratch* to perform specific tasks. For example, a Transformer was trained\nwith English--French pairs (:numref:`sec_transformer`)\nso that this model can translate input English text into French. As a result, each model becomes a *specific expert*\nthat is sensitive to even a slight shift in data distribution\n(:numref:`sec_environment-and-distribution-shift`). For better generalized models, or even more competent *generalists*\nthat can perform multiple tasks with or without adaptation,\n*pretraining* models on large data has been increasingly common. Given larger data for pretraining, the Transformer architecture\nperforms better with an increased model size and training compute,\ndemonstrating superior *scaling* behavior. Specifically, performance of Transformer-based language models\nscales as a power law with the amount of model parameters,\ntraining tokens, and training compute :cite:`kaplan2020scaling`. The scalability of Transformers is also evidenced\nby the significantly boosted performance\nfrom larger vision Transformers trained on larger data\n(discussed in :numref:`sec_vision-transformer`). More recent success stories include Gato, a *generalist* model\nthat can play Atari, caption images, chat, and act as a robot :cite:`reed2022generalist`. Gato is a single  Transformer that scales well when pretrained on diverse modalities,\nincluding text, images, joint torques, and button presses. Notably, all such multimodal data is serialized into a flat sequence of tokens,\nwhich can be processed akin to text tokens (:numref:`sec_transformer`)\nor image patches (:numref:`sec_vision-transformer`) by Transformers. Prior to the compelling success of pretraining Transformers for multimodal data,\nTransformers were extensively pretrained  with a wealth of text."
    },
    {
      "chunk_id": "0fad9a14f46d_1",
      "chapter": "large-pretraining-transformers",
      "heading": "large-pretraining-transformers",
      "text": "Prior to the compelling success of pretraining Transformers for multimodal data,\nTransformers were extensively pretrained  with a wealth of text. Originally proposed for machine translation,\nthe Transformer architecture in :numref:`fig_transformer`\nconsists of an encoder for representing input sequences\nand a decoder for generating target sequences. Primarily, Transformers can be used in three different modes:\n*encoder-only*, *encoder--decoder*, and *decoder-only*. To conclude this chapter, we will review these three modes\nand explain the scalability in pretraining Transformers."
    },
    {
      "chunk_id": "3c993137049f_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Encoder-Only",
      "text": "When only the Transformer encoder is used,\na sequence of input tokens is converted\ninto the same number of representations\nthat can be further projected into output\n(e.g., classification). A Transformer encoder\nconsists of  self-attention layers,\nwhere all input tokens attend to each other.\nFor example, vision Transformers depicted in :numref:`fig_vit`\nare encoder-only, converting a sequence of input image patches into\nthe representation of a special \u201c&lt;cls&gt;\u201d token.\nSince this representation depends on all input tokens,\nit is further projected into classification labels.\nThis design was inspired by an earlier encoder-only Transformer\npretrained on text: BERT (Bidirectional Encoder Representations from Transformers) :cite:`Devlin.Chang.Lee.ea.2018`."
    },
    {
      "chunk_id": "142c364c6560_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Pretraining BERT",
      "text": "![Left: Pretraining BERT with masked language modeling. Prediction of the masked \"love\" token depends on all input tokens before and after \"love\". Right: Attention pattern in the Transformer encoder. Each token along the vertical axis attends to all input tokens along the horizontal axis.](../img/bert-encoder-only.svg)\n:label:`fig_bert-encoder-only`\n\nBERT is pretrained on text sequences using *masked language modeling*:\ninput text with randomly masked tokens is fed\ninto a Transformer encoder to predict the masked tokens.\nAs illustrated in :numref:`fig_bert-encoder-only`,\nan original text sequence \"I\", \"love\", \"this\", \"red\", \"car\"\nis prepended with the \u201c&lt;cls&gt;\u201d token, and the \u201c&lt;mask&gt;\u201d token\nrandomly replaces \"love\"; then the cross-entropy loss between the masked token \"love\"\nand its prediction is to be minimized during pretraining.\nNote that there is no constraint in the attention pattern of Transformer encoders\n(right of :numref:`fig_bert-encoder-only`)\nso all tokens can attend to each other.\nThus, prediction of \"love\" depends on input tokens before and after it in the sequence.\nThis is why BERT is a \"bidirectional encoder\".\nWithout need for manual labeling, large-scale text data\nfrom books and Wikipedia can be used for pretraining BERT."
    },
    {
      "chunk_id": "71eaca82bb09_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Fine-Tuning BERT",
      "text": "The pretrained BERT can be *fine-tuned* to downstream encoding tasks involving single text or text pairs. During fine-tuning, additional layers can be added to BERT with randomized parameters: these parameters and those pretrained BERT parameters will be *updated* to fit training data of downstream tasks. ![Fine-tuning BERT for sentiment analysis.](../img/bert-finetune-classification.svg)\n:label:`fig_bert-finetune-classification`\n\n:numref:`fig_bert-finetune-classification` illustrates\nfine-tuning of BERT for sentiment analysis. The Transformer encoder is a pretrained BERT,\nwhich takes a text sequence as input\nand feeds the \u201c&lt;cls&gt;\u201d representation\n(global representation of the input)\ninto an additional fully connected layer\nto predict the sentiment. During fine-tuning, the cross-entropy loss\nbetween the prediction and the label\non sentiment analysis data\nis minimized via gradient-based algorithms,\nwhere the additional layer is trained from scratch\nwhile pretrained parameters of BERT are updated. BERT does more than sentiment analysis. The general language representations learned\nby the 350-million-parameter BERT\nfrom 250 billion training tokens\nadvanced the state of the art for natural language tasks\nsuch as single text classification,\ntext pair classification or regression,\ntext tagging, and question answering. You may note that these downstream tasks include text pair understanding. BERT pretraining has another loss for predicting\nwhether one sentence immediately follows the other. However, this loss was later found to be less useful when pretraining RoBERTa,\na BERT variant of the same size, on 2000 billion tokens :cite:`Liu.Ott.Goyal.ea.2019`."
    },
    {
      "chunk_id": "71eaca82bb09_1",
      "chapter": "large-pretraining-transformers",
      "heading": "Fine-Tuning BERT",
      "text": "However, this loss was later found to be less useful when pretraining RoBERTa,\na BERT variant of the same size, on 2000 billion tokens :cite:`Liu.Ott.Goyal.ea.2019`. Other derivatives of BERT improved model architectures or pretraining objectives,\nsuch as ALBERT (enforcing parameter sharing) :cite:`lan2019albert`,\nSpanBERT (representing and predicting spans of text) :cite:`joshi2020spanbert`,\nDistilBERT (lightweight via knowledge distillation) :cite:`sanh2019distilbert`,\nand ELECTRA (replaced token detection) :cite:`clark2019electra`. Moreover, BERT inspired Transformer pretraining in computer vision,\nsuch as with vision Transformers :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`,\nSwin Transformers :cite:`liu2021swin`,\nand MAE (masked autoencoders) :cite:`he2022masked`."
    },
    {
      "chunk_id": "7abbbbed25da_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Encoder--Decoder",
      "text": "Since a Transformer encoder converts a sequence of input tokens\ninto the same number of output representations,\nthe encoder-only mode cannot generate a sequence of arbitrary length as in machine translation.\nAs originally proposed for machine translation,\nthe Transformer architecture can be outfitted with a decoder\nthat autoregressively predicts the target sequence\nof arbitrary length, token by token,\nconditional on both encoder output and decoder output:\n(i) for conditioning on encoder output, encoder--decoder cross-attention\n(multi-head attention of decoder in :numref:`fig_transformer`)\nallows target tokens to attend to *all* input tokens;\n(ii) conditioning on decoder output is achieved\nby a so-called *causal* attention\n(this name is common in the literature but is misleading\nas it has little connection to the proper study of causality)\npattern (masked multi-head attention of decoder in :numref:`fig_transformer`),\nwhere any target token can only attend to *past* and *present* tokens in the target sequence.\n\nTo pretrain encoder--decoder Transformers beyond human-labeled machine translation data,\nBART :cite:`lewis2019bart` and T5 :cite:`raffel2020exploring`\nare two concurrently proposed encoder--decoder Transformers\npretrained on large-scale text corpora.\nBoth attempt to reconstruct original text in their pretraining objectives,\nwhile the former emphasizes noising input\n(e.g., masking, deletion, permutation, and rotation)\nand the latter highlights multitask unification\nwith comprehensive ablation studies."
    },
    {
      "chunk_id": "9590550a721e_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Pretraining T5",
      "text": "As an example of the pretrained Transformer encoder--decoder,\nT5 (Text-to-Text Transfer Transformer)\nunifies many tasks as the same text-to-text problem:\nfor any task, the input of the encoder is a task description\n(e.g., \"Summarize\", \":\") followed by task input\n(e.g., a sequence of tokens from an article),\nand the decoder predicts the task output\n(e.g., a sequence of tokens summarizing the input article). To perform as text-to-text, T5 is trained\nto generate some target text conditional on input text. ![Left: Pretraining T5 by predicting consecutive spans. The original sentence is \"I\", \"love\", \"this\", \"red\", \"car\", where \"love\" is replaced by a special \u201c&lt;X&gt;\u201d token, and consecutive \"red\", \"car\" are replaced by a special \u201c&lt;Y&gt;\u201d token. The target sequence ends with a special \u201c&lt;Z&gt;\u201d token. Right: Attention pattern in the Transformer encoder--decoder. In the encoder self-attention (lower square), all input tokens attend to each other; In the encoder--decoder cross-attention (upper rectangle), each target token attends to all input tokens; In the decoder self-attention (upper triangle), each target token  attends to present and past target tokens only (causal).](../img/t5-encoder-decoder.svg)\n:label:`fig_t5-encoder-decoder`\n\nTo obtain input and output from any original text,\nT5 is pretrained to predict consecutive spans. Specifically, tokens from text are randomly replaced\nby special tokens where each consecutive span\nis replaced by the same special token. Consider the example in :numref:`fig_t5-encoder-decoder`,\nwhere the original text is \"I\", \"love\", \"this\", \"red\", \"car\". Tokens \"love\", \"red\", \"car\" are randomly replaced by special tokens. Since \"red\" and \"car\" are a consecutive span,\nthey are replaced by the same special token. As a result, the input sequence is \"I\", \"&lt;X&gt;\", \"this\", \"&lt;Y&gt;\",\nand the target sequence is\n\"&lt;X&gt;\", \"love\", \"&lt;Y&gt;\", \"red\", \"car\", \"&lt;Z&gt;\",\nwhere \"&lt;Z&gt;\" is another special token marking the end."
    },
    {
      "chunk_id": "9590550a721e_1",
      "chapter": "large-pretraining-transformers",
      "heading": "Pretraining T5",
      "text": "As a result, the input sequence is \"I\", \"&lt;X&gt;\", \"this\", \"&lt;Y&gt;\",\nand the target sequence is\n\"&lt;X&gt;\", \"love\", \"&lt;Y&gt;\", \"red\", \"car\", \"&lt;Z&gt;\",\nwhere \"&lt;Z&gt;\" is another special token marking the end. As shown in :numref:`fig_t5-encoder-decoder`,\nthe decoder has a causal attention pattern to prevent itself\nfrom attending to future tokens during sequence prediction. In T5, predicting consecutive span is also referred to\nas reconstructing corrupted text. With this objective, T5 is pretrained\nwith 1000 billion tokens from the C4\n(Colossal Clean Crawled Corpus) data,\nwhich consists of clean English text\nfrom the web :cite:`raffel2020exploring`."
    },
    {
      "chunk_id": "4dd47209ff2f_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Fine-Tuning T5",
      "text": "Similar to BERT, T5 needs to be fine-tuned (updating T5 parameters)\non task-specific training data to perform this task.\nMajor differences from BERT fine-tuning include:\n(i) T5 input includes task descriptions;\n(ii) T5 can generate sequences\nwith arbitrary length\nwith its Transformer decoder;\n(iii) No additional layers are required.\n\n![Fine-tuning T5 for text summarization. Both the task description and article tokens are fed into the Transformer encoder for predicting the summary.](../img/t5-finetune-summarization.svg)\n:label:`fig_t5-finetune-summarization`\n\n:numref:`fig_t5-finetune-summarization`\nexplains fine-tuning T5\nusing text summarization as an example.\nIn this downstream task,\nthe task description tokens \"Summarize\", \":\"\nfollowed by the article tokens are input to the encoder.\n\nAfter fine-tuning, the 11-billion-parameter T5 (T5-11B)\nachieved state-of-the-art results on multiple encoding (e.g., classification)\nand generation (e.g., summarization) benchmarks.\nSince released, T5 has been extensively used in later research.\nFor example, switch Transformers are designed based on T5\nto activate a subset of the parameters\nfor better computational efficiency :cite:`fedus2022switch`.\nIn a text-to-image model called Imagen,\ntext is input to a frozen T5 encoder (T5-XXL)\nwith 4.6 billion parameters :cite:`saharia2022photorealistic`.\nThe photorealistic text-to-image examples in :numref:`fig_imagen`\nsuggest that the T5 encoder alone may effectively\nrepresent text even without fine-tuning.\n\n![Text-to-image examples by the Imagen model, whose text encoder is from T5 (figures taken from :citet:`saharia2022photorealistic`).](../img/imagen.png)\n:width:`700px`\n:label:`fig_imagen`"
    },
    {
      "chunk_id": "5c0101247adc_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Decoder-Only",
      "text": "We have reviewed encoder-only and encoder--decoder Transformers.\nAlternatively, decoder-only Transformers\nremove the entire encoder and the decoder sublayer\nwith the encoder--decoder cross-attention\nfrom the original encoder--decoder architecture\ndepicted in :numref:`fig_transformer`.\nNowadays, decoder-only Transformers have been the *de facto* architecture\nin large-scale language modeling (:numref:`sec_language-model`),\nwhich leverages the world's abundant unlabeled text corpora via self-supervised learning."
    },
    {
      "chunk_id": "9418ac38872a_0",
      "chapter": "large-pretraining-transformers",
      "heading": "GPT and GPT-2",
      "text": "Using language modeling as the training objective,\nthe GPT (generative pre-training) model\nchooses a Transformer decoder\nas its backbone :cite:`Radford.Narasimhan.Salimans.ea.2018`.\n\n![Left: Pretraining GPT with language modeling. The target sequence is the input sequence shifted by one token. Both \u201c&lt;bos&gt;\u201d and \u201c&lt;eos&gt;\u201d are special tokens marking the beginning and end of sequences, respectively. Right: Attention pattern in the Transformer decoder. Each token along the vertical axis attends to only its past tokens along the horizontal axis (causal).](../img/gpt-decoder-only.svg)\n:label:`fig_gpt-decoder-only`\n\nFollowing the autoregressive language model training\nas described in :numref:`subsec_partitioning-seqs`,\n:numref:`fig_gpt-decoder-only` illustrates\nGPT pretraining with a Transformer encoder,\nwhere the target sequence is the input sequence shifted by one token.\nNote that the attention pattern in the Transformer decoder\nenforces that each token can only attend to its past tokens\n(future tokens cannot be attended to because they have not yet been chosen).\n\n\nGPT has 100 million parameters and needs to be\nfine-tuned for individual downstream tasks.\nA much larger Transformer-decoder language model,\nGPT-2, was introduced one year later :cite:`Radford.Wu.Child.ea.2019`.\nCompared with the original Transformer decoder in GPT, pre-normalization\n(discussed in :numref:`subsec_vit-encoder`)\nand improved initialization and weight-scaling were adopted in GPT-2.\nPretrained on 40 GB of text, the 1.5-billion-parameter\nGPT-2 obtained the state-of-the-art results on language modeling benchmarks\nand promising results on multiple other tasks\n*without updating the parameters or architecture*."
    },
    {
      "chunk_id": "3ec322a38638_0",
      "chapter": "large-pretraining-transformers",
      "heading": "GPT-3 and Beyond",
      "text": "GPT-2 demonstrated potential of using the same language model\nfor multiple tasks without updating the model. This is more computationally efficient than fine-tuning,\nwhich requires model updates via gradient computation. ![Zero-shot, one-shot, few-shot in-context learning with language models (Transformer decoders). No parameter update is needed.](../img/gpt-3-xshot.svg)\n:label:`fig_gpt-3-xshot`\n\nBefore explaining the more computationally efficient use\nof language models without parameter update,\nrecall :numref:`sec_rnn-scratch` that a language model\ncan be trained to generate a text sequence\nconditional on some prefix text sequence. Thus, a pretrained language model may generate the task output\nas a sequence *without parameter update*,\nconditional on an input sequence with the task description,\ntask-specific input--output examples, and a prompt (task input). This learning paradigm is called *in-context learning* :cite:`brown2020language`,\nwhich can be further categorized\ninto *zero-shot*, *one-shot*, and *few-shot*,\nwhen there is no, one, and a few task-specific input--output examples (:numref:`fig_gpt-3-xshot`). ![Aggregate performance of GPT-3 for all 42 accuracy-denominated benchmarks (caption adapted and figure taken from :citet:`brown2020language`).](../img/gpt3-xshot-scaling.png)\n:width:`400px`\n:label:`fig_gpt3-xshot-scaling`\n\nThese three settings were tested in GPT-3 :cite:`brown2020language`,\nwhose largest version uses data and model size\nabout two orders of magnitude larger than those in GPT-2. GPT-3 uses the same Transformer decoder architecture\nas its direct predecessor GPT-2\nexcept that attention patterns\n(at the right in :numref:`fig_gpt-decoder-only`)\nare sparser at alternating layers. Pretrained with 300 billion tokens,\nGPT-3 performs better with larger model size,\nwhere few-shot performance increases most rapidly (:numref:`fig_gpt3-xshot-scaling`). The subsequent GPT-4 model did not fully disclose technical details in its report :cite:`openai2023gpt4`."
    },
    {
      "chunk_id": "3ec322a38638_1",
      "chapter": "large-pretraining-transformers",
      "heading": "GPT-3 and Beyond",
      "text": "The subsequent GPT-4 model did not fully disclose technical details in its report :cite:`openai2023gpt4`. By contrast with its predecessors, GPT-4\nis a large-scale, multimodal model that\ncan take both text and images as input\nand generate text output."
    },
    {
      "chunk_id": "da2a4167b944_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Scalability",
      "text": ":numref:`fig_gpt3-xshot-scaling` empirically demonstrates scalability\nof Transformers in the GPT-3 language model. For language modeling, more comprehensive empirical studies\non the scalability of Transformers have led researchers to see promise\nin training larger Transformers with more data and compute :cite:`kaplan2020scaling`. ![Transformer language model performance improves smoothly as we increase the model size, dataset size, and amount of compute used for training. For optimal performance all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two (caption adapted and figure taken from :citet:`kaplan2020scaling`).](../img/scaling-power-law.png)\n:width:`700px`\n:label:`fig_scaling-power-law3`\n\nAs shown in :numref:`fig_scaling-power-law3`,\n*power-law scaling* can be observed in the performance\nwith respect to the model size (number of parameters, excluding embedding layers),\ndataset size (number of training tokens),\nand amount of training compute (PetaFLOP/s-days, excluding embedding layers). In general, increasing all these three factors in tandem leads to better performance. However, *how* to increase them in tandem\nstill remains a matter of debate :cite:`hoffmann2022training`. ![Transformer language model training runs (figure taken from :citet:`kaplan2020scaling`).](../img/scaling-sample-conv.png)\n:width:`700px`\n:label:`fig_scaling-sample-conv`\n\nAs well as increased performance, large models also enjoy better sample efficiency than small models. :numref:`fig_scaling-sample-conv` shows that large models need fewer training samples (tokens processed) to perform at the same level achieved by small models, and performance is scaled smoothly with compute. ![GPT-3 performance (cross-entropy validation loss) follows a power-law trend with the amount of compute used for training."
    },
    {
      "chunk_id": "da2a4167b944_1",
      "chapter": "large-pretraining-transformers",
      "heading": "Scalability",
      "text": "![GPT-3 performance (cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in :citet:`kaplan2020scaling` continues for an additional two orders of magnitude with only small deviations from the predicted curve. Embedding parameters are excluded from compute and parameter counts (caption adapted and figure taken from :citet:`brown2020language`).](../img/scaling-gpt3.png)\n:width:`250px`\n:label:`fig_scaling-gpt3`\n\n\nThe empirical scaling behaviors in :citet:`kaplan2020scaling` have been tested in subsequent large Transformer models. For example, GPT-3 supported this hypothesis with two more orders of magnitude in :numref:`fig_scaling-gpt3`."
    },
    {
      "chunk_id": "8f3b5b10559b_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Large Language Models",
      "text": "The scalability of Transformers in the GPT series has inspired subsequent large language models. The GPT-2 Transformer decoder was used for training the 530-billion-parameter Megatron-Turing NLG :cite:`smith2022using` with 270 billion training tokens. Following the GPT-2 design, the 280-billion-parameter Gopher :cite:`rae2021scaling` pretrained with 300 billion tokens, performed competitively across diverse tasks. Inheriting the same architecture and using the same compute budget of Gopher, Chinchilla :cite:`hoffmann2022training` is a substantially smaller (70 billion parameters) model that trains for much longer (1.4 trillion training tokens), outperforming Gopher on many tasks and with more emphasis on the number of tokens than on the number of parameters. To continue the scaling line of language modeling, \nPaLM (Pathway Language Model) :cite:`chowdhery2022palm`, a 540-billion-parameter Transformer decoder with modified designs pretrained on 780 billion tokens, outperformed average human performance on the BIG-Bench benchmark :cite:`srivastava2022beyond`. Its later version, PaLM 2 :cite:`anil2023palm`, scaled data and model roughly 1:1 and improved multilingual and reasoning capabilities. Other large language models, such as Minerva  :cite:`lewkowycz2022solving` that further trains a generalist (PaLM) and Galactica :cite:`taylor2022galactica` that is not trained on a general corpus, have shown promising quantitative and scientific reasoning capabilities. Open-sourced releases, such as OPT (Open Pretrained Transformers) :cite:`zhang2022opt`, BLOOM :cite:` scao2022bloom`, and FALCON :cite:`penedo2023refinedweb`,\ndemocratized research and use of large language models. Focusing on computational efficiency at inference time,\nthe open-sourced Llama 1 :cite:`touvron2023llama` outperformed much larger models by training on more tokens than had been typically used."
    },
    {
      "chunk_id": "8f3b5b10559b_1",
      "chapter": "large-pretraining-transformers",
      "heading": "Large Language Models",
      "text": "Focusing on computational efficiency at inference time,\nthe open-sourced Llama 1 :cite:`touvron2023llama` outperformed much larger models by training on more tokens than had been typically used. The updated Llama 2 :cite:`touvron2023llama2` further increased the pretraining corpus by 40%, leading to product models that may match the performance of competitive close-sourced models. :citet:`wei2022emergent` discussed emergent abilities of large language models that are present in larger models, but not in smaller models. However, simply increasing model size does not inherently make models follow human instructions better. :citet:`wei2021finetuned,sanh2021multitask` have found that fine-tuning large language models\non a range of datasets described via *instructions*\ncan improve zero-shot performance on held-out tasks. Using *reinforcement learning from human feedback*,\n:citet:`ouyang2022training` fine-tuned GPT-3\nto follow a diverse set of instructions. Following the resultant InstructGPT which\naligns language models with human intent\nvia fine-tuning :cite:`ouyang2022training`,\n[ChatGPT](https://chat.openai.com/)\ncan generate human-like responses (e.g., code debugging and creative writing)\nbased on conversations with humans\nand can perform many natural language processing\ntasks zero-shot :cite:`qin2023chatgpt`. :citet:`bai2022constitutional` replaced human inputs (e.g., human-labeled data) with model outputs\nto partially automate the instruction tuning process, which is also known as *reinforcement learning from AI feedback*. Large language models offer an exciting prospect\nof formulating text input to induce models to perform desired tasks via in-context learning,\nwhich is also known as *prompting*. Notably,\n*chain-of-thought prompting* :cite:`wei2022chain`,\nan in-context learning method\nwith few-shot \"question, intermediate reasoning steps, answer\" demonstrations,\nelicits the complex reasoning capabilities of\nlarge language models\nin order to solve mathematical, commonsense, and symbolic reasoning tasks."
    },
    {
      "chunk_id": "8f3b5b10559b_2",
      "chapter": "large-pretraining-transformers",
      "heading": "Large Language Models",
      "text": "Sampling multiple reasoning paths :cite:`wang2023self`, diversifying few-shot demonstrations :cite:`zhang2023automatic`, \nand reducing complex problems to sub-problems :cite:`zhou2023least`\ncan all improve the reasoning accuracy. In fact, with simple prompts like \"Let's think step by step\" just before each answer,\nlarge language models can even perform *zero-shot*\nchain-of-thought reasoning with decent accuracy :cite:`kojima2022large`. Even for multimodal inputs consisting of both text and images,\nlanguage models can perform multimodal chain-of-thought reasoning with higher accuracy than using text input only :cite:`zhang2023multicot`."
    },
    {
      "chunk_id": "af39b2ab0a32_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Summary and Discussion",
      "text": "Transformers have been pretrained as encoder-only (e.g., BERT), encoder--decoder (e.g., T5), and decoder-only (e.g., GPT series). Pretrained models may be adapted to perform different tasks with model update (e.g., fine-tuning) or not (e.g., few-shot). Scalability of Transformers suggests that better performance benefits from larger models, more training data, and more training compute. Since Transformers were first designed and pretrained for text data, this section leans slightly towards natural language processing. Nonetheless, those models discussed above can be often found in more recent models across multiple modalities. For example,\n(i) Chinchilla :cite:`hoffmann2022training` was further extended to Flamingo :cite:`alayrac2022flamingo`, a visual language model for few-shot learning;\n(ii) GPT-2 :cite:`Radford.Wu.Child.ea.2019` and the vision Transformer encode text and images in CLIP (Contrastive Language-Image Pre-training) :cite:`radford2021learning`, whose image and text embeddings were later adopted in the DALL-E 2 text-to-image system :cite:`ramesh2022hierarchical`. Although there have been no systematic studies on Transformer scalability in multimodal pretraining yet, an all-Transformer text-to-image model called Parti :cite:`yu2022scaling` shows potential of scalability across modalities:\na larger Parti is more capable of high-fidelity image generation and content-rich text understanding (:numref:`fig_parti`).\n\n\n![Image examples generated from the same text by the Parti model of increasing sizes (350M, 750M, 3B, 20B) (examples taken from :citet:`yu2022scaling`).](../img/parti.png)\n:width:`700px`\n:label:`fig_parti`"
    },
    {
      "chunk_id": "a5e9ea76f19a_0",
      "chapter": "large-pretraining-transformers",
      "heading": "Exercises",
      "text": "1. Is it possible to fine-tune T5 using a minibatch consisting of different tasks? Why or why not? How about for GPT-2?\n1. Given a powerful language model, what applications can you think of?\n1. Say that you are asked to fine-tune a language model to perform text classification by adding additional layers. Where will you add them? Why?\n1. Consider sequence-to-sequence problems (e.g., machine translation) where the input sequence is always available throughout the target sequence prediction. What could be limitations of modeling with decoder-only Transformers? Why?\n\n\n[Discussions](https://discuss.d2l.ai/t/9232)"
    },
    {
      "chunk_id": "b8994abf13af_0",
      "chapter": "multihead-attention",
      "heading": "multihead-attention",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n# Multi-Head Attention\n:label:`sec_multihead-attention`\n\n\nIn practice, given the same set of queries, keys, and values we may want our model to combine knowledge from\ndifferent behaviors of the same attention mechanism,\nsuch as capturing dependencies of various ranges\n(e.g., shorter-range vs. longer-range) within a sequence.\nThus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.\n\n\nTo this end, instead of performing \na single attention pooling,\nqueries, keys, and values\ncan be transformed\nwith $h$ independently learned linear projections.\nThen these $h$ projected queries, keys, and values\nare fed into attention pooling in parallel.\nIn the end,\n$h$ attention-pooling outputs\nare concatenated and \ntransformed with another learned linear projection\nto produce the final output.\nThis design\nis called *multi-head attention*,\nwhere each of the $h$ attention pooling outputs\nis a *head* :cite:`Vaswani.Shazeer.Parmar.ea.2017`.\nUsing fully connected layers\nto perform learnable linear transformations,\n:numref:`fig_multi-head-attention`\ndescribes multi-head attention.\n\n![Multi-head attention, where multiple heads are concatenated then linearly transformed.](../img/multi-head-attention.svg)\n:label:`fig_multi-head-attention`\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import autograd, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport math\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\n```"
    },
    {
      "chunk_id": "56d1f16a9c51_0",
      "chapter": "multihead-attention",
      "heading": "Model",
      "text": "Before providing the implementation of multi-head attention,\nlet's formalize this model mathematically.\nGiven a query $\\mathbf{q} \\in \\mathbb{R}^{d_q}$,\na key $\\mathbf{k} \\in \\mathbb{R}^{d_k}$,\nand a value $\\mathbf{v} \\in \\mathbb{R}^{d_v}$,\neach attention head $\\mathbf{h}_i$  ($i = 1, \\ldots, h$)\nis computed as\n\n$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n\nwhere \n$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$,\n$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$,\nand $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$\nare learnable parameters and\n$f$ is attention pooling,\nsuch as\nadditive attention and scaled dot product attention\nin :numref:`sec_attention-scoring-functions`.\nThe multi-head attention output\nis another linear transformation via \nlearnable parameters\n$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\nof the concatenation of $h$ heads:\n\n$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$\n\nBased on this design, each head may attend\nto different parts of the input.\nMore sophisticated functions \nthan the simple weighted average can be expressed."
    },
    {
      "chunk_id": "84a00f0afe88_0",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "In our implementation,\nwe [**choose the scaled dot product attention\nfor each head**] of the multi-head attention. To avoid significant growth of computational cost and parametrization cost,\nwe set $p_q = p_k = p_v = p_o / h$. Note that $h$ heads can be computed in parallel\nif we set the number of outputs \nof linear transformations\nfor the query, key, and value\nto $p_q h = p_k h = p_v h = p_o$. In the following implementation,\n$p_o$ is specified via the argument `num_hiddens`. ```{.python .input}\n%%tab mxnet\nclass MultiHeadAttention(d2l.Module):  #@save\n    \"\"\"Multi-head attention.\"\"\"\n    def __init__(self, num_hiddens, num_heads, dropout, use_bias=False,\n                 **kwargs):\n        super().__init__()\n        self.num_heads = num_heads\n        self.attention = d2l.DotProductAttention(dropout)\n        self.W_q = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n        self.W_k = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n        self.W_v = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n        self.W_o = nn.Dense(num_hiddens, use_bias=use_bias, flatten=False)\n\n    def forward(self, queries, keys, values, valid_lens):\n        # Shape of queries, keys, or values:\n        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n        # After transposing, shape of output queries, keys, or values:\n        # (batch_size * num_heads, no. of queries or key-value pairs,\n        # num_hiddens / num_heads)\n        queries = self.transpose_qkv(self.W_q(queries))\n        keys = self.transpose_qkv(self.W_k(keys))\n        values = self.transpose_qkv(self.W_v(values))\n\n        if valid_lens is not None:\n            # On axis 0, copy the first item (scalar or vector) for num_heads\n            # times, then copy the next item, and so on\n            valid_lens = valid_lens.repeat(self.num_heads, axis=0)\n\n        # Shape of output: (batch_size * num_heads, no."
    },
    {
      "chunk_id": "84a00f0afe88_1",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "of queries,\n        # num_hiddens / num_heads)\n        output = self.attention(queries, keys, values, valid_lens)\n        \n        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n        output_concat = self.transpose_output(output)\n        return self.W_o(output_concat)\n```\n\n```{.python .input}\n%%tab pytorch\nclass MultiHeadAttention(d2l.Module):  #@save\n    \"\"\"Multi-head attention.\"\"\"\n    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n        super().__init__()\n        self.num_heads = num_heads\n        self.attention = d2l.DotProductAttention(dropout)\n        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n\n    def forward(self, queries, keys, values, valid_lens):\n        # Shape of queries, keys, or values:\n        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n        # After transposing, shape of output queries, keys, or values:\n        # (batch_size * num_heads, no. of queries or key-value pairs,\n        # num_hiddens / num_heads)\n        queries = self.transpose_qkv(self.W_q(queries))\n        keys = self.transpose_qkv(self.W_k(keys))\n        values = self.transpose_qkv(self.W_v(values))\n\n        if valid_lens is not None:\n            # On axis 0, copy the first item (scalar or vector) for num_heads\n            # times, then copy the next item, and so on\n            valid_lens = torch.repeat_interleave(\n                valid_lens, repeats=self.num_heads, dim=0)\n\n        # Shape of output: (batch_size * num_heads, no. of queries,\n        # num_hiddens / num_heads)\n        output = self.attention(queries, keys, values, valid_lens)\n        # Shape of output_concat: (batch_size, no."
    },
    {
      "chunk_id": "84a00f0afe88_2",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "of queries,\n        # num_hiddens / num_heads)\n        output = self.attention(queries, keys, values, valid_lens)\n        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n        output_concat = self.transpose_output(output)\n        return self.W_o(output_concat)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MultiHeadAttention(d2l.Module):  #@save\n    \"\"\"Multi-head attention.\"\"\"\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 num_heads, dropout, bias=False, **kwargs):\n        super().__init__()\n        self.num_heads = num_heads\n        self.attention = d2l.DotProductAttention(dropout)\n        self.W_q = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n        self.W_k = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n        self.W_v = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n        self.W_o = tf.keras.layers.Dense(num_hiddens, use_bias=bias)\n    \n    def call(self, queries, keys, values, valid_lens, **kwargs):\n        # Shape of queries, keys, or values:\n        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n        # After transposing, shape of output queries, keys, or values:\n        # (batch_size * num_heads, no. of queries or key-value pairs,\n        # num_hiddens / num_heads)\n        queries = self.transpose_qkv(self.W_q(queries))\n        keys = self.transpose_qkv(self.W_k(keys))\n        values = self.transpose_qkv(self.W_v(values))\n        \n        if valid_lens is not None:\n            # On axis 0, copy the first item (scalar or vector) for num_heads\n            # times, then copy the next item, and so on\n            valid_lens = tf.repeat(valid_lens, repeats=self.num_heads, axis=0)\n            \n        # Shape of output: (batch_size * num_heads, no. of queries,\n        # num_hiddens / num_heads)\n        output = self.attention(queries, keys, values, valid_lens, **kwargs)\n        \n        # Shape of output_concat: (batch_size, no."
    },
    {
      "chunk_id": "84a00f0afe88_3",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "of queries,\n        # num_hiddens / num_heads)\n        output = self.attention(queries, keys, values, valid_lens, **kwargs)\n        \n        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n        output_concat = self.transpose_output(output)\n        return self.W_o(output_concat)\n```\n\n```{.python .input}\n%%tab jax\nclass MultiHeadAttention(nn.Module):  #@save\n    num_hiddens: int\n    num_heads: int\n    dropout: float\n    bias: bool = False\n\n    def setup(self):\n        self.attention = d2l.DotProductAttention(self.dropout)\n        self.W_q = nn.Dense(self.num_hiddens, use_bias=self.bias)\n        self.W_k = nn.Dense(self.num_hiddens, use_bias=self.bias)\n        self.W_v = nn.Dense(self.num_hiddens, use_bias=self.bias)\n        self.W_o = nn.Dense(self.num_hiddens, use_bias=self.bias)\n\n    @nn.compact\n    def __call__(self, queries, keys, values, valid_lens, training=False):\n        # Shape of queries, keys, or values:\n        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n        # After transposing, shape of output queries, keys, or values:\n        # (batch_size * num_heads, no. of queries or key-value pairs,\n        # num_hiddens / num_heads)\n        queries = self.transpose_qkv(self.W_q(queries))\n        keys = self.transpose_qkv(self.W_k(keys))\n        values = self.transpose_qkv(self.W_v(values))\n\n        if valid_lens is not None:\n            # On axis 0, copy the first item (scalar or vector) for num_heads\n            # times, then copy the next item, and so on\n            valid_lens = jnp.repeat(valid_lens, self.num_heads, axis=0)\n\n        # Shape of output: (batch_size * num_heads, no. of queries,\n        # num_hiddens / num_heads)\n        output, attention_weights = self.attention(\n            queries, keys, values, valid_lens, training=training)\n        # Shape of output_concat: (batch_size, no."
    },
    {
      "chunk_id": "84a00f0afe88_4",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "of queries,\n        # num_hiddens / num_heads)\n        output, attention_weights = self.attention(\n            queries, keys, values, valid_lens, training=training)\n        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n        output_concat = self.transpose_output(output)\n        return self.W_o(output_concat), attention_weights\n```\n\nTo allow for [**parallel computation of multiple heads**],\nthe above `MultiHeadAttention` class uses two transposition methods as defined below. Specifically,\nthe `transpose_output` method reverses the operation\nof the `transpose_qkv` method. ```{.python .input}\n%%tab mxnet\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_qkv(self, X):\n    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n    # key-value pairs, num_heads, num_hiddens / num_heads)\n    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    X = X.transpose(0, 2, 1, 3)\n    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    return X.reshape(-1, X.shape[2], X.shape[3])\n\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_output(self, X):\n    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n    X = X.transpose(0, 2, 1, 3)\n    return X.reshape(X.shape[0], X.shape[1], -1)\n```\n\n```{.python .input}\n%%tab pytorch\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_qkv(self, X):\n    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n    # num_hiddens). Shape of output X: (batch_size, no."
    },
    {
      "chunk_id": "84a00f0afe88_5",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "of queries or key-value pairs,\n    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n    # key-value pairs, num_heads, num_hiddens / num_heads)\n    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    X = X.permute(0, 2, 1, 3)\n    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    return X.reshape(-1, X.shape[2], X.shape[3])\n\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_output(self, X):\n    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n    X = X.permute(0, 2, 1, 3)\n    return X.reshape(X.shape[0], X.shape[1], -1)\n```\n\n```{.python .input}\n%%tab tensorflow\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_qkv(self, X):\n    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n    # key-value pairs, num_heads, num_hiddens / num_heads)\n    X = tf.reshape(X, shape=(X.shape[0], X.shape[1], self.num_heads, -1))\n    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    X = tf.transpose(X, perm=(0, 2, 1, 3))\n    # Shape of output: (batch_size * num_heads, no."
    },
    {
      "chunk_id": "84a00f0afe88_6",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    X = tf.transpose(X, perm=(0, 2, 1, 3))\n    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    return tf.reshape(X, shape=(-1, X.shape[2], X.shape[3]))\n\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_output(self, X):\n    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n    X = tf.reshape(X, shape=(-1, self.num_heads, X.shape[1], X.shape[2]))\n    X = tf.transpose(X, perm=(0, 2, 1, 3))\n    return tf.reshape(X, shape=(X.shape[0], X.shape[1], -1))\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_qkv(self, X):\n    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n    # key-value pairs, num_heads, num_hiddens / num_heads)\n    X = X.reshape((X.shape[0], X.shape[1], self.num_heads, -1))\n    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    X = jnp.transpose(X, (0, 2, 1, 3))\n    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n    # pairs, num_hiddens / num_heads)\n    return X.reshape((-1, X.shape[2], X.shape[3]))\n\n@d2l.add_to_class(MultiHeadAttention)  #@save\ndef transpose_output(self, X):\n    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n    X = X.reshape((-1, self.num_heads, X.shape[1], X.shape[2]))\n    X = jnp.transpose(X, (0, 2, 1, 3))\n    return X.reshape((X.shape[0], X.shape[1], -1))\n```\n\nLet's [**test our implemented**] `MultiHeadAttention` class\nusing a toy example where keys and values are the same. As a result,\nthe shape of the multi-head attention output\nis (`batch_size`, `num_queries`, `num_hiddens`)."
    },
    {
      "chunk_id": "84a00f0afe88_7",
      "chapter": "multihead-attention",
      "heading": "Implementation",
      "text": "As a result,\nthe shape of the multi-head attention output\nis (`batch_size`, `num_queries`, `num_hiddens`). ```{.python .input}\n%%tab pytorch\nnum_hiddens, num_heads = 100, 5\nattention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, num_kvpairs = 2, 4, 6\nvalid_lens = d2l.tensor([3, 2])\nX = d2l.ones((batch_size, num_queries, num_hiddens))\nY = d2l.ones((batch_size, num_kvpairs, num_hiddens))\nd2l.check_shape(attention(X, Y, Y, valid_lens),\n                (batch_size, num_queries, num_hiddens))\n```\n\n```{.python .input}\n%%tab mxnet\nnum_hiddens, num_heads = 100, 5\nattention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\nattention.initialize()\n```\n\n```{.python .input}\n%%tab jax\nnum_hiddens, num_heads = 100, 5\nattention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n```\n\n```{.python .input}\n%%tab tensorflow\nnum_hiddens, num_heads = 100, 5\nattention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n                               num_hiddens, num_heads, 0.5)\n```\n\n```{.python .input}\n%%tab mxnet\nbatch_size, num_queries, num_kvpairs = 2, 4, 6\nvalid_lens = d2l.tensor([3, 2])\nX = d2l.ones((batch_size, num_queries, num_hiddens))\nY = d2l.ones((batch_size, num_kvpairs, num_hiddens))\nd2l.check_shape(attention(X, Y, Y, valid_lens),\n                (batch_size, num_queries, num_hiddens))\n```\n\n```{.python .input}\n%%tab tensorflow\nbatch_size, num_queries, num_kvpairs = 2, 4, 6\nvalid_lens = d2l.tensor([3, 2])\nX = tf.ones((batch_size, num_queries, num_hiddens))\nY = tf.ones((batch_size, num_kvpairs, num_hiddens))\nd2l.check_shape(attention(X, Y, Y, valid_lens, training=False),\n                (batch_size, num_queries, num_hiddens))\n```\n\n```{.python .input}\n%%tab jax\nbatch_size, num_queries, num_kvpairs = 2, 4, 6\nvalid_lens = d2l.tensor([3, 2])\nX = d2l.ones((batch_size, num_queries, num_hiddens))\nY = d2l.ones((batch_size, num_kvpairs, num_hiddens))\nd2l.check_shape(attention.init_with_output(d2l.get_key(), X, Y, Y, valid_lens,\n                                           training=False)[0][0],\n                (batch_size, num_queries, num_hiddens))\n```"
    },
    {
      "chunk_id": "798c42a94d98_0",
      "chapter": "multihead-attention",
      "heading": "Summary",
      "text": "Multi-head attention combines knowledge of the same attention pooling \nvia different representation subspaces of queries, keys, and values.\nTo compute multiple heads of multi-head attention in parallel, \nproper tensor manipulation is needed."
    },
    {
      "chunk_id": "462d50f8f312_0",
      "chapter": "multihead-attention",
      "heading": "Exercises",
      "text": "1. Visualize attention weights of multiple heads in this experiment.\n1. Suppose that we have a trained model based on multi-head attention and we want to prune less important attention heads to increase the prediction speed. How can we design experiments to measure the importance of an attention head?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/1634)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1635)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3869)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18029)\n:end_tab:"
    },
    {
      "chunk_id": "b0e4e31d357e_0",
      "chapter": "queries-keys-values",
      "heading": "queries-keys-values",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow')\n```\n\n# Queries, Keys, and Values\n:label:`sec_queries-keys-values`\n\nSo far all the networks we have reviewed crucially relied on the input being of a well-defined size. For instance, the images in ImageNet are of size $224 \\times 224$ pixels and CNNs are specifically tuned to this size. Even in natural language processing the input size for RNNs is well defined and fixed. Variable size is addressed by sequentially processing one token at a time, or by specially designed convolution kernels :cite:`Kalchbrenner.Grefenstette.Blunsom.2014`. This approach can lead to significant problems when the input is truly of varying size with varying information content, such as in :numref:`sec_seq2seq` in the transformation of text :cite:`Sutskever.Vinyals.Le.2014`. In particular, for long sequences it becomes quite difficult to keep track of everything that has already been generated or even viewed by the network. Even explicit tracking heuristics such as proposed by :citet:`yang2016neural` only offer limited benefit. Compare this to databases. In their simplest form they are collections of keys ($k$) and values ($v$). For instance, our database $\\mathcal{D}$ might consist of tuples \\{(\"Zhang\", \"Aston\"), (\"Lipton\", \"Zachary\"), (\"Li\", \"Mu\"), (\"Smola\", \"Alex\"), (\"Hu\", \"Rachel\"), (\"Werness\", \"Brent\")\\} with the last name being the key and the first name being the value. We can operate on $\\mathcal{D}$, for instance with the exact query ($q$) for \"Li\" which would return the value \"Mu\". If (\"Li\", \"Mu\") was not a record in $\\mathcal{D}$, there would be no valid answer. If we also allowed for approximate matches, we would retrieve (\"Lipton\", \"Zachary\") instead. This quite simple and trivial example nonetheless teaches us a number of useful things:\n\n* We can design queries $q$ that operate on ($k$,$v$) pairs in such a manner as to be valid regardless of the  database size."
    },
    {
      "chunk_id": "b0e4e31d357e_1",
      "chapter": "queries-keys-values",
      "heading": "queries-keys-values",
      "text": "This quite simple and trivial example nonetheless teaches us a number of useful things:\n\n* We can design queries $q$ that operate on ($k$,$v$) pairs in such a manner as to be valid regardless of the  database size. * The same query can receive different answers, according to the contents of the database. * The \"code\" being executed for operating on a large state space (the database) can be quite simple (e.g., exact match, approximate match, top-$k$). * There is no need to compress or simplify the database to make the operations effective. Clearly we would not have introduced a simple database here if it wasn't for the purpose of explaining deep learning. Indeed, this leads to one of the most exciting concepts introduced in deep learning in the past decade: the *attention mechanism* :cite:`Bahdanau.Cho.Bengio.2014`. We will cover the specifics of its application to machine translation later. For now, simply consider the following: denote by $\\mathcal{D} \\stackrel{\\textrm{def}}{=} \\{(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots (\\mathbf{k}_m, \\mathbf{v}_m)\\}$ a database of $m$ tuples of *keys* and *values*. Moreover, denote by $\\mathbf{q}$ a *query*. Then we can define the *attention* over $\\mathcal{D}$ as\n\n$$\\textrm{Attention}(\\mathbf{q}, \\mathcal{D}) \\stackrel{\\textrm{def}}{=} \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i,$$\n:eqlabel:`eq_attention_pooling`\n\nwhere $\\alpha(\\mathbf{q}, \\mathbf{k}_i) \\in \\mathbb{R}$ ($i = 1, \\ldots, m$) are scalar attention weights. The operation itself is typically referred to as *attention pooling*. The name *attention* derives from the fact that the operation pays particular attention to the terms for which the weight $\\alpha$ is significant (i.e., large). As such, the attention over $\\mathcal{D}$ generates a linear combination of values contained in the database. In fact, this contains the above example as a special case where all but one weight is zero. We have a number of special cases:\n\n* The weights $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ are nonnegative."
    },
    {
      "chunk_id": "b0e4e31d357e_2",
      "chapter": "queries-keys-values",
      "heading": "queries-keys-values",
      "text": "In fact, this contains the above example as a special case where all but one weight is zero. We have a number of special cases:\n\n* The weights $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ are nonnegative. In this case the output of the attention mechanism is contained in the convex cone spanned by the values $\\mathbf{v}_i$. * The weights $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ form a convex combination, i.e., $\\sum_i \\alpha(\\mathbf{q}, \\mathbf{k}_i) = 1$ and $\\alpha(\\mathbf{q}, \\mathbf{k}_i) \\geq 0$ for all $i$. This is the most common setting in deep learning. * Exactly one of the weights $\\alpha(\\mathbf{q}, \\mathbf{k}_i)$ is $1$, while all others are $0$. This is akin to a traditional database query. * All weights are equal, i.e., $\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{1}{m}$ for all $i$. This amounts to averaging across the entire database, also called average pooling in deep learning. A common strategy for ensuring that the weights sum up to $1$ is to normalize them via \n\n$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\alpha(\\mathbf{q}, \\mathbf{k}_i)}{{\\sum_j} \\alpha(\\mathbf{q}, \\mathbf{k}_j)}.$$\n\nIn particular, to ensure that the weights are also nonnegative, one can resort to exponentiation. This means that we can now pick *any* function  $a(\\mathbf{q}, \\mathbf{k})$ and then apply the softmax operation used for multinomial models to it via\n\n$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_j \\exp(a(\\mathbf{q}, \\mathbf{k}_j))}. $$\n:eqlabel:`eq_softmax_attention`\n\nThis operation is readily available in all deep learning frameworks. It is differentiable and its gradient never vanishes, all of which are desirable properties in a model. Note though, the attention mechanism introduced above is not the only option. For instance, we can design a non-differentiable attention model that can be trained using reinforcement learning methods :cite:`Mnih.Heess.Graves.ea.2014`. As one would expect, training such a model is quite complex."
    },
    {
      "chunk_id": "b0e4e31d357e_3",
      "chapter": "queries-keys-values",
      "heading": "queries-keys-values",
      "text": "For instance, we can design a non-differentiable attention model that can be trained using reinforcement learning methods :cite:`Mnih.Heess.Graves.ea.2014`. As one would expect, training such a model is quite complex. Consequently the bulk of modern attention research \nfollows the framework outlined in :numref:`fig_qkv`. We thus focus our exposition on this family of differentiable mechanisms. ![The attention mechanism computes a linear combination over values $\\mathbf{v}_\\mathit{i}$ via attention pooling,\nwhere weights are derived according to the compatibility between a query $\\mathbf{q}$ and keys $\\mathbf{k}_\\mathit{i}$.](../img/qkv.svg)\n:label:`fig_qkv`\n\nWhat is quite remarkable is that the actual \"code\" for executing on the set of keys and values, namely the query, can be quite concise, even though the space to operate on is significant. This is a desirable property for a network layer as it does not require too many parameters to learn. Just as convenient is the fact that attention can operate on arbitrarily large databases without the need to change the way the attention pooling operation is performed. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input  n=2}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "eaecdeb57541_0",
      "chapter": "queries-keys-values",
      "heading": "Visualization",
      "text": "One of the benefits of the attention mechanism is that it can be quite intuitive, particularly when the weights are nonnegative and sum to $1$. In this case we might *interpret* large weights as a way for the model to select components of relevance. While this is a good intuition, it is important to remember that it is just that, an *intuition*. Regardless, we may want to visualize its effect on the given set of keys when applying a variety of different queries. This function will come in handy later. We thus define the `show_heatmaps` function. Note that it does not take a matrix (of attention weights) as its input but rather a tensor with four axes, allowing for an array of different queries and weights. Consequently the input `matrices` has the shape (number of rows for display, number of columns for display, number of queries, number of keys). This will come in handy later on when we want to visualize the workings that are to design Transformers."
    },
    {
      "chunk_id": "eaecdeb57541_1",
      "chapter": "queries-keys-values",
      "heading": "Visualization",
      "text": "Consequently the input `matrices` has the shape (number of rows for display, number of columns for display, number of queries, number of keys). This will come in handy later on when we want to visualize the workings that are to design Transformers. ```{.python .input  n=17}\n%%tab all\n#@save\ndef show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\n                  cmap='Reds'):\n    \"\"\"Show heatmaps of matrices.\"\"\"\n    d2l.use_svg_display()\n    num_rows, num_cols, _, _ = matrices.shape\n    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,\n                                 sharex=True, sharey=True, squeeze=False)\n    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n            if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n                pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)\n            if tab.selected('jax'):\n                pcm = ax.imshow(matrix, cmap=cmap)\n            if i == num_rows - 1:\n                ax.set_xlabel(xlabel)\n            if j == 0:\n                ax.set_ylabel(ylabel)\n            if titles:\n                ax.set_title(titles[j])\n    fig.colorbar(pcm, ax=axes, shrink=0.6);\n```\n\nAs a quick sanity check let's visualize the identity matrix, representing a case \nwhere the attention weight is $1$ only when the query and the key are the same. ```{.python .input  n=20}\n%%tab all\nattention_weights = d2l.reshape(d2l.eye(10), (1, 1, 10, 10))\nshow_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')\n```"
    },
    {
      "chunk_id": "9e1ba9bb685e_0",
      "chapter": "queries-keys-values",
      "heading": "Summary",
      "text": "The attention mechanism allows us to aggregate data from many (key, value) pairs. So far our discussion was \nquite abstract, simply describing a way to pool data. We have not explained yet where those mysterious queries, keys, and values might arise from. Some intuition might help here: for instance, in a regression setting, the query might correspond to the location where the regression should be carried out. The keys are the locations where past data was observed and the values are the (regression) values themselves. This is the so-called Nadaraya--Watson estimator :cite:`Nadaraya.1964,Watson.1964` that we will be studying in the next section. \n\nBy design, the attention mechanism provides a *differentiable* means of control \nby which a neural network can select elements from a set and to construct an associated weighted sum over representations."
    },
    {
      "chunk_id": "90f0b325ac8d_0",
      "chapter": "queries-keys-values",
      "heading": "Exercises",
      "text": "1. Suppose that you wanted to reimplement approximate (key, query) matches as used in classical databases, which attention function would you pick? \n1. Suppose that the attention function is given by $a(\\mathbf{q}, \\mathbf{k}_i) = \\mathbf{q}^\\top \\mathbf{k}_i$ and that $\\mathbf{k}_i = \\mathbf{v}_i$ for $i = 1, \\ldots, m$. Denote by $p(\\mathbf{k}_i; \\mathbf{q})$ the probability distribution over keys when using the softmax normalization in :eqref:`eq_softmax_attention`. Prove that $\\nabla_{\\mathbf{q}} \\mathop{\\textrm{Attention}}(\\mathbf{q}, \\mathcal{D}) = \\textrm{Cov}_{p(\\mathbf{k}_i; \\mathbf{q})}[\\mathbf{k}_i]$.\n1. Design a differentiable search engine using the attention mechanism. \n1. Review the design of the Squeeze and Excitation Networks :cite:`Hu.Shen.Sun.2018` and interpret them through the lens of the attention mechanism. \n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/1596)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1592)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1710)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18024)\n:end_tab:"
    },
    {
      "chunk_id": "632dd012cbbc_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "self-attention-and-positional-encoding",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n# Self-Attention and Positional Encoding\n:label:`sec_self-attention-and-positional-encoding`\n\nIn deep learning, we often use CNNs or RNNs to encode sequences.\nNow with attention mechanisms in mind, \nimagine feeding a sequence of tokens \ninto an attention mechanism\nsuch that at every step,\neach token has its own query, keys, and values.\nHere, when computing the value of a token's representation at the next layer,\nthe token can attend (via its query vector) to any other's token \n(matching based on their key vectors).\nUsing the full set of query-key compatibility scores,\nwe can compute, for each token, a representation\nby building the appropriate weighted sum\nover the other tokens. \nBecause every token is attending to each other token\n(unlike the case where decoder steps attend to encoder steps),\nsuch architectures are typically described as *self-attention* models :cite:`Lin.Feng.Santos.ea.2017,Vaswani.Shazeer.Parmar.ea.2017`, \nand elsewhere described as *intra-attention* model :cite:`Cheng.Dong.Lapata.2016,Parikh.Tackstrom.Das.ea.2016,Paulus.Xiong.Socher.2017`.\nIn this section, we will discuss sequence encoding using self-attention,\nincluding using additional information for the sequence order.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import autograd, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport math\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport numpy as np\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\n```"
    },
    {
      "chunk_id": "53c8be670e87_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "[**Self-Attention**]",
      "text": "Given a sequence of input tokens\n$\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$ where any $\\mathbf{x}_i \\in \\mathbb{R}^d$ ($1 \\leq i \\leq n$),\nits self-attention outputs\na sequence of the same length\n$\\mathbf{y}_1, \\ldots, \\mathbf{y}_n$,\nwhere\n\n$$\\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d$$\n\naccording to the definition of attention pooling in\n:eqref:`eq_attention_pooling`. Using multi-head attention,\nthe following code snippet\ncomputes the self-attention of a tensor\nwith shape (batch size, number of time steps or sequence length in tokens, $d$). The output tensor has the same shape."
    },
    {
      "chunk_id": "53c8be670e87_1",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "[**Self-Attention**]",
      "text": "Using multi-head attention,\nthe following code snippet\ncomputes the self-attention of a tensor\nwith shape (batch size, number of time steps or sequence length in tokens, $d$). The output tensor has the same shape. ```{.python .input}\n%%tab pytorch\nnum_hiddens, num_heads = 100, 5\nattention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, valid_lens = 2, 4, d2l.tensor([3, 2])\nX = d2l.ones((batch_size, num_queries, num_hiddens))\nd2l.check_shape(attention(X, X, X, valid_lens),\n                (batch_size, num_queries, num_hiddens))\n```\n\n```{.python .input}\n%%tab mxnet\nnum_hiddens, num_heads = 100, 5\nattention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)\nattention.initialize()\n```\n\n```{.python .input}\n%%tab jax\nnum_hiddens, num_heads = 100, 5\nattention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)\n```\n\n```{.python .input}\n%%tab tensorflow\nnum_hiddens, num_heads = 100, 5\nattention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n                                   num_hiddens, num_heads, 0.5)\n```\n\n```{.python .input}\n%%tab mxnet\nbatch_size, num_queries, valid_lens = 2, 4, d2l.tensor([3, 2])\nX = d2l.ones((batch_size, num_queries, num_hiddens))\nd2l.check_shape(attention(X, X, X, valid_lens),\n                (batch_size, num_queries, num_hiddens))\n```\n\n```{.python .input}\n%%tab tensorflow\nbatch_size, num_queries, valid_lens = 2, 4, tf.constant([3, 2])\nX = tf.ones((batch_size, num_queries, num_hiddens))\nd2l.check_shape(attention(X, X, X, valid_lens, training=False),\n                (batch_size, num_queries, num_hiddens))\n```\n\n```{.python .input}\n%%tab jax\nbatch_size, num_queries, valid_lens = 2, 4, d2l.tensor([3, 2])\nX = d2l.ones((batch_size, num_queries, num_hiddens))\nd2l.check_shape(attention.init_with_output(d2l.get_key(), X, X, X, valid_lens,\n                                           training=False)[0][0],\n                (batch_size, num_queries, num_hiddens))\n```"
    },
    {
      "chunk_id": "e3088f338128_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "Comparing CNNs, RNNs, and Self-Attention",
      "text": ":label:`subsec_cnn-rnn-self-attention`\n\nLet's\ncompare architectures for mapping\na sequence of $n$ tokens\nto another one of equal length,\nwhere each input or output token is represented by\na $d$-dimensional vector. Specifically,\nwe will consider CNNs, RNNs, and self-attention. We will compare their\ncomputational complexity, \nsequential operations,\nand maximum path lengths. Note that sequential operations prevent parallel computation,\nwhile a shorter path between\nany combination of sequence positions\nmakes it easier to learn long-range dependencies \nwithin the sequence :cite:`Hochreiter.Bengio.Frasconi.ea.2001`. ![Comparing CNN (padding tokens are omitted), RNN, and self-attention architectures.](../img/cnn-rnn-self-attention.svg)\n:label:`fig_cnn-rnn-self-attention`\n\n\n\nLet's regard any text sequence as a \"one-dimensional image\". Similarly, one-dimensional CNNs can process local features such as $n$-grams in text. Given a sequence of length $n$,\nconsider a convolutional layer whose kernel size is $k$,\nand whose numbers of input and output channels are both $d$. The computational complexity of the convolutional layer is $\\mathcal{O}(knd^2)$. As :numref:`fig_cnn-rnn-self-attention` shows,\nCNNs are hierarchical,\nso there are $\\mathcal{O}(1)$ sequential operations\nand the maximum path length is $\\mathcal{O}(n/k)$. For example, $\\mathbf{x}_1$ and $\\mathbf{x}_5$\nare within the receptive field of a two-layer CNN\nwith kernel size 3 in :numref:`fig_cnn-rnn-self-attention`. When updating the hidden state of RNNs,\nmultiplication of the $d \\times d$ weight matrix\nand the $d$-dimensional hidden state has \na computational complexity of $\\mathcal{O}(d^2)$. Since the sequence length is $n$,\nthe computational complexity of the recurrent layer\nis $\\mathcal{O}(nd^2)$. According to :numref:`fig_cnn-rnn-self-attention`,\nthere are $\\mathcal{O}(n)$ sequential operations\nthat cannot be parallelized\nand the maximum path length is also $\\mathcal{O}(n)$. In self-attention,\nthe queries, keys, and values \nare all $n \\times d$ matrices."
    },
    {
      "chunk_id": "e3088f338128_1",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "Comparing CNNs, RNNs, and Self-Attention",
      "text": "In self-attention,\nthe queries, keys, and values \nare all $n \\times d$ matrices. Consider the scaled dot product attention in\n:eqref:`eq_softmax_QK_V`,\nwhere an $n \\times d$ matrix is multiplied by\na $d \\times n$ matrix,\nthen the output $n \\times n$ matrix is multiplied\nby an $n \\times d$ matrix. As a result,\nthe self-attention\nhas a $\\mathcal{O}(n^2d)$ computational complexity. As we can see from :numref:`fig_cnn-rnn-self-attention`,\neach token is directly connected\nto any other token via self-attention. Therefore,\ncomputation can be parallel with $\\mathcal{O}(1)$ sequential operations\nand the maximum path length is also $\\mathcal{O}(1)$. All in all,\nboth CNNs and self-attention enjoy parallel computation\nand self-attention has the shortest maximum path length. However, the quadratic computational complexity with respect to the sequence length\nmakes self-attention prohibitively slow for very long sequences."
    },
    {
      "chunk_id": "518ccef6f71b_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "[**Positional Encoding**]",
      "text": ":label:`subsec_positional-encoding`\n\n\nUnlike RNNs, which recurrently process\ntokens of a sequence one-by-one,\nself-attention ditches\nsequential operations in favor of \nparallel computation. Note that self-attention by itself\ndoes not preserve the order of the sequence. What do we do if it really matters \nthat the model knows in which order\nthe input sequence arrived? The dominant approach for preserving \ninformation about the order of tokens\nis to represent this to the model \nas an additional input associated \nwith each token. These inputs are called *positional encodings*,\nand they can either be learned or fixed *a priori*. We now describe a simple scheme for fixed positional encodings\nbased on sine and cosine functions :cite:`Vaswani.Shazeer.Parmar.ea.2017`. Suppose that the input representation \n$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ \ncontains the $d$-dimensional embeddings \nfor $n$ tokens of a sequence. The positional encoding outputs\n$\\mathbf{X} + \\mathbf{P}$\nusing a positional embedding matrix \n$\\mathbf{P} \\in \\mathbb{R}^{n \\times d}$ of the same shape,\nwhose element on the $i^\\textrm{th}$ row \nand the $(2j)^\\textrm{th}$\nor the $(2j + 1)^\\textrm{th}$ column is\n\n$$\\begin{aligned} p_{i, 2j} &= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\\\p_{i, 2j+1} &= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned}$$\n:eqlabel:`eq_positional-encoding-def`\n\nAt first glance,\nthis trigonometric function\ndesign looks weird. Before we give explanations of this design,\nlet's first implement it in the following `PositionalEncoding` class."
    },
    {
      "chunk_id": "518ccef6f71b_1",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "[**Positional Encoding**]",
      "text": "Before we give explanations of this design,\nlet's first implement it in the following `PositionalEncoding` class. ```{.python .input}\n%%tab mxnet\nclass PositionalEncoding(nn.Block):  #@save\n    \"\"\"Positional encoding.\"\"\"\n    def __init__(self, num_hiddens, dropout, max_len=1000):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        # Create a long enough P\n        self.P = d2l.zeros((1, max_len, num_hiddens))\n        X = d2l.arange(max_len).reshape(-1, 1) / np.power(\n            10000, np.arange(0, num_hiddens, 2) / num_hiddens)\n        self.P[:, :, 0::2] = np.sin(X)\n        self.P[:, :, 1::2] = np.cos(X)\n\n    def forward(self, X):\n        X = X + self.P[:, :X.shape[1], :].as_in_ctx(X.ctx)\n        return self.dropout(X)\n```\n\n```{.python .input}\n%%tab pytorch\nclass PositionalEncoding(nn.Module):  #@save\n    \"\"\"Positional encoding.\"\"\"\n    def __init__(self, num_hiddens, dropout, max_len=1000):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        # Create a long enough P\n        self.P = d2l.zeros((1, max_len, num_hiddens))\n        X = d2l.arange(max_len, dtype=torch.float32).reshape(\n            -1, 1) / torch.pow(10000, torch.arange(\n            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n        self.P[:, :, 0::2] = torch.sin(X)\n        self.P[:, :, 1::2] = torch.cos(X)\n\n    def forward(self, X):\n        X = X + self.P[:, :X.shape[1], :].to(X.device)\n        return self.dropout(X)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass PositionalEncoding(tf.keras.layers.Layer):  #@save\n    \"\"\"Positional encoding.\"\"\"\n    def __init__(self, num_hiddens, dropout, max_len=1000):\n        super().__init__()\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        # Create a long enough P\n        self.P = np.zeros((1, max_len, num_hiddens))\n        X = np.arange(max_len, dtype=np.float32).reshape(\n            -1,1)/np.power(10000, np.arange(\n            0, num_hiddens, 2, dtype=np.float32) / num_hiddens)\n        self.P[:, :, 0::2] = np.sin(X)\n        self.P[:, :, 1::2] = np.cos(X)\n        \n    def call(self, X, **kwargs):\n        X = X + self.P[:, :X.shape[1], :]\n        return self.dropout(X, **kwargs)\n```\n\n```{.python .input}\n%%tab jax\nclass PositionalEncoding(nn.Module):  #@save\n    \"\"\"Positional encoding.\"\"\"\n    num_hiddens: int\n    dropout: float\n    max_len: int = 1000\n\n    def setup(self):\n        # Create a long enough P\n        self.P = d2l.zeros((1, self.max_len, self.num_hiddens))\n        X = d2l.arange(self.max_len, dtype=jnp.float32).reshape(\n            -1, 1) / jnp.power(10000, jnp.arange(\n            0, self.num_hiddens, 2, dtype=jnp.float32) / self.num_hiddens)\n        self.P = self.P.at[:, :, 0::2].set(jnp.sin(X))\n        self.P = self.P.at[:, :, 1::2].set(jnp.cos(X))\n\n    @nn.compact\n    def __call__(self, X, training=False):\n        # Flax sow API is used to capture intermediate variables\n        self.sow('intermediates', 'P', self.P)\n        X = X + self.P[:, :X.shape[1], :]\n        return nn.Dropout(self.dropout)(X, deterministic=not training)\n```\n\nIn the positional embedding matrix $\\mathbf{P}$,\n[**rows correspond to positions within a sequence\nand columns represent different positional encoding dimensions**]."
    },
    {
      "chunk_id": "518ccef6f71b_2",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "[**Positional Encoding**]",
      "text": "In the example below,\nwe can see that\nthe $6^{\\textrm{th}}$ and the $7^{\\textrm{th}}$\ncolumns of the positional embedding matrix \nhave a higher frequency than \nthe $8^{\\textrm{th}}$ and the $9^{\\textrm{th}}$\ncolumns. The offset between \nthe $6^{\\textrm{th}}$ and the $7^{\\textrm{th}}$ (same for the $8^{\\textrm{th}}$ and the $9^{\\textrm{th}}$) columns\nis due to the alternation of sine and cosine functions."
    },
    {
      "chunk_id": "518ccef6f71b_3",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "[**Positional Encoding**]",
      "text": "The offset between \nthe $6^{\\textrm{th}}$ and the $7^{\\textrm{th}}$ (same for the $8^{\\textrm{th}}$ and the $9^{\\textrm{th}}$) columns\nis due to the alternation of sine and cosine functions. ```{.python .input}\n%%tab mxnet\nencoding_dim, num_steps = 32, 60\npos_encoding = PositionalEncoding(encoding_dim, 0)\npos_encoding.initialize()\nX = pos_encoding(np.zeros((1, num_steps, encoding_dim)))\nP = pos_encoding.P[:, :X.shape[1], :]\nd2l.plot(d2l.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\n         figsize=(6, 2.5), legend=[\"Col %d\" % d for d in d2l.arange(6, 10)])\n```\n\n```{.python .input}\n%%tab pytorch\nencoding_dim, num_steps = 32, 60\npos_encoding = PositionalEncoding(encoding_dim, 0)\nX = pos_encoding(d2l.zeros((1, num_steps, encoding_dim)))\nP = pos_encoding.P[:, :X.shape[1], :]\nd2l.plot(d2l.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\n         figsize=(6, 2.5), legend=[\"Col %d\" % d for d in d2l.arange(6, 10)])\n```\n\n```{.python .input}\n%%tab tensorflow\nencoding_dim, num_steps = 32, 60\npos_encoding = PositionalEncoding(encoding_dim, 0)\nX = pos_encoding(tf.zeros((1, num_steps, encoding_dim)), training=False)\nP = pos_encoding.P[:, :X.shape[1], :]\nd2l.plot(np.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\n         figsize=(6, 2.5), legend=[\"Col %d\" % d for d in np.arange(6, 10)])\n```\n\n```{.python .input}\n%%tab jax\nencoding_dim, num_steps = 32, 60\npos_encoding = PositionalEncoding(encoding_dim, 0)\nparams = pos_encoding.init(d2l.get_key(), d2l.zeros((1, num_steps, encoding_dim)))\nX, inter_vars = pos_encoding.apply(params, d2l.zeros((1, num_steps, encoding_dim)),\n                                   mutable='intermediates')\nP = inter_vars['intermediates']['P'][0]  # retrieve intermediate value P\nP = P[:, :X.shape[1], :]\nd2l.plot(d2l.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\n         figsize=(6, 2.5), legend=[\"Col %d\" % d for d in d2l.arange(6, 10)])\n```"
    },
    {
      "chunk_id": "0debc9c1b550_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "Absolute Positional Information",
      "text": "To see how the monotonically decreased frequency\nalong the encoding dimension relates to absolute positional information,\nlet's print out [**the binary representations**] of $0, 1, \\ldots, 7$.\nAs we can see, the lowest bit, the second-lowest bit, \nand the third-lowest bit alternate on every number, \nevery two numbers, and every four numbers, respectively.\n\n```{.python .input}\n%%tab all\nfor i in range(8):\n    print(f'{i} in binary is {i:>03b}')\n```\n\nIn binary representations, a higher bit \nhas a lower frequency than a lower bit.\nSimilarly, as demonstrated in the heat map below,\n[**the positional encoding decreases\nfrequencies along the encoding dimension**]\nby using trigonometric functions.\nSince the outputs are float numbers,\nsuch continuous representations\nare more space-efficient\nthan binary representations.\n\n```{.python .input}\n%%tab mxnet\nP = np.expand_dims(np.expand_dims(P[0, :, :], 0), 0)\nd2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\n                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')\n```\n\n```{.python .input}\n%%tab pytorch\nP = P[0, :, :].unsqueeze(0).unsqueeze(0)\nd2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\n                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')\n```\n\n```{.python .input}\n%%tab tensorflow\nP = tf.expand_dims(tf.expand_dims(P[0, :, :], axis=0), axis=0)\nd2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\n                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')\n```\n\n```{.python .input}\n%%tab jax\nP = jnp.expand_dims(jnp.expand_dims(P[0, :, :], axis=0), axis=0)\nd2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\n                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')\n```"
    },
    {
      "chunk_id": "40618ac0ec1f_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "Relative Positional Information",
      "text": "Besides capturing absolute positional information,\nthe above positional encoding\nalso allows\na model to easily learn to attend by relative positions.\nThis is because\nfor any fixed position offset $\\delta$,\nthe positional encoding at position $i + \\delta$\ncan be represented by a linear projection\nof that at position $i$.\n\n\nThis projection can be explained\nmathematically.\nDenoting\n$\\omega_j = 1/10000^{2j/d}$,\nany pair of $(p_{i, 2j}, p_{i, 2j+1})$ \nin :eqref:`eq_positional-encoding-def`\ncan \nbe linearly projected to $(p_{i+\\delta, 2j}, p_{i+\\delta, 2j+1})$\nfor any fixed offset $\\delta$:\n\n$$\\begin{aligned}\n\\begin{bmatrix} \\cos(\\delta \\omega_j) & \\sin(\\delta \\omega_j) \\\\  -\\sin(\\delta \\omega_j) & \\cos(\\delta \\omega_j) \\\\ \\end{bmatrix}\n\\begin{bmatrix} p_{i, 2j} \\\\  p_{i, 2j+1} \\\\ \\end{bmatrix}\n=&\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\\\  -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\\\ \\end{bmatrix}\\\\\n=&\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\\\  \\cos\\left((i+\\delta) \\omega_j\\right) \\\\ \\end{bmatrix}\\\\\n=& \n\\begin{bmatrix} p_{i+\\delta, 2j} \\\\  p_{i+\\delta, 2j+1} \\\\ \\end{bmatrix},\n\\end{aligned}$$\n\nwhere the $2\\times 2$ projection matrix does not depend on any position index $i$."
    },
    {
      "chunk_id": "bf8ff9d6456f_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "Summary",
      "text": "In self-attention, the queries, keys, and values all come from the same place.\nBoth CNNs and self-attention enjoy parallel computation\nand self-attention has the shortest maximum path length.\nHowever, the quadratic computational complexity\nwith respect to the sequence length\nmakes self-attention prohibitively slow\nfor very long sequences.\nTo use the sequence order information, \nwe can inject absolute or relative positional information \nby adding positional encoding to the input representations."
    },
    {
      "chunk_id": "06c56fb709b2_0",
      "chapter": "self-attention-and-positional-encoding",
      "heading": "Exercises",
      "text": "1. Suppose that we design a deep architecture to represent a sequence by stacking self-attention layers with positional encoding. What could the possible issues be?\n1. Can you design a learnable positional encoding method?\n1. Can we assign different learned embeddings according to different offsets between queries and keys that are compared in self-attention? Hint: you may refer to relative position embeddings :cite:`shaw2018self,huang2018music`.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/1651)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1652)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3870)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18030)\n:end_tab:"
    },
    {
      "chunk_id": "ccdbd66cc89f_0",
      "chapter": "transformer",
      "heading": "transformer",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n# The Transformer Architecture\n:label:`sec_transformer`\n\n\nWe have compared CNNs, RNNs, and self-attention in\n:numref:`subsec_cnn-rnn-self-attention`.\nNotably, self-attention\nenjoys both parallel computation and\nthe shortest maximum path length.\nTherefore,\nit is appealing to design deep architectures\nby using self-attention.\nUnlike earlier self-attention models\nthat still rely on RNNs for input representations :cite:`Cheng.Dong.Lapata.2016,Lin.Feng.Santos.ea.2017,Paulus.Xiong.Socher.2017`,\nthe Transformer model\nis solely based on attention mechanisms\nwithout any convolutional or recurrent layer :cite:`Vaswani.Shazeer.Parmar.ea.2017`.\nThough originally proposed\nfor sequence-to-sequence learning on text data,\nTransformers have been\npervasive in a wide range of\nmodern deep learning applications,\nsuch as in areas to do with language, vision, speech, and reinforcement learning.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import autograd, init, np, npx\nfrom mxnet.gluon import nn\nimport pandas as pd\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport math\nimport pandas as pd\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\nimport math\nimport pandas as pd\n```"
    },
    {
      "chunk_id": "6e4a9170dbf2_0",
      "chapter": "transformer",
      "heading": "Model",
      "text": "As an instance of the encoder--decoder\narchitecture,\nthe overall architecture of\nthe Transformer\nis presented in :numref:`fig_transformer`. As we can see,\nthe Transformer is composed of an encoder and a decoder. In contrast to\nBahdanau attention\nfor sequence-to-sequence learning\nin :numref:`fig_s2s_attention_details`,\nthe input (source) and output (target)\nsequence embeddings\nare added with positional encoding\nbefore being fed into\nthe encoder and the decoder\nthat stack modules based on self-attention. ![The Transformer architecture.](../img/transformer.svg)\n:width:`320px`\n:label:`fig_transformer`\n\n\nNow we provide an overview of the\nTransformer architecture in :numref:`fig_transformer`. At a high level,\nthe Transformer encoder is a stack of multiple identical layers,\nwhere each layer\nhas two sublayers (either is denoted as $\\textrm{sublayer}$). The first\nis a multi-head self-attention pooling\nand the second is a positionwise feed-forward network. Specifically,\nin the encoder self-attention,\nqueries, keys, and values are all from the\noutputs of the previous encoder layer. Inspired by the ResNet design of :numref:`sec_resnet`,\na residual connection is employed\naround both sublayers. In the Transformer,\nfor any input $\\mathbf{x} \\in \\mathbb{R}^d$ at any position of the sequence,\nwe require that $\\textrm{sublayer}(\\mathbf{x}) \\in \\mathbb{R}^d$ so that\nthe residual connection $\\mathbf{x} + \\textrm{sublayer}(\\mathbf{x}) \\in \\mathbb{R}^d$ is feasible. This addition from the residual connection is immediately\nfollowed by layer normalization :cite:`Ba.Kiros.Hinton.2016`. As a result, the Transformer encoder outputs a $d$-dimensional vector representation\nfor each position of the input sequence. The Transformer decoder is also a stack of multiple identical layers\nwith residual connections and layer normalizations. As well as the two sublayers described in\nthe encoder, the decoder inserts\na third sublayer, known as\nthe encoder--decoder attention,\nbetween these two."
    },
    {
      "chunk_id": "6e4a9170dbf2_1",
      "chapter": "transformer",
      "heading": "Model",
      "text": "As well as the two sublayers described in\nthe encoder, the decoder inserts\na third sublayer, known as\nthe encoder--decoder attention,\nbetween these two. In the encoder--decoder attention,\nqueries are from the\noutputs of the decoder's self-attention sublayer,\nand the keys and values are\nfrom the Transformer encoder outputs. In the decoder self-attention,\nqueries, keys, and values are all from the\noutputs of the previous decoder layer. However, each position in the decoder is\nallowed only to attend to all positions in the decoder\nup to that position. This *masked* attention\npreserves the autoregressive property,\nensuring that the prediction only depends\non those output tokens that have been generated. We have already described and implemented\nmulti-head attention based on scaled dot products\nin :numref:`sec_multihead-attention`\nand positional encoding in :numref:`subsec_positional-encoding`. In the following, we will implement\nthe rest of the Transformer model."
    },
    {
      "chunk_id": "956ae85daff3_0",
      "chapter": "transformer",
      "heading": "[**Positionwise Feed-Forward Networks**]",
      "text": ":label:`subsec_positionwise-ffn`\n\nThe positionwise feed-forward network transforms\nthe representation at all the sequence positions\nusing the same MLP. This is why we call it *positionwise*. In the implementation below,\nthe input `X` with shape\n(batch size, number of time steps or sequence length in tokens,\nnumber of hidden units or feature dimension)\nwill be transformed by a two-layer MLP into\nan output tensor of shape\n(batch size, number of time steps, `ffn_num_outputs`)."
    },
    {
      "chunk_id": "956ae85daff3_1",
      "chapter": "transformer",
      "heading": "[**Positionwise Feed-Forward Networks**]",
      "text": "```{.python .input}\n%%tab mxnet\nclass PositionWiseFFN(nn.Block):  #@save\n    \"\"\"The positionwise feed-forward network.\"\"\"\n    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n        super().__init__()\n        self.dense1 = nn.Dense(ffn_num_hiddens, flatten=False,\n                               activation='relu')\n        self.dense2 = nn.Dense(ffn_num_outputs, flatten=False)\n\n    def forward(self, X):\n        return self.dense2(self.dense1(X))\n```\n\n```{.python .input}\n%%tab pytorch\nclass PositionWiseFFN(nn.Module):  #@save\n    \"\"\"The positionwise feed-forward network.\"\"\"\n    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n        super().__init__()\n        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n\n    def forward(self, X):\n        return self.dense2(self.relu(self.dense1(X)))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass PositionWiseFFN(tf.keras.layers.Layer):  #@save\n    \"\"\"The positionwise feed-forward network.\"\"\"\n    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n        super().__init__()\n        self.dense1 = tf.keras.layers.Dense(ffn_num_hiddens)\n        self.relu = tf.keras.layers.ReLU()\n        self.dense2 = tf.keras.layers.Dense(ffn_num_outputs)\n\n    def call(self, X):\n        return self.dense2(self.relu(self.dense1(X)))\n```\n\n```{.python .input}\n%%tab jax\nclass PositionWiseFFN(nn.Module):  #@save\n    \"\"\"The positionwise feed-forward network.\"\"\"\n    ffn_num_hiddens: int\n    ffn_num_outputs: int\n\n    def setup(self):\n        self.dense1 = nn.Dense(self.ffn_num_hiddens)\n        self.dense2 = nn.Dense(self.ffn_num_outputs)\n\n    def __call__(self, X):\n        return self.dense2(nn.relu(self.dense1(X)))\n```\n\nThe following example\nshows that [**the innermost dimension\nof a tensor changes**] to\nthe number of outputs in\nthe positionwise feed-forward network. Since the same MLP transforms\nat all the positions,\nwhen the inputs at all these positions are the same,\ntheir outputs are also identical."
    },
    {
      "chunk_id": "956ae85daff3_2",
      "chapter": "transformer",
      "heading": "[**Positionwise Feed-Forward Networks**]",
      "text": "Since the same MLP transforms\nat all the positions,\nwhen the inputs at all these positions are the same,\ntheir outputs are also identical. ```{.python .input}\n%%tab mxnet\nffn = PositionWiseFFN(4, 8)\nffn.initialize()\nffn(np.ones((2, 3, 4)))[0]\n```\n\n```{.python .input}\n%%tab pytorch\nffn = PositionWiseFFN(4, 8)\nffn.eval()\nffn(d2l.ones((2, 3, 4)))[0]\n```\n\n```{.python .input}\n%%tab tensorflow\nffn = PositionWiseFFN(4, 8)\nffn(tf.ones((2, 3, 4)))[0]\n```\n\n```{.python .input}\n%%tab jax\nffn = PositionWiseFFN(4, 8)\nffn.init_with_output(d2l.get_key(), jnp.ones((2, 3, 4)))[0][0]\n```"
    },
    {
      "chunk_id": "2bc9fdbd9482_0",
      "chapter": "transformer",
      "heading": "Residual Connection and Layer Normalization",
      "text": "Now let's focus on the \"add & norm\" component in :numref:`fig_transformer`. As we described at the beginning of this section,\nthis is a residual connection immediately\nfollowed by layer normalization. Both are key to effective deep architectures. In :numref:`sec_batch_norm`,\nwe explained how batch normalization\nrecenters and rescales across the examples within\na minibatch. As discussed in :numref:`subsec_layer-normalization-in-bn`,\nlayer normalization is the same as batch normalization\nexcept that the former\nnormalizes across the feature dimension,\nthus enjoying benefits of scale independence and batch size independence. Despite its pervasive applications\nin computer vision,\nbatch normalization\nis usually empirically\nless effective than layer normalization\nin natural language processing\ntasks, where the inputs are often\nvariable-length sequences. The following code snippet\n[**compares the normalization across different dimensions\nby layer normalization and batch normalization**]."
    },
    {
      "chunk_id": "2bc9fdbd9482_1",
      "chapter": "transformer",
      "heading": "Residual Connection and Layer Normalization",
      "text": "The following code snippet\n[**compares the normalization across different dimensions\nby layer normalization and batch normalization**]. ```{.python .input}\n%%tab mxnet\nln = nn.LayerNorm()\nln.initialize()\nbn = nn.BatchNorm()\nbn.initialize()\nX = d2l.tensor([[1, 2], [2, 3]])\n# Compute mean and variance from X in the training mode\nwith autograd.record():\n    print('layer norm:', ln(X), '\\nbatch norm:', bn(X))\n```\n\n```{.python .input}\n%%tab pytorch\nln = nn.LayerNorm(2)\nbn = nn.LazyBatchNorm1d()\nX = d2l.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n# Compute mean and variance from X in the training mode\nprint('layer norm:', ln(X), '\\nbatch norm:', bn(X))\n```\n\n```{.python .input}\n%%tab tensorflow\nln = tf.keras.layers.LayerNormalization()\nbn = tf.keras.layers.BatchNormalization()\nX = tf.constant([[1, 2], [2, 3]], dtype=tf.float32)\nprint('layer norm:', ln(X), '\\nbatch norm:', bn(X, training=True))\n```\n\n```{.python .input}\n%%tab jax\nln = nn.LayerNorm()\nbn = nn.BatchNorm()\nX = d2l.tensor([[1, 2], [2, 3]], dtype=d2l.float32)\n# Compute mean and variance from X in the training mode\nprint('layer norm:', ln.init_with_output(d2l.get_key(), X)[0],\n      '\\nbatch norm:', bn.init_with_output(d2l.get_key(), X,\n                                           use_running_average=False)[0])\n```\n\nNow we can implement the `AddNorm` class\n[**using a residual connection followed by layer normalization**]. Dropout is also applied for regularization."
    },
    {
      "chunk_id": "2bc9fdbd9482_2",
      "chapter": "transformer",
      "heading": "Residual Connection and Layer Normalization",
      "text": "Dropout is also applied for regularization. ```{.python .input}\n%%tab mxnet\nclass AddNorm(nn.Block):  #@save\n    \"\"\"The residual connection followed by layer normalization.\"\"\"\n    def __init__(self, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm()\n\n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n```\n\n```{.python .input}\n%%tab pytorch\nclass AddNorm(nn.Module):  #@save\n    \"\"\"The residual connection followed by layer normalization.\"\"\"\n    def __init__(self, norm_shape, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(norm_shape)\n\n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass AddNorm(tf.keras.layers.Layer):  #@save\n    \"\"\"The residual connection followed by layer normalization.\"\"\"\n    def __init__(self, norm_shape, dropout):\n        super().__init__()\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.ln = tf.keras.layers.LayerNormalization(norm_shape)\n\n    def call(self, X, Y, **kwargs):\n        return self.ln(self.dropout(Y, **kwargs) + X)\n```\n\n```{.python .input}\n%%tab jax\nclass AddNorm(nn.Module):  #@save\n    \"\"\"The residual connection followed by layer normalization.\"\"\"\n    dropout: int\n\n    @nn.compact\n    def __call__(self, X, Y, training=False):\n        return nn.LayerNorm()(\n            nn.Dropout(self.dropout)(Y, deterministic=not training) + X)\n```\n\nThe residual connection requires that\nthe two inputs are of the same shape\nso that [**the output tensor also has the same shape after the addition operation**]."
    },
    {
      "chunk_id": "2bc9fdbd9482_3",
      "chapter": "transformer",
      "heading": "Residual Connection and Layer Normalization",
      "text": "```{.python .input}\n%%tab mxnet\nadd_norm = AddNorm(0.5)\nadd_norm.initialize()\nshape = (2, 3, 4)\nd2l.check_shape(add_norm(d2l.ones(shape), d2l.ones(shape)), shape)\n```\n\n```{.python .input}\n%%tab pytorch\nadd_norm = AddNorm(4, 0.5)\nshape = (2, 3, 4)\nd2l.check_shape(add_norm(d2l.ones(shape), d2l.ones(shape)), shape)\n```\n\n```{.python .input}\n%%tab tensorflow\n# Normalized_shape is: [i for i in range(len(input.shape))][1:]\nadd_norm = AddNorm([1, 2], 0.5)\nshape = (2, 3, 4)\nd2l.check_shape(add_norm(tf.ones(shape), tf.ones(shape), training=False),\n                shape)\n```\n\n```{.python .input}\n%%tab jax\nadd_norm = AddNorm(0.5)\nshape = (2, 3, 4)\noutput, _ = add_norm.init_with_output(d2l.get_key(), d2l.ones(shape),\n                                      d2l.ones(shape))\nd2l.check_shape(output, shape)\n```"
    },
    {
      "chunk_id": "aa17246ce936_0",
      "chapter": "transformer",
      "heading": "Encoder",
      "text": ":label:`subsec_transformer-encoder`\n\nWith all the essential components to assemble\nthe Transformer encoder,\nlet's start by\nimplementing [**a single layer within the encoder**]. The following `TransformerEncoderBlock` class\ncontains two sublayers: multi-head self-attention and positionwise feed-forward networks,\nwhere a residual connection followed by layer normalization is employed\naround both sublayers."
    },
    {
      "chunk_id": "aa17246ce936_1",
      "chapter": "transformer",
      "heading": "Encoder",
      "text": "The following `TransformerEncoderBlock` class\ncontains two sublayers: multi-head self-attention and positionwise feed-forward networks,\nwhere a residual connection followed by layer normalization is employed\naround both sublayers. ```{.python .input}\n%%tab mxnet\nclass TransformerEncoderBlock(nn.Block):  #@save\n    \"\"\"The Transformer encoder block.\"\"\"\n    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\n                 use_bias=False):\n        super().__init__()\n        self.attention = d2l.MultiHeadAttention(\n            num_hiddens, num_heads, dropout, use_bias)\n        self.addnorm1 = AddNorm(dropout)\n        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(dropout)\n\n    def forward(self, X, valid_lens):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n        return self.addnorm2(Y, self.ffn(Y))\n```\n\n```{.python .input}\n%%tab pytorch\nclass TransformerEncoderBlock(nn.Module):  #@save\n    \"\"\"The Transformer encoder block.\"\"\"\n    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\n                 use_bias=False):\n        super().__init__()\n        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\n                                                dropout, use_bias)\n        self.addnorm1 = AddNorm(num_hiddens, dropout)\n        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(num_hiddens, dropout)\n\n    def forward(self, X, valid_lens):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n        return self.addnorm2(Y, self.ffn(Y))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass TransformerEncoderBlock(tf.keras.layers.Layer):  #@save\n    \"\"\"The Transformer encoder block.\"\"\"\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_hiddens, num_heads, dropout, bias=False):\n        super().__init__()\n        self.attention = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n            bias)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n\n    def call(self, X, valid_lens, **kwargs):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens, **kwargs),\n                          **kwargs)\n        return self.addnorm2(Y, self.ffn(Y), **kwargs)\n```\n\n```{.python .input}\n%%tab jax\nclass TransformerEncoderBlock(nn.Module):  #@save\n    \"\"\"The Transformer encoder block.\"\"\"\n    num_hiddens: int\n    ffn_num_hiddens: int\n    num_heads: int\n    dropout: float\n    use_bias: bool = False\n\n    def setup(self):\n        self.attention = d2l.MultiHeadAttention(self.num_hiddens, self.num_heads,\n                                                self.dropout, self.use_bias)\n        self.addnorm1 = AddNorm(self.dropout)\n        self.ffn = PositionWiseFFN(self.ffn_num_hiddens, self.num_hiddens)\n        self.addnorm2 = AddNorm(self.dropout)\n\n    def __call__(self, X, valid_lens, training=False):\n        output, attention_weights = self.attention(X, X, X, valid_lens,\n                                                   training=training)\n        Y = self.addnorm1(X, output, training=training)\n        return self.addnorm2(Y, self.ffn(Y), training=training), attention_weights\n```\n\nAs we can see,\n[**no layer in the Transformer encoder\nchanges the shape of its input.**]\n\n```{.python .input}\n%%tab mxnet\nX = d2l.ones((2, 100, 24))\nvalid_lens = d2l.tensor([3, 2])\nencoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)\nencoder_blk.initialize()\nd2l.check_shape(encoder_blk(X, valid_lens), X.shape)\n```\n\n```{.python .input}\n%%tab pytorch\nX = d2l.ones((2, 100, 24))\nvalid_lens = d2l.tensor([3, 2])\nencoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)\nencoder_blk.eval()\nd2l.check_shape(encoder_blk(X, valid_lens), X.shape)\n```\n\n```{.python .input}\n%%tab tensorflow\nX = tf.ones((2, 100, 24))\nvalid_lens = tf.constant([3, 2])\nnorm_shape = [i for i in range(len(X.shape))][1:]\nencoder_blk = TransformerEncoderBlock(24, 24, 24, 24, norm_shape, 48, 8, 0.5)\nd2l.check_shape(encoder_blk(X, valid_lens, training=False), X.shape)\n```\n\n```{.python .input}\n%%tab jax\nX = jnp.ones((2, 100, 24))\nvalid_lens = jnp.array([3, 2])\nencoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)\n(output, _), _ = encoder_blk.init_with_output(d2l.get_key(), X, valid_lens,\n                                              training=False)\nd2l.check_shape(output, X.shape)\n```\n\nIn the following [**Transformer encoder**] implementation,\nwe stack `num_blks` instances of the above `TransformerEncoderBlock` classes."
    },
    {
      "chunk_id": "aa17246ce936_2",
      "chapter": "transformer",
      "heading": "Encoder",
      "text": "Since we use the fixed positional encoding\nwhose values are always between $-1$ and $1$,\nwe multiply values of the learnable input embeddings\nby the square root of the embedding dimension\nto rescale before summing up the input embedding and the positional encoding."
    },
    {
      "chunk_id": "aa17246ce936_3",
      "chapter": "transformer",
      "heading": "Encoder",
      "text": "```{.python .input}\n%%tab mxnet\nclass TransformerEncoder(d2l.Encoder):  #@save\n    \"\"\"The Transformer encoder.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n                 num_heads, num_blks, dropout, use_bias=False):\n        super().__init__()\n        self.num_hiddens = num_hiddens\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for _ in range(num_blks):\n            self.blks.add(TransformerEncoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\n        self.initialize()\n\n    def forward(self, X, valid_lens):\n        # Since positional encoding values are between -1 and 1, the embedding\n        # values are multiplied by the square root of the embedding dimension\n        # to rescale before they are summed up\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self.attention_weights = [None] * len(self.blks)\n        for i, blk in enumerate(self.blks):\n            X = blk(X, valid_lens)\n            self.attention_weights[\n                i] = blk.attention.attention.attention_weights\n        return X\n```\n\n```{.python .input}\n%%tab pytorch\nclass TransformerEncoder(d2l.Encoder):  #@save\n    \"\"\"The Transformer encoder.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n                 num_heads, num_blks, dropout, use_bias=False):\n        super().__init__()\n        self.num_hiddens = num_hiddens\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_blks):\n            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\n\n    def forward(self, X, valid_lens):\n        # Since positional encoding values are between -1 and 1, the embedding\n        # values are multiplied by the square root of the embedding dimension\n        # to rescale before they are summed up\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self.attention_weights = [None] * len(self.blks)\n        for i, blk in enumerate(self.blks):\n            X = blk(X, valid_lens)\n            self.attention_weights[\n                i] = blk.attention.attention.attention_weights\n        return X\n```\n\n```{.python .input}\n%%tab tensorflow\nclass TransformerEncoder(d2l.Encoder):  #@save\n    \"\"\"The Transformer encoder.\"\"\"\n    def __init__(self, vocab_size, key_size, query_size, value_size,\n                 num_hiddens, norm_shape, ffn_num_hiddens, num_heads,\n                 num_blks, dropout, bias=False):\n        super().__init__()\n        self.num_hiddens = num_hiddens\n        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = [TransformerEncoderBlock(\n            key_size, query_size, value_size, num_hiddens, norm_shape,\n            ffn_num_hiddens, num_heads, dropout, bias) for _ in range(\n            num_blks)]\n\n    def call(self, X, valid_lens, **kwargs):\n        # Since positional encoding values are between -1 and 1, the embedding\n        # values are multiplied by the square root of the embedding dimension\n        # to rescale before they are summed up\n        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(\n            tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)\n        self.attention_weights = [None] * len(self.blks)\n        for i, blk in enumerate(self.blks):\n            X = blk(X, valid_lens, **kwargs)\n            self.attention_weights[\n                i] = blk.attention.attention.attention_weights\n        return X\n```\n\n```{.python .input}\n%%tab jax\nclass TransformerEncoder(d2l.Encoder):  #@save\n    \"\"\"The Transformer encoder.\"\"\"\n    vocab_size: int\n    num_hiddens:int\n    ffn_num_hiddens: int\n    num_heads: int\n    num_blks: int\n    dropout: float\n    use_bias: bool = False\n\n    def setup(self):\n        self.embedding = nn.Embed(self.vocab_size, self.num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(self.num_hiddens, self.dropout)\n        self.blks = [TransformerEncoderBlock(self.num_hiddens,\n                                             self.ffn_num_hiddens,\n                                             self.num_heads,\n                                             self.dropout, self.use_bias)\n                     for _ in range(self.num_blks)]\n\n    def __call__(self, X, valid_lens, training=False):\n        # Since positional encoding values are between -1 and 1, the embedding\n        # values are multiplied by the square root of the embedding dimension\n        # to rescale before they are summed up\n        X = self.embedding(X) * math.sqrt(self.num_hiddens)\n        X = self.pos_encoding(X, training=training)\n        attention_weights = [None] * len(self.blks)\n        for i, blk in enumerate(self.blks):\n            X, attention_w = blk(X, valid_lens, training=training)\n            attention_weights[i] = attention_w\n        # Flax sow API is used to capture intermediate variables\n        self.sow('intermediates', 'enc_attention_weights', attention_weights)\n        return X\n```\n\nBelow we specify hyperparameters to [**create a two-layer Transformer encoder**]."
    },
    {
      "chunk_id": "aa17246ce936_4",
      "chapter": "transformer",
      "heading": "Encoder",
      "text": "The shape of the Transformer encoder output\nis (batch size, number of time steps, `num_hiddens`). ```{.python .input}\n%%tab mxnet\nencoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\nd2l.check_shape(encoder(np.ones((2, 100)), valid_lens), (2, 100, 24))\n```\n\n```{.python .input}\n%%tab pytorch\nencoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\nd2l.check_shape(encoder(d2l.ones((2, 100), dtype=torch.long), valid_lens),\n                (2, 100, 24))\n```\n\n```{.python .input}\n%%tab tensorflow\nencoder = TransformerEncoder(200, 24, 24, 24, 24, [1, 2], 48, 8, 2, 0.5)\nd2l.check_shape(encoder(tf.ones((2, 100)), valid_lens, training=False),\n                (2, 100, 24))\n```\n\n```{.python .input}\n%%tab jax\nencoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\nd2l.check_shape(encoder.init_with_output(d2l.get_key(),\n                                         jnp.ones((2, 100), dtype=jnp.int32),\n                                         valid_lens)[0],\n                (2, 100, 24))\n```"
    },
    {
      "chunk_id": "c7bb0b2a248b_0",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "As shown in :numref:`fig_transformer`,\n[**the Transformer decoder\nis composed of multiple identical layers**]. Each layer is implemented in the following\n`TransformerDecoderBlock` class,\nwhich contains three sublayers:\ndecoder self-attention,\nencoder--decoder attention,\nand positionwise feed-forward networks. These sublayers employ\na residual connection around them\nfollowed by layer normalization. As we described earlier in this section,\nin the masked multi-head decoder self-attention\n(the first sublayer),\nqueries, keys, and values\nall come from the outputs of the previous decoder layer. When training sequence-to-sequence models,\ntokens at all the positions (time steps)\nof the output sequence\nare known. However,\nduring prediction\nthe output sequence is generated token by token;\nthus,\nat any decoder time step\nonly the generated tokens\ncan be used in the decoder self-attention. To preserve autoregression in the decoder,\nits masked self-attention\nspecifies  `dec_valid_lens` so that\nany query\nonly attends to\nall positions in the decoder\nup to the query position. ```{.python .input}\n%%tab mxnet\nclass TransformerDecoderBlock(nn.Block):\n    # The i-th block in the Transformer decoder\n    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\n        super().__init__()\n        self.i = i\n        self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads,\n                                                 dropout)\n        self.addnorm1 = AddNorm(dropout)\n        self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads,\n                                                 dropout)\n        self.addnorm2 = AddNorm(dropout)\n        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n        self.addnorm3 = AddNorm(dropout)\n\n    def forward(self, X, state):\n        enc_outputs, enc_valid_lens = state[0], state[1]\n        # During training, all the tokens of any output sequence are processed\n        # at the same time, so state[2][self.i] is None as initialized."
    },
    {
      "chunk_id": "c7bb0b2a248b_1",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "When\n        # decoding any output sequence token by token during prediction,\n        # state[2][self.i] contains representations of the decoded output at\n        # the i-th block up to the current time step\n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            key_values = np.concatenate((state[2][self.i], X), axis=1)\n        state[2][self.i] = key_values\n\n        if autograd.is_training():\n            batch_size, num_steps, _ = X.shape\n            # Shape of dec_valid_lens: (batch_size, num_steps), where every\n            # row is [1, 2, ..., num_steps]\n            dec_valid_lens = np.tile(np.arange(1, num_steps + 1, ctx=X.ctx),\n                                     (batch_size, 1))\n        else:\n            dec_valid_lens = None\n        # Self-attention\n        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n        Y = self.addnorm1(X, X2)\n        # Encoder-decoder attention."
    },
    {
      "chunk_id": "c7bb0b2a248b_2",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "Shape of enc_outputs:\n        # (batch_size, num_steps, num_hiddens)\n        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n        Z = self.addnorm2(Y, Y2)\n        return self.addnorm3(Z, self.ffn(Z)), state\n```\n\n```{.python .input}\n%%tab pytorch\nclass TransformerDecoderBlock(nn.Module):\n    # The i-th block in the Transformer decoder\n    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\n        super().__init__()\n        self.i = i\n        self.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads,\n                                                 dropout)\n        self.addnorm1 = AddNorm(num_hiddens, dropout)\n        self.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads,\n                                                 dropout)\n        self.addnorm2 = AddNorm(num_hiddens, dropout)\n        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n        self.addnorm3 = AddNorm(num_hiddens, dropout)\n\n    def forward(self, X, state):\n        enc_outputs, enc_valid_lens = state[0], state[1]\n        # During training, all the tokens of any output sequence are processed\n        # at the same time, so state[2][self.i] is None as initialized."
    },
    {
      "chunk_id": "c7bb0b2a248b_3",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "When\n        # decoding any output sequence token by token during prediction,\n        # state[2][self.i] contains representations of the decoded output at\n        # the i-th block up to the current time step\n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            key_values = torch.cat((state[2][self.i], X), dim=1)\n        state[2][self.i] = key_values\n        if self.training:\n            batch_size, num_steps, _ = X.shape\n            # Shape of dec_valid_lens: (batch_size, num_steps), where every\n            # row is [1, 2, ..., num_steps]\n            dec_valid_lens = torch.arange(\n                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n        else:\n            dec_valid_lens = None\n        # Self-attention\n        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n        Y = self.addnorm1(X, X2)\n        # Encoder-decoder attention."
    },
    {
      "chunk_id": "c7bb0b2a248b_4",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "Shape of enc_outputs:\n        # (batch_size, num_steps, num_hiddens)\n        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n        Z = self.addnorm2(Y, Y2)\n        return self.addnorm3(Z, self.ffn(Z)), state\n```\n\n```{.python .input}\n%%tab tensorflow\nclass TransformerDecoderBlock(tf.keras.layers.Layer):\n    # The i-th block in the Transformer decoder\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_hiddens, num_heads, dropout, i):\n        super().__init__()\n        self.i = i\n        self.attention1 = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.attention2 = d2l.MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n        self.addnorm3 = AddNorm(norm_shape, dropout)\n\n    def call(self, X, state, **kwargs):\n        enc_outputs, enc_valid_lens = state[0], state[1]\n        # During training, all the tokens of any output sequence are processed\n        # at the same time, so state[2][self.i] is None as initialized."
    },
    {
      "chunk_id": "c7bb0b2a248b_5",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "When\n        # decoding any output sequence token by token during prediction,\n        # state[2][self.i] contains representations of the decoded output at\n        # the i-th block up to the current time step\n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            key_values = tf.concat((state[2][self.i], X), axis=1)\n        state[2][self.i] = key_values\n        if kwargs[\"training\"]:\n            batch_size, num_steps, _ = X.shape\n            # Shape of dec_valid_lens: (batch_size, num_steps), where every\n            # row is [1, 2, ..., num_steps]\n            dec_valid_lens = tf.repeat(\n                tf.reshape(tf.range(1, num_steps + 1),\n                           shape=(-1, num_steps)), repeats=batch_size, axis=0)\n        else:\n            dec_valid_lens = None\n        # Self-attention\n        X2 = self.attention1(X, key_values, key_values, dec_valid_lens,\n                             **kwargs)\n        Y = self.addnorm1(X, X2, **kwargs)\n        # Encoder-decoder attention."
    },
    {
      "chunk_id": "c7bb0b2a248b_6",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "Shape of enc_outputs:\n        # (batch_size, num_steps, num_hiddens)\n        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens,\n                             **kwargs)\n        Z = self.addnorm2(Y, Y2, **kwargs)\n        return self.addnorm3(Z, self.ffn(Z), **kwargs), state\n```\n\n```{.python .input}\n%%tab jax\nclass TransformerDecoderBlock(nn.Module):\n    # The i-th block in the Transformer decoder\n    num_hiddens: int\n    ffn_num_hiddens: int\n    num_heads: int\n    dropout: float\n    i: int\n\n    def setup(self):\n        self.attention1 = d2l.MultiHeadAttention(self.num_hiddens,\n                                                 self.num_heads,\n                                                 self.dropout)\n        self.addnorm1 = AddNorm(self.dropout)\n        self.attention2 = d2l.MultiHeadAttention(self.num_hiddens,\n                                                 self.num_heads,\n                                                 self.dropout)\n        self.addnorm2 = AddNorm(self.dropout)\n        self.ffn = PositionWiseFFN(self.ffn_num_hiddens, self.num_hiddens)\n        self.addnorm3 = AddNorm(self.dropout)\n\n    def __call__(self, X, state, training=False):\n        enc_outputs, enc_valid_lens = state[0], state[1]\n        # During training, all the tokens of any output sequence are processed\n        # at the same time, so state[2][self.i] is None as initialized."
    },
    {
      "chunk_id": "c7bb0b2a248b_7",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "When\n        # decoding any output sequence token by token during prediction,\n        # state[2][self.i] contains representations of the decoded output at\n        # the i-th block up to the current time step\n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            key_values = jnp.concatenate((state[2][self.i], X), axis=1)\n        state[2][self.i] = key_values\n        if training:\n            batch_size, num_steps, _ = X.shape\n            # Shape of dec_valid_lens: (batch_size, num_steps), where every\n            # row is [1, 2, ..., num_steps]\n            dec_valid_lens = jnp.tile(jnp.arange(1, num_steps + 1),\n                                      (batch_size, 1))\n        else:\n            dec_valid_lens = None\n        # Self-attention\n        X2, attention_w1 = self.attention1(X, key_values, key_values,\n                                           dec_valid_lens, training=training)\n        Y = self.addnorm1(X, X2, training=training)\n        # Encoder-decoder attention."
    },
    {
      "chunk_id": "c7bb0b2a248b_8",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "Shape of enc_outputs:\n        # (batch_size, num_steps, num_hiddens)\n        Y2, attention_w2 = self.attention2(Y, enc_outputs, enc_outputs,\n                                           enc_valid_lens, training=training)\n        Z = self.addnorm2(Y, Y2, training=training)\n        return self.addnorm3(Z, self.ffn(Z), training=training), state, attention_w1, attention_w2\n```\n\nTo facilitate scaled dot product operations\nin the encoder--decoder attention\nand addition operations in the residual connections,\n[**the feature dimension (`num_hiddens`) of the decoder is\nthe same as that of the encoder.**]\n\n```{.python .input}\n%%tab mxnet\ndecoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)\ndecoder_blk.initialize()\nX = np.ones((2, 100, 24))\nstate = [encoder_blk(X, valid_lens), valid_lens, [None]]\nd2l.check_shape(decoder_blk(X, state)[0], X.shape)\n```\n\n```{.python .input}\n%%tab pytorch\ndecoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)\nX = d2l.ones((2, 100, 24))\nstate = [encoder_blk(X, valid_lens), valid_lens, [None]]\nd2l.check_shape(decoder_blk(X, state)[0], X.shape)\n```\n\n```{.python .input}\n%%tab tensorflow\ndecoder_blk = TransformerDecoderBlock(24, 24, 24, 24, [1, 2], 48, 8, 0.5, 0)\nX = tf.ones((2, 100, 24))\nstate = [encoder_blk(X, valid_lens), valid_lens, [None]]\nd2l.check_shape(decoder_blk(X, state, training=False)[0], X.shape)\n```\n\n```{.python .input}\n%%tab jax\ndecoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)\nX = d2l.ones((2, 100, 24))\nstate = [encoder_blk.init_with_output(d2l.get_key(), X, valid_lens)[0][0],\n         valid_lens, [None]]\nd2l.check_shape(decoder_blk.init_with_output(d2l.get_key(), X, state)[0][0],\n                X.shape)\n```\n\nNow we [**construct the entire Transformer decoder**]\ncomposed of `num_blks` instances of `TransformerDecoderBlock`. In the end,\na fully connected layer computes the prediction\nfor all the `vocab_size` possible output tokens. Both of the decoder self-attention weights\nand the encoder--decoder attention weights\nare stored for later visualization."
    },
    {
      "chunk_id": "c7bb0b2a248b_9",
      "chapter": "transformer",
      "heading": "Decoder",
      "text": "In the end,\na fully connected layer computes the prediction\nfor all the `vocab_size` possible output tokens. Both of the decoder self-attention weights\nand the encoder--decoder attention weights\nare stored for later visualization. ```{.python .input}\n%%tab mxnet\nclass TransformerDecoder(d2l.AttentionDecoder):\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                 num_blks, dropout):\n        super().__init__()\n        self.num_hiddens = num_hiddens\n        self.num_blks = num_blks\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_blks):\n            self.blks.add(TransformerDecoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, i))\n        self.dense = nn.Dense(vocab_size, flatten=False)\n        self.initialize()\n\n    def init_state(self, enc_outputs, enc_valid_lens):\n        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n\n    def forward(self, X, state):\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n        for i, blk in enumerate(self.blks):\n            X, state = blk(X, state)\n            # Decoder self-attention weights\n            self._attention_weights[0][\n                i] = blk.attention1.attention.attention_weights\n            # Encoder-decoder attention weights\n            self._attention_weights[1][\n                i] = blk.attention2.attention.attention_weights\n        return self.dense(X), state\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n```\n\n```{.python .input}\n%%tab pytorch\nclass TransformerDecoder(d2l.AttentionDecoder):\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                 num_blks, dropout):\n        super().__init__()\n        self.num_hiddens = num_hiddens\n        self.num_blks = num_blks\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_blks):\n            self.blks.add_module(\"block\"+str(i), TransformerDecoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, i))\n        self.dense = nn.LazyLinear(vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens):\n        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n\n    def forward(self, X, state):\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n        for i, blk in enumerate(self.blks):\n            X, state = blk(X, state)\n            # Decoder self-attention weights\n            self._attention_weights[0][\n                i] = blk.attention1.attention.attention_weights\n            # Encoder-decoder attention weights\n            self._attention_weights[1][\n                i] = blk.attention2.attention.attention_weights\n        return self.dense(X), state\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n```\n\n```{.python .input}\n%%tab tensorflow\nclass TransformerDecoder(d2l.AttentionDecoder):\n    def __init__(self, vocab_size, key_size, query_size, value_size,\n                 num_hiddens, norm_shape, ffn_num_hiddens, num_heads,\n                 num_blks, dropout):\n        super().__init__()\n        self.num_hiddens = num_hiddens\n        self.num_blks = num_blks\n        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n        self.blks = [TransformerDecoderBlock(\n            key_size, query_size, value_size, num_hiddens, norm_shape,\n            ffn_num_hiddens, num_heads, dropout, i)\n                     for i in range(num_blks)]\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens):\n        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n\n    def call(self, X, state, **kwargs):\n        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(\n            tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)\n        # 2 attention layers in decoder\n        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n        for i, blk in enumerate(self.blks):\n            X, state = blk(X, state, **kwargs)\n            # Decoder self-attention weights\n            self._attention_weights[0][i] = (\n                blk.attention1.attention.attention_weights)\n            # Encoder-decoder attention weights\n            self._attention_weights[1][i] = (\n                blk.attention2.attention.attention_weights)\n        return self.dense(X), state\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights\n```\n\n```{.python .input}\n%%tab jax\nclass TransformerDecoder(nn.Module):\n    vocab_size: int\n    num_hiddens: int\n    ffn_num_hiddens: int\n    num_heads: int\n    num_blks: int\n    dropout: float\n\n    def setup(self):\n        self.embedding = nn.Embed(self.vocab_size, self.num_hiddens)\n        self.pos_encoding = d2l.PositionalEncoding(self.num_hiddens,\n                                                   self.dropout)\n        self.blks = [TransformerDecoderBlock(self.num_hiddens,\n                                             self.ffn_num_hiddens,\n                                             self.num_heads, self.dropout, i)\n                     for i in range(self.num_blks)]\n        self.dense = nn.Dense(self.vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens):\n        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n\n    def __call__(self, X, state, training=False):\n        X = self.embedding(X) * jnp.sqrt(jnp.float32(self.num_hiddens))\n        X = self.pos_encoding(X, training=training)\n        attention_weights = [[None] * len(self.blks) for _ in range(2)]\n        for i, blk in enumerate(self.blks):\n            X, state, attention_w1, attention_w2 = blk(X, state,\n                                                       training=training)\n            # Decoder self-attention weights\n            attention_weights[0][i] = attention_w1\n            # Encoder-decoder attention weights\n            attention_weights[1][i] = attention_w2\n        # Flax sow API is used to capture intermediate variables\n        self.sow('intermediates', 'dec_attention_weights', attention_weights)\n        return self.dense(X), state\n```"
    },
    {
      "chunk_id": "0e66174db4e0_0",
      "chapter": "transformer",
      "heading": "[**Training**]",
      "text": "Let's instantiate an encoder--decoder model\nby following the Transformer architecture. Here we specify that\nboth the Transformer encoder and the Transformer decoder\nhave two layers using 4-head attention. As in :numref:`sec_seq2seq_training`,\nwe train the Transformer model\nfor sequence-to-sequence learning on the English--French machine translation dataset."
    },
    {
      "chunk_id": "0e66174db4e0_1",
      "chapter": "transformer",
      "heading": "[**Training**]",
      "text": "As in :numref:`sec_seq2seq_training`,\nwe train the Transformer model\nfor sequence-to-sequence learning on the English--French machine translation dataset. ```{.python .input}\n%%tab all\ndata = d2l.MTFraEng(batch_size=128)\nnum_hiddens, num_blks, dropout = 256, 2, 0.2\nffn_num_hiddens, num_heads = 64, 4\nif tab.selected('tensorflow'):\n    key_size, query_size, value_size = 256, 256, 256\n    norm_shape = [2]\nif tab.selected('pytorch', 'mxnet', 'jax'):\n    encoder = TransformerEncoder(\n        len(data.src_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n        num_blks, dropout)\n    decoder = TransformerDecoder(\n        len(data.tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n        num_blks, dropout)\nif tab.selected('mxnet', 'pytorch'):\n    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                        lr=0.001)\n    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\nif tab.selected('jax'):\n    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                        lr=0.001, training=True)\n    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        encoder = TransformerEncoder(\n            len(data.src_vocab), key_size, query_size, value_size, num_hiddens,\n            norm_shape, ffn_num_hiddens, num_heads, num_blks, dropout)\n        decoder = TransformerDecoder(\n            len(data.tgt_vocab), key_size, query_size, value_size, num_hiddens,\n            norm_shape, ffn_num_hiddens, num_heads, num_blks, dropout)\n        model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                            lr=0.001)\n    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)\ntrainer.fit(model, data)\n```\n\nAfter training,\nwe use the Transformer model\nto [**translate a few English sentences**] into French and compute their BLEU scores."
    },
    {
      "chunk_id": "0e66174db4e0_2",
      "chapter": "transformer",
      "heading": "[**Training**]",
      "text": "```{.python .input}\n%%tab all\nengs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\nfras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    preds, _ = model.predict_step(\n        data.build(engs, fras), d2l.try_gpu(), data.num_steps)\nif tab.selected('jax'):\n    preds, _ = model.predict_step(\n        trainer.state.params, data.build(engs, fras), data.num_steps)\nfor en, fr, p in zip(engs, fras, preds):\n    translation = []\n    for token in data.tgt_vocab.to_tokens(p):\n        if token == '<eos>':\n            break\n        translation.append(token)\n    print(f'{en} => {translation}, bleu,'\n          f'{d2l.bleu(\" \".join(translation), fr, k=2):.3f}')\n```\n\nLet's [**visualize the Transformer attention weights**] when translating the final English sentence into French. The shape of the encoder self-attention weights\nis (number of encoder layers, number of attention heads, `num_steps` or number of queries, `num_steps` or number of key-value pairs)."
    },
    {
      "chunk_id": "0e66174db4e0_3",
      "chapter": "transformer",
      "heading": "[**Training**]",
      "text": "The shape of the encoder self-attention weights\nis (number of encoder layers, number of attention heads, `num_steps` or number of queries, `num_steps` or number of key-value pairs). ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n_, dec_attention_weights = model.predict_step(\n    data.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)\nenc_attention_weights = d2l.concat(model.encoder.attention_weights, 0)\nshape = (num_blks, num_heads, -1, data.num_steps)\nenc_attention_weights = d2l.reshape(enc_attention_weights, shape)\nd2l.check_shape(enc_attention_weights,\n                (num_blks, num_heads, data.num_steps, data.num_steps))\n```\n\n```{.python .input}\n%%tab jax\n_, (dec_attention_weights, enc_attention_weights) = model.predict_step(\n    trainer.state.params, data.build([engs[-1]], [fras[-1]]),\n    data.num_steps, True)\nenc_attention_weights = d2l.concat(enc_attention_weights, 0)\nshape = (num_blks, num_heads, -1, data.num_steps)\nenc_attention_weights = d2l.reshape(enc_attention_weights, shape)\nd2l.check_shape(enc_attention_weights,\n                (num_blks, num_heads, data.num_steps, data.num_steps))\n```\n\nIn the encoder self-attention,\nboth queries and keys come from the same input sequence. Since padding tokens do not carry meaning,\nwith specified valid length of the input sequence\nno query attends to positions of padding tokens. In the following,\ntwo layers of multi-head attention weights\nare presented row by row. Each head independently attends\nbased on a separate representation subspace of queries, keys, and values."
    },
    {
      "chunk_id": "0e66174db4e0_4",
      "chapter": "transformer",
      "heading": "[**Training**]",
      "text": "In the following,\ntwo layers of multi-head attention weights\nare presented row by row. Each head independently attends\nbased on a separate representation subspace of queries, keys, and values. ```{.python .input}\n%%tab mxnet, tensorflow, jax\nd2l.show_heatmaps(\n    enc_attention_weights, xlabel='Key positions', ylabel='Query positions',\n    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))\n```\n\n```{.python .input}\n%%tab pytorch\nd2l.show_heatmaps(\n    enc_attention_weights.cpu(), xlabel='Key positions',\n    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\n    figsize=(7, 3.5))\n```\n\n[**To visualize the decoder self-attention weights and the encoder--decoder attention weights,\nwe need more data manipulations.**]\nFor example,\nwe fill the masked attention weights with zero. Note that\nthe decoder self-attention weights\nand the encoder--decoder attention weights\nboth have the same queries:\nthe beginning-of-sequence token followed by\nthe output tokens and possibly\nend-of-sequence tokens."
    },
    {
      "chunk_id": "0e66174db4e0_5",
      "chapter": "transformer",
      "heading": "[**Training**]",
      "text": "Note that\nthe decoder self-attention weights\nand the encoder--decoder attention weights\nboth have the same queries:\nthe beginning-of-sequence token followed by\nthe output tokens and possibly\nend-of-sequence tokens. ```{.python .input}\n%%tab mxnet\ndec_attention_weights_2d = [d2l.tensor(head[0]).tolist()\n                            for step in dec_attention_weights\n                            for attn in step for blk in attn for head in blk]\ndec_attention_weights_filled = d2l.tensor(\n    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)\ndec_attention_weights = d2l.reshape(dec_attention_weights_filled, (\n    -1, 2, num_blks, num_heads, data.num_steps))\ndec_self_attention_weights, dec_inter_attention_weights = \\\n    dec_attention_weights.transpose(1, 2, 3, 0, 4)\n```\n\n```{.python .input}\n%%tab pytorch\ndec_attention_weights_2d = [head[0].tolist()\n                            for step in dec_attention_weights\n                            for attn in step for blk in attn for head in blk]\ndec_attention_weights_filled = d2l.tensor(\n    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)\nshape = (-1, 2, num_blks, num_heads, data.num_steps)\ndec_attention_weights = d2l.reshape(dec_attention_weights_filled, shape)\ndec_self_attention_weights, dec_inter_attention_weights = \\\n    dec_attention_weights.permute(1, 2, 3, 0, 4)\n```\n\n```{.python .input}\n%%tab tensorflow\ndec_attention_weights_2d = [head[0] for step in dec_attention_weights\n                            for attn in step\n                            for blk in attn for head in blk]\ndec_attention_weights_filled = tf.convert_to_tensor(\n    np.asarray(pd.DataFrame(dec_attention_weights_2d).fillna(\n        0.0).values).astype(np.float32))\ndec_attention_weights = tf.reshape(dec_attention_weights_filled, shape=(\n    -1, 2, num_blks, num_heads, data.num_steps))\ndec_self_attention_weights, dec_inter_attention_weights = tf.transpose(\n    dec_attention_weights, perm=(1, 2, 3, 0, 4))\n```\n\n```{.python .input}\n%%tab jax\ndec_attention_weights_2d = [head[0].tolist() for step in dec_attention_weights\n                            for attn in step\n                            for blk in attn for head in blk]\ndec_attention_weights_filled = d2l.tensor(\n    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)\ndec_attention_weights = dec_attention_weights_filled.reshape(\n    (-1, 2, num_blks, num_heads, data.num_steps))\ndec_self_attention_weights, dec_inter_attention_weights = \\\n    dec_attention_weights.transpose(1, 2, 3, 0, 4)\n```\n\n```{.python .input}\n%%tab all\nd2l.check_shape(dec_self_attention_weights,\n                (num_blks, num_heads, data.num_steps, data.num_steps))\nd2l.check_shape(dec_inter_attention_weights,\n                (num_blks, num_heads, data.num_steps, data.num_steps))\n```\n\nBecause of the autoregressive property of the decoder self-attention,\nno query attends to key--value pairs after the query position."
    },
    {
      "chunk_id": "0e66174db4e0_6",
      "chapter": "transformer",
      "heading": "[**Training**]",
      "text": "```{.python .input}\n%%tab all\nd2l.show_heatmaps(\n    dec_self_attention_weights[:, :, :, :],\n    xlabel='Key positions', ylabel='Query positions',\n    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))\n```\n\nSimilar to the case in the encoder self-attention,\nvia the specified valid length of the input sequence,\n[**no query from the output sequence\nattends to those padding tokens from the input sequence.**]\n\n```{.python .input}\n%%tab all\nd2l.show_heatmaps(\n    dec_inter_attention_weights, xlabel='Key positions',\n    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\n    figsize=(7, 3.5))\n```\n\nAlthough the Transformer architecture\nwas originally proposed for sequence-to-sequence learning,\nas we will discover later in the book,\neither the Transformer encoder\nor the Transformer decoder\nis often individually used\nfor different deep learning tasks."
    },
    {
      "chunk_id": "77e85db5405b_0",
      "chapter": "transformer",
      "heading": "Summary",
      "text": "The Transformer is an instance of the encoder--decoder architecture,\nthough either the encoder or the decoder can be used individually in practice.\nIn the Transformer architecture, multi-head self-attention is used\nfor representing the input sequence and the output sequence,\nthough the decoder has to preserve the autoregressive property via a masked version.\nBoth the residual connections and the layer normalization in the Transformer\nare important for training a very deep model.\nThe positionwise feed-forward network in the Transformer model\ntransforms the representation at all the sequence positions using the same MLP."
    },
    {
      "chunk_id": "67dab87010e8_0",
      "chapter": "transformer",
      "heading": "Exercises",
      "text": "1. Train a deeper Transformer in the experiments. How does it affect the training speed and the translation performance?\n1. Is it a good idea to replace scaled dot product attention with additive attention in the Transformer? Why?\n1. For language modeling, should we use the Transformer encoder, decoder, or both? How would you design this method?\n1. What challenges can Transformers face if input sequences are very long? Why?\n1. How would you improve the computational and memory efficiency of Transformers? Hint: you may refer to the survey paper by :citet:`Tay.Dehghani.Bahri.ea.2020`.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/348)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1066)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3871)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18031)\n:end_tab:"
    },
    {
      "chunk_id": "2c805b2b23aa_0",
      "chapter": "vision-transformer",
      "heading": "vision-transformer",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['pytorch', 'jax'])\n```\n\n# Transformers for Vision\n:label:`sec_vision-transformer`\n\nThe Transformer architecture was initially proposed\nfor sequence-to-sequence learning,\nwith a focus on machine translation. Subsequently, Transformers emerged as the model of choice\nin various natural language processing tasks :cite:`Radford.Narasimhan.Salimans.ea.2018,Radford.Wu.Child.ea.2019,brown2020language,Devlin.Chang.Lee.ea.2018,raffel2020exploring`. However, in the field of computer vision\nthe dominant architecture has remained\nthe CNN (:numref:`chap_modern_cnn`). Naturally, researchers started to wonder\nif it might be possible to do better\nby adapting Transformer models to image data. This question sparked immense interest\nin the computer vision community. Recently, :citet:`ramachandran2019stand` proposed\na scheme for replacing convolution with self-attention. However, its use of specialized patterns in attention\nmakes it hard to scale up models on hardware accelerators. Then, :citet:`cordonnier2020relationship` theoretically proved\nthat self-attention can learn to behave similarly to convolution. Empirically, $2 \\times 2$ patches were taken from images as inputs,\nbut the small patch size makes the model\nonly applicable to image data with low resolutions. Without specific constraints on patch size,\n*vision Transformers* (ViTs)\nextract patches from images\nand feed them into a Transformer encoder\nto obtain a global representation,\nwhich will finally be transformed for classification :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`. Notably, Transformers show better scalability than CNNs:\nand when training larger models on larger datasets,\nvision Transformers outperform ResNets by a significant margin. Similar to the landscape of network architecture design in natural language processing,\nTransformers have also become a game-changer in computer vision."
    },
    {
      "chunk_id": "2c805b2b23aa_1",
      "chapter": "vision-transformer",
      "heading": "vision-transformer",
      "text": "Similar to the landscape of network architecture design in natural language processing,\nTransformers have also become a game-changer in computer vision. ```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "4e43a6d2dc69_0",
      "chapter": "vision-transformer",
      "heading": "Model",
      "text": ":numref:`fig_vit` depicts\nthe model architecture of vision Transformers.\nThis architecture consists of a stem\nthat patchifies images,\na body based on the multilayer Transformer encoder,\nand a head that transforms the global representation\ninto the output label.\n\n![The vision Transformer architecture. In this example, an image is split into nine patches. A special \u201c&lt;cls&gt;\u201d token and the nine flattened image patches are transformed via patch embedding and $\\mathit{n}$ Transformer encoder blocks into ten representations, respectively. The \u201c&lt;cls&gt;\u201d representation is further transformed into the output label.](../img/vit.svg)\n:label:`fig_vit`\n\nConsider an input image with height $h$, width $w$,\nand $c$ channels.\nSpecifying the patch height and width both as $p$,\nthe image is split into a sequence of $m = hw/p^2$ patches,\nwhere each patch is flattened to a vector of length $cp^2$.\nIn this way, image patches can be treated similarly to tokens in text sequences by Transformer encoders.\nA special \u201c&lt;cls&gt;\u201d (class) token and\nthe $m$ flattened image patches are linearly projected\ninto a sequence of $m+1$ vectors,\nsummed with learnable positional embeddings.\nThe multilayer Transformer encoder\ntransforms $m+1$ input vectors\ninto the same number of output vector representations of the same length.\nIt works exactly the same way as the original Transformer encoder in :numref:`fig_transformer`,\nonly differing in the position of normalization.\nSince the \u201c&lt;cls&gt;\u201d token attends to all the image patches\nvia self-attention (see :numref:`fig_cnn-rnn-self-attention`),\nits representation from the Transformer encoder output\nwill be further transformed into the output label."
    },
    {
      "chunk_id": "901fc562e6da_0",
      "chapter": "vision-transformer",
      "heading": "Patch Embedding",
      "text": "To implement a vision Transformer, let's start\nwith patch embedding in :numref:`fig_vit`. Splitting an image into patches\nand linearly projecting these flattened patches\ncan be simplified as a single convolution operation,\nwhere both the kernel size and the stride size are set to the patch size. ```{.python .input}\n%%tab pytorch\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=96, patch_size=16, num_hiddens=512):\n        super().__init__()\n        def _make_tuple(x):\n            if not isinstance(x, (list, tuple)):\n                return (x, x)\n            return x\n        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\n        self.num_patches = (img_size[0] // patch_size[0]) * (\n            img_size[1] // patch_size[1])\n        self.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,\n                                  stride=patch_size)\n\n    def forward(self, X):\n        # Output shape: (batch size, no. of patches, no. of channels)\n        return self.conv(X).flatten(2).transpose(1, 2)\n```\n\n```{.python .input}\n%%tab jax\nclass PatchEmbedding(nn.Module):\n    img_size: int = 96\n    patch_size: int = 16\n    num_hiddens: int = 512\n\n    def setup(self):\n        def _make_tuple(x):\n            if not isinstance(x, (list, tuple)):\n                return (x, x)\n            return x\n        img_size, patch_size = _make_tuple(self.img_size), _make_tuple(self.patch_size)\n        self.num_patches = (img_size[0] // patch_size[0]) * (\n            img_size[1] // patch_size[1])\n        self.conv = nn.Conv(self.num_hiddens, kernel_size=patch_size,\n                            strides=patch_size, padding='SAME')\n\n    def __call__(self, X):\n        # Output shape: (batch size, no. of patches, no."
    },
    {
      "chunk_id": "901fc562e6da_1",
      "chapter": "vision-transformer",
      "heading": "Patch Embedding",
      "text": "of patches, no. of channels)\n        X = self.conv(X)\n        return X.reshape((X.shape[0], -1, X.shape[3]))\n```\n\nIn the following example, taking images with height and width of `img_size` as inputs,\nthe patch embedding outputs `(img_size//patch_size)**2` patches\nthat are linearly projected to vectors of length `num_hiddens`. ```{.python .input}\n%%tab pytorch\nimg_size, patch_size, num_hiddens, batch_size = 96, 16, 512, 4\npatch_emb = PatchEmbedding(img_size, patch_size, num_hiddens)\nX = d2l.zeros(batch_size, 3, img_size, img_size)\nd2l.check_shape(patch_emb(X),\n                (batch_size, (img_size//patch_size)**2, num_hiddens))\n```\n\n```{.python .input}\n%%tab jax\nimg_size, patch_size, num_hiddens, batch_size = 96, 16, 512, 4\npatch_emb = PatchEmbedding(img_size, patch_size, num_hiddens)\nX = d2l.zeros((batch_size, img_size, img_size, 3))\noutput, _ = patch_emb.init_with_output(d2l.get_key(), X)\nd2l.check_shape(output, (batch_size, (img_size//patch_size)**2, num_hiddens))\n```"
    },
    {
      "chunk_id": "46a0abc6f391_0",
      "chapter": "vision-transformer",
      "heading": "Vision Transformer Encoder",
      "text": ":label:`subsec_vit-encoder`\n\nThe MLP of the vision Transformer encoder is slightly different\nfrom the positionwise FFN of the original Transformer encoder\n(see :numref:`subsec_positionwise-ffn`). First, here the activation function uses the Gaussian error linear unit (GELU),\nwhich can be considered as a smoother version of the ReLU :cite:`Hendrycks.Gimpel.2016`. Second, dropout is applied to the output of each fully connected layer in the MLP for regularization. ```{.python .input}\n%%tab pytorch\nclass ViTMLP(nn.Module):\n    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):\n        super().__init__()\n        self.dense1 = nn.LazyLinear(mlp_num_hiddens)\n        self.gelu = nn.GELU()\n        self.dropout1 = nn.Dropout(dropout)\n        self.dense2 = nn.LazyLinear(mlp_num_outputs)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.dropout2(self.dense2(self.dropout1(self.gelu(\n            self.dense1(x)))))\n```\n\n```{.python .input}\n%%tab jax\nclass ViTMLP(nn.Module):\n    mlp_num_hiddens: int\n    mlp_num_outputs: int\n    dropout: float = 0.5\n\n    @nn.compact\n    def __call__(self, x, training=False):\n        x = nn.Dense(self.mlp_num_hiddens)(x)\n        x = nn.gelu(x)\n        x = nn.Dropout(self.dropout, deterministic=not training)(x)\n        x = nn.Dense(self.mlp_num_outputs)(x)\n        x = nn.Dropout(self.dropout, deterministic=not training)(x)\n        return x\n```\n\nThe vision Transformer encoder block implementation\njust follows the pre-normalization design in :numref:`fig_vit`,\nwhere normalization is applied right *before* multi-head attention or the MLP. In contrast to post-normalization (\"add & norm\" in :numref:`fig_transformer`),\nwhere normalization is placed right *after* residual connections,\npre-normalization leads to more effective or efficient training for Transformers :cite:`baevski2018adaptive,wang2019learning,xiong2020layer`."
    },
    {
      "chunk_id": "46a0abc6f391_1",
      "chapter": "vision-transformer",
      "heading": "Vision Transformer Encoder",
      "text": "```{.python .input}\n%%tab pytorch\nclass ViTBlock(nn.Module):\n    def __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,\n                 num_heads, dropout, use_bias=False):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(norm_shape)\n        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\n                                                dropout, use_bias)\n        self.ln2 = nn.LayerNorm(norm_shape)\n        self.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\n\n    def forward(self, X, valid_lens=None):\n        X = X + self.attention(*([self.ln1(X)] * 3), valid_lens)\n        return X + self.mlp(self.ln2(X))\n```\n\n```{.python .input}\n%%tab jax\nclass ViTBlock(nn.Module):\n    num_hiddens: int\n    mlp_num_hiddens: int\n    num_heads: int\n    dropout: float\n    use_bias: bool = False\n\n    def setup(self):\n        self.attention = d2l.MultiHeadAttention(self.num_hiddens, self.num_heads,\n                                                self.dropout, self.use_bias)\n        self.mlp = ViTMLP(self.mlp_num_hiddens, self.num_hiddens, self.dropout)\n\n    @nn.compact\n    def __call__(self, X, valid_lens=None, training=False):\n        X = X + self.attention(*([nn.LayerNorm()(X)] * 3),\n                               valid_lens, training=training)[0]\n        return X + self.mlp(nn.LayerNorm()(X), training=training)\n```\n\nJust as in :numref:`subsec_transformer-encoder`,\nno vision Transformer encoder block changes its input shape. ```{.python .input}\n%%tab pytorch\nX = d2l.ones((2, 100, 24))\nencoder_blk = ViTBlock(24, 24, 48, 8, 0.5)\nencoder_blk.eval()\nd2l.check_shape(encoder_blk(X), X.shape)\n```\n\n```{.python .input}\n%%tab jax\nX = d2l.ones((2, 100, 24))\nencoder_blk = ViTBlock(24, 48, 8, 0.5)\nd2l.check_shape(encoder_blk.init_with_output(d2l.get_key(), X)[0], X.shape)\n```"
    },
    {
      "chunk_id": "e966ac9b0837_0",
      "chapter": "vision-transformer",
      "heading": "Putting It All Together",
      "text": "The forward pass of vision Transformers below is straightforward. First, input images are fed into an `PatchEmbedding` instance,\nwhose output is concatenated with the \u201c&lt;cls&gt;\u201d  token embedding. They are summed with learnable positional embeddings before dropout. Then the output is fed into the Transformer encoder that stacks `num_blks` instances of the `ViTBlock` class. Finally, the representation of the \u201c&lt;cls&gt;\u201d  token is projected by the network head."
    },
    {
      "chunk_id": "e966ac9b0837_1",
      "chapter": "vision-transformer",
      "heading": "Putting It All Together",
      "text": "Then the output is fed into the Transformer encoder that stacks `num_blks` instances of the `ViTBlock` class. Finally, the representation of the \u201c&lt;cls&gt;\u201d  token is projected by the network head. ```{.python .input}\n%%tab pytorch\nclass ViT(d2l.Classifier):\n    \"\"\"Vision Transformer.\"\"\"\n    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens,\n                 num_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,\n                 use_bias=False, num_classes=10):\n        super().__init__()\n        self.save_hyperparameters()\n        self.patch_embedding = PatchEmbedding(\n            img_size, patch_size, num_hiddens)\n        self.cls_token = nn.Parameter(d2l.zeros(1, 1, num_hiddens))\n        num_steps = self.patch_embedding.num_patches + 1  # Add the cls token\n        # Positional embeddings are learnable\n        self.pos_embedding = nn.Parameter(\n            torch.randn(1, num_steps, num_hiddens))\n        self.dropout = nn.Dropout(emb_dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_blks):\n            self.blks.add_module(f\"{i}\", ViTBlock(\n                num_hiddens, num_hiddens, mlp_num_hiddens,\n                num_heads, blk_dropout, use_bias))\n        self.head = nn.Sequential(nn.LayerNorm(num_hiddens),\n                                  nn.Linear(num_hiddens, num_classes))\n\n    def forward(self, X):\n        X = self.patch_embedding(X)\n        X = d2l.concat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\n        X = self.dropout(X + self.pos_embedding)\n        for blk in self.blks:\n            X = blk(X)\n        return self.head(X[:, 0])\n```\n\n```{.python .input}\n%%tab jax\nclass ViT(d2l.Classifier):\n    \"\"\"Vision Transformer.\"\"\"\n    img_size: int\n    patch_size: int\n    num_hiddens: int\n    mlp_num_hiddens: int\n    num_heads: int\n    num_blks: int\n    emb_dropout: float\n    blk_dropout: float\n    lr: float = 0.1\n    use_bias: bool = False\n    num_classes: int = 10\n    training: bool = False\n\n    def setup(self):\n        self.patch_embedding = PatchEmbedding(self.img_size, self.patch_size,\n                                              self.num_hiddens)\n        self.cls_token = self.param('cls_token', nn.initializers.zeros,\n                                    (1, 1, self.num_hiddens))\n        num_steps = self.patch_embedding.num_patches + 1  # Add the cls token\n        # Positional embeddings are learnable\n        self.pos_embedding = self.param('pos_embed', nn.initializers.normal(),\n                                        (1, num_steps, self.num_hiddens))\n        self.blks = [ViTBlock(self.num_hiddens, self.mlp_num_hiddens,\n                              self.num_heads, self.blk_dropout, self.use_bias)\n                    for _ in range(self.num_blks)]\n        self.head = nn.Sequential([nn.LayerNorm(), nn.Dense(self.num_classes)])\n\n    @nn.compact\n    def __call__(self, X):\n        X = self.patch_embedding(X)\n        X = d2l.concat((jnp.tile(self.cls_token, (X.shape[0], 1, 1)), X), 1)\n        X = nn.Dropout(emb_dropout, deterministic=not self.training)(X + self.pos_embedding)\n        for blk in self.blks:\n            X = blk(X, training=self.training)\n        return self.head(X[:, 0])\n```"
    },
    {
      "chunk_id": "e040eb813df0_0",
      "chapter": "vision-transformer",
      "heading": "Training",
      "text": "Training a vision Transformer on the Fashion-MNIST dataset is just like how CNNs were trained in :numref:`chap_modern_cnn`.\n\n```{.python .input}\n%%tab all\nimg_size, patch_size = 96, 16\nnum_hiddens, mlp_num_hiddens, num_heads, num_blks = 512, 2048, 8, 2\nemb_dropout, blk_dropout, lr = 0.1, 0.1, 0.1\nmodel = ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,\n            num_blks, emb_dropout, blk_dropout, lr)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(img_size, img_size))\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "053f8a07f7be_0",
      "chapter": "vision-transformer",
      "heading": "Summary and Discussion",
      "text": "You may have noticed that for small datasets like Fashion-MNIST,\nour implemented vision Transformer\ndoes not outperform the ResNet in :numref:`sec_resnet`.\nSimilar observations can be made even on the ImageNet dataset (1.2 million images).\nThis is because Transformers *lack* those useful principles in convolution,\nsuch as translation invariance and locality (:numref:`sec_why-conv`).\nHowever, the picture changes when training larger models on larger datasets (e.g., 300 million images),\nwhere vision Transformers outperform ResNets by a large margin in image classification, demonstrating\nintrinsic superiority of Transformers in scalability :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`.\nThe introduction of vision Transformers\nhas changed the landscape of network design for modeling image data.\nThey were soon shown to be effective on the ImageNet dataset\nwith data-efficient training strategies of DeiT :cite:`touvron2021training`.\nHowever, the quadratic complexity of self-attention\n(:numref:`sec_self-attention-and-positional-encoding`)\nmakes the Transformer architecture\nless suitable for higher-resolution images.\nTowards a general-purpose backbone network in computer vision,\nSwin Transformers addressed the quadratic computational complexity\nwith respect to image size (:numref:`subsec_cnn-rnn-self-attention`)\nand reinstated convolution-like priors,\nextending the applicability of Transformers to a range of computer vision tasks\nbeyond image classification with state-of-the-art results :cite:`liu2021swin`."
    },
    {
      "chunk_id": "222f7a3dfb49_0",
      "chapter": "vision-transformer",
      "heading": "Exercises",
      "text": "1. How does the value of `img_size` affect training time?\n1. Instead of projecting the \u201c&lt;cls&gt;\u201d token representation to the output, how would you project the averaged patch representations? Implement this change and see how it affects the accuracy.\n1. Can you modify hyperparameters to improve the accuracy of the vision Transformer?\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/8943)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18032)\n:end_tab:"
    },
    {
      "chunk_id": "7574c219a0be_0",
      "chapter": "custom-layer",
      "heading": "custom-layer",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Custom Layers\n\nOne factor behind deep learning's success\nis the availability of a wide range of layers\nthat can be composed in creative ways\nto design architectures suitable\nfor a wide variety of tasks.\nFor instance, researchers have invented layers\nspecifically for handling images, text,\nlooping over sequential data,\nand\nperforming dynamic programming.\nSooner or later, you will need\na layer that does not exist yet in the deep learning framework.\nIn these cases, you must build a custom layer.\nIn this section, we show you how.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "c10e3ad1a8fc_0",
      "chapter": "custom-layer",
      "heading": "(**Layers without Parameters**)",
      "text": "To start, we construct a custom layer\nthat does not have any parameters of its own. This should look familiar if you recall our\nintroduction to modules in :numref:`sec_model_construction`. The following `CenteredLayer` class simply\nsubtracts the mean from its input. To build it, we simply need to inherit\nfrom the base layer class and implement the forward propagation function. ```{.python .input}\n%%tab mxnet\nclass CenteredLayer(nn.Block):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def forward(self, X):\n        return X - X.mean()\n```\n\n```{.python .input}\n%%tab pytorch\nclass CenteredLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, X):\n        return X - X.mean()\n```\n\n```{.python .input}\n%%tab tensorflow\nclass CenteredLayer(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, X):\n        return X - tf.reduce_mean(X)\n```\n\n```{.python .input}\n%%tab jax\nclass CenteredLayer(nn.Module):\n    def __call__(self, X):\n        return X - X.mean()\n```\n\nLet's verify that our layer works as intended by feeding some data through it. ```{.python .input}\n%%tab all\nlayer = CenteredLayer()\nlayer(d2l.tensor([1.0, 2, 3, 4, 5]))\n```\n\nWe can now [**incorporate our layer as a component\nin constructing more complex models.**]\n\n```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\nnet.add(nn.Dense(128), CenteredLayer())\nnet.initialize()\n```\n\n```{.python .input}\n%%tab pytorch\nnet = nn.Sequential(nn.LazyLinear(128), CenteredLayer())\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.Sequential([tf.keras.layers.Dense(128), CenteredLayer()])\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([nn.Dense(128), CenteredLayer()])\n```\n\nAs an extra sanity check, we can send random data\nthrough the network and check that the mean is in fact 0. Because we are dealing with floating point numbers,\nwe may still see a very small nonzero number\ndue to quantization."
    },
    {
      "chunk_id": "c10e3ad1a8fc_1",
      "chapter": "custom-layer",
      "heading": "(**Layers without Parameters**)",
      "text": "Because we are dealing with floating point numbers,\nwe may still see a very small nonzero number\ndue to quantization. :begin_tab:`jax`\nHere we utilize the `init_with_output` method which returns both the output of\nthe network as well as the parameters. In this case we only focus on the\noutput. :end_tab:\n\n```{.python .input}\n%%tab pytorch, mxnet\nY = net(d2l.rand(4, 8))\nY.mean()\n```\n\n```{.python .input}\n%%tab tensorflow\nY = net(tf.random.uniform((4, 8)))\ntf.reduce_mean(Y)\n```\n\n```{.python .input}\n%%tab jax\nY, _ = net.init_with_output(d2l.get_key(), jax.random.uniform(d2l.get_key(),\n                                                              (4, 8)))\nY.mean()\n```"
    },
    {
      "chunk_id": "a173d6150b4c_0",
      "chapter": "custom-layer",
      "heading": "[**Layers with Parameters**]",
      "text": "Now that we know how to define simple layers,\nlet's move on to defining layers with parameters\nthat can be adjusted through training. We can use built-in functions to create parameters, which\nprovide some basic housekeeping functionality. In particular, they govern access, initialization,\nsharing, saving, and loading model parameters. This way, among other benefits, we will not need to write\ncustom serialization routines for every custom layer. Now let's implement our own version of the  fully connected layer. Recall that this layer requires two parameters,\none to represent the weight and the other for the bias. In this implementation, we bake in the ReLU activation as a default. This layer requires two input arguments: `in_units` and `units`, which\ndenote the number of inputs and outputs, respectively."
    },
    {
      "chunk_id": "a173d6150b4c_1",
      "chapter": "custom-layer",
      "heading": "[**Layers with Parameters**]",
      "text": "In this implementation, we bake in the ReLU activation as a default. This layer requires two input arguments: `in_units` and `units`, which\ndenote the number of inputs and outputs, respectively. ```{.python .input}\n%%tab mxnet\nclass MyDense(nn.Block):\n    def __init__(self, units, in_units, **kwargs):\n        super().__init__(**kwargs)\n        self.weight = self.params.get('weight', shape=(in_units, units))\n        self.bias = self.params.get('bias', shape=(units,))\n\n    def forward(self, x):\n        linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(\n            ctx=x.ctx)\n        return npx.relu(linear)\n```\n\n```{.python .input}\n%%tab pytorch\nclass MyLinear(nn.Module):\n    def __init__(self, in_units, units):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(in_units, units))\n        self.bias = nn.Parameter(torch.randn(units,))\n        \n    def forward(self, X):\n        linear = torch.matmul(X, self.weight.data) + self.bias.data\n        return F.relu(linear)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MyDense(tf.keras.Model):\n    def __init__(self, units):\n        super().__init__()\n        self.units = units\n\n    def build(self, X_shape):\n        self.weight = self.add_weight(name='weight',\n            shape=[X_shape[-1], self.units],\n            initializer=tf.random_normal_initializer())\n        self.bias = self.add_weight(\n            name='bias', shape=[self.units],\n            initializer=tf.zeros_initializer())\n\n    def call(self, X):\n        linear = tf.matmul(X, self.weight) + self.bias\n        return tf.nn.relu(linear)\n```\n\n```{.python .input}\n%%tab jax\nclass MyDense(nn.Module):\n    in_units: int\n    units: int\n\n    def setup(self):\n        self.weight = self.param('weight', nn.initializers.normal(stddev=1),\n                                 (self.in_units, self.units))\n        self.bias = self.param('bias', nn.initializers.zeros, self.units)\n\n    def __call__(self, X):\n        linear = jnp.matmul(X, self.weight) + self.bias\n        return nn.relu(linear)\n```\n\n:begin_tab:`mxnet, tensorflow, jax`\nNext, we instantiate the `MyDense` class\nand access its model parameters."
    },
    {
      "chunk_id": "a173d6150b4c_2",
      "chapter": "custom-layer",
      "heading": "[**Layers with Parameters**]",
      "text": ":end_tab:\n\n:begin_tab:`pytorch`\nNext, we instantiate the `MyLinear` class\nand access its model parameters. :end_tab:\n\n```{.python .input}\n%%tab mxnet\ndense = MyDense(units=3, in_units=5)\ndense.params\n```\n\n```{.python .input}\n%%tab pytorch\nlinear = MyLinear(5, 3)\nlinear.weight\n```\n\n```{.python .input}\n%%tab tensorflow\ndense = MyDense(3)\ndense(tf.random.uniform((2, 5)))\ndense.get_weights()\n```\n\n```{.python .input}\n%%tab jax\ndense = MyDense(5, 3)\nparams = dense.init(d2l.get_key(), jnp.zeros((3, 5)))\nparams\n```\n\nWe can [**directly carry out forward propagation calculations using custom layers.**]\n\n```{.python .input}\n%%tab mxnet\ndense.initialize()\ndense(np.random.uniform(size=(2, 5)))\n```\n\n```{.python .input}\n%%tab pytorch\nlinear(torch.rand(2, 5))\n```\n\n```{.python .input}\n%%tab tensorflow\ndense(tf.random.uniform((2, 5)))\n```\n\n```{.python .input}\n%%tab jax\ndense.apply(params, jax.random.uniform(d2l.get_key(),\n                                       (2, 5)))\n```\n\nWe can also (**construct models using custom layers.**)\nOnce we have that we can use it just like the built-in fully connected layer. ```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\nnet.add(MyDense(8, in_units=64),\n        MyDense(1, in_units=8))\nnet.initialize()\nnet(np.random.uniform(size=(2, 64)))\n```\n\n```{.python .input}\n%%tab pytorch\nnet = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\nnet(torch.rand(2, 64))\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([MyDense(8), MyDense(1)])\nnet(tf.random.uniform((2, 64)))\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([MyDense(64, 8), MyDense(8, 1)])\nY, _ = net.init_with_output(d2l.get_key(), jax.random.uniform(d2l.get_key(),\n                                                              (2, 64)))\nY\n```"
    },
    {
      "chunk_id": "6c575a9e8de5_0",
      "chapter": "custom-layer",
      "heading": "Summary",
      "text": "We can design custom layers via the basic layer class. This allows us to define flexible new layers that behave differently from any existing layers in the library.\nOnce defined, custom layers can be invoked in arbitrary contexts and architectures.\nLayers can have local parameters, which can be created through built-in functions."
    },
    {
      "chunk_id": "f4a74bc50dac_0",
      "chapter": "custom-layer",
      "heading": "Exercises",
      "text": "1. Design a layer that takes an input and computes a tensor reduction,\n   i.e., it returns $y_k = \\sum_{i, j} W_{ijk} x_i x_j$.\n1. Design a layer that returns the leading half of the Fourier coefficients of the data.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/58)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/59)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/279)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17993)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Builders' Guide\n:label:`chap_computation`\n\nAlongside giant datasets and powerful hardware,\ngreat software tools have played an indispensable role\nin the rapid progress of deep learning. Starting with the pathbreaking Theano library released in 2007,\nflexible open-source tools have enabled researchers\nto rapidly prototype models, avoiding repetitive work\nwhen recycling standard components\nwhile still maintaining the ability to make low-level modifications. Over time, deep learning's libraries have evolved\nto offer increasingly coarse abstractions. Just as semiconductor designers went from specifying transistors\nto logical circuits to writing code,\nneural networks researchers have moved from thinking about\nthe behavior of individual artificial neurons\nto conceiving of networks in terms of whole layers,\nand now often design architectures with far coarser *blocks* in mind. So far, we have introduced some basic machine learning concepts,\nramping up to fully-functional deep learning models. In the last chapter,\nwe implemented each component of an MLP from scratch\nand even showed how to leverage high-level APIs\nto roll out the same models effortlessly. To get you that far that fast, we *called upon* the libraries,\nbut skipped over more advanced details about *how they work*. In this chapter, we will peel back the curtain,\ndigging deeper into the key components of deep learning computation,\nnamely model construction, parameter access and initialization,\ndesigning custom layers and blocks, reading and writing models to disk,\nand leveraging GPUs to achieve dramatic speedups. These insights will move you from *end user* to *power user*,\ngiving you the tools needed to reap the benefits\nof a mature deep learning library while retaining the flexibility\nto implement more complex models, including those you invent yourself! While this chapter does not introduce any new models or datasets,\nthe advanced modeling chapters that follow rely heavily on these techniques."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "While this chapter does not introduce any new models or datasets,\nthe advanced modeling chapters that follow rely heavily on these techniques. ```toc\n:maxdepth: 2\n\nmodel-construction\nparameters\ninit-param\nlazy-init\ncustom-layer\nread-write\nuse-gpu\n```"
    },
    {
      "chunk_id": "4285443ecdfa_0",
      "chapter": "init-param",
      "heading": "init-param",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Parameter Initialization\n\nNow that we know how to access the parameters,\nlet's look at how to initialize them properly. We discussed the need for proper initialization in :numref:`sec_numerical_stability`. The deep learning framework provides default random initializations to its layers. However, we often want to initialize our weights\naccording to various other protocols. The framework provides most commonly\nused protocols, and also allows to create a custom initializer. ```{.python .input}\n%%tab mxnet\nfrom mxnet import init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```\n\n:begin_tab:`mxnet`\nBy default, MXNet initializes weight parameters by randomly drawing from a uniform distribution $U(-0.07, 0.07)$,\nclearing bias parameters to zero. MXNet's `init` module provides a variety\nof preset initialization methods. :end_tab:\n\n:begin_tab:`pytorch`\nBy default, PyTorch initializes weight and bias matrices\nuniformly by drawing from a range that is computed according to the input and output dimension. PyTorch's `nn.init` module provides a variety\nof preset initialization methods. :end_tab:\n\n:begin_tab:`tensorflow`\nBy default, Keras initializes weight matrices uniformly by drawing from a range that is computed according to the input and output dimension, and the bias parameters are all set to zero. TensorFlow provides a variety of initialization methods both in the root module and the `keras.initializers` module."
    },
    {
      "chunk_id": "4285443ecdfa_1",
      "chapter": "init-param",
      "heading": "init-param",
      "text": "TensorFlow provides a variety of initialization methods both in the root module and the `keras.initializers` module. :end_tab:\n\n:begin_tab:`jax`\nBy default, Flax initializes weights using `jax.nn.initializers.lecun_normal`,\ni.e., by drawing samples from a truncated normal distribution centered on 0 with\nthe standard deviation set as the squared root of $1 / \\textrm{fan}_{\\textrm{in}}$\nwhere `fan_in` is the number of input units in the weight tensor. The bias\nparameters are all set to zero. Jax's `nn.initializers` module provides a variety\nof preset initialization methods. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\nnet.add(nn.Dense(8, activation='relu'))\nnet.add(nn.Dense(1))\nnet.initialize()  # Use the default initialization method\n\nX = np.random.uniform(size=(2, 4))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab pytorch\nnet = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\nX = torch.rand(size=(2, 4))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1),\n])\n\nX = tf.random.uniform((2, 4))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([nn.Dense(8), nn.relu, nn.Dense(1)])\nX = jax.random.uniform(d2l.get_key(), (2, 4))\nparams = net.init(d2l.get_key(), X)\nnet.apply(params, X).shape\n```"
    },
    {
      "chunk_id": "f9faf211f990_0",
      "chapter": "init-param",
      "heading": "[**Built-in Initialization**]",
      "text": "Let's begin by calling on built-in initializers. The code below initializes all weight parameters\nas Gaussian random variables\nwith standard deviation 0.01, while bias parameters are cleared to zero. ```{.python .input}\n%%tab mxnet\n# Here force_reinit ensures that parameters are freshly initialized even if\n# they were already initialized previously\nnet.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\nnet[0].weight.data()[0]\n```\n\n```{.python .input}\n%%tab pytorch\ndef init_normal(module):\n    if type(module) == nn.Linear:\n        nn.init.normal_(module.weight, mean=0, std=0.01)\n        nn.init.zeros_(module.bias)\n\nnet.apply(init_normal)\nnet[0].weight.data[0], net[0].bias.data[0]\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(\n        4, activation=tf.nn.relu,\n        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n        bias_initializer=tf.zeros_initializer()),\n    tf.keras.layers.Dense(1)])\n\nnet(X)\nnet.weights[0], net.weights[1]\n```\n\n```{.python .input}\n%%tab jax\nweight_init = nn.initializers.normal(0.01)\nbias_init = nn.initializers.zeros\n\nnet = nn.Sequential([nn.Dense(8, kernel_init=weight_init, bias_init=bias_init),\n                     nn.relu,\n                     nn.Dense(1, kernel_init=weight_init, bias_init=bias_init)])\n\nparams = net.init(jax.random.PRNGKey(d2l.get_seed()), X)\nlayer_0 = params['params']['layers_0']\nlayer_0['kernel'][:, 0], layer_0['bias'][0]\n```\n\nWe can also initialize all the parameters\nto a given constant value (say, 1)."
    },
    {
      "chunk_id": "f9faf211f990_1",
      "chapter": "init-param",
      "heading": "[**Built-in Initialization**]",
      "text": "```{.python .input}\n%%tab mxnet\nnet.initialize(init=init.Constant(1), force_reinit=True)\nnet[0].weight.data()[0]\n```\n\n```{.python .input}\n%%tab pytorch\ndef init_constant(module):\n    if type(module) == nn.Linear:\n        nn.init.constant_(module.weight, 1)\n        nn.init.zeros_(module.bias)\n\nnet.apply(init_constant)\nnet[0].weight.data[0], net[0].bias.data[0]\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(\n        4, activation=tf.nn.relu,\n        kernel_initializer=tf.keras.initializers.Constant(1),\n        bias_initializer=tf.zeros_initializer()),\n    tf.keras.layers.Dense(1),\n])\n\nnet(X)\nnet.weights[0], net.weights[1]\n```\n\n```{.python .input}\n%%tab jax\nweight_init = nn.initializers.constant(1)\n\nnet = nn.Sequential([nn.Dense(8, kernel_init=weight_init, bias_init=bias_init),\n                     nn.relu,\n                     nn.Dense(1, kernel_init=weight_init, bias_init=bias_init)])\n\nparams = net.init(jax.random.PRNGKey(d2l.get_seed()), X)\nlayer_0 = params['params']['layers_0']\nlayer_0['kernel'][:, 0], layer_0['bias'][0]\n```\n\n[**We can also apply different initializers for certain blocks.**]\nFor example, below we initialize the first layer\nwith the Xavier initializer\nand initialize the second layer\nto a constant value of 42."
    },
    {
      "chunk_id": "f9faf211f990_2",
      "chapter": "init-param",
      "heading": "[**Built-in Initialization**]",
      "text": "```{.python .input}\n%%tab mxnet\nnet[0].weight.initialize(init=init.Xavier(), force_reinit=True)\nnet[1].initialize(init=init.Constant(42), force_reinit=True)\nprint(net[0].weight.data()[0])\nprint(net[1].weight.data())\n```\n\n```{.python .input}\n%%tab pytorch\ndef init_xavier(module):\n    if type(module) == nn.Linear:\n        nn.init.xavier_uniform_(module.weight)\n\ndef init_42(module):\n    if type(module) == nn.Linear:\n        nn.init.constant_(module.weight, 42)\n\nnet[0].apply(init_xavier)\nnet[2].apply(init_42)\nprint(net[0].weight.data[0])\nprint(net[2].weight.data)\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(\n        4,\n        activation=tf.nn.relu,\n        kernel_initializer=tf.keras.initializers.GlorotUniform()),\n    tf.keras.layers.Dense(\n        1, kernel_initializer=tf.keras.initializers.Constant(42)),\n])\n\nnet(X)\nprint(net.layers[1].weights[0])\nprint(net.layers[2].weights[0])\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([nn.Dense(8, kernel_init=nn.initializers.xavier_uniform(),\n                              bias_init=bias_init),\n                     nn.relu,\n                     nn.Dense(1, kernel_init=nn.initializers.constant(42),\n                              bias_init=bias_init)])\n\nparams = net.init(jax.random.PRNGKey(d2l.get_seed()), X)\nparams['params']['layers_0']['kernel'][:, 0], params['params']['layers_2']['kernel']\n```"
    },
    {
      "chunk_id": "c13e74929ea9_0",
      "chapter": "init-param",
      "heading": "[**Custom Initialization**]",
      "text": "Sometimes, the initialization methods we need\nare not provided by the deep learning framework. In the example below, we define an initializer\nfor any weight parameter $w$ using the following strange distribution:\n\n$$\n\\begin{aligned}\n    w \\sim \\begin{cases}\n        U(5, 10) & \\textrm{ with probability } \\frac{1}{4} \\\\\n            0    & \\textrm{ with probability } \\frac{1}{2} \\\\\n        U(-10, -5) & \\textrm{ with probability } \\frac{1}{4}\n    \\end{cases}\n\\end{aligned}\n$$\n\n:begin_tab:`mxnet`\nHere we define a subclass of the `Initializer` class. Usually, we only need to implement the `_init_weight` function\nwhich takes a tensor argument (`data`)\nand assigns to it the desired initialized values. :end_tab:\n\n:begin_tab:`pytorch`\nAgain, we implement a `my_init` function to apply to `net`. :end_tab:\n\n:begin_tab:`tensorflow`\nHere we define a subclass of `Initializer` and implement the `__call__`\nfunction that return a desired tensor given the shape and data type. :end_tab:\n\n:begin_tab:`jax`\nJax initialization functions take as arguments the `PRNGKey`, `shape` and\n`dtype`. Here we implement the function `my_init` that returns a desired\ntensor given the shape and data type."
    },
    {
      "chunk_id": "c13e74929ea9_1",
      "chapter": "init-param",
      "heading": "[**Custom Initialization**]",
      "text": ":end_tab:\n\n:begin_tab:`jax`\nJax initialization functions take as arguments the `PRNGKey`, `shape` and\n`dtype`. Here we implement the function `my_init` that returns a desired\ntensor given the shape and data type. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nclass MyInit(init.Initializer):\n    def _init_weight(self, name, data):\n        print('Init', name, data.shape)\n        data[:] = np.random.uniform(-10, 10, data.shape)\n        data *= np.abs(data) >= 5\n\nnet.initialize(MyInit(), force_reinit=True)\nnet[0].weight.data()[:2]\n```\n\n```{.python .input}\n%%tab pytorch\ndef my_init(module):\n    if type(module) == nn.Linear:\n        print(\"Init\", *[(name, param.shape)\n                        for name, param in module.named_parameters()][0])\n        nn.init.uniform_(module.weight, -10, 10)\n        module.weight.data *= module.weight.data.abs() >= 5\n\nnet.apply(my_init)\nnet[0].weight[:2]\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MyInit(tf.keras.initializers.Initializer):\n    def __call__(self, shape, dtype=None):\n        data=tf.random.uniform(shape, -10, 10, dtype=dtype)\n        factor=(tf.abs(data) >= 5)\n        factor=tf.cast(factor, tf.float32)\n        return data * factor\n\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(\n        4,\n        activation=tf.nn.relu,\n        kernel_initializer=MyInit()),\n    tf.keras.layers.Dense(1),\n])\n\nnet(X)\nprint(net.layers[1].weights[0])\n```\n\n```{.python .input}\n%%tab jax\ndef my_init(key, shape, dtype=jnp.float_):\n    data = jax.random.uniform(key, shape, minval=-10, maxval=10)\n    return data * (jnp.abs(data) >= 5)\n\nnet = nn.Sequential([nn.Dense(8, kernel_init=my_init), nn.relu, nn.Dense(1)])\nparams = net.init(d2l.get_key(), X)\nprint(params['params']['layers_0']['kernel'][:, :2])\n```\n\n:begin_tab:`mxnet, pytorch, tensorflow`\nNote that we always have the option\nof setting parameters directly."
    },
    {
      "chunk_id": "c13e74929ea9_2",
      "chapter": "init-param",
      "heading": "[**Custom Initialization**]",
      "text": ":end_tab:\n\n:begin_tab:`jax`\nWhen initializing parameters in JAX and Flax, the the dictionary of parameters\nreturned has a `flax.core.frozen_dict.FrozenDict` type. It is not advisable in\nthe Jax ecosystem to directly alter the values of an array, hence the datatypes\nare generally immutable. One might use `params.unfreeze()` to make changes. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nnet[0].weight.data()[:] += 1\nnet[0].weight.data()[0, 0] = 42\nnet[0].weight.data()[0]\n```\n\n```{.python .input}\n%%tab pytorch\nnet[0].weight.data[:] += 1\nnet[0].weight.data[0, 0] = 42\nnet[0].weight.data[0]\n```\n\n```{.python .input}\n%%tab tensorflow\nnet.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)\nnet.layers[1].weights[0][0, 0].assign(42)\nnet.layers[1].weights[0]\n```"
    },
    {
      "chunk_id": "3ab63ec0204b_0",
      "chapter": "init-param",
      "heading": "Summary",
      "text": "We can initialize parameters using built-in and custom initializers."
    },
    {
      "chunk_id": "6347485670f2_0",
      "chapter": "init-param",
      "heading": "Exercises",
      "text": "Look up the online documentation for more built-in initializers.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/8089)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/8090)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/8091)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17991)\n:end_tab:"
    },
    {
      "chunk_id": "8482119879c8_0",
      "chapter": "lazy-init",
      "heading": "lazy-init",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Lazy Initialization\n:label:`sec_lazy_init`\n\nSo far, it might seem that we got away\nwith being sloppy in setting up our networks. Specifically, we did the following unintuitive things,\nwhich might not seem like they should work:\n\n* We defined the network architectures\n  without specifying the input dimensionality. * We added layers without specifying\n  the output dimension of the previous layer. * We even \"initialized\" these parameters\n  before providing enough information to determine\n  how many parameters our models should contain. You might be surprised that our code runs at all. After all, there is no way the deep learning framework\ncould tell what the input dimensionality of a network would be. The trick here is that the framework *defers initialization*,\nwaiting until the first time we pass data through the model,\nto infer the sizes of each layer on the fly. Later on, when working with convolutional neural networks,\nthis technique will become even more convenient\nsince the input dimensionality\n(e.g., the resolution of an image)\nwill affect the dimensionality\nof each subsequent layer. Hence the ability to set parameters\nwithout the need to know,\nat the time of writing the code,\nthe value of the dimension\ncan greatly simplify the task of specifying\nand subsequently modifying our models. Next, we go deeper into the mechanics of initialization. ```{.python .input}\n%%tab mxnet\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```\n\nTo begin, let's instantiate an MLP."
    },
    {
      "chunk_id": "8482119879c8_1",
      "chapter": "lazy-init",
      "heading": "lazy-init",
      "text": "```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\nnet.add(nn.Dense(256, activation='relu'))\nnet.add(nn.Dense(10))\n```\n\n```{.python .input}\n%%tab pytorch\nnet = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10),\n])\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(10)])\n```\n\nAt this point, the network cannot possibly know\nthe dimensions of the input layer's weights\nbecause the input dimension remains unknown. :begin_tab:`mxnet, pytorch, tensorflow`\nConsequently the framework has not yet initialized any parameters. We confirm by attempting to access the parameters below. :end_tab:\n\n:begin_tab:`jax`\nAs mentioned in :numref:`subsec_param-access`, parameters and the network definition are decoupled\nin Jax and Flax, and the user handles both manually. Flax models are stateless\nhence there is no `parameters` attribute. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nprint(net.collect_params)\nprint(net.collect_params())\n```\n\n```{.python .input}\n%%tab pytorch\nnet[0].weight\n```\n\n```{.python .input}\n%%tab tensorflow\n[net.layers[i].get_weights() for i in range(len(net.layers))]\n```\n\n:begin_tab:`mxnet`\nNote that while the parameter objects exist,\nthe input dimension to each layer is listed as -1. MXNet uses the special value -1 to indicate\nthat the parameter dimension remains unknown. At this point, attempts to access `net[0].weight.data()`\nwould trigger a runtime error stating that the network\nmust be initialized before the parameters can be accessed. Now let's see what happens when we attempt to initialize\nparameters via the `initialize` method. :end_tab:\n\n:begin_tab:`tensorflow`\nNote that each layer objects exist but the weights are empty. Using `net.get_weights()` would throw an error since the weights\nhave not been initialized yet."
    },
    {
      "chunk_id": "8482119879c8_2",
      "chapter": "lazy-init",
      "heading": "lazy-init",
      "text": ":end_tab:\n\n:begin_tab:`tensorflow`\nNote that each layer objects exist but the weights are empty. Using `net.get_weights()` would throw an error since the weights\nhave not been initialized yet. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nnet.initialize()\nnet.collect_params()\n```\n\n:begin_tab:`mxnet`\nAs we can see, nothing has changed. When input dimensions are unknown,\ncalls to initialize do not truly initialize the parameters. Instead, this call registers to MXNet that we wish\n(and optionally, according to which distribution)\nto initialize the parameters. :end_tab:\n\nNext let's pass data through the network\nto make the framework finally initialize parameters. ```{.python .input}\n%%tab mxnet\nX = np.random.uniform(size=(2, 20))\nnet(X)\n\nnet.collect_params()\n```\n\n```{.python .input}\n%%tab pytorch\nX = torch.rand(2, 20)\nnet(X)\n\nnet[0].weight.shape\n```\n\n```{.python .input}\n%%tab tensorflow\nX = tf.random.uniform((2, 20))\nnet(X)\n[w.shape for w in net.get_weights()]\n```\n\n```{.python .input}\n%%tab jax\nparams = net.init(d2l.get_key(), jnp.zeros((2, 20)))\njax.tree_util.tree_map(lambda x: x.shape, params).tree_flatten_with_keys()\n```\n\nAs soon as we know the input dimensionality,\n20,\nthe framework can identify the shape of the first layer's weight matrix by plugging in the value of 20. Having recognized the first layer's shape, the framework proceeds\nto the second layer,\nand so on through the computational graph\nuntil all shapes are known. Note that in this case,\nonly the first layer requires lazy initialization,\nbut the framework initializes sequentially. Once all parameter shapes are known,\nthe framework can finally initialize the parameters. :begin_tab:`pytorch`\nThe following method\npasses in dummy inputs\nthrough the network\nfor a dry run\nto infer all parameter shapes\nand subsequently initializes the parameters. It will be used later when default random initializations are not desired. :end_tab:\n\n:begin_tab:`jax`\nParameter initialization in Flax is always done manually and handled by the\nuser."
    },
    {
      "chunk_id": "8482119879c8_3",
      "chapter": "lazy-init",
      "heading": "lazy-init",
      "text": "It will be used later when default random initializations are not desired. :end_tab:\n\n:begin_tab:`jax`\nParameter initialization in Flax is always done manually and handled by the\nuser. The following method takes a dummy input and a key dictionary as argument. This key dictionary has the rngs for initializing the model parameters\nand dropout rng for generating the dropout mask for the models with\ndropout layers. More about dropout will be covered later in :numref:`sec_dropout`. Ultimately the method initializes the model returning the parameters. We have been using it under the hood in the previous sections as well. :end_tab:\n\n```{.python .input}\n%%tab pytorch\n@d2l.add_to_class(d2l.Module)  #@save\ndef apply_init(self, inputs, init=None):\n    self.forward(*inputs)\n    if init is not None:\n        self.net.apply(init)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.Module)  #@save\ndef apply_init(self, dummy_input, key):\n    params = self.init(key, *dummy_input)  # dummy_input tuple unpacked\n    return params\n```"
    },
    {
      "chunk_id": "ce1c6894515b_0",
      "chapter": "lazy-init",
      "heading": "Summary",
      "text": "Lazy initialization can be convenient, allowing the framework to infer parameter shapes automatically, making it easy to modify architectures and eliminating one common source of errors.\nWe can pass data through the model to make the framework finally initialize parameters."
    },
    {
      "chunk_id": "2e2e387dc3ed_0",
      "chapter": "lazy-init",
      "heading": "Exercises",
      "text": "1. What happens if you specify the input dimensions to the first layer but not to subsequent layers? Do you get immediate initialization?\n1. What happens if you specify mismatching dimensions?\n1. What would you need to do if you have input of varying dimensionality? Hint: look at the parameter tying.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/280)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/8092)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/281)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17992)\n:end_tab:"
    },
    {
      "chunk_id": "130d95af1971_0",
      "chapter": "model-construction",
      "heading": "model-construction",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Layers and Modules\n:label:`sec_model_construction`\n\nWhen we first introduced neural networks,\nwe focused on linear models with a single output. Here, the entire model consists of just a single neuron. Note that a single neuron\n(i) takes some set of inputs;\n(ii) generates a corresponding scalar output;\nand (iii) has a set of associated parameters that can be updated\nto optimize some objective function of interest. Then, once we started thinking about networks with multiple outputs,\nwe leveraged vectorized arithmetic\nto characterize an entire layer of neurons. Just like individual neurons,\nlayers (i) take a set of inputs,\n(ii) generate corresponding outputs,\nand (iii) are described by a set of tunable parameters. When we worked through softmax regression,\na single layer was itself the model. However, even when we subsequently\nintroduced MLPs,\nwe could still think of the model as\nretaining this same basic structure. Interestingly, for MLPs,\nboth the entire model and its constituent layers\nshare this structure. The entire model takes in raw inputs (the features),\ngenerates outputs (the predictions),\nand possesses parameters\n(the combined parameters from all constituent layers). Likewise, each individual layer ingests inputs\n(supplied by the previous layer)\ngenerates outputs (the inputs to the subsequent layer),\nand possesses a set of tunable parameters that are updated\naccording to the signal that flows backwards\nfrom the subsequent layer. While you might think that neurons, layers, and models\ngive us enough abstractions to go about our business,\nit turns out that we often find it convenient\nto speak about components that are\nlarger than an individual layer\nbut smaller than the entire model. For example, the ResNet-152 architecture,\nwhich is wildly popular in computer vision,\npossesses hundreds of layers. These layers consist of repeating patterns of *groups of layers*."
    },
    {
      "chunk_id": "130d95af1971_1",
      "chapter": "model-construction",
      "heading": "model-construction",
      "text": "For example, the ResNet-152 architecture,\nwhich is wildly popular in computer vision,\npossesses hundreds of layers. These layers consist of repeating patterns of *groups of layers*. Implementing such a network one layer at a time can grow tedious. This concern is not just hypothetical---such\ndesign patterns are common in practice. The ResNet architecture mentioned above\nwon the 2015 ImageNet and COCO computer vision competitions\nfor both recognition and detection :cite:`He.Zhang.Ren.ea.2016`\nand remains a go-to architecture for many vision tasks. Similar architectures in which layers are arranged\nin various repeating patterns\nare now ubiquitous in other domains,\nincluding natural language processing and speech. To implement these complex networks,\nwe introduce the concept of a neural network *module*. A module could describe a single layer,\na component consisting of multiple layers,\nor the entire model itself! One benefit of working with the module abstraction\nis that they can be combined into larger artifacts,\noften recursively. This is illustrated in :numref:`fig_blocks`. By defining code to generate modules\nof arbitrary complexity on demand,\nwe can write surprisingly compact code\nand still implement complex neural networks. ![Multiple layers are combined into modules, forming repeating patterns of larger models.](../img/blocks.svg)\n:label:`fig_blocks`\n\n\nFrom a programming standpoint, a module is represented by a *class*. Any subclass of it must define a forward propagation method\nthat transforms its input into output\nand must store any necessary parameters. Note that some modules do not require any parameters at all. Finally a module must possess a backpropagation method,\nfor purposes of calculating gradients. Fortunately, due to some behind-the-scenes magic\nsupplied by the auto differentiation\n(introduced in :numref:`sec_autograd`)\nwhen defining our own module,\nwe only need to worry about parameters\nand the forward propagation method."
    },
    {
      "chunk_id": "130d95af1971_2",
      "chapter": "model-construction",
      "heading": "model-construction",
      "text": "Fortunately, due to some behind-the-scenes magic\nsupplied by the auto differentiation\n(introduced in :numref:`sec_autograd`)\nwhen defining our own module,\nwe only need to worry about parameters\nand the forward propagation method. ```{.python .input}\n%%tab mxnet\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom typing import List\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```\n\n[**To begin, we revisit the code\nthat we used to implement MLPs**]\n(:numref:`sec_mlp`). The following code generates a network\nwith one fully connected hidden layer\nwith 256 units and ReLU activation,\nfollowed by a fully connected output layer\nwith ten units (no activation function). ```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\nnet.add(nn.Dense(256, activation='relu'))\nnet.add(nn.Dense(10))\nnet.initialize()\n\nX = np.random.uniform(size=(2, 20))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab pytorch\nnet = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n\nX = torch.rand(2, 20)\nnet(X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10),\n])\n\nX = tf.random.uniform((2, 20))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(10)])\n\n# get_key is a d2l saved function returning jax.random.PRNGKey(random_seed)\nX = jax.random.uniform(d2l.get_key(), (2, 20))\nparams = net.init(d2l.get_key(), X)\nnet.apply(params, X).shape\n```\n\n:begin_tab:`mxnet`\nIn this example, we constructed\nour model by instantiating an `nn.Sequential`,\nassigning the returned object to the `net` variable. Next, we repeatedly call its `add` method,\nappending layers in the order\nthat they should be executed."
    },
    {
      "chunk_id": "130d95af1971_3",
      "chapter": "model-construction",
      "heading": "model-construction",
      "text": "Next, we repeatedly call its `add` method,\nappending layers in the order\nthat they should be executed. In short, `nn.Sequential` defines a special kind of `Block`,\nthe class that presents a *module* in Gluon. It maintains an ordered list of constituent `Block`s. The `add` method simply facilitates\nthe addition of each successive `Block` to the list. Note that each layer is an instance of the `Dense` class\nwhich is itself a subclass of `Block`. The forward propagation (`forward`) method is also remarkably simple:\nit chains each `Block` in the list together,\npassing the output of each as input to the next. Note that until now, we have been invoking our models\nvia the construction `net(X)` to obtain their outputs. This is actually just shorthand for `net.forward(X)`,\na slick Python trick achieved via\nthe `Block` class's `__call__` method. :end_tab:\n\n:begin_tab:`pytorch`\nIn this example, we constructed\nour model by instantiating an `nn.Sequential`, with layers in the order\nthat they should be executed passed as arguments. In short, (**`nn.Sequential` defines a special kind of `Module`**),\nthe class that presents a module in PyTorch. It maintains an ordered list of constituent `Module`s. Note that each of the two fully connected layers is an instance of the `Linear` class\nwhich is itself a subclass of `Module`. The forward propagation (`forward`) method is also remarkably simple:\nit chains each module in the list together,\npassing the output of each as input to the next. Note that until now, we have been invoking our models\nvia the construction `net(X)` to obtain their outputs. This is actually just shorthand for `net.__call__(X)`. :end_tab:\n\n:begin_tab:`tensorflow`\nIn this example, we constructed\nour model by instantiating an `keras.models.Sequential`, with layers in the order\nthat they should be executed passed as arguments. In short, `Sequential` defines a special kind of `keras.Model`,\nthe class that presents a module in Keras. It maintains an ordered list of constituent `Model`s."
    },
    {
      "chunk_id": "130d95af1971_4",
      "chapter": "model-construction",
      "heading": "model-construction",
      "text": "In short, `Sequential` defines a special kind of `keras.Model`,\nthe class that presents a module in Keras. It maintains an ordered list of constituent `Model`s. Note that each of the two fully connected layers is an instance of the `Dense` class\nwhich is itself a subclass of `Model`. The forward propagation (`call`) method is also remarkably simple:\nit chains each module in the list together,\npassing the output of each as input to the next. Note that until now, we have been invoking our models\nvia the construction `net(X)` to obtain their outputs. This is actually just shorthand for `net.call(X)`,\na slick Python trick achieved via\nthe module class's `__call__` method. :end_tab:"
    },
    {
      "chunk_id": "b1c3dff1e14d_0",
      "chapter": "model-construction",
      "heading": "[**A Custom Module**]",
      "text": "Perhaps the easiest way to develop intuition\nabout how a module works\nis to implement one ourselves. Before we do that,\nwe briefly summarize the basic functionality\nthat each module must provide:\n\n\n1. Ingest input data as arguments to its forward propagation method. 1. Generate an output by having the forward propagation method return a value. Note that the output may have a different shape from the input. For example, the first fully connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256. 1. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation method. Typically this happens automatically. 1. Store and provide access to those parameters necessary\n   for executing the forward propagation computation. 1. Initialize model parameters as needed. In the following snippet,\nwe code up a module from scratch\ncorresponding to an MLP\nwith one hidden layer with 256 hidden units,\nand a 10-dimensional output layer. Note that the `MLP` class below inherits the class that represents a module. We will heavily rely on the parent class's methods,\nsupplying only our own constructor (the `__init__` method in Python) and the forward propagation method."
    },
    {
      "chunk_id": "b1c3dff1e14d_1",
      "chapter": "model-construction",
      "heading": "[**A Custom Module**]",
      "text": "Note that the `MLP` class below inherits the class that represents a module. We will heavily rely on the parent class's methods,\nsupplying only our own constructor (the `__init__` method in Python) and the forward propagation method. ```{.python .input}\n%%tab mxnet\nclass MLP(nn.Block):\n    def __init__(self):\n        # Call the constructor of the MLP parent class nn.Block to perform\n        # the necessary initialization\n        super().__init__()\n        self.hidden = nn.Dense(256, activation='relu')\n        self.out = nn.Dense(10)\n\n    # Define the forward propagation of the model, that is, how to return the\n    # required model output based on the input X\n    def forward(self, X):\n        return self.out(self.hidden(X))\n```\n\n```{.python .input}\n%%tab pytorch\nclass MLP(nn.Module):\n    def __init__(self):\n        # Call the constructor of the parent class nn.Module to perform\n        # the necessary initialization\n        super().__init__()\n        self.hidden = nn.LazyLinear(256)\n        self.out = nn.LazyLinear(10)\n\n    # Define the forward propagation of the model, that is, how to return the\n    # required model output based on the input X\n    def forward(self, X):\n        return self.out(F.relu(self.hidden(X)))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MLP(tf.keras.Model):\n    def __init__(self):\n        # Call the constructor of the parent class tf.keras.Model to perform\n        # the necessary initialization\n        super().__init__()\n        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n        self.out = tf.keras.layers.Dense(units=10)\n\n    # Define the forward propagation of the model, that is, how to return the\n    # required model output based on the input X\n    def call(self, X):\n        return self.out(self.hidden((X)))\n```\n\n```{.python .input}\n%%tab jax\nclass MLP(nn.Module):\n    def setup(self):\n        # Define the layers\n        self.hidden = nn.Dense(256)\n        self.out = nn.Dense(10)\n\n    # Define the forward propagation of the model, that is, how to return the\n    # required model output based on the input X\n    def __call__(self, X):\n        return self.out(nn.relu(self.hidden(X)))\n```\n\nLet's first focus on the forward propagation method."
    },
    {
      "chunk_id": "b1c3dff1e14d_2",
      "chapter": "model-construction",
      "heading": "[**A Custom Module**]",
      "text": "Note that it takes `X` as input,\ncalculates the hidden representation\nwith the activation function applied,\nand outputs its logits. In this `MLP` implementation,\nboth layers are instance variables. To see why this is reasonable, imagine\ninstantiating two MLPs, `net1` and `net2`,\nand training them on different data. Naturally, we would expect them\nto represent two different learned models. We [**instantiate the MLP's layers**]\nin the constructor\n(**and subsequently invoke these layers**)\non each call to the forward propagation method. Note a few key details. First, our customized `__init__` method\ninvokes the parent class's `__init__` method\nvia `super().__init__()`\nsparing us the pain of restating\nboilerplate code applicable to most modules. We then instantiate our two fully connected layers,\nassigning them to `self.hidden` and `self.out`. Note that unless we implement a new layer,\nwe need not worry about the backpropagation method\nor parameter initialization. The system will generate these methods automatically. Let's try this out. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nnet = MLP()\nif tab.selected('mxnet'):\n    net.initialize()\nnet(X).shape\n```\n\n```{.python .input}\n%%tab jax\nnet = MLP()\nparams = net.init(d2l.get_key(), X)\nnet.apply(params, X).shape\n```\n\nA key virtue of the module abstraction is its versatility. We can subclass a module to create layers\n(such as the fully connected layer class),\nentire models (such as the `MLP` class above),\nor various components of intermediate complexity. We exploit this versatility\nthroughout the coming chapters,\nsuch as when addressing\nconvolutional neural networks."
    },
    {
      "chunk_id": "450eb6dfcb36_0",
      "chapter": "model-construction",
      "heading": "[**The Sequential Module**]",
      "text": ":label:`subsec_model-construction-sequential`\n\nWe can now take a closer look\nat how the `Sequential` class works. Recall that `Sequential` was designed\nto daisy-chain other modules together. To build our own simplified `MySequential`,\nwe just need to define two key methods:\n\n1. A method for appending modules one by one to a list. 1. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended. The following `MySequential` class delivers the same\nfunctionality of the default `Sequential` class. ```{.python .input}\n%%tab mxnet\nclass MySequential(nn.Block):\n    def add(self, block):\n        # Here, block is an instance of a Block subclass, and we assume that\n        # it has a unique name. We save it in the member variable _children of\n        # the Block class, and its type is OrderedDict."
    },
    {
      "chunk_id": "450eb6dfcb36_1",
      "chapter": "model-construction",
      "heading": "[**The Sequential Module**]",
      "text": "We save it in the member variable _children of\n        # the Block class, and its type is OrderedDict. When the MySequential\n        # instance calls the initialize method, the system automatically\n        # initializes all members of _children\n        self._children[block.name] = block\n\n    def forward(self, X):\n        # OrderedDict guarantees that members will be traversed in the order\n        # they were added\n        for block in self._children.values():\n            X = block(X)\n        return X\n```\n\n```{.python .input}\n%%tab pytorch\nclass MySequential(nn.Module):\n    def __init__(self, *args):\n        super().__init__()\n        for idx, module in enumerate(args):\n            self.add_module(str(idx), module)\n\n    def forward(self, X):\n        for module in self.children():            \n            X = module(X)\n        return X\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MySequential(tf.keras.Model):\n    def __init__(self, *args):\n        super().__init__()\n        self.modules = args\n\n    def call(self, X):\n        for module in self.modules:\n            X = module(X)\n        return X\n```\n\n```{.python .input}\n%%tab jax\nclass MySequential(nn.Module):\n    modules: List\n\n    def __call__(self, X):\n        for module in self.modules:\n            X = module(X)\n        return X\n```\n\n:begin_tab:`mxnet`\nThe `add` method adds a single block\nto the ordered dictionary `_children`. You might wonder why every Gluon `Block`\npossesses a `_children` attribute\nand why we used it rather than just\ndefine a Python list ourselves. In short the chief advantage of `_children`\nis that during our block's parameter initialization,\nGluon knows to look inside the `_children`\ndictionary to find sub-blocks whose\nparameters also need to be initialized. :end_tab:\n\n:begin_tab:`pytorch`\nIn the `__init__` method, we add every module\nby calling the `add_modules` method. These modules can be accessed by the `children` method at a later date."
    },
    {
      "chunk_id": "450eb6dfcb36_2",
      "chapter": "model-construction",
      "heading": "[**The Sequential Module**]",
      "text": ":end_tab:\n\n:begin_tab:`pytorch`\nIn the `__init__` method, we add every module\nby calling the `add_modules` method. These modules can be accessed by the `children` method at a later date. In this way the system knows the added modules,\nand it will properly initialize each module's parameters. :end_tab:\n\nWhen our `MySequential`'s forward propagation method is invoked,\neach added module is executed\nin the order in which they were added. We can now reimplement an MLP\nusing our `MySequential` class. ```{.python .input}\n%%tab mxnet\nnet = MySequential()\nnet.add(nn.Dense(256, activation='relu'))\nnet.add(nn.Dense(10))\nnet.initialize()\nnet(X).shape\n```\n\n```{.python .input}\n%%tab pytorch\nnet = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = MySequential(\n    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab jax\nnet = MySequential([nn.Dense(256), nn.relu, nn.Dense(10)])\nparams = net.init(d2l.get_key(), X)\nnet.apply(params, X).shape\n```\n\nNote that this use of `MySequential`\nis identical to the code we previously wrote\nfor the `Sequential` class\n(as described in :numref:`sec_mlp`)."
    },
    {
      "chunk_id": "99f06c037f8c_0",
      "chapter": "model-construction",
      "heading": "[**Executing Code in the Forward Propagation Method**]",
      "text": "The `Sequential` class makes model construction easy,\nallowing us to assemble new architectures\nwithout having to define our own class. However, not all architectures are simple daisy chains. When greater flexibility is required,\nwe will want to define our own blocks. For example, we might want to execute\nPython's control flow within the forward propagation method. Moreover, we might want to perform\narbitrary mathematical operations,\nnot simply relying on predefined neural network layers. You may have noticed that until now,\nall of the operations in our networks\nhave acted upon our network's activations\nand its parameters. Sometimes, however, we might want to\nincorporate terms\nthat are neither the result of previous layers\nnor updatable parameters. We call these *constant parameters*. Say for example that we want a layer\nthat calculates the function\n$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$,\nwhere $\\mathbf{x}$ is the input, $\\mathbf{w}$ is our parameter,\nand $c$ is some specified constant\nthat is not updated during optimization. So we implement a `FixedHiddenMLP` class as follows. ```{.python .input}\n%%tab mxnet\nclass FixedHiddenMLP(nn.Block):\n    def __init__(self):\n        super().__init__()\n        # Random weight parameters created with the get_constant method\n        # are not updated during training (i.e., constant parameters)\n        self.rand_weight = self.params.get_constant(\n            'rand_weight', np.random.uniform(size=(20, 20)))\n        self.dense = nn.Dense(20, activation='relu')\n\n    def forward(self, X):\n        X = self.dense(X)\n        # Use the created constant parameters, as well as the relu and dot\n        # functions\n        X = npx.relu(np.dot(X, self.rand_weight.data()) + 1)\n        # Reuse the fully connected layer."
    },
    {
      "chunk_id": "99f06c037f8c_1",
      "chapter": "model-construction",
      "heading": "[**Executing Code in the Forward Propagation Method**]",
      "text": "This is equivalent to sharing\n        # parameters with two fully connected layers\n        X = self.dense(X)\n        # Control flow\n        while np.abs(X).sum() > 1:\n            X /= 2\n        return X.sum()\n```\n\n```{.python .input}\n%%tab pytorch\nclass FixedHiddenMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Random weight parameters that will not compute gradients and\n        # therefore keep constant during training\n        self.rand_weight = torch.rand((20, 20))\n        self.linear = nn.LazyLinear(20)\n\n    def forward(self, X):\n        X = self.linear(X)        \n        X = F.relu(X @ self.rand_weight + 1)\n        # Reuse the fully connected layer. This is equivalent to sharing\n        # parameters with two fully connected layers\n        X = self.linear(X)\n        # Control flow\n        while X.abs().sum() > 1:\n            X /= 2\n        return X.sum()\n```\n\n```{.python .input}\n%%tab tensorflow\nclass FixedHiddenMLP(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.flatten = tf.keras.layers.Flatten()\n        # Random weight parameters created with tf.constant are not updated\n        # during training (i.e., constant parameters)\n        self.rand_weight = tf.constant(tf.random.uniform((20, 20)))\n        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n\n    def call(self, inputs):\n        X = self.flatten(inputs)\n        # Use the created constant parameters, as well as the relu and\n        # matmul functions\n        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)\n        # Reuse the fully connected layer."
    },
    {
      "chunk_id": "99f06c037f8c_2",
      "chapter": "model-construction",
      "heading": "[**Executing Code in the Forward Propagation Method**]",
      "text": "This is equivalent to sharing\n        # parameters with two fully connected layers\n        X = self.dense(X)\n        # Control flow\n        while tf.reduce_sum(tf.math.abs(X)) > 1:\n            X /= 2\n        return tf.reduce_sum(X)\n```\n\n```{.python .input}\n%%tab jax\nclass FixedHiddenMLP(nn.Module):\n    # Random weight parameters that will not compute gradients and\n    # therefore keep constant during training\n    rand_weight: jnp.array = jax.random.uniform(d2l.get_key(), (20, 20))\n\n    def setup(self):\n        self.dense = nn.Dense(20)\n\n    def __call__(self, X):\n        X = self.dense(X)\n        X = nn.relu(X @ self.rand_weight + 1)\n        # Reuse the fully connected layer. This is equivalent to sharing\n        # parameters with two fully connected layers\n        X = self.dense(X)\n        # Control flow\n        while jnp.abs(X).sum() > 1:\n            X /= 2\n        return X.sum()\n```\n\nIn this model,\nwe implement a hidden layer whose weights\n(`self.rand_weight`) are initialized randomly\nat instantiation and are thereafter constant. This weight is not a model parameter\nand thus it is never updated by backpropagation. The network then passes the output of this \"fixed\" layer\nthrough a fully connected layer. Note that before returning the output,\nour model did something unusual. We ran a while-loop, testing\non the condition its $\\ell_1$ norm is larger than $1$,\nand dividing our output vector by $2$\nuntil it satisfied the condition. Finally, we returned the sum of the entries in `X`. To our knowledge, no standard neural network\nperforms this operation. Note that this particular operation may not be useful\nin any real-world task. Our point is only to show you how to integrate\narbitrary code into the flow of your\nneural network computations."
    },
    {
      "chunk_id": "99f06c037f8c_3",
      "chapter": "model-construction",
      "heading": "[**Executing Code in the Forward Propagation Method**]",
      "text": "Note that this particular operation may not be useful\nin any real-world task. Our point is only to show you how to integrate\narbitrary code into the flow of your\nneural network computations. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nnet = FixedHiddenMLP()\nif tab.selected('mxnet'):\n    net.initialize()\nnet(X)\n```\n\n```{.python .input}\n%%tab jax\nnet = FixedHiddenMLP()\nparams = net.init(d2l.get_key(), X)\nnet.apply(params, X)\n```\n\nWe can [**mix and match various\nways of assembling modules together.**]\nIn the following example, we nest modules\nin some creative ways."
    },
    {
      "chunk_id": "99f06c037f8c_4",
      "chapter": "model-construction",
      "heading": "[**Executing Code in the Forward Propagation Method**]",
      "text": "```{.python .input}\n%%tab mxnet\nclass NestMLP(nn.Block):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.net = nn.Sequential()\n        self.net.add(nn.Dense(64, activation='relu'),\n                     nn.Dense(32, activation='relu'))\n        self.dense = nn.Dense(16, activation='relu')\n\n    def forward(self, X):\n        return self.dense(self.net(X))\n\nchimera = nn.Sequential()\nchimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP())\nchimera.initialize()\nchimera(X)\n```\n\n```{.python .input}\n%%tab pytorch\nclass NestMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n                                 nn.LazyLinear(32), nn.ReLU())\n        self.linear = nn.LazyLinear(16)\n\n    def forward(self, X):\n        return self.linear(self.net(X))\n\nchimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\nchimera(X)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass NestMLP(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.net = tf.keras.Sequential()\n        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n\n    def call(self, inputs):\n        return self.dense(self.net(inputs))\n\nchimera = tf.keras.Sequential()\nchimera.add(NestMLP())\nchimera.add(tf.keras.layers.Dense(20))\nchimera.add(FixedHiddenMLP())\nchimera(X)\n```\n\n```{.python .input}\n%%tab jax\nclass NestMLP(nn.Module):\n    def setup(self):\n        self.net = nn.Sequential([nn.Dense(64), nn.relu,\n                                  nn.Dense(32), nn.relu])\n        self.dense = nn.Dense(16)\n\n    def __call__(self, X):\n        return self.dense(self.net(X))\n\n\nchimera = nn.Sequential([NestMLP(), nn.Dense(20), FixedHiddenMLP()])\nparams = chimera.init(d2l.get_key(), X)\nchimera.apply(params, X)\n```"
    },
    {
      "chunk_id": "fedc4b08f97a_0",
      "chapter": "model-construction",
      "heading": "Summary",
      "text": "Individual layers can be modules.\nMany layers can comprise a module.\nMany modules can comprise a module.\n\nA module can contain code.\nModules take care of lots of housekeeping, including parameter initialization and backpropagation.\nSequential concatenations of layers and modules are handled by the `Sequential` module."
    },
    {
      "chunk_id": "4fda6649178a_0",
      "chapter": "model-construction",
      "heading": "Exercises",
      "text": "1. What kinds of problems will occur if you change `MySequential` to store modules in a Python list?\n1. Implement a module that takes two modules as an argument, say `net1` and `net2` and returns the concatenated output of both networks in the forward propagation. This is also called a *parallel module*.\n1. Assume that you want to concatenate multiple instances of the same network. Implement a factory function that generates multiple instances of the same module and build a larger network from it.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/54)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/55)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/264)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17989)\n:end_tab:"
    },
    {
      "chunk_id": "0584a6d28c5c_0",
      "chapter": "parameters",
      "heading": "parameters",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Parameter Management\n\nOnce we have chosen an architecture\nand set our hyperparameters,\nwe proceed to the training loop,\nwhere our goal is to find parameter values\nthat minimize our loss function. After training, we will need these parameters\nin order to make future predictions. Additionally, we will sometimes wish\nto extract the parameters\nperhaps to reuse them in some other context,\nto save our model to disk so that\nit may be executed in other software,\nor for examination in the hope of\ngaining scientific understanding. Most of the time, we will be able\nto ignore the nitty-gritty details\nof how parameters are declared\nand manipulated, relying on deep learning frameworks\nto do the heavy lifting. However, when we move away from\nstacked architectures with standard layers,\nwe will sometimes need to get into the weeds\nof declaring and manipulating parameters. In this section, we cover the following:\n\n* Accessing parameters for debugging, diagnostics, and visualizations. * Sharing parameters across different model components."
    },
    {
      "chunk_id": "0584a6d28c5c_1",
      "chapter": "parameters",
      "heading": "parameters",
      "text": "In this section, we cover the following:\n\n* Accessing parameters for debugging, diagnostics, and visualizations. * Sharing parameters across different model components. ```{.python .input}\n%%tab mxnet\nfrom mxnet import init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```\n\n(**We start by focusing on an MLP with one hidden layer.**)\n\n```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\nnet.add(nn.Dense(8, activation='relu'))\nnet.add(nn.Dense(1))\nnet.initialize()  # Use the default initialization method\n\nX = np.random.uniform(size=(2, 4))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab pytorch\nnet = nn.Sequential(nn.LazyLinear(8),\n                    nn.ReLU(),\n                    nn.LazyLinear(1))\n\nX = torch.rand(size=(2, 4))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1),\n])\n\nX = tf.random.uniform((2, 4))\nnet(X).shape\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([nn.Dense(8), nn.relu, nn.Dense(1)])\n\nX = jax.random.uniform(d2l.get_key(), (2, 4))\nparams = net.init(d2l.get_key(), X)\nnet.apply(params, X).shape\n```"
    },
    {
      "chunk_id": "cd120fd8b274_0",
      "chapter": "parameters",
      "heading": "[**Parameter Access**]",
      "text": ":label:`subsec_param-access`\n\nLet's start with how to access parameters\nfrom the models that you already know.\n\n:begin_tab:`mxnet, pytorch, tensorflow`\nWhen a model is defined via the `Sequential` class,\nwe can first access any layer by indexing\ninto the model as though it were a list.\nEach layer's parameters are conveniently\nlocated in its attribute.\n:end_tab:\n\n:begin_tab:`jax`\nFlax and JAX decouple the model and the parameters as you\nmight have observed in the models defined previously.\nWhen a model is defined via the `Sequential` class,\nwe first need to initialize the network to generate\nthe parameters dictionary. We can access\nany layer's parameters through the keys of this dictionary.\n:end_tab:\n\nWe can inspect the parameters of the second fully connected layer as follows.\n\n```{.python .input}\n%%tab mxnet\nnet[1].params\n```\n\n```{.python .input}\n%%tab pytorch\nnet[2].state_dict()\n```\n\n```{.python .input}\n%%tab tensorflow\nnet.layers[2].weights\n```\n\n```{.python .input}\n%%tab jax\nparams['params']['layers_2']\n```\n\nWe can see that this fully connected layer\ncontains two parameters,\ncorresponding to that layer's\nweights and biases, respectively."
    },
    {
      "chunk_id": "d8c94ac29ede_0",
      "chapter": "parameters",
      "heading": "[**Targeted Parameters**]",
      "text": "Note that each parameter is represented\nas an instance of the parameter class.\nTo do anything useful with the parameters,\nwe first need to access the underlying numerical values.\nThere are several ways to do this.\nSome are simpler while others are more general.\nThe following code extracts the bias\nfrom the second neural network layer, which returns a parameter class instance, and\nfurther accesses that parameter's value.\n\n```{.python .input}\n%%tab mxnet\ntype(net[1].bias), net[1].bias.data()\n```\n\n```{.python .input}\n%%tab pytorch\ntype(net[2].bias), net[2].bias.data\n```\n\n```{.python .input}\n%%tab tensorflow\ntype(net.layers[2].weights[1]), tf.convert_to_tensor(net.layers[2].weights[1])\n```\n\n```{.python .input}\n%%tab jax\nbias = params['params']['layers_2']['bias']\ntype(bias), bias\n```\n\n:begin_tab:`mxnet,pytorch`\nParameters are complex objects,\ncontaining values, gradients,\nand additional information.\nThat is why we need to request the value explicitly.\n\nIn addition to the value, each parameter also allows us to access the gradient. Because we have not invoked backpropagation for this network yet, it is in its initial state.\n:end_tab:\n\n:begin_tab:`jax`\nUnlike the other frameworks, JAX does not keep a track of the gradients over the\nneural network parameters, instead the parameters and the network are decoupled.\nIt allows the user to express their computation as a\nPython function, and use the `grad` transformation for the same purpose.\n:end_tab:\n\n```{.python .input}\n%%tab mxnet\nnet[1].weight.grad()\n```\n\n```{.python .input}\n%%tab pytorch\nnet[2].weight.grad == None\n```"
    },
    {
      "chunk_id": "9b6dbe0da8f0_0",
      "chapter": "parameters",
      "heading": "[**All Parameters at Once**]",
      "text": "When we need to perform operations on all parameters,\naccessing them one-by-one can grow tedious.\nThe situation can grow especially unwieldy\nwhen we work with more complex, e.g., nested, modules,\nsince we would need to recurse\nthrough the entire tree to extract\neach sub-module's parameters. Below we demonstrate accessing the parameters of all layers.\n\n```{.python .input}\n%%tab mxnet\nnet.collect_params()\n```\n\n```{.python .input}\n%%tab pytorch\n[(name, param.shape) for name, param in net.named_parameters()]\n```\n\n```{.python .input}\n%%tab tensorflow\nnet.get_weights()\n```\n\n```{.python .input}\n%%tab jax\njax.tree_util.tree_map(lambda x: x.shape, params)\n```"
    },
    {
      "chunk_id": "7cf5afabe3cb_0",
      "chapter": "parameters",
      "heading": "[**Tied Parameters**]",
      "text": "Often, we want to share parameters across multiple layers. Let's see how to do this elegantly. In the following we allocate a fully connected layer\nand then use its parameters specifically\nto set those of another layer. Here we need to run the forward propagation\n`net(X)` before accessing the parameters. ```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\n# We need to give the shared layer a name so that we can refer to its\n# parameters\nshared = nn.Dense(8, activation='relu')\nnet.add(nn.Dense(8, activation='relu'),\n        shared,\n        nn.Dense(8, activation='relu', params=shared.params),\n        nn.Dense(10))\nnet.initialize()\n\nX = np.random.uniform(size=(2, 20))\n\nnet(X)\n# Check whether the parameters are the same\nprint(net[1].weight.data()[0] == net[2].weight.data()[0])\nnet[1].weight.data()[0, 0] = 100\n# Make sure that they are actually the same object rather than just having the\n# same value\nprint(net[1].weight.data()[0] == net[2].weight.data()[0])\n```\n\n```{.python .input}\n%%tab pytorch\n# We need to give the shared layer a name so that we can refer to its\n# parameters\nshared = nn.LazyLinear(8)\nnet = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n                    shared, nn.ReLU(),\n                    shared, nn.ReLU(),\n                    nn.LazyLinear(1))\n\nnet(X)\n# Check whether the parameters are the same\nprint(net[2].weight.data[0] == net[4].weight.data[0])\nnet[2].weight.data[0, 0] = 100\n# Make sure that they are actually the same object rather than just having the\n# same value\nprint(net[2].weight.data[0] == net[4].weight.data[0])\n```\n\n```{.python .input}\n%%tab tensorflow\n# tf.keras behaves a bit differently."
    },
    {
      "chunk_id": "7cf5afabe3cb_1",
      "chapter": "parameters",
      "heading": "[**Tied Parameters**]",
      "text": "It removes the duplicate layer\n# automatically\nshared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\nnet = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    shared,\n    shared,\n    tf.keras.layers.Dense(1),\n])\n\nnet(X)\n# Check whether the parameters are different\nprint(len(net.layers) == 3)\n```\n\n```{.python .input}\n%%tab jax\n# We need to give the shared layer a name so that we can refer to its\n# parameters\nshared = nn.Dense(8)\nnet = nn.Sequential([nn.Dense(8), nn.relu,\n                     shared, nn.relu,\n                     shared, nn.relu,\n                     nn.Dense(1)])\n\nparams = net.init(jax.random.PRNGKey(d2l.get_seed()), X)\n\n# Check whether the parameters are different\nprint(len(params['params']) == 3)\n```\n\nThis example shows that the parameters\nof the second and third layer are tied. They are not just equal, they are\nrepresented by the same exact tensor. Thus, if we change one of the parameters,\nthe other one changes, too. :begin_tab:`mxnet, pytorch, tensorflow`\nYou might wonder,\nwhen parameters are tied\nwhat happens to the gradients? Since the model parameters contain gradients,\nthe gradients of the second hidden layer\nand the third hidden layer are added together\nduring backpropagation. :end_tab:"
    },
    {
      "chunk_id": "ae26da98414a_0",
      "chapter": "parameters",
      "heading": "Summary",
      "text": "We have several ways of accessing and tying model parameters."
    },
    {
      "chunk_id": "ec568d7deb98_0",
      "chapter": "parameters",
      "heading": "Exercises",
      "text": "1. Use the `NestMLP` model defined in :numref:`sec_model_construction` and access the parameters of the various layers.\n1. Construct an MLP containing a shared parameter layer and train it. During the training process, observe the model parameters and gradients of each layer.\n1. Why is sharing parameters a good idea?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/56)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/57)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/269)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17990)\n:end_tab:"
    },
    {
      "chunk_id": "765c6084cb37_0",
      "chapter": "read-write",
      "heading": "read-write",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# File I/O\n\nSo far we have discussed how to process data and how\nto build, train, and test deep learning models.\nHowever, at some point we will hopefully be happy enough\nwith the learned models that we will want\nto save the results for later use in various contexts\n(perhaps even to make predictions in deployment).\nAdditionally, when running a long training process,\nthe best practice is to periodically save intermediate results (checkpointing)\nto ensure that we do not lose several days' worth of computation\nif we trip over the power cord of our server.\nThus it is time to learn how to load and store\nboth individual weight vectors and entire models.\nThis section addresses both issues.\n\n```{.python .input}\n%%tab mxnet\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\nimport numpy as np\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nimport flax\nfrom flax import linen as nn\nfrom flax.training import checkpoints\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "5992d35b937d_0",
      "chapter": "read-write",
      "heading": "(**Loading and Saving Tensors**)",
      "text": "For individual tensors, we can directly\ninvoke the `load` and `save` functions\nto read and write them respectively. Both functions require that we supply a name,\nand `save` requires as input the variable to be saved. ```{.python .input}\n%%tab mxnet\nx = np.arange(4)\nnpx.save('x-file', x)\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.arange(4)\ntorch.save(x, 'x-file')\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.range(4)\nnp.save('x-file.npy', x)\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.arange(4)\njnp.save('x-file.npy', x)\n```\n\nWe can now read the data from the stored file back into memory. ```{.python .input}\n%%tab mxnet\nx2 = npx.load('x-file')\nx2\n```\n\n```{.python .input}\n%%tab pytorch\nx2 = torch.load('x-file')\nx2\n```\n\n```{.python .input}\n%%tab tensorflow\nx2 = np.load('x-file.npy', allow_pickle=True)\nx2\n```\n\n```{.python .input}\n%%tab jax\nx2 = jnp.load('x-file.npy', allow_pickle=True)\nx2\n```\n\nWe can [**store a list of tensors and read them back into memory.**]\n\n```{.python .input}\n%%tab mxnet\ny = np.zeros(4)\nnpx.save('x-files', [x, y])\nx2, y2 = npx.load('x-files')\n(x2, y2)\n```\n\n```{.python .input}\n%%tab pytorch\ny = torch.zeros(4)\ntorch.save([x, y],'x-files')\nx2, y2 = torch.load('x-files')\n(x2, y2)\n```\n\n```{.python .input}\n%%tab tensorflow\ny = tf.zeros(4)\nnp.save('xy-files.npy', [x, y])\nx2, y2 = np.load('xy-files.npy', allow_pickle=True)\n(x2, y2)\n```\n\n```{.python .input}\n%%tab jax\ny = jnp.zeros(4)\njnp.save('xy-files.npy', [x, y])\nx2, y2 = jnp.load('xy-files.npy', allow_pickle=True)\n(x2, y2)\n```\n\nWe can even [**write and read a dictionary that maps\nfrom strings to tensors.**]\nThis is convenient when we want\nto read or write all the weights in a model."
    },
    {
      "chunk_id": "5992d35b937d_1",
      "chapter": "read-write",
      "heading": "(**Loading and Saving Tensors**)",
      "text": "```{.python .input}\n%%tab mxnet\nmydict = {'x': x, 'y': y}\nnpx.save('mydict', mydict)\nmydict2 = npx.load('mydict')\nmydict2\n```\n\n```{.python .input}\n%%tab pytorch\nmydict = {'x': x, 'y': y}\ntorch.save(mydict, 'mydict')\nmydict2 = torch.load('mydict')\nmydict2\n```\n\n```{.python .input}\n%%tab tensorflow\nmydict = {'x': x, 'y': y}\nnp.save('mydict.npy', mydict)\nmydict2 = np.load('mydict.npy', allow_pickle=True)\nmydict2\n```\n\n```{.python .input}\n%%tab jax\nmydict = {'x': x, 'y': y}\njnp.save('mydict.npy', mydict)\nmydict2 = jnp.load('mydict.npy', allow_pickle=True)\nmydict2\n```"
    },
    {
      "chunk_id": "fbee2b264b15_0",
      "chapter": "read-write",
      "heading": "[**Loading and Saving Model Parameters**]",
      "text": "Saving individual weight vectors (or other tensors) is useful,\nbut it gets very tedious if we want to save\n(and later load) an entire model. After all, we might have hundreds of\nparameter groups sprinkled throughout. For this reason the deep learning framework provides built-in functionalities\nto load and save entire networks. An important detail to note is that this\nsaves model *parameters* and not the entire model. For example, if we have a 3-layer MLP,\nwe need to specify the architecture separately. The reason for this is that the models themselves can contain arbitrary code,\nhence they cannot be serialized as naturally. Thus, in order to reinstate a model, we need\nto generate the architecture in code\nand then load the parameters from disk."
    },
    {
      "chunk_id": "fbee2b264b15_1",
      "chapter": "read-write",
      "heading": "[**Loading and Saving Model Parameters**]",
      "text": "The reason for this is that the models themselves can contain arbitrary code,\nhence they cannot be serialized as naturally. Thus, in order to reinstate a model, we need\nto generate the architecture in code\nand then load the parameters from disk. (**Let's start with our familiar MLP.**)\n\n```{.python .input}\n%%tab mxnet\nclass MLP(nn.Block):\n    def __init__(self, **kwargs):\n        super(MLP, self).__init__(**kwargs)\n        self.hidden = nn.Dense(256, activation='relu')\n        self.output = nn.Dense(10)\n\n    def forward(self, x):\n        return self.output(self.hidden(x))\n\nnet = MLP()\nnet.initialize()\nX = np.random.uniform(size=(2, 20))\nY = net(X)\n```\n\n```{.python .input}\n%%tab pytorch\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden = nn.LazyLinear(256)\n        self.output = nn.LazyLinear(10)\n\n    def forward(self, x):\n        return self.output(F.relu(self.hidden(x)))\n\nnet = MLP()\nX = torch.randn(size=(2, 20))\nY = net(X)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MLP(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.flatten = tf.keras.layers.Flatten()\n        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n        self.out = tf.keras.layers.Dense(units=10)\n\n    def call(self, inputs):\n        x = self.flatten(inputs)\n        x = self.hidden(x)\n        return self.out(x)\n\nnet = MLP()\nX = tf.random.uniform((2, 20))\nY = net(X)\n```\n\n```{.python .input}\n%%tab jax\nclass MLP(nn.Module):\n    def setup(self):\n        self.hidden = nn.Dense(256)\n        self.output = nn.Dense(10)\n\n    def __call__(self, x):\n        return self.output(nn.relu(self.hidden(x)))\n\nnet = MLP()\nX = jax.random.normal(jax.random.PRNGKey(d2l.get_seed()), (2, 20))\nY, params = net.init_with_output(jax.random.PRNGKey(d2l.get_seed()), X)\n```\n\nNext, we [**store the parameters of the model as a file**] with the name \"mlp.params\"."
    },
    {
      "chunk_id": "fbee2b264b15_2",
      "chapter": "read-write",
      "heading": "[**Loading and Saving Model Parameters**]",
      "text": "```{.python .input}\n%%tab mxnet\nnet.save_parameters('mlp.params')\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.save(net.state_dict(), 'mlp.params')\n```\n\n```{.python .input}\n%%tab tensorflow\nnet.save_weights('mlp.params')\n```\n\n```{.python .input}\n%%tab jax\ncheckpoints.save_checkpoint('ckpt_dir', params, step=1, overwrite=True)\n```\n\nTo recover the model, we instantiate a clone\nof the original MLP model. Instead of randomly initializing the model parameters,\nwe [**read the parameters stored in the file directly**]. ```{.python .input}\n%%tab mxnet\nclone = MLP()\nclone.load_parameters('mlp.params')\n```\n\n```{.python .input}\n%%tab pytorch\nclone = MLP()\nclone.load_state_dict(torch.load('mlp.params'))\nclone.eval()\n```\n\n```{.python .input}\n%%tab tensorflow\nclone = MLP()\nclone.load_weights('mlp.params')\n```\n\n```{.python .input}\n%%tab jax\nclone = MLP()\ncloned_params = flax.core.freeze(checkpoints.restore_checkpoint('ckpt_dir',\n                                                                target=None))\n```\n\nSince both instances have the same model parameters,\nthe computational result of the same input `X` should be the same. Let's verify this. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nY_clone = clone(X)\nY_clone == Y\n```\n\n```{.python .input}\n%%tab jax\nY_clone = clone.apply(cloned_params, X)\nY_clone == Y\n```"
    },
    {
      "chunk_id": "942f90758eda_0",
      "chapter": "read-write",
      "heading": "Summary",
      "text": "The `save` and `load` functions can be used to perform file I/O for tensor objects.\nWe can save and load the entire sets of parameters for a network via a parameter dictionary.\nSaving the architecture has to be done in code rather than in parameters."
    },
    {
      "chunk_id": "f0fb0a2f5cb4_0",
      "chapter": "read-write",
      "heading": "Exercises",
      "text": "1. Even if there is no need to deploy trained models to a different device, what are the practical benefits of storing model parameters?\n1. Assume that we want to reuse only parts of a network to be incorporated into a network having a different architecture. How would you go about using, say the first two layers from a previous network in a new network?\n1. How would you go about saving the network architecture and parameters? What restrictions would you impose on the architecture?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/60)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/61)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/327)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17994)\n:end_tab:"
    },
    {
      "chunk_id": "6aaea8ba715d_0",
      "chapter": "use-gpu",
      "heading": "use-gpu",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# GPUs\n:label:`sec_use_gpu`\n\nIn :numref:`tab_intro_decade`, we illustrated the rapid growth\nof computation over the past two decades. In a nutshell, GPU performance has increased\nby a factor of 1000 every decade since 2000. This offers great opportunities but it also suggests\nthat there was significant demand for such performance. In this section, we begin to discuss how to harness\nthis computational performance for your research. First by using a single GPU and at a later point,\nhow to use multiple GPUs and multiple servers (with multiple GPUs). Specifically, we will discuss how\nto use a single NVIDIA GPU for calculations. First, make sure you have at least one NVIDIA GPU installed. Then, download the [NVIDIA driver and CUDA](https://developer.nvidia.com/cuda-downloads)\nand follow the prompts to set the appropriate path. Once these preparations are complete,\nthe `nvidia-smi` command can be used\nto (**view the graphics card information**). :begin_tab:`mxnet`\nYou might have noticed that a MXNet tensor\nlooks almost identical to a NumPy `ndarray`. But there are a few crucial differences. One of the key features that distinguishes MXNet\nfrom NumPy is its support for diverse hardware devices. In MXNet, every array has a context. So far, by default, all variables\nand associated computation\nhave been assigned to the CPU. Typically, other contexts might be various GPUs. Things can get even hairier when\nwe deploy jobs across multiple servers. By assigning arrays to contexts intelligently,\nwe can minimize the time spent\ntransferring data between devices. For example, when training neural networks on a server with a GPU,\nwe typically prefer for the model's parameters to live on the GPU. Next, we need to confirm that\nthe GPU version of MXNet is installed. If a CPU version of MXNet is already installed,\nwe need to uninstall it first."
    },
    {
      "chunk_id": "6aaea8ba715d_1",
      "chapter": "use-gpu",
      "heading": "use-gpu",
      "text": "Next, we need to confirm that\nthe GPU version of MXNet is installed. If a CPU version of MXNet is already installed,\nwe need to uninstall it first. For example, use the `pip uninstall mxnet` command,\nthen install the corresponding MXNet version\naccording to your CUDA version. Assuming you have CUDA 10.0 installed,\nyou can install the MXNet version\nthat supports CUDA 10.0 via `pip install mxnet-cu100`. :end_tab:\n\n:begin_tab:`pytorch`\nIn PyTorch, every array has a device; we often refer it as a *context*. So far, by default, all variables\nand associated computation\nhave been assigned to the CPU. Typically, other contexts might be various GPUs. Things can get even hairier when\nwe deploy jobs across multiple servers. By assigning arrays to contexts intelligently,\nwe can minimize the time spent\ntransferring data between devices. For example, when training neural networks on a server with a GPU,\nwe typically prefer for the model's parameters to live on the GPU. :end_tab:\n\nTo run the programs in this section,\nyou need at least two GPUs. Note that this might be extravagant for most desktop computers\nbut it is easily available in the cloud, e.g.,\nby using the AWS EC2 multi-GPU instances. Almost all other sections do *not* require multiple GPUs, but here we simply wish to illustrate data flow between different devices. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "1b67f03faed9_0",
      "chapter": "use-gpu",
      "heading": "[**Computing Devices**]",
      "text": "We can specify devices, such as CPUs and GPUs,\nfor storage and calculation. By default, tensors are created in the main memory\nand then the CPU is used for calculations. :begin_tab:`mxnet`\nIn MXNet, the CPU and GPU can be indicated by `cpu()` and `gpu()`. It should be noted that `cpu()`\n(or any integer in the parentheses)\nmeans all physical CPUs and memory. This means that MXNet's calculations\nwill try to use all CPU cores. However, `gpu()` only represents one card\nand the corresponding memory. If there are multiple GPUs, we use `gpu(i)`\nto represent the $i^\\textrm{th}$ GPU ($i$ starts from 0). Also, `gpu(0)` and `gpu()` are equivalent. :end_tab:\n\n:begin_tab:`pytorch`\nIn PyTorch, the CPU and GPU can be indicated by `torch.device('cpu')` and `torch.device('cuda')`. It should be noted that the `cpu` device\nmeans all physical CPUs and memory. This means that PyTorch's calculations\nwill try to use all CPU cores. However, a `gpu` device only represents one card\nand the corresponding memory. If there are multiple GPUs, we use `torch.device(f'cuda:{i}')`\nto represent the $i^\\textrm{th}$ GPU ($i$ starts at 0). Also, `gpu:0` and `gpu` are equivalent."
    },
    {
      "chunk_id": "1b67f03faed9_1",
      "chapter": "use-gpu",
      "heading": "[**Computing Devices**]",
      "text": "However, a `gpu` device only represents one card\nand the corresponding memory. If there are multiple GPUs, we use `torch.device(f'cuda:{i}')`\nto represent the $i^\\textrm{th}$ GPU ($i$ starts at 0). Also, `gpu:0` and `gpu` are equivalent. :end_tab:\n\n```{.python .input}\n%%tab pytorch\ndef cpu():  #@save\n    \"\"\"Get the CPU device.\"\"\"\n    return torch.device('cpu')\n\ndef gpu(i=0):  #@save\n    \"\"\"Get a GPU device.\"\"\"\n    return torch.device(f'cuda:{i}')\n\ncpu(), gpu(), gpu(1)\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow, jax\ndef cpu():  #@save\n    \"\"\"Get the CPU device.\"\"\"\n    if tab.selected('mxnet'):\n        return npx.cpu()\n    if tab.selected('tensorflow'):\n        return tf.device('/CPU:0')\n    if tab.selected('jax'):\n        return jax.devices('cpu')[0]\n\ndef gpu(i=0):  #@save\n    \"\"\"Get a GPU device.\"\"\"\n    if tab.selected('mxnet'):\n        return npx.gpu(i)\n    if tab.selected('tensorflow'):\n        return tf.device(f'/GPU:{i}')\n    if tab.selected('jax'):\n        return jax.devices('gpu')[i]\n\ncpu(), gpu(), gpu(1)\n```\n\nWe can (**query the number of available GPUs.**)\n\n```{.python .input}\n%%tab pytorch\ndef num_gpus():  #@save\n    \"\"\"Get the number of available GPUs.\"\"\"\n    return torch.cuda.device_count()\n\nnum_gpus()\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow, jax\ndef num_gpus():  #@save\n    \"\"\"Get the number of available GPUs.\"\"\"\n    if tab.selected('mxnet'):\n        return npx.num_gpus()\n    if tab.selected('tensorflow'):\n        return len(tf.config.experimental.list_physical_devices('GPU'))\n    if tab.selected('jax'):\n        try:\n            return jax.device_count('gpu')\n        except:\n            return 0  # No GPU backend found\n\nnum_gpus()\n```\n\nNow we [**define two convenient functions that allow us\nto run code even if the requested GPUs do not exist.**]\n\n```{.python .input}\n%%tab all\ndef try_gpu(i=0):  #@save\n    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n    if num_gpus() >= i + 1:\n        return gpu(i)\n    return cpu()\n\ndef try_all_gpus():  #@save\n    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n    return [gpu(i) for i in range(num_gpus())]\n\ntry_gpu(), try_gpu(10), try_all_gpus()\n```"
    },
    {
      "chunk_id": "c83770559256_0",
      "chapter": "use-gpu",
      "heading": "Tensors and GPUs",
      "text": ":begin_tab:`pytorch`\nBy default, tensors are created on the CPU.\nWe can [**query the device where the tensor is located.**]\n:end_tab:\n\n:begin_tab:`mxnet`\nBy default, tensors are created on the CPU.\nWe can [**query the device where the tensor is located.**]\n:end_tab:\n\n:begin_tab:`tensorflow, jax`\nBy default, tensors are created on the GPU/TPU if they are available,\nelse CPU is used if not available.\nWe can [**query the device where the tensor is located.**]\n:end_tab:\n\n```{.python .input}\n%%tab mxnet\nx = np.array([1, 2, 3])\nx.ctx\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.tensor([1, 2, 3])\nx.device\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.constant([1, 2, 3])\nx.device\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.array([1, 2, 3])\nx.device()\n```\n\nIt is important to note that whenever we want\nto operate on multiple terms,\nthey need to be on the same device.\nFor instance, if we sum two tensors,\nwe need to make sure that both arguments\nlive on the same device---otherwise the framework\nwould not know where to store the result\nor even how to decide where to perform the computation."
    },
    {
      "chunk_id": "11c684b2296d_0",
      "chapter": "use-gpu",
      "heading": "Storage on the GPU",
      "text": "There are several ways to [**store a tensor on the GPU.**]\nFor example, we can specify a storage device when creating a tensor.\nNext, we create the tensor variable `X` on the first `gpu`.\nThe tensor created on a GPU only consumes the memory of this GPU.\nWe can use the `nvidia-smi` command to view GPU memory usage.\nIn general, we need to make sure that we do not create data that exceeds the GPU memory limit.\n\n```{.python .input}\n%%tab mxnet\nX = np.ones((2, 3), ctx=try_gpu())\nX\n```\n\n```{.python .input}\n%%tab pytorch\nX = torch.ones(2, 3, device=try_gpu())\nX\n```\n\n```{.python .input}\n%%tab tensorflow\nwith try_gpu():\n    X = tf.ones((2, 3))\nX\n```\n\n```{.python .input}\n%%tab jax\n# By default JAX puts arrays to GPUs or TPUs if available\nX = jax.device_put(jnp.ones((2, 3)), try_gpu())\nX\n```\n\nAssuming that you have at least two GPUs, the following code will (**create a random tensor, `Y`, on the second GPU.**)\n\n```{.python .input}\n%%tab mxnet\nY = np.random.uniform(size=(2, 3), ctx=try_gpu(1))\nY\n```\n\n```{.python .input}\n%%tab pytorch\nY = torch.rand(2, 3, device=try_gpu(1))\nY\n```\n\n```{.python .input}\n%%tab tensorflow\nwith try_gpu(1):\n    Y = tf.random.uniform((2, 3))\nY\n```\n\n```{.python .input}\n%%tab jax\nY = jax.device_put(jax.random.uniform(jax.random.PRNGKey(0), (2, 3)),\n                   try_gpu(1))\nY\n```"
    },
    {
      "chunk_id": "30d81cdcdcda_0",
      "chapter": "use-gpu",
      "heading": "Copying",
      "text": "[**If we want to compute `X + Y`,\nwe need to decide where to perform this operation.**]\nFor instance, as shown in :numref:`fig_copyto`,\nwe can transfer `X` to the second GPU\nand perform the operation there. *Do not* simply add `X` and `Y`,\nsince this will result in an exception. The runtime engine would not know what to do:\nit cannot find data on the same device and it fails. Since `Y` lives on the second GPU,\nwe need to move `X` there before we can add the two. ![Copy data to perform an operation on the same device.](../img/copyto.svg)\n:label:`fig_copyto`\n\n```{.python .input}\n%%tab mxnet\nZ = X.copyto(try_gpu(1))\nprint(X)\nprint(Z)\n```\n\n```{.python .input}\n%%tab pytorch\nZ = X.cuda(1)\nprint(X)\nprint(Z)\n```\n\n```{.python .input}\n%%tab tensorflow\nwith try_gpu(1):\n    Z = X\nprint(X)\nprint(Z)\n```\n\n```{.python .input}\n%%tab jax\nZ = jax.device_put(X, try_gpu(1))\nprint(X)\nprint(Z)\n```\n\nNow that [**the data (both `Z` and `Y`) are on the same GPU), we can add them up.**]\n\n```{.python .input}\n%%tab all\nY + Z\n```\n\n:begin_tab:`mxnet`\nImagine that your variable `Z` already lives on your second GPU. What happens if we still call  `Z.copyto(gpu(1))`? It will make a copy and allocate new memory,\neven though that variable already lives on the desired device. There are times where, depending on the environment our code is running in,\ntwo variables may already live on the same device. So we want to make a copy only if the variables\ncurrently live in different devices. In these cases, we can call `as_in_ctx`. If the variable already live in the specified device\nthen this is a no-op. Unless you specifically want to make a copy,\n`as_in_ctx` is the method of choice. :end_tab:\n\n:begin_tab:`pytorch`\nBut what if your variable `Z` already lived on your second GPU? What happens if we still call `Z.cuda(1)`? It will return `Z` instead of making a copy and allocating new memory. :end_tab:\n\n:begin_tab:`tensorflow`\nImagine that your variable `Z` already lives on your second GPU. What happens if we still call `Z2 = Z` under the same device scope?"
    },
    {
      "chunk_id": "30d81cdcdcda_1",
      "chapter": "use-gpu",
      "heading": "Copying",
      "text": "It will return `Z` instead of making a copy and allocating new memory. :end_tab:\n\n:begin_tab:`tensorflow`\nImagine that your variable `Z` already lives on your second GPU. What happens if we still call `Z2 = Z` under the same device scope? It will return `Z` instead of making a copy and allocating new memory. :end_tab:\n\n:begin_tab:`jax`\nImagine that your variable `Z` already lives on your second GPU. What happens if we still call `Z2 = Z` under the same device scope? It will return `Z` instead of making a copy and allocating new memory. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nZ.as_in_ctx(try_gpu(1)) is Z\n```\n\n```{.python .input}\n%%tab pytorch\nZ.cuda(1) is Z\n```\n\n```{.python .input}\n%%tab tensorflow\nwith try_gpu(1):\n    Z2 = Z\nZ2 is Z\n```\n\n```{.python .input}\n%%tab jax\nZ2 = jax.device_put(Z, try_gpu(1))\nZ2 is Z\n```"
    },
    {
      "chunk_id": "70109cf1125c_0",
      "chapter": "use-gpu",
      "heading": "Side Notes",
      "text": "People use GPUs to do machine learning\nbecause they expect them to be fast.\nBut transferring variables between devices is slow: much slower than computation.\nSo we want you to be 100% certain\nthat you want to do something slow before we let you do it.\nIf the deep learning framework just did the copy automatically\nwithout crashing then you might not realize\nthat you had written some slow code.\n\nTransferring data is not only slow, it also makes parallelization a lot more difficult,\nsince we have to wait for data to be sent (or rather to be received)\nbefore we can proceed with more operations.\nThis is why copy operations should be taken with great care.\nAs a rule of thumb, many small operations\nare much worse than one big operation.\nMoreover, several operations at a time\nare much better than many single operations interspersed in the code\nunless you know what you are doing.\nThis is the case since such operations can block if one device\nhas to wait for the other before it can do something else.\nIt is a bit like ordering your coffee in a queue\nrather than pre-ordering it by phone\nand finding out that it is ready when you are.\n\nLast, when we print tensors or convert tensors to the NumPy format,\nif the data is not in the main memory,\nthe framework will copy it to the main memory first,\nresulting in additional transmission overhead.\nEven worse, it is now subject to the dreaded global interpreter lock\nthat makes everything wait for Python to complete."
    },
    {
      "chunk_id": "dbb0a7499336_0",
      "chapter": "use-gpu",
      "heading": "[**Neural Networks and GPUs**]",
      "text": "Similarly, a neural network model can specify devices. The following code puts the model parameters on the GPU. ```{.python .input}\n%%tab mxnet\nnet = nn.Sequential()\nnet.add(nn.Dense(1))\nnet.initialize(ctx=try_gpu())\n```\n\n```{.python .input}\n%%tab pytorch\nnet = nn.Sequential(nn.LazyLinear(1))\nnet = net.to(device=try_gpu())\n```\n\n```{.python .input}\n%%tab tensorflow\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n    net = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(1)])\n```\n\n```{.python .input}\n%%tab jax\nnet = nn.Sequential([nn.Dense(1)])\n\nkey1, key2 = jax.random.split(jax.random.PRNGKey(0))\nx = jax.random.normal(key1, (10,))  # Dummy input\nparams = net.init(key2, x)  # Initialization call\n```\n\nWe will see many more examples of\nhow to run models on GPUs in the following chapters,\nsimply because the models will become somewhat more computationally intensive. For example, when the input is a tensor on the GPU, the model will calculate the result on the same GPU. ```{.python .input}\n%%tab mxnet, pytorch, tensorflow\nnet(X)\n```\n\n```{.python .input}\n%%tab jax\nnet.apply(params, x)\n```\n\nLet's (**confirm that the model parameters are stored on the same GPU.**)\n\n```{.python .input}\n%%tab mxnet\nnet[0].weight.data().ctx\n```\n\n```{.python .input}\n%%tab pytorch\nnet[0].weight.data.device\n```\n\n```{.python .input}\n%%tab tensorflow\nnet.layers[0].weights[0].device, net.layers[0].weights[1].device\n```\n\n```{.python .input}\n%%tab jax\nprint(jax.tree_util.tree_map(lambda x: x.device(), params))\n```\n\nLet the trainer support GPU."
    },
    {
      "chunk_id": "dbb0a7499336_1",
      "chapter": "use-gpu",
      "heading": "[**Neural Networks and GPUs**]",
      "text": "```{.python .input}\n%%tab mxnet\n@d2l.add_to_class(d2l.Module)  #@save\ndef set_scratch_params_device(self, device):\n    for attr in dir(self):\n        a = getattr(self, attr)\n        if isinstance(a, np.ndarray):\n            with autograd.record():\n                setattr(self, attr, a.as_in_ctx(device))\n            getattr(self, attr).attach_grad()\n        if isinstance(a, d2l.Module):\n            a.set_scratch_params_device(device)\n        if isinstance(a, list):\n            for elem in a:\n                elem.set_scratch_params_device(device)\n```\n\n```{.python .input}\n%%tab mxnet, pytorch\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n    self.save_hyperparameters()\n    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef prepare_batch(self, batch):\n    if self.gpus:\n        batch = [d2l.to(a, self.gpus[0]) for a in batch]\n    return batch\n\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef prepare_model(self, model):\n    model.trainer = self\n    model.board.xlim = [0, self.max_epochs]\n    if self.gpus:\n        if tab.selected('mxnet'):\n            model.collect_params().reset_ctx(self.gpus[0])\n            model.set_scratch_params_device(self.gpus[0])\n        if tab.selected('pytorch'):\n            model.to(self.gpus[0])\n    self.model = model\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n    self.save_hyperparameters()\n    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef prepare_batch(self, batch):\n    if self.gpus:\n        batch = [d2l.to(a, self.gpus[0]) for a in batch]\n    return batch\n```\n\nIn short, as long as all data and parameters are on the same device, we can learn models efficiently. In the following chapters we will see several such examples."
    },
    {
      "chunk_id": "6ee8714c458d_0",
      "chapter": "use-gpu",
      "heading": "Summary",
      "text": "We can specify devices for storage and calculation, such as the CPU or GPU.\n  By default, data is created in the main memory\n  and then uses the CPU for calculations.\nThe deep learning framework requires all input data for calculation\n  to be on the same device,\n  be it CPU or the same GPU.\nYou can lose significant performance by moving data without care.\n  A typical mistake is as follows: computing the loss\n  for every minibatch on the GPU and reporting it back\n  to the user on the command line (or logging it in a NumPy `ndarray`)\n  will trigger a global interpreter lock which stalls all GPUs.\n  It is much better to allocate memory\n  for logging inside the GPU and only move larger logs."
    },
    {
      "chunk_id": "74cf4c3da332_0",
      "chapter": "use-gpu",
      "heading": "Exercises",
      "text": "1. Try a larger computation task, such as the multiplication of large matrices,\n   and see the difference in speed between the CPU and GPU.\n   What about a task with a small number of calculations?\n1. How should we read and write model parameters on the GPU?\n1. Measure the time it takes to compute 1000\n   matrix--matrix multiplications of $100 \\times 100$ matrices\n   and log the Frobenius norm of the output matrix one result at a time. Compare it with keeping a log on the GPU and transferring only the final result.\n1. Measure how much time it takes to perform two matrix--matrix multiplications\n   on two GPUs at the same time. Compare it with computing in in sequence\n   on one GPU. Hint: you should see almost linear scaling.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/62)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/63)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/270)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17995)\n:end_tab:"
    },
    {
      "chunk_id": "242dd99585e3_0",
      "chapter": "async-computation",
      "heading": "async-computation",
      "text": "# Asynchronous Computation\n:label:`sec_async`\n\nToday's computers are highly parallel systems, consisting of multiple CPU cores (often multiple threads per core), multiple processing elements per GPU, and often multiple GPUs per device. In short, we can process many different things at the same time, often on different devices. Unfortunately Python is not a great way of writing parallel and asynchronous code, at least not without some extra help. After all, Python is single-threaded and this is unlikely to change in the future. Deep learning frameworks such as MXNet and TensorFlow adopt an *asynchronous programming* model to improve performance,\nwhile PyTorch uses Python's own scheduler leading to a different performance trade-off.\nFor PyTorch, by default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on the CPU or other GPUs.\n\nHence, understanding how asynchronous programming works helps us to develop more efficient programs, by proactively reducing computational requirements and mutual dependencies. This allows us to reduce memory overhead and increase processor utilization.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nimport numpy, os, subprocess\nfrom mxnet import autograd, gluon, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport numpy, os, subprocess\nimport torch\nfrom torch import nn\n```"
    },
    {
      "chunk_id": "8934a972cfdf_0",
      "chapter": "async-computation",
      "heading": "Asynchrony via Backend",
      "text": ":begin_tab:`mxnet`\nFor a warmup consider the following toy problem: we want to generate a random matrix and multiply it. Let's do that both in NumPy and in `mxnet.np` to see the difference. :end_tab:\n\n:begin_tab:`pytorch`\nFor a warmup consider the following toy problem: we want to generate a random matrix and multiply it. Let's do that both in NumPy and in PyTorch tensor to see the difference. Note that PyTorch `tensor` is defined on a GPU. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nwith d2l.Benchmark('numpy'):\n    for _ in range(10):\n        a = numpy.random.normal(size=(1000, 1000))\n        b = numpy.dot(a, a)\n\nwith d2l.Benchmark('mxnet.np'):\n    for _ in range(10):\n        a = np.random.normal(size=(1000, 1000))\n        b = np.dot(a, a)\n```\n\n```{.python .input}\n#@tab pytorch\n# Warmup for GPU computation\ndevice = d2l.try_gpu()\na = torch.randn(size=(1000, 1000), device=device)\nb = torch.mm(a, a)\n\nwith d2l.Benchmark('numpy'):\n    for _ in range(10):\n        a = numpy.random.normal(size=(1000, 1000))\n        b = numpy.dot(a, a)\n\nwith d2l.Benchmark('torch'):\n    for _ in range(10):\n        a = torch.randn(size=(1000, 1000), device=device)\n        b = torch.mm(a, a)\n```\n\n:begin_tab:`mxnet`\nThe benchmark output via MXNet is orders of magnitude faster. Since both are executed on the same processor something else must be going on. Forcing MXNet to finish all the backend computation prior to returning shows what happened previously: computation is executed by the backend while the frontend returns control to Python. :end_tab:\n\n:begin_tab:`pytorch`\nThe benchmark output via PyTorch is orders of magnitude faster. NumPy dot product is executed on the CPU processor while\nPyTorch matrix multiplication is executed on GPU and hence the latter\nis expected to be much faster. But the huge time difference suggests something\nelse must be going on. By default, GPU operations are asynchronous in PyTorch."
    },
    {
      "chunk_id": "8934a972cfdf_1",
      "chapter": "async-computation",
      "heading": "Asynchrony via Backend",
      "text": "But the huge time difference suggests something\nelse must be going on. By default, GPU operations are asynchronous in PyTorch. Forcing PyTorch to finish all computation prior to returning shows\nwhat happened previously: computation is being executed by the backend\nwhile the frontend returns control to Python. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nwith d2l.Benchmark():\n    for _ in range(10):\n        a = np.random.normal(size=(1000, 1000))\n        b = np.dot(a, a)\n    npx.waitall()\n```\n\n```{.python .input}\n#@tab pytorch\nwith d2l.Benchmark():\n    for _ in range(10):\n        a = torch.randn(size=(1000, 1000), device=device)\n        b = torch.mm(a, a)\n    torch.cuda.synchronize(device)\n```\n\n:begin_tab:`mxnet`\nBroadly speaking, MXNet has a frontend for direct interactions with users, e.g., via Python, as well as a backend used by the system to perform the computation. As shown in :numref:`fig_frontends`, users can write MXNet programs in various frontend languages, such as Python, R, Scala, and C++. Regardless of the frontend programming language used, the execution of MXNet programs occurs primarily in the backend of C++ implementations. Operations issued by the frontend language are passed on to the backend for execution. The backend manages its own threads that continuously collect and execute queued tasks. Note that for this to work the backend must be able to keep track of the dependencies between various steps in the computational graph. Hence, it is not possible to parallelize operations that depend on each other. :end_tab:\n\n:begin_tab:`pytorch`\nBroadly speaking, PyTorch has a frontend for direct interaction with the users, e.g., via Python, as well as a backend used by the system to perform the computation. As shown in :numref:`fig_frontends`, users can write PyTorch programs in various frontend languages, such as Python and C++. Regardless of the frontend programming language used, the execution of PyTorch programs occurs primarily in the backend of C++ implementations."
    },
    {
      "chunk_id": "8934a972cfdf_2",
      "chapter": "async-computation",
      "heading": "Asynchrony via Backend",
      "text": "Regardless of the frontend programming language used, the execution of PyTorch programs occurs primarily in the backend of C++ implementations. Operations issued by the frontend language are passed on to the backend for execution. The backend manages its own threads that continuously collect and execute queued tasks. Note that for this to work the backend must be able to keep track of the\ndependencies between various steps in the computational graph. Hence, it is not possible to parallelize operations that depend on each other. :end_tab:\n\n![Programming language frontends and deep learning framework backends.](../img/frontends.png)\n:width:`300px`\n:label:`fig_frontends`\n\nLet's look at another toy example to understand the dependency graph a bit better. ```{.python .input}\n#@tab mxnet\nx = np.ones((1, 2))\ny = np.ones((1, 2))\nz = x * y + 2\nz\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.ones((1, 2), device=device)\ny = torch.ones((1, 2), device=device)\nz = x * y + 2\nz\n```\n\n![The backend tracks dependencies between various steps in the computational graph.](../img/asyncgraph.svg)\n:label:`fig_asyncgraph`\n\n\n\nThe code snippet above is also illustrated in :numref:`fig_asyncgraph`. Whenever the Python frontend thread executes one of the first three statements, it simply returns the task to the backend queue. When the last statement's results need to be *printed*, the Python frontend thread will wait for the C++ backend thread to finish computing the result of the variable `z`. One benefit of this design is that the Python frontend thread does not need to perform actual computations. Thus, there is little impact on the program's overall performance, regardless of Python's performance. :numref:`fig_threading` illustrates how frontend and backend interact. ![Interactions of the frontend and backend.](../img/threading.svg)\n:label:`fig_threading`"
    },
    {
      "chunk_id": "8cc8ee82c339_0",
      "chapter": "async-computation",
      "heading": "Barriers and Blockers",
      "text": ":begin_tab:`mxnet`\nThere are a number of operations that will force Python to wait for completion:\n\n* Most obviously `npx.waitall()` waits until all computation has completed, regardless of when the compute instructions were issued. In practice it is a bad idea to use this operator unless absolutely necessary since it can lead to poor performance.\n* If we just want to wait until a specific variable is available we can call `z.wait_to_read()`. In this case MXNet blocks return to Python until the variable `z` has been computed. Other computation may well continue afterwards.\n\nLet's see how this works in practice.\n:end_tab:\n\n```{.python .input}\n#@tab mxnet\nwith d2l.Benchmark('waitall'):\n    b = np.dot(a, a)\n    npx.waitall()\n\nwith d2l.Benchmark('wait_to_read'):\n    b = np.dot(a, a)\n    b.wait_to_read()\n```\n\n:begin_tab:`mxnet`\nBoth operations take approximately the same time to complete. Besides the obvious blocking operations we recommend that you are aware of *implicit* blockers. Printing a variable clearly requires the variable to be available and is thus a blocker. Last, conversions to NumPy via `z.asnumpy()` and conversions to scalars via `z.item()` are blocking, since NumPy has no notion of asynchrony. It needs access to the values just like the `print` function. \n\nCopying small amounts of data frequently from MXNet's scope to NumPy and back can destroy performance of an otherwise efficient code, since each such operation requires the computational graph to evaluate all intermediate results needed to get the relevant term *before* anything else can be done.\n:end_tab:\n\n```{.python .input}\n#@tab mxnet\nwith d2l.Benchmark('numpy conversion'):\n    b = np.dot(a, a)\n    b.asnumpy()\n\nwith d2l.Benchmark('scalar conversion'):\n    b = np.dot(a, a)\n    b.sum().item()\n```"
    },
    {
      "chunk_id": "0bdf6ff57d61_0",
      "chapter": "async-computation",
      "heading": "Improving Computation",
      "text": ":begin_tab:`mxnet`\nOn a heavily multithreaded system (even regular laptops have 4 threads or more and on multi-socket servers this number can exceed 256) the overhead of scheduling operations can become significant. This is why it is highly desirable to have computation and scheduling occur asynchronously and in parallel. To illustrate the benefit of doing so let's see what happens if we increment a variable by 1 multiple times, both in sequence or asynchronously. We simulate synchronous execution by inserting a `wait_to_read` barrier in between each addition.\n:end_tab:\n\n```{.python .input}\n#@tab mxnet\nwith d2l.Benchmark('synchronous'):\n    for _ in range(10000):\n        y = x + 1\n        y.wait_to_read()\n\nwith d2l.Benchmark('asynchronous'):\n    for _ in range(10000):\n        y = x + 1\n    npx.waitall()\n```\n\n:begin_tab:`mxnet`\nA slightly simplified interaction between the Python frontend thread and the C++ backend thread can be summarized as follows:\n1. The frontend orders the backend to insert the computation task `y = x + 1` into the queue.\n1. The backend then receives the computation tasks from the queue and performs the actual computations.\n1. The backend then returns the computation results to the frontend.\nAssume that the durations of these three stages are $t_1, t_2$ and $t_3$, respectively. If we do not use asynchronous programming, the total time taken to perform 10000 computations is approximately $10000 (t_1+ t_2 + t_3)$. If asynchronous programming is used, the total time taken to perform 10000 computations can be reduced to $t_1 + 10000 t_2 + t_3$ (assuming $10000 t_2 > 9999t_1$), since the frontend does not have to wait for the backend to return computation results for each loop.\n:end_tab:"
    },
    {
      "chunk_id": "f82de6bae42a_0",
      "chapter": "async-computation",
      "heading": "Summary",
      "text": "* Deep learning frameworks may decouple the Python frontend from an execution backend. This allows for fast asynchronous insertion of commands into the backend and associated parallelism.\n* Asynchrony leads to a rather responsive frontend. However, use caution not to overfill the task queue since it may lead to excessive memory consumption. It is recommended to synchronize for each minibatch to keep frontend and backend approximately synchronized.\n* Chip vendors offer sophisticated performance analysis tools to obtain a much more fine-grained insight into the efficiency of deep learning.\n\n:begin_tab:`mxnet`\n* Be aware of the fact that conversions from MXNet's memory management to Python will force the backend to wait until  the specific variable is ready. Functions such as `print`, `asnumpy` and `item` all have this effect. This can be desirable but a careless use of synchronization can ruin performance.\n:end_tab:"
    },
    {
      "chunk_id": "cdec5e8324e4_0",
      "chapter": "async-computation",
      "heading": "Exercises",
      "text": ":begin_tab:`mxnet`\n1. We mentioned above that using asynchronous computation can reduce the total amount of time needed to perform 10000 computations to $t_1 + 10000 t_2 + t_3$. Why do we have to assume $10000 t_2 > 9999 t_1$ here?\n1. Measure the difference between `waitall` and `wait_to_read`. Hint: perform a number of instructions and synchronize for an intermediate result.\n:end_tab:\n\n:begin_tab:`pytorch`\n1. On the CPU, benchmark the same matrix multiplication operations in this section. Can you still observe asynchrony via the backend?\n:end_tab:\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/361)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/2564)\n:end_tab:"
    },
    {
      "chunk_id": "b9f54e6e93ac_0",
      "chapter": "auto-parallelism",
      "heading": "auto-parallelism",
      "text": "# Automatic Parallelism\n:label:`sec_auto_para`\n\n\nDeep learning frameworks (e.g., MXNet and PyTorch) automatically construct computational graphs at the backend. Using a\ncomputational graph, the system is aware of all the dependencies,\nand can selectively execute multiple non-interdependent tasks in parallel to\nimprove speed. For instance, :numref:`fig_asyncgraph` in :numref:`sec_async` initializes two variables independently. Consequently the system can choose to execute them in parallel.\n\n\nTypically, a single operator will use all the computational resources on all CPUs or on a single GPU. For example, the `dot` operator will use all cores (and threads) on all CPUs, even if there are multiple CPU processors on a single machine. The same applies to a single GPU. Hence parallelization is not quite so useful for single-device computers. With multiple devices things matter more. While parallelization is typically most relevant between multiple GPUs, adding the local CPU will increase performance slightly. For example, see :citet:`Hadjis.Zhang.Mitliagkas.ea.2016` that focuses on training computer vision models combining a GPU and a CPU. With the convenience of an automatically parallelizing framework we can accomplish the same goal in a few lines of Python code. More broadly, our discussion of automatic parallel computation focuses on parallel computation using both CPUs and GPUs, as well as the parallelization of computation and communication.\n\nNote that we need at least two GPUs to run the experiments in this section.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\n```"
    },
    {
      "chunk_id": "7d0f2806b3f1_0",
      "chapter": "auto-parallelism",
      "heading": "Parallel Computation on GPUs",
      "text": "Let's start by defining a reference workload to test: the `run` function below performs 10 matrix-matrix multiplications on the device of our choice using data allocated into two variables: `x_gpu1` and `x_gpu2`. ```{.python .input}\n#@tab mxnet\ndevices = d2l.try_all_gpus()\ndef run(x):\n    return [x.dot(x) for _ in range(50)]\n\nx_gpu1 = np.random.uniform(size=(4000, 4000), ctx=devices[0])\nx_gpu2 = np.random.uniform(size=(4000, 4000), ctx=devices[1])\n```\n\n```{.python .input}\n#@tab pytorch\ndevices = d2l.try_all_gpus()\ndef run(x):\n    return [x.mm(x) for _ in range(50)]\n\nx_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])\nx_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])\n```\n\n:begin_tab:`mxnet`\nNow we apply the function to the data. To ensure that caching does not play a role in the results we warm up the devices by performing a single pass on either of them prior to measuring. :end_tab:\n\n:begin_tab:`pytorch`\nNow we apply the function to the data. To ensure that caching does not play a role in the results we warm up the devices by performing a single pass on either of them prior to measuring. `torch.cuda.synchronize()` waits for all kernels in all streams on a CUDA device to complete. It takes in a `device` argument, the device for which we need to synchronize. It uses the current device, given by `current_device()`, if the device argument is `None` (default)."
    },
    {
      "chunk_id": "7d0f2806b3f1_1",
      "chapter": "auto-parallelism",
      "heading": "Parallel Computation on GPUs",
      "text": "It takes in a `device` argument, the device for which we need to synchronize. It uses the current device, given by `current_device()`, if the device argument is `None` (default). :end_tab:\n\n```{.python .input}\n#@tab mxnet\nrun(x_gpu1)  # Warm-up both devices\nrun(x_gpu2)\nnpx.waitall()\n\nwith d2l.Benchmark('GPU1 time'):\n    run(x_gpu1)\n    npx.waitall()\n\nwith d2l.Benchmark('GPU2 time'):\n    run(x_gpu2)\n    npx.waitall()\n```\n\n```{.python .input}\n#@tab pytorch\nrun(x_gpu1)\nrun(x_gpu2)  # Warm-up all devices\ntorch.cuda.synchronize(devices[0])\ntorch.cuda.synchronize(devices[1])\n\nwith d2l.Benchmark('GPU1 time'):\n    run(x_gpu1)\n    torch.cuda.synchronize(devices[0])\n\nwith d2l.Benchmark('GPU2 time'):\n    run(x_gpu2)\n    torch.cuda.synchronize(devices[1])\n```\n\n:begin_tab:`mxnet`\nIf we remove the `waitall` statement between both tasks the system is free to parallelize computation on both devices automatically. :end_tab:\n\n:begin_tab:`pytorch`\nIf we remove the `synchronize` statement between both tasks the system is free to parallelize computation on both devices automatically. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nwith d2l.Benchmark('GPU1 & GPU2'):\n    run(x_gpu1)\n    run(x_gpu2)\n    npx.waitall()\n```\n\n```{.python .input}\n#@tab pytorch\nwith d2l.Benchmark('GPU1 & GPU2'):\n    run(x_gpu1)\n    run(x_gpu2)\n    torch.cuda.synchronize()\n```\n\nIn the above case the total execution time is less than the sum of its parts, since the deep learning framework automatically schedules computation on both GPU devices without the need for sophisticated code on behalf of the user."
    },
    {
      "chunk_id": "2b2ba13705e1_0",
      "chapter": "auto-parallelism",
      "heading": "Parallel Computation and Communication",
      "text": "In many cases we need to move data between different devices, say between the CPU and GPU, or between different GPUs. For instance,\nthis occurs when we want to perform distributed optimization where we need to aggregate the gradients over multiple accelerator cards. Let's simulate this by computing on the GPU and then copying the results back to the CPU. ```{.python .input}\n#@tab mxnet\ndef copy_to_cpu(x):\n    return [y.copyto(npx.cpu()) for y in x]\n\nwith d2l.Benchmark('Run on GPU1'):\n    y = run(x_gpu1)\n    npx.waitall()\n\nwith d2l.Benchmark('Copy to CPU'):\n    y_cpu = copy_to_cpu(y)\n    npx.waitall()\n```\n\n```{.python .input}\n#@tab pytorch\ndef copy_to_cpu(x, non_blocking=False):\n    return [y.to('cpu', non_blocking=non_blocking) for y in x]\n\nwith d2l.Benchmark('Run on GPU1'):\n    y = run(x_gpu1)\n    torch.cuda.synchronize()\n\nwith d2l.Benchmark('Copy to CPU'):\n    y_cpu = copy_to_cpu(y)\n    torch.cuda.synchronize()\n```\n\n:begin_tab:`mxnet`\nThis is somewhat inefficient. Note that we could already start copying parts of `y` to the CPU while the remainder of the list is still being computed. This situation occurs, e.g., when we compute the gradient on a minibatch. The gradients of some of the parameters will be available earlier than that of others. Hence it works to our advantage to start using PCI-Express bus bandwidth while the GPU is still running. Removing `waitall` between both parts allows us to simulate this scenario. :end_tab:\n\n:begin_tab:`pytorch`\nThis is somewhat inefficient. Note that we could already start copying parts of `y` to the CPU while the remainder of the list is still being computed. This situation occurs, e.g., when we compute the (backprop) gradient on a minibatch. The gradients of some of the parameters will be available earlier than that of others. Hence it works to our advantage to start using PCI-Express bus bandwidth while the GPU is still running."
    },
    {
      "chunk_id": "2b2ba13705e1_1",
      "chapter": "auto-parallelism",
      "heading": "Parallel Computation and Communication",
      "text": "The gradients of some of the parameters will be available earlier than that of others. Hence it works to our advantage to start using PCI-Express bus bandwidth while the GPU is still running. In PyTorch, several functions such as `to()` and `copy_()` admit an explicit `non_blocking` argument, which lets the caller bypass synchronization when it is unnecessary. Setting `non_blocking=True` allows us to simulate this scenario. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nwith d2l.Benchmark('Run on GPU1 and copy to CPU'):\n    y = run(x_gpu1)\n    y_cpu = copy_to_cpu(y)\n    npx.waitall()\n```\n\n```{.python .input}\n#@tab pytorch\nwith d2l.Benchmark('Run on GPU1 and copy to CPU'):\n    y = run(x_gpu1)\n    y_cpu = copy_to_cpu(y, True)\n    torch.cuda.synchronize()\n```\n\nThe total time required for both operations is (as expected) less than the sum of their parts. Note that this task is different from parallel computation as it uses a different resource: the bus between the CPU and GPUs. In fact, we could compute on both devices and communicate, all at the same time. As noted above, there is a dependency between computation and communication: `y[i]` must be computed before it can be copied to the CPU. Fortunately, the system can copy `y[i-1]` while computing `y[i]` to reduce the total running time. We conclude with an illustration of the computational graph and its dependencies for a simple two-layer MLP when training on a CPU and two GPUs, as depicted in :numref:`fig_twogpu`. It would be quite painful to schedule the parallel program resulting from this manually. This is where it is advantageous to have a graph-based computing backend for optimization. ![The computational graph and its dependencies of a two-layer MLP on a CPU and two GPUs.](../img/twogpu.svg)\n:label:`fig_twogpu`"
    },
    {
      "chunk_id": "2e5b1421ae8d_0",
      "chapter": "auto-parallelism",
      "heading": "Summary",
      "text": "* Modern systems have a variety of devices, such as multiple GPUs and CPUs. They can be used in parallel, asynchronously.\n* Modern systems also have a variety of resources for communication, such as PCI Express, storage (typically solid-state drives or via networks), and network bandwidth. They can be used in parallel for peak efficiency.\n* The backend can improve performance through automatic parallel computation and communication."
    },
    {
      "chunk_id": "c93d37c4f1a5_0",
      "chapter": "auto-parallelism",
      "heading": "Exercises",
      "text": "1. Eight operations were performed in the `run` function defined in this section. There are no dependencies between them. Design an experiment to see if the deep learning framework will automatically execute them in parallel.\n1. When the workload of an individual operator is sufficiently small, parallelization can help even on a single CPU or GPU. Design an experiment to verify this.\n1. Design an experiment that uses parallel computation on CPUs, GPUs, and communication between both devices.\n1. Use a debugger such as NVIDIA's [Nsight](https://developer.nvidia.com/nsight-compute-2019_5) to verify that your code is efficient.\n1. Designing computation tasks that include more complex data dependencies, and run experiments to see if you can obtain the correct results while improving performance.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/362)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1681)\n:end_tab:"
    },
    {
      "chunk_id": "3affed682334_0",
      "chapter": "hardware",
      "heading": "hardware",
      "text": "# Hardware\n:label:`sec_hardware`\n\nBuilding systems with great performance requires a good understanding of the algorithms and models to capture the statistical aspects of the problem. At the same time it is also indispensable to have at least a modicum of knowledge of the underlying hardware. The current section is no substitute for a proper course on hardware and system design. Instead, it might serve as a starting point for understanding why some algorithms are more efficient than others and how to achieve good throughput. A good design can easily make a difference of an order of magnitude and, in turn, this can make the difference between being able to train a network (e.g., in a week) and not at all (in 3 months, thus missing the deadline). We will start by looking at computers. Then we will zoom in to look more carefully at CPUs and GPUs. Lastly we zoom out to review how multiple computers are connected in a server center or in the cloud. ![Latency Numbers that every programmer should know.](../img/latencynumbers.png)\n:label:`fig_latencynumbers`\n\nImpatient readers may be able to get by with :numref:`fig_latencynumbers`. It is taken from Colin Scott's [interactive post](https://people.eecs.berkeley.edu/%7Ercs/research/interactive_latency.html) that gives a good overview of the progress over the past decade. The original numbers are due to Jeff Dean's [Stanford talk from 2010](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf). The discussion below explains some of the rationale for these numbers and how they can guide us in designing algorithms. The discussion below is very high level and cursory. It is clearly *no substitute* for a proper course but rather just meant to provide enough information for a statistical modeler to make suitable design decisions."
    },
    {
      "chunk_id": "3affed682334_1",
      "chapter": "hardware",
      "heading": "hardware",
      "text": "The discussion below is very high level and cursory. It is clearly *no substitute* for a proper course but rather just meant to provide enough information for a statistical modeler to make suitable design decisions. For an in-depth overview of computer architecture we refer the reader to :cite:`Hennessy.Patterson.2011` or a recent course on the subject, such as the one by [Arste Asanovic](http://inst.eecs.berkeley.edu/%7Ecs152/sp19/)."
    },
    {
      "chunk_id": "df9374365487_0",
      "chapter": "hardware",
      "heading": "Computers",
      "text": "Most deep learning researchers and practitioners have access to a computer with a fair amount of memory, computation, some form of an accelerator such as a GPU, or multiples thereof. A computer consists of the following key components:\n\n* A processor (also referred to as a CPU) that is able to execute the programs we give it (in addition to running an operating system and many other things), typically consisting of 8 or more cores. * Memory (RAM) to store and retrieve the results from computation, such as weight vectors and activations, and training data. * An Ethernet network connection (sometimes multiple) with speeds ranging from 1 GB/s to 100 GB/s. On high end servers more advanced interconnects can be found. * A high speed expansion bus (PCIe) to connect the system to one or more GPUs. Servers have up to 8 accelerators, often connected in an advanced topology, while desktop systems have 1 or 2, depending on the budget of the user and the size of the power supply. * Durable storage, such as a magnetic hard disk drive, a solid state drive, in many cases connected using the PCIe bus. It provides efficient transfer of training data to the system and storage of intermediate checkpoints as needed. ![Connectivity of components of a computer.](../img/mobo-symbol.svg)\n:label:`fig_mobo-symbol`\n\nAs :numref:`fig_mobo-symbol` indicates, most components (network, GPU, and storage) are connected to the CPU across the PCIe bus. It consists of multiple lanes that are directly attached to the CPU. For instance AMD's Threadripper 3 has 64 PCIe 4.0 lanes, each of which is capable 16 Gbit/s data transfer in both directions. The memory is directly attached to the CPU with a total bandwidth of up to 100 GB/s. When we run code on a computer we need to shuffle data to the processors (CPUs or GPUs), perform computation, and then move the results off the processor back to RAM and durable storage."
    },
    {
      "chunk_id": "df9374365487_1",
      "chapter": "hardware",
      "heading": "Computers",
      "text": "When we run code on a computer we need to shuffle data to the processors (CPUs or GPUs), perform computation, and then move the results off the processor back to RAM and durable storage. Hence, in order to get good performance we need to make sure that this works seamlessly without any one of the systems becoming a major bottleneck. For instance, if we cannot load images quickly enough the processor will not have any work to do. Likewise, if we cannot move matrices quickly enough to the CPU (or GPU), its processing elements will starve. Finally, if we want to synchronize multiple computers across the network, the latter should not slow down computation. One option is to interleave communication and computation. Let's have a look at the various components in more detail."
    },
    {
      "chunk_id": "598a02a7ce58_0",
      "chapter": "hardware",
      "heading": "Memory",
      "text": "At its most basic memory is used to store data that needs to be readily accessible. At present CPU RAM is typically of the [DDR4](https://en.wikipedia.org/wiki/DDR4_SDRAM) variety, offering 20--25 GB/s bandwidth per module. Each module has a 64-bit-wide bus. Typically pairs of memory modules are used to allow for multiple channels. CPUs have between 2 and 4 memory channels, i.e., they have between 4 0GB/s and 100 GB/s peak memory bandwidth. Often there are two banks per channel. For instance AMD's Zen 3 Threadripper has 8 slots. While these numbers are impressive, indeed, they only tell part of the story. When we want to read a portion from memory we first need to tell the memory module where the information can be found. That is, we first need to send the *address* to RAM. Once this is accomplished we can choose to read just a single 64 bit record or a long sequence of records. The latter is called *burst read*. In a nutshell, sending an address to memory and setting up the transfer takes approximately 100 ns (details depend on the specific timing coefficients of the memory chips used), every subsequent transfer takes only 0.2 ns. In short, the first read is 500 times as expensive as subsequent ones! Note that we could perform up to 10,000,000 random reads per second. This suggests that we avoid random memory access as far as possible and use burst reads (and writes) instead. Matters are a bit more complex when we take into account that we have multiple *banks*. Each bank can read memory largely independently. This means two things. On the one hand, the effective number of random reads is up to 4 times higher, provided that they are spread evenly across memory. It also means that it is still a bad idea to perform random reads since burst reads are 4 times faster, too. On the other hand, due to memory alignment to 64 bit boundaries it is a good idea to align any data structures with the same boundaries."
    },
    {
      "chunk_id": "598a02a7ce58_1",
      "chapter": "hardware",
      "heading": "Memory",
      "text": "It also means that it is still a bad idea to perform random reads since burst reads are 4 times faster, too. On the other hand, due to memory alignment to 64 bit boundaries it is a good idea to align any data structures with the same boundaries. Compilers do this pretty much [automatically](https://en.wikipedia.org/wiki/Data_structure_alignment) when the appropriate flags are set. Curious readers are encouraged to review a lecture on DRAMs such as the one by [Zeshan Chishti](http://web.cecs.pdx.edu/%7Ezeshan/ece585_lec5.pdf). GPU memory is subject to even higher bandwidth requirements since they have many more processing elements than CPUs. By and large there are two options to address them. The first is to make the memory bus significantly wider. For instance, NVIDIA's RTX 2080 Ti has a 352-bit-wide bus. This allows for much more information to be transferred at the same time. Second, GPUs use specific high-performance memory. Consumer-grade devices, such as NVIDIA's RTX and Titan series typically use [GDDR6](https://en.wikipedia.org/wiki/GDDR6_SDRAM) chips with over 500 GB/s aggregate bandwidth. An alternative is to use HBM (high bandwidth memory) modules. They use a very different interface and connect directly with GPUs on a dedicated silicon wafer. This makes them very expensive and their use is typically limited to high-end server chips, such as the NVIDIA Volta V100 series of accelerators. Quite unsurprisingly, GPU memory is generally *much* smaller than CPU memory due to the higher cost of the former. For our purposes, by and large their performance characteristics are similar, just a lot faster. We can safely ignore the details for the purpose of this book. They only matter when tuning GPU kernels for high throughput."
    },
    {
      "chunk_id": "b830244f770a_0",
      "chapter": "hardware",
      "heading": "Storage",
      "text": "We saw that some of the key characteristics of RAM are *bandwidth* and *latency*. The same is true for storage devices, just that the differences can be even more extreme."
    },
    {
      "chunk_id": "a76ff577ccc9_0",
      "chapter": "hardware",
      "heading": "Hard Disk Drives",
      "text": "*Hard disk drives* (HDDs) have been in use for over half a century. In a nutshell they contain a number of spinning platters with heads that can be positioned to read or write at any given track. High-end disks hold up to 16 TB on 9 platters. One of the key benefits of HDDs is that they are relatively inexpensive. One of their many downsides are their typically catastrophic failure modes and their relatively high read latency.\n\nTo understand the latter, consider the fact that HDDs spin at around 7,200 RPM (revolutions per minute). If they were much faster they would shatter due to the centrifugal force exerted on the platters. This has a major downside when it comes to accessing a specific sector on the disk: we need to wait until the platter has rotated in position (we can move the heads but not accelerate the actual disks). Hence it can take over 8 ms until the requested data is available. A common way this is expressed is to say that HDDs can operate at approximately 100 IOPs (input/output operations per second). This number has essentially remained unchanged for the past two decades. Worse still, it is equally difficult to increase bandwidth (it is in the order of 100--200 MB/s). After all, each head reads a track of bits, hence the bit rate only scales with the square root of the information density. As a result, HDDs are quickly becoming relegated to archival storage and low-grade storage for very large datasets."
    },
    {
      "chunk_id": "47cd3264f588_0",
      "chapter": "hardware",
      "heading": "Solid State Drives",
      "text": "Solid state drives (SSDs) use flash memory to store information persistently. This allows for *much faster* access to stored records. Modern SSDs can operate at 100,000 to 500,000 IOPs, i.e., up to 3 orders of magnitude faster than HDDs. Furthermore, their bandwidth can reach 1--3GB/s, i.e., one order of magnitude faster than HDDs. These improvements sound almost too good to be true. Indeed, they come with the following caveats, due to the way SSDs are designed.\n\n* SSDs store information in blocks (256 KB or larger). They can only be written as a whole, which takes significant time. Consequently bit-wise random writes on SSD have very poor performance. Likewise, writing data in general takes significant time since the block has to be read, erased and then rewritten with new information. By now SSD controllers and firmware have developed algorithms to mitigate this. Nonetheless, writes can be much slower, in particular for QLC (quad level cell) SSDs. The key for improved performance is to maintain a *queue* of operations, to prefer reads and to write in large blocks if possible.\n* The memory cells in SSDs wear out relatively quickly (often already after a few thousand writes). Wear-level protection algorithms are able to spread the degradation over many cells. That said, it is not recommended to use SSDs for swapping files or for large aggregations of log-files.\n* Lastly, the massive increase in bandwidth has forced computer designers to attach SSDs directly to the PCIe bus. The drives capable of handling this, referred to as NVMe (Non Volatile Memory enhanced), can use up to 4 PCIe lanes. This amounts to up to 8GB/s on PCIe 4.0."
    },
    {
      "chunk_id": "cb5f0337b8b2_0",
      "chapter": "hardware",
      "heading": "Cloud Storage",
      "text": "Cloud storage provides a configurable range of performance. That is, the assignment of storage to virtual machines is dynamic, both in terms of quantity and in terms of speed, as chosen by users. We recommend that users increase the provisioned number of IOPs whenever latency is too high, e.g., during training with many small records."
    },
    {
      "chunk_id": "a16cab2c7354_0",
      "chapter": "hardware",
      "heading": "CPUs",
      "text": "Central processing units (CPUs) are the centerpiece of any computer. They consist of a number of key components: *processor cores* that are able to execute machine code, a *bus* connecting them (the specific topology differs significantly between processor models, generations, and vendors), and *caches* to allow for higher bandwidth and lower latency memory access than what is possible by reads from main memory. Lastly, almost all modern CPUs contain *vector processing units* to aid with high performance linear algebra and convolutions, as they are common in media processing and machine learning.\n\n![Intel Skylake consumer quad-core CPU.](../img/skylake.svg)\n:label:`fig_skylake`\n\n:numref:`fig_skylake` depicts an Intel Skylake consumer-grade quad-core CPU. It has an integrated GPU, caches, and a ringbus connecting the four cores. Peripherals, such as Ethernet, WiFi, Bluetooth, SSD controller, and USB, are either part of the chipset or directly attached (PCIe) to the CPU."
    },
    {
      "chunk_id": "ac4322e0f9f9_0",
      "chapter": "hardware",
      "heading": "Microarchitecture",
      "text": "Each of the processor cores consists of a rather sophisticated set of components. While details differ between generations and vendors, the basic functionality is pretty much standard. The front-end loads instructions and tries to predict which path will be taken (e.g., for control flow). Instructions are then decoded from assembly code to microinstructions. Assembly code is often not the lowest level code that a processor executes. Instead, complex instructions may be decoded into a set of more lower level operations. These are then processed by the actual execution core. Often the latter is capable of performing many operations simultaneously. For instance, the ARM Cortex A77 core of :numref:`fig_cortexa77` is able to perform up to 8 operations simultaneously.\n\n![ARM Cortex A77 Microarchitecture.](../img/a77.svg)\n:label:`fig_cortexa77`\n\nThis means that efficient programs might be able to perform more than one instruction per clock cycle, provided that they can be carried out independently. Not all units are created equal. Some specialize in integer instructions whereas others are optimized for floating point performance. To increase throughput, the processor might also follow  multiple code paths simultaneously in a branching instruction and then discard the results of the branches not taken. This is why branch prediction units matter (on the front-end) such that only the most promising paths are pursued."
    },
    {
      "chunk_id": "97c6c72fc36f_0",
      "chapter": "hardware",
      "heading": "Vectorization",
      "text": "Deep learning is extremely compute-hungry. Hence, to make CPUs suitable for machine learning, one needs to perform many operations in one clock cycle. This is achieved via vector units. They have different names: on ARM they are called NEON, on x86 they (a recent generation) are referred to as [AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) units. A common aspect is that they are able to perform SIMD (single instruction multiple data) operations. :numref:`fig_neon128` shows how 8 short integers can be added in one clock cycle on ARM.\n\n![128 bit NEON vectorization.](../img/neon128.svg)\n:label:`fig_neon128`\n\nDepending on architecture choices, such registers are up to 512 bits long, allowing for the combination of up to 64 pairs of numbers. For instance, we might be multiplying two numbers and adding them to a third, which is also known as a fused multiply-add. Intel's [OpenVino](https://01.org/openvinotoolkit) uses these to achieve respectable throughput for deep learning on server-grade CPUs. Note, though, that this number is entirely dwarfed by what GPUs are capable of achieving. For instance, NVIDIA's RTX 2080 Ti has 4,352 CUDA cores, each of which is capable of processing such an operation at any time."
    },
    {
      "chunk_id": "97cd5af576d8_0",
      "chapter": "hardware",
      "heading": "Cache",
      "text": "Consider the following situation: we have a modest CPU core with 4 cores as depicted in :numref:`fig_skylake` above, running at 2 GHz frequency. Moreover, let's assume that we have an IPC (instructions per clock) count of 1 and that the units have AVX2 with 256-bit width enabled. Let's furthermore assume that at least one of the registers used for AVX2 operations needs to be retrieved from memory. This means that the CPU consumes $4 \\times 256 \\textrm{ bit} = 128 \\textrm{ bytes}$ of data per clock cycle. Unless we are able to transfer $2 \\times 10^9 \\times 128 = 256 \\times 10^9$ bytes to the processor per second the processing elements are going to starve. Unfortunately the memory interface of such a chip only supports 20--40 GB/s data transfer, i.e., one order of magnitude less. The fix is to avoid loading *new* data from memory as far as possible and rather to cache it locally on the CPU. This is where caches come in handy. Commonly the following names or concepts are used:\n\n* **Registers** are strictly speaking not part of the cache. They help stage instructions. That said, CPU registers are memory locations that a CPU can access at clock speed without any delay penalty. CPUs have tens of registers. It is up to the compiler (or programmer) to use registers efficiently. For instance the C programming language has a `register` keyword. * **L1 caches** are the first line of defense against high memory bandwidth requirements. L1 caches are tiny (typical sizes might be 32--64 KB) and often split into data and instructions caches. When data is found in the L1 cache, access is very fast. If they cannot be found there, the search progresses down the cache hierarchy. * **L2 caches** are the next stop. Depending on architecture design and processor size they might be exclusive. They might be accessible only by a given core or shared among multiple cores. L2 caches are larger (typically 256--512 KB per core) and slower than L1."
    },
    {
      "chunk_id": "97cd5af576d8_1",
      "chapter": "hardware",
      "heading": "Cache",
      "text": "Depending on architecture design and processor size they might be exclusive. They might be accessible only by a given core or shared among multiple cores. L2 caches are larger (typically 256--512 KB per core) and slower than L1. Furthermore, to access something in L2 we first need to check to realize that the data is not in L1, which adds a small amount of extra latency. * **L3 caches** are shared among multiple cores and can be quite large. AMD's Epyc 3 server CPUs have a whopping 256 MB of cache spread across multiple chiplets. More typical numbers are in the 4--8 MB range. Predicting which memory elements will be needed next is one of the key optimization parameters in chip design. For instance, it is advisable to traverse memory in a *forward* direction since most caching algorithms will try to *read ahead* rather than backwards. Likewise, keeping memory access patterns local is a good way of improving performance. Adding caches is a double-edge sword. On the one hand they ensure that the processor cores do not starve of data. At the same time they increase chip size, using up area that otherwise could have been spent on increasing processing power. Moreover, *cache misses* can be expensive. Consider the worst case scenario, *false sharing*, as depicted in :numref:`fig_falsesharing`. A memory location is cached on processor 0 when a thread on processor 1 requests the data. To obtain it, processor 0 needs to stop what it is doing, write the information back to main memory and then let processor 1 read it from memory. During this operation both processors wait. Quite potentially such code runs *more slowly* on multiple processors when compared with an efficient single-processor implementation. This is one more reason for why there is a practical limit to cache sizes (besides their physical size). ![False sharing (image courtesy of Intel).](../img/falsesharing.svg)\n:label:`fig_falsesharing`"
    },
    {
      "chunk_id": "465ab4675a46_0",
      "chapter": "hardware",
      "heading": "GPUs and other Accelerators",
      "text": "It is not an exaggeration to claim that deep learning would not have been successful without GPUs. By the same token, it is quite reasonable to argue that GPU manufacturers' fortunes have increased significantly due to deep learning. This co-evolution of hardware and algorithms has led to a situation where for better or worse deep learning is the preferable statistical modeling paradigm. Hence it pays to understand the specific benefits that GPUs and related accelerators such as the TPU :cite:`Jouppi.Young.Patil.ea.2017`. Of note is a distinction that is often made in practice: accelerators are optimized either for training or inference. For the latter we only need to compute the forward propagation in a network. No storage of intermediate data is needed for backpropagation. Moreover, we may not need very precise computation (FP16 or INT8 typically suffice). On the other hand, during training all intermediate results need storage to compute gradients. Moreover, accumulating gradients requires higher precision to avoid numerical underflow (or overflow). This means that FP16 (or mixed precision with FP32) is the minimum requirement. All of this necessitates faster and larger memory (HBM2 vs. GDDR6) and more processing power. For instance, NVIDIA's [Turing](https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/) T4 GPUs are optimized for inference whereas the V100 GPUs are preferable for training. Recall vectorization as illustrated in :numref:`fig_neon128`. Adding vector units to a processor core allowed us to increase throughput significantly. For example, in the example in :numref:`fig_neon128` we were able to perform 16 operations simultaneously. First,\nwhat if we added operations that optimized not just operations between vectors but also between matrices? This strategy led to tensor cores (to be covered shortly). Second, what if we added many more cores? In a nutshell, these two strategies summarize the design decisions in GPUs."
    },
    {
      "chunk_id": "465ab4675a46_1",
      "chapter": "hardware",
      "heading": "GPUs and other Accelerators",
      "text": "This strategy led to tensor cores (to be covered shortly). Second, what if we added many more cores? In a nutshell, these two strategies summarize the design decisions in GPUs. :numref:`fig_turing_processing_block` gives an overview of a basic processing block. It contains 16 integer and 16 floating point units. In addition to that, two tensor cores accelerate a narrow subset of additional operations relevant for deep learning. Each streaming multiprocessor consists of four such blocks. ![NVIDIA Turing processing block (image courtesy of NVIDIA).](../img/turing-processing-block.png)\n:width:`150px`\n:label:`fig_turing_processing_block`\n\nNext, 12 streaming multiprocessors are grouped into graphics processing clusters which make up the high-end TU102 processors. Ample memory channels and an L2 cache complement the setup. :numref:`fig_turing` has the relevant details. One of the reasons for designing such a device is that individual blocks can be added or removed as needed to allow for more compact chips and to deal with yield issues (faulty modules might not be activated). Fortunately programming such devices is well hidden from the casual deep learning researcher beneath layers of CUDA and framework code. In particular, more than one of the programs might well be executed simultaneously on the GPU, provided that there are available resources. Nonetheless it pays to be aware of the limitations of the devices to avoid picking models that do not fit into device memory. ![NVIDIA Turing architecture (image courtesy of NVIDIA)](../img/turing.png)\n:width:`350px`\n:label:`fig_turing`\n\nA last aspect that is worth mentioning in more detail are *tensor cores*. They are an example of a recent trend of adding more optimized circuits that are specifically effective for deep learning. For instance, the TPU added a systolic array :cite:`Kung.1988` for fast matrix multiplication. There the design was to support a very small number (one for the first generation of TPUs) of large operations. Tensor cores are at the other end."
    },
    {
      "chunk_id": "465ab4675a46_2",
      "chapter": "hardware",
      "heading": "GPUs and other Accelerators",
      "text": "For instance, the TPU added a systolic array :cite:`Kung.1988` for fast matrix multiplication. There the design was to support a very small number (one for the first generation of TPUs) of large operations. Tensor cores are at the other end. They are optimized for small operations involving between $4 \\times 4$ and $16 \\times 16$ matrices, depending on their numerical precision. :numref:`fig_tensorcore` gives an overview of the optimizations. ![NVIDIA tensor cores in Turing (image courtesy of NVIDIA).](../img/tensorcore.jpg)\n:width:`400px`\n:label:`fig_tensorcore`\n\nObviously when optimizing for computation we end up making certain compromises. One of them is that GPUs are not very good at handling interrupts and sparse data. While there are notable exceptions, such as [Gunrock](https://github.com/gunrock/gunrock) :cite:`Wang.Davidson.Pan.ea.2016`, the access pattern of sparse matrices and vectors do not go well with the high bandwidth burst read operations where GPUs excel. Matching both goals is an area of active research. See e.g., [DGL](http://dgl.ai), a library tuned for deep learning on graphs."
    },
    {
      "chunk_id": "6c25bc9708da_0",
      "chapter": "hardware",
      "heading": "Networks and Buses",
      "text": "Whenever a single device is insufficient for optimization we need to transfer data to and from it to synchronize processing. This is where networks and buses come in handy. We have a number of design parameters: bandwidth, cost, distance, and flexibility. On one end we have WiFi that has a pretty good range, is very easy to use (no wires, after all), cheap but it offers comparatively mediocre bandwidth and latency. No machine learning researcher within their right mind would use it to build a cluster of servers. In what follows we focus on interconnects that are suitable for deep learning. * **PCIe** is a dedicated bus for very high bandwidth point-to-point connections (up to 32 GB/s on PCIe 4.0 in a 16-lane slot) per lane. Latency is in the order of single-digit microseconds (5 \u03bcs). PCIe links are precious. Processors only have a limited number of them: AMD's EPYC 3 has 128 lanes, Intel's Xeon has up to 48 lanes per chip; on desktop-grade CPUs the numbers are 20 (Ryzen 9) and 16 (Core i9) respectively. Since GPUs have typically 16 lanes, this limits the number of GPUs that can connect to the CPU at full bandwidth. After all, they need to share the links with other high bandwidth peripherals such as storage and Ethernet. Just like with RAM access, large bulk transfers are preferable due to reduced packet overhead. * **Ethernet** is the most commonly used way of connecting computers. While it is significantly slower than PCIe, it is very cheap and resilient to install and covers much longer distances. Typical bandwidth for low-grade servers is 1 GBit/s. Higher-end devices (e.g., [C5 instances](https://aws.amazon.com/ec2/instance-types/c5/) in the cloud) offer between 10 and 100 GBit/s bandwidth. As in all previous cases data transmission has significant overheads. Note that we almost never use raw Ethernet directly but rather a protocol that is executed on top of the physical interconnect (such as UDP or TCP/IP). This adds further overhead."
    },
    {
      "chunk_id": "6c25bc9708da_1",
      "chapter": "hardware",
      "heading": "Networks and Buses",
      "text": "As in all previous cases data transmission has significant overheads. Note that we almost never use raw Ethernet directly but rather a protocol that is executed on top of the physical interconnect (such as UDP or TCP/IP). This adds further overhead. Like PCIe, Ethernet is designed to connect two devices, e.g., a computer and a switch. * **Switches** allow us to connect multiple devices in a manner where any pair of them can carry out a (typically full bandwidth) point-to-point connection simultaneously. For instance, Ethernet switches might connect 40 servers at high cross-sectional bandwidth. Note that switches are not unique to traditional computer networks. Even PCIe lanes can be [switched](https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches). This occurs, e.g., to connect a large number of GPUs to a host processor, as is the case for the [P2 instances](https://aws.amazon.com/ec2/instance-types/p2/). * **NVLink** is an alternative to PCIe when it comes to very high bandwidth interconnects. It offers up to 300 Gbit/s data transfer rate per link. Server GPUs (Volta V100) have six links whereas consumer-grade GPUs (RTX 2080 Ti) have only one link, operating at a reduced 100 Gbit/s rate. We recommend to use [NCCL](https://github.com/NVIDIA/nccl) to achieve high data transfer between GPUs."
    },
    {
      "chunk_id": "93dfe372fc32_0",
      "chapter": "hardware",
      "heading": "More Latency Numbers",
      "text": "The summary in :numref:`table_latency_numbers` and :numref:`table_latency_numbers_tesla` are from [Eliot Eshelman](https://gist.github.com/eshelman) who maintains an updated version of the numbers as a [GitHub gist](https://gist.github.com/eshelman/343a1c46cb3fba142c1afdcdeec17646). :Common Latency Numbers. | Action | Time | Notes |\n| :----------------------------------------- | -----: | :---------------------------------------------- |\n| L1 cache reference/hit                     | 1.5 ns | 4 cycles                                        |\n| Floating-point add/mult/FMA                | 1.5 ns | 4 cycles                                        |\n| L2 cache reference/hit                     |   5 ns | 12 ~ 17 cycles                                  |\n| Branch mispredict                          |   6 ns | 15 ~ 20 cycles                                  |\n| L3 cache hit (unshared cache)              |  16 ns | 42 cycles                                       |\n| L3 cache hit (shared in another core)      |  25 ns | 65 cycles                                       |\n| Mutex lock/unlock                          |  25 ns |                                                 |\n| L3 cache hit (modified in another core)    |  29 ns | 75 cycles                                       |\n| L3 cache hit (on a remote CPU socket)      |  40 ns | 100 ~ 300 cycles (40 ~ 116 ns)                  |\n| QPI hop to a another CPU (per hop)         |  40 ns |                                                 |\n| 64MB memory ref. (local CPU)          |  46 ns | TinyMemBench on Broadwell E5-2690v4             |\n| 64MB memory ref. (remote CPU)         |  70 ns | TinyMemBench on Broadwell E5-2690v4             |\n| 256MB memory ref. (local CPU)         |  75 ns | TinyMemBench on Broadwell E5-2690v4             |\n| Intel Optane random write                  |  94 ns | UCSD Non-Volatile Systems Lab                   |\n| 256MB memory ref."
    },
    {
      "chunk_id": "93dfe372fc32_1",
      "chapter": "hardware",
      "heading": "More Latency Numbers",
      "text": "(local CPU)         |  75 ns | TinyMemBench on Broadwell E5-2690v4             |\n| Intel Optane random write                  |  94 ns | UCSD Non-Volatile Systems Lab                   |\n| 256MB memory ref. (remote CPU)        | 120 ns | TinyMemBench on Broadwell E5-2690v4             |\n| Intel Optane random read                   | 305 ns | UCSD Non-Volatile Systems Lab                   |\n| Send 4KB over 100 Gbps HPC fabric          |   1 \u03bcs | MVAPICH2 over Intel Omni-Path                   |\n| Compress 1KB with Google Snappy            |   3 \u03bcs |                                                 |\n| Send 4KB over 10 Gbps ethernet             |  10 \u03bcs |                                                 |\n| Write 4KB randomly to NVMe SSD             |  30 \u03bcs | DC P3608 NVMe SSD (QOS 99% is 500\u03bcs)            |\n| Transfer 1MB to/from NVLink GPU            |  30 \u03bcs | ~33GB/s on NVIDIA 40GB NVLink                 |\n| Transfer 1MB to/from PCI-E GPU             |  80 \u03bcs | ~12GB/s on PCIe 3.0 x16 link                  |\n| Read 4KB randomly from NVMe SSD            | 120 \u03bcs | DC P3608 NVMe SSD (QOS 99%)                     |\n| Read 1MB sequentially from NVMe SSD        | 208 \u03bcs | ~4.8GB/s DC P3608 NVMe SSD                    |\n| Write 4KB randomly to SATA SSD             | 500 \u03bcs | DC S3510 SATA SSD (QOS 99.9%)                   |\n| Read 4KB randomly from SATA SSD            | 500 \u03bcs | DC S3510 SATA SSD (QOS 99.9%)                   |\n| Round trip within same data center          | 500 \u03bcs | One-way ping is ~250\u03bcs                          |\n| Read 1MB sequentially from SATA SSD        |   2 ms | ~550MB/s DC S3510 SATA SSD                    |\n| Read 1MB sequentially from disk            |   5 ms | ~200MB/s server HDD                           |\n| Random Disk Access (seek+rotation)         |  10 ms |                                                 |\n| Send packet CA->Netherlands->CA            | 150 ms |                                                 |\n:label:`table_latency_numbers`\n\n:Latency Numbers for NVIDIA Tesla GPUs."
    },
    {
      "chunk_id": "93dfe372fc32_2",
      "chapter": "hardware",
      "heading": "More Latency Numbers",
      "text": "| Action | Time | Notes |\n| :------------------------------ | -----: | :---------------------------------------- |\n| GPU Shared Memory access        |  30 ns | 30~90 cycles (bank conflicts add latency) |\n| GPU Global Memory access        | 200 ns | 200~800 cycles                            |\n| Launch CUDA kernel on GPU       |  10 \u03bcs | Host CPU instructs GPU to start kernel    |\n| Transfer 1MB to/from NVLink GPU |  30 \u03bcs | ~33GB/s on NVIDIA 40GB NVLink           |\n| Transfer 1MB to/from PCI-E GPU  |  80 \u03bcs | ~12GB/s on PCI-Express x16 link         |\n:label:`table_latency_numbers_tesla`"
    },
    {
      "chunk_id": "24e4133a6ffb_0",
      "chapter": "hardware",
      "heading": "Summary",
      "text": "* Devices have overheads for operations. Hence it is important to aim for a small number of large transfers rather than many small ones. This applies to RAM, SSDs, networks and GPUs.\n* Vectorization is key for performance. Make sure you are aware of the specific abilities of your accelerator. E.g., some Intel Xeon CPUs are particularly good for INT8 operations, NVIDIA Volta GPUs excel at FP16 matrix-matrix operations and NVIDIA Turing shines at FP16, INT8, and INT4 operations.\n* Numerical overflow due to small data types can be a problem during training (and to a lesser extent during inference).\n* Aliasing can significantly degrade performance. For instance, memory alignment on 64 bit CPUs should be done with respect to 64 bit boundaries. On GPUs it is a good idea to keep convolution sizes aligned, e.g., to tensor cores.\n* Match your algorithms to the hardware (e.g., memory footprint, and bandwidth). Great speedup (orders of magnitude) can be achieved when fitting the parameters into caches.\n* We recommend that you sketch out the performance of a novel algorithm on paper before verifying the experimental results. Discrepancies of an order-of-magnitude or more are reasons for concern.\n* Use profilers to debug performance bottlenecks.\n* Training and inference hardware have different sweet spots in terms of price and performance."
    },
    {
      "chunk_id": "3fee666e8889_0",
      "chapter": "hardware",
      "heading": "Exercises",
      "text": "1. Write C code to test whether there is any difference in speed between accessing memory aligned or misaligned relative to the external memory interface. Hint: be careful of caching effects. 1. Test the difference in speed between accessing memory in sequence or with a given stride. 1. How could you measure the cache sizes on a CPU? 1. How would you lay out data across multiple memory channels for maximum bandwidth? How would you lay it out if you had many small threads? 1. An enterprise-class HDD is spinning at 10,000 rpm. What is the absolutely minimum time an HDD needs to spend worst case before it can read data (you can assume that heads move almost instantaneously)? Why are 2.5\" HDDs becoming popular for commercial servers (relative to 3.5\" and 5.25\" drives)? 1. Assume that an HDD manufacturer increases the storage density from 1 Tbit per square inch to 5 Tbit per square inch. How much information can you store on a ring on a 2.5\" HDD? Is there a difference between the inner and outer tracks? 1. Going from 8 bit to 16 bit data types increases the amount of silicon approximately by four times. Why? Why might NVIDIA have added INT4 operations to their Turing GPUs? 1. How much faster is it to read forward through memory vs. reading backwards? Does this number differ between different computers and CPU vendors? Why? Write C code and experiment with it. 1. Can you measure the cache size of your disk? What is it for a typical HDD? Do SSDs need a cache? 1. Measure the packet overhead when sending messages across the Ethernet. Look up the difference between UDP and TCP/IP connections. 1. Direct memory access allows devices other than the CPU to write (and read) directly to (from) memory. Why is this a good idea? 1. Look at the performance numbers for the Turing T4 GPU. Why does the performance \"only\" double as you go from FP16 to INT8 and INT4? 1. What is the shortest time it should take for a packet on a round trip between San Francisco and Amsterdam? Hint: you can assume that the distance is 10,000 km."
    },
    {
      "chunk_id": "3fee666e8889_1",
      "chapter": "hardware",
      "heading": "Exercises",
      "text": "Why does the performance \"only\" double as you go from FP16 to INT8 and INT4? 1. What is the shortest time it should take for a packet on a round trip between San Francisco and Amsterdam? Hint: you can assume that the distance is 10,000 km. [Discussions](https://discuss.d2l.ai/t/363)"
    },
    {
      "chunk_id": "93acf1555bea_0",
      "chapter": "hybridize",
      "heading": "hybridize",
      "text": "# Compilers and Interpreters\n:label:`sec_hybridize`\n\nSo far, this book has focused on imperative programming, which makes use of statements such as `print`, `+`, and `if` to change a program's state. Consider the following example of a simple imperative program.\n\n```{.python .input}\n#@tab all\ndef add(a, b):\n    return a + b\n\ndef fancy_func(a, b, c, d):\n    e = add(a, b)\n    f = add(c, d)\n    g = add(e, f)\n    return g\n\nprint(fancy_func(1, 2, 3, 4))\n```\n\nPython is an *interpreted language*. When evaluating the above `fancy_func` function it performs the operations making up the function's body *in sequence*. That is, it will evaluate `e = add(a, b)` and store the results as variable `e`, thereby changing the program's state. The next two statements `f = add(c, d)` and `g = add(e, f)` will be executed similarly, performing additions and storing the results as variables. :numref:`fig_compute_graph` illustrates the flow of data.\n\n![Data flow in an imperative program.](../img/computegraph.svg)\n:label:`fig_compute_graph`\n\nAlthough imperative programming is convenient, it may be inefficient. On the one hand, even if the `add` function is repeatedly called throughout `fancy_func`, Python will execute the three function calls individually. If these are executed, say, on a GPU (or even on multiple GPUs), the overhead arising from the Python interpreter can become overwhelming. Moreover, it will need to save the variable values of `e` and `f` until all the statements in `fancy_func` have been executed. This is because we do not know whether the variables `e` and `f` will be used by other parts of the program after the statements `e = add(a, b)` and `f = add(c, d)` are executed."
    },
    {
      "chunk_id": "4f03003722cf_0",
      "chapter": "hybridize",
      "heading": "Symbolic Programming",
      "text": "Consider the alternative, *symbolic programming*, where computation is usually performed only once the process has been fully defined. This strategy is used by multiple deep learning frameworks, including Theano and TensorFlow (the latter has acquired imperative extensions). It usually involves the following steps:\n\n1. Define the operations to be executed. 1. Compile the operations into an executable program. 1. Provide the required inputs and call the compiled program for execution. This allows for a significant amount of optimization. First, we can skip the Python interpreter in many cases, thus removing a performance bottleneck that can become significant on multiple fast GPUs paired with a single Python thread on a CPU. Second, a compiler might optimize and rewrite the above code into `print((1 + 2) + (3 + 4))` or even `print(10)`. This is possible since a compiler gets to see the full code before turning it into machine instructions. For instance, it can release memory (or never allocate it) whenever a variable is no longer needed. Or it can transform the code entirely into an equivalent piece. To get a better idea, consider the following simulation of imperative programming (it is Python after all) below. ```{.python .input}\n#@tab all\ndef add_():\n    return '''\ndef add(a, b):\n    return a + b\n'''\n\ndef fancy_func_():\n    return '''\ndef fancy_func(a, b, c, d):\n    e = add(a, b)\n    f = add(c, d)\n    g = add(e, f)\n    return g\n'''\n\ndef evoke_():\n    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n\nprog = evoke_()\nprint(prog)\ny = compile(prog, '', 'exec')\nexec(y)\n```\n\nThe differences between imperative (interpreted) programming and symbolic programming are as follows:\n\n* Imperative programming is easier. When imperative programming is used in Python, the majority of the code is straightforward and easy to write. It is also easier to debug imperative programming code."
    },
    {
      "chunk_id": "4f03003722cf_1",
      "chapter": "hybridize",
      "heading": "Symbolic Programming",
      "text": "When imperative programming is used in Python, the majority of the code is straightforward and easy to write. It is also easier to debug imperative programming code. This is because it is easier to obtain and print all relevant intermediate variable values, or use Python's built-in debugging tools. * Symbolic programming is more efficient and easier to port. Symbolic programming makes it easier to optimize the code during compilation, while also having the ability to port the program into a format independent of Python. This allows the program to be run in a non-Python environment, thus avoiding any potential performance issues related to the Python interpreter."
    },
    {
      "chunk_id": "cd72a64a22ee_0",
      "chapter": "hybridize",
      "heading": "Hybrid Programming",
      "text": "Historically most deep learning frameworks choose between an imperative or a symbolic approach. For example, Theano, TensorFlow (inspired by the former), Keras, and CNTK formulate models symbolically. Conversely, Chainer and PyTorch take an imperative approach. An imperative mode was added to TensorFlow 2.0 and Keras in later revisions. :begin_tab:`mxnet`\nWhen designing Gluon, developers considered whether it would be possible to combine the benefits of both programming paradigms. This led to a hybrid model that lets users develop and debug with pure imperative programming, while having the ability to convert most programs into symbolic programs to be run when product-level computing performance and deployment are required. In practice this means that we build models using the `HybridBlock` or `HybridSequential` class. By default, either of them is executed in the same way the `Block` or `Sequential` class is executed in imperative programming. The `HybridSequential` class is a subclass of `HybridBlock` (just like `Sequential` subclasses `Block`). When the `hybridize` function is called, Gluon compiles the model into the form used in symbolic programming. This allows one to optimize the computation-intensive components without sacrifices in the way a model is implemented. We will illustrate the benefits below, focusing on sequential models and blocks. :end_tab:\n\n:begin_tab:`pytorch`\nAs mentioned above, PyTorch is based on imperative programming and uses dynamic computation graphs. In an effort to leverage the portability and efficiency of symbolic programming, developers considered whether it would be possible to combine the benefits of both programming paradigms. This led to a torchscript that lets users develop and debug using pure imperative programming, while having the ability to convert most programs into symbolic programs to be run when product-level computing performance and deployment are required."
    },
    {
      "chunk_id": "cd72a64a22ee_1",
      "chapter": "hybridize",
      "heading": "Hybrid Programming",
      "text": "This led to a torchscript that lets users develop and debug using pure imperative programming, while having the ability to convert most programs into symbolic programs to be run when product-level computing performance and deployment are required. :end_tab:\n\n:begin_tab:`tensorflow`\nThe imperative programming paradigm is now the default in Tensorflow 2, a welcoming change for those new to the language. However, the same symbolic programming techniques and subsequent computational graphs still exist in TensorFlow, and can be accessed by the easy-to-use `tf.function` decorator. This brought the imperative programming paradigm to TensorFlow, allowed users to define more intuitive functions, then wrap them and compile them into computational graphs automatically using a feature the TensorFlow team refers to as [autograph](https://www.tensorflow.org/api_docs/python/tf/autograph). :end_tab:"
    },
    {
      "chunk_id": "1dbadc1b7aaf_0",
      "chapter": "hybridize",
      "heading": "Hybridizing the `Sequential` Class",
      "text": "The easiest way to get a feel for how hybridization works is to consider deep networks with multiple layers. Conventionally the Python interpreter will need to execute the code for all layers to generate an instruction that can then be forwarded to a CPU or a GPU. For a single (fast) computing device this does not cause any major issues. On the other hand, if we use an advanced 8-GPU server such as an AWS P3dn.24xlarge instance Python will struggle to keep all GPUs busy. The single-threaded Python interpreter becomes the bottleneck here. Let's see how we can address this for significant parts of the code by replacing `Sequential` with `HybridSequential`. We begin by defining a simple MLP. ```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n\n# Factory for networks\ndef get_net():\n    net = nn.HybridSequential()  \n    net.add(nn.Dense(256, activation='relu'),\n            nn.Dense(128, activation='relu'),\n            nn.Dense(2))\n    net.initialize()\n    return net\n\nx = np.random.normal(size=(1, 512))\nnet = get_net()\nnet(x)\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n\n# Factory for networks\ndef get_net():\n    net = nn.Sequential(nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2))\n    return net\n\nx = torch.randn(size=(1, 512))\nnet = get_net()\nnet(x)\n```\n\n```{.python .input}\n#@tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\n\n# Factory for networks\ndef get_net():\n    net = tf.keras.Sequential()\n    net.add(Dense(256, input_shape = (512,), activation = \"relu\"))\n    net.add(Dense(128, activation = \"relu\"))\n    net.add(Dense(2, activation = \"linear\"))\n    return net\n\nx = tf.random.normal([1,512])\nnet = get_net()\nnet(x)\n```\n\n:begin_tab:`mxnet`\nBy calling the `hybridize` function, we are able to compile and optimize the computation in the MLP."
    },
    {
      "chunk_id": "1dbadc1b7aaf_1",
      "chapter": "hybridize",
      "heading": "Hybridizing the `Sequential` Class",
      "text": "The model's computation result remains unchanged. :end_tab:\n\n:begin_tab:`pytorch`\nBy converting the model using `torch.jit.script` function, we are able to compile and optimize the computation in the MLP. The model's computation result remains unchanged. :end_tab:\n\n:begin_tab:`tensorflow`\nFormerly, all functions built in TensorFlow were built as a computational graph, and therefore JIT compiled by default. However, with the release of TensorFlow 2.X and EagerTensor, this is no longer the default behavor. We cen re-enable this functionality with tf.function. tf.function is more commonly used as a function decorator, however it is possible to call it direcly as a normal python function, shown below. The model's computation result remains unchanged. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nnet.hybridize()\nnet(x)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = torch.jit.script(net)\nnet(x)\n```\n\n```{.python .input}\n#@tab tensorflow\nnet = tf.function(net)\nnet(x)\n```\n\n:begin_tab:`mxnet`\nThis seems almost too good to be true: simply designate a block to be `HybridSequential`, write the same code as before and invoke `hybridize`. Once this happens the network is optimized (we will benchmark the performance below). Unfortunately this does not work magically for every layer. That said, a layer will not be optimized if it inherits from the `Block` class instead of the `HybridBlock` class. :end_tab:\n\n:begin_tab:`pytorch`\nThis seems almost too good to be true: write the same code as before and simply convert the model using `torch.jit.script`. Once this happens the network is optimized (we will benchmark the performance below). :end_tab:\n\n:begin_tab:`tensorflow`\nThis seems almost too good to be true: write the same code as before and simply convert the model using `tf.function`. Once this happens the network is built as a computational graph in TensorFlow's MLIR intermediate representation and is heavily optimized at the compiler level for rapid execution (we will benchmark the performance below)."
    },
    {
      "chunk_id": "1dbadc1b7aaf_2",
      "chapter": "hybridize",
      "heading": "Hybridizing the `Sequential` Class",
      "text": "Once this happens the network is built as a computational graph in TensorFlow's MLIR intermediate representation and is heavily optimized at the compiler level for rapid execution (we will benchmark the performance below). Explicitly adding the `jit_compile = True` flag to the `tf.function()` call enables XLA (Accelerated Linear Algebra) functionality in TensorFlow. XLA can further optimize JIT compiled code in certain instances. Graph-mode execution is enabled without this explicit definition, however XLA can make certain large linear algebra operations (in the vein of those we see in deep learning applications) much faster, particularly in a GPU environment. :end_tab:"
    },
    {
      "chunk_id": "637e286ddbe3_0",
      "chapter": "hybridize",
      "heading": "Acceleration by Hybridization",
      "text": "To demonstrate the performance improvement gained by compilation we compare the time needed to evaluate `net(x)` before and after hybridization. Let's define a class to measure this time first. It will come handy throughout the chapter as we set out to measure (and improve) performance. ```{.python .input}\n#@tab all\n#@save\nclass Benchmark:\n    \"\"\"For measuring running time.\"\"\"\n    def __init__(self, description='Done'):\n        self.description = description\n\n    def __enter__(self):\n        self.timer = d2l.Timer()\n        return self\n\n    def __exit__(self, *args):\n        print(f'{self.description}: {self.timer.stop():.4f} sec')\n```\n\n:begin_tab:`mxnet`\nNow we can invoke the network twice, once with and once without hybridization. :end_tab:\n\n:begin_tab:`pytorch`\nNow we can invoke the network twice, once with and once without torchscript. :end_tab:\n\n:begin_tab:`tensorflow`\nNow we can invoke the network three times, once executed eagerly, once with graph-mode execution, and again using JIT compiled XLA. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nnet = get_net()\nwith Benchmark('Without hybridization'):\n    for i in range(1000): net(x)\n    npx.waitall()\n\nnet.hybridize()\nwith Benchmark('With hybridization'):\n    for i in range(1000): net(x)\n    npx.waitall()\n```\n\n```{.python .input}\n#@tab pytorch\nnet = get_net()\nwith Benchmark('Without torchscript'):\n    for i in range(1000): net(x)\n\nnet = torch.jit.script(net)\nwith Benchmark('With torchscript'):\n    for i in range(1000): net(x)\n```\n\n```{.python .input}\n#@tab tensorflow\nnet = get_net()\nwith Benchmark('Eager Mode'):\n    for i in range(1000): net(x)\n\nnet = tf.function(net)\nwith Benchmark('Graph Mode'):\n    for i in range(1000): net(x)\n```\n\n:begin_tab:`mxnet`\nAs is observed in the above results, after a `HybridSequential` instance calls the `hybridize` function, computing performance is improved through the use of symbolic programming."
    },
    {
      "chunk_id": "637e286ddbe3_1",
      "chapter": "hybridize",
      "heading": "Acceleration by Hybridization",
      "text": ":end_tab:\n\n:begin_tab:`pytorch`\nAs is observed in the above results, after an `nn.Sequential` instance is scripted using the `torch.jit.script` function, computing performance is improved through the use of symbolic programming. :end_tab:\n\n:begin_tab:`tensorflow`\nAs is observed in the above results, after a `tf.keras.Sequential` instance is scripted using the `tf.function` function, computing performance is improved through the use of symbolic programming via graph-mode execution in tensorflow. :end_tab:"
    },
    {
      "chunk_id": "ad4446a81774_0",
      "chapter": "hybridize",
      "heading": "Serialization",
      "text": ":begin_tab:`mxnet`\nOne of the benefits of compiling the models is that we can serialize (save) the model and its parameters to disk. This allows us to store a model in a manner that is independent of the front-end language of choice. This allows us to deploy trained models to other devices and easily use other front-end programming languages. At the same time the code is often faster than what can be achieved in imperative programming. Let's see the `export` function in action. :end_tab:\n\n:begin_tab:`pytorch`\nOne of the benefits of compiling the models is that we can serialize (save) the model and its parameters to disk. This allows us to store a model in a manner that is independent of the front-end language of choice. This allows us to deploy trained models to other devices and easily use other front-end programming languages. At the same time the code is often faster than what can be achieved in imperative programming. Let's see the `save` function in action. :end_tab:\n\n:begin_tab:`tensorflow`\nOne of the benefits of compiling the models is that we can serialize (save) the model and its parameters to disk. This allows us to store a model in a manner that is independent of the front-end language of choice. This allows us to deploy trained models to other devices and easily use other front-end programming languages or execute a trained model on a server. At the same time the code is often faster than what can be achieved in imperative programming. The low-level API that allows us to save in tensorflow is `tf.saved_model`. Let's see the `saved_model` instance in action. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nnet.export('my_mlp')\n!ls -lh my_mlp*\n```\n\n```{.python .input}\n#@tab pytorch\nnet.save('my_mlp')\n!ls -lh my_mlp*\n```\n\n```{.python .input}\n#@tab tensorflow\nnet = get_net()\ntf.saved_model.save(net, 'my_mlp')\n!ls -lh my_mlp*\n```\n\n:begin_tab:`mxnet`\nThe model is decomposed into a (large binary) parameter file and a JSON description of the program required to execute the model computation."
    },
    {
      "chunk_id": "ad4446a81774_1",
      "chapter": "hybridize",
      "heading": "Serialization",
      "text": "The files can be read by other front-end languages supported by Python or MXNet, such as C++, R, Scala, and Perl. Let's have a look at the first few lines in the model description. :end_tab:\n\n```{.python .input}\n#@tab mxnet\n!head my_mlp-symbol.json\n```\n\n:begin_tab:`mxnet`\nEarlier, we demonstrated that, after calling the `hybridize` function, the model is able to achieve superior computing performance and portability. Note, though that hybridization can affect model flexibility, in particular in terms of control flow. Besides, contrary to the `Block` instance, which needs to use the `forward` function, for a `HybridBlock` instance we need to use the `hybrid_forward` function. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nclass HybridNet(nn.HybridBlock):\n    def __init__(self, **kwargs):\n        super(HybridNet, self).__init__(**kwargs)\n        self.hidden = nn.Dense(4)\n        self.output = nn.Dense(2)\n\n    def hybrid_forward(self, F, x):\n        print('module F: ', F)\n        print('value  x: ', x)\n        x = F.npx.relu(self.hidden(x))\n        print('result  : ', x)\n        return self.output(x)\n```\n\n:begin_tab:`mxnet`\nThe code above implements a simple network with 4 hidden units and 2 outputs. The `hybrid_forward` function takes an additional argument `F`. This is needed since, depending on whether the code has been hybridized or not, it will use a slightly different library (`ndarray` or `symbol`) for processing. Both classes perform very similar functions and MXNet automatically determines the argument. To understand what is going on we print the arguments as part of the function invocation. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nnet = HybridNet()\nnet.initialize()\nx = np.random.normal(size=(1, 3))\nnet(x)\n```\n\n:begin_tab:`mxnet`\nRepeating the forward computation will lead to the same output (we omit details). Now let's see what happens if we invoke the `hybridize` function."
    },
    {
      "chunk_id": "ad4446a81774_2",
      "chapter": "hybridize",
      "heading": "Serialization",
      "text": "Now let's see what happens if we invoke the `hybridize` function. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nnet.hybridize()\nnet(x)\n```\n\n:begin_tab:`mxnet`\nInstead of using `ndarray` we now use the `symbol` module for `F`. Moreover, even though the input is of `ndarray` type, the data flowing through the network is now converted to `symbol` type as part of the compilation process. Repeating the function call leads to a surprising outcome:\n:end_tab:\n\n```{.python .input}\n#@tab mxnet\nnet(x)\n```\n\n:begin_tab:`mxnet` \nThis is quite different from what we saw previously. All print statements, as defined in `hybrid_forward`, are omitted. Indeed, after hybridization the execution of `net(x)` does not involve the Python interpreter any longer. This means that any spurious Python code is omitted (such as print statements) in favor of a much more streamlined execution and better performance. Instead, MXNet directly calls the C++ backend. Also note that some functions are not supported in the `symbol` module (e.g.,  `asnumpy`) and operations in-place such as `a += b` and `a[:] = a + b` must be rewritten as `a = a + b`. Nonetheless, compilation of models is worth the effort whenever speed matters. The benefit can range from small percentage points to more than twice the speed, depending on the complexity of the model, the speed of the CPU, and the speed and number of GPUs. :end_tab:"
    },
    {
      "chunk_id": "7f2380ee3061_0",
      "chapter": "hybridize",
      "heading": "Summary",
      "text": "* Imperative programming makes it easy to design new models since it is possible to write code with control flow and the ability to use a large amount of the Python software ecosystem.\n* Symbolic programming requires that we specify the program and compile it before executing it. The benefit is improved performance.\n\n:begin_tab:`mxnet` \n* MXNet is able to combine the advantages of both approaches as needed.\n* Models constructed by the `HybridSequential` and `HybridBlock` classes are able to convert imperative programs into symbolic programs by calling the `hybridize` function.\n:end_tab:"
    },
    {
      "chunk_id": "dc13da8799c6_0",
      "chapter": "hybridize",
      "heading": "Exercises",
      "text": ":begin_tab:`mxnet` \n1. Add `x.asnumpy()` to the first line of the `hybrid_forward` function of the `HybridNet` class in this section. Execute the code and observe the errors you encounter. Why do they happen?\n1. What happens if we add control flow, i.e., the Python statements `if` and `for` in the `hybrid_forward` function?\n1. Review the models that interest you in the previous chapters. Can you improve their computational performance by reimplementing them?\n:end_tab:\n\n:begin_tab:`pytorch,tensorflow` \n1. Review the models that interest you in the previous chapters. Can you improve their computational performance by reimplementing them?\n:end_tab:\n\n\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/360)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/2490)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/2492)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Computational Performance\n:label:`chap_performance`\n\nIn deep learning, \ndatasets and models are usually large,\nwhich involves heavy computation.\nTherefore, computational performance matters a lot.\nThis chapter will focus on the major factors that affect computational performance:\nimperative programming, symbolic programming, asynchronous computing, automatic parallelism, and multi-GPU computation.\nBy studying this chapter, you may further improve computational performance of those models implemented in the previous chapters,\nfor example, by reducing training time without affecting accuracy.\n\n```toc\n:maxdepth: 2\n\nhybridize\nasync-computation\nauto-parallelism\nhardware\nmultiple-gpus\nmultiple-gpus-concise\nparameterserver\n```"
    },
    {
      "chunk_id": "0895d86968cf_0",
      "chapter": "multiple-gpus-concise",
      "heading": "multiple-gpus-concise",
      "text": "# Concise Implementation for Multiple GPUs\n:label:`sec_multi_gpu_concise`\n\nImplementing parallelism from scratch for every new model is no fun. Moreover, there is significant benefit in optimizing synchronization tools for high performance. In the following we will show how to do this using high-level APIs of deep learning frameworks.\nThe mathematics and the algorithms are the same as in :numref:`sec_multi_gpu`.\nQuite unsurprisingly you will need at least two GPUs to run code of this section.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```"
    },
    {
      "chunk_id": "5dbcfc358782_0",
      "chapter": "multiple-gpus-concise",
      "heading": "[**A Toy Network**]",
      "text": "Let's use a slightly more meaningful network than LeNet from :numref:`sec_multi_gpu` that is still sufficiently easy and quick to train. We pick a ResNet-18 variant :cite:`He.Zhang.Ren.ea.2016`. Since the input images are tiny we modify it slightly. In particular, the difference from :numref:`sec_resnet` is that we use a smaller convolution kernel, stride, and padding at the beginning. Moreover, we remove the max-pooling layer."
    },
    {
      "chunk_id": "5dbcfc358782_1",
      "chapter": "multiple-gpus-concise",
      "heading": "[**A Toy Network**]",
      "text": "Since the input images are tiny we modify it slightly. In particular, the difference from :numref:`sec_resnet` is that we use a smaller convolution kernel, stride, and padding at the beginning. Moreover, we remove the max-pooling layer. ```{.python .input}\n#@tab mxnet\n#@save\ndef resnet18(num_classes):\n    \"\"\"A slightly modified ResNet-18 model.\"\"\"\n    def resnet_block(num_channels, num_residuals, first_block=False):\n        blk = nn.Sequential()\n        for i in range(num_residuals):\n            if i == 0 and not first_block:\n                blk.add(d2l.Residual(\n                    num_channels, use_1x1conv=True, strides=2))\n            else:\n                blk.add(d2l.Residual(num_channels))\n        return blk\n\n    net = nn.Sequential()\n    # This model uses a smaller convolution kernel, stride, and padding and\n    # removes the max-pooling layer\n    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n            nn.BatchNorm(), nn.Activation('relu'))\n    net.add(resnet_block(64, 2, first_block=True),\n            resnet_block(128, 2),\n            resnet_block(256, 2),\n            resnet_block(512, 2))\n    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n    return net\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef resnet18(num_classes, in_channels=1):\n    \"\"\"A slightly modified ResNet-18 model.\"\"\"\n    def resnet_block(in_channels, out_channels, num_residuals,\n                     first_block=False):\n        blk = []\n        for i in range(num_residuals):\n            if i == 0 and not first_block:\n                blk.append(d2l.Residual(out_channels, use_1x1conv=True, \n                                        strides=2))\n            else:\n                blk.append(d2l.Residual(out_channels))\n        return nn.Sequential(*blk)\n\n    # This model uses a smaller convolution kernel, stride, and padding and\n    # removes the max-pooling layer\n    net = nn.Sequential(\n        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(64),\n        nn.ReLU())\n    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n    net.add_module(\"fc\", nn.Sequential(nn.Flatten(),\n                                       nn.Linear(512, num_classes)))\n    return net\n```"
    },
    {
      "chunk_id": "6f68931a5bec_0",
      "chapter": "multiple-gpus-concise",
      "heading": "Network Initialization",
      "text": ":begin_tab:`mxnet`\nThe `initialize` function allows us to initialize parameters on a device of our choice. For a refresher on initialization methods see :numref:`sec_numerical_stability`. What is particularly convenient is that it also allows us to initialize the network on *multiple* devices simultaneously. Let's try how this works in practice. :end_tab:\n\n:begin_tab:`pytorch`\nWe will initialize the network inside the training loop. For a refresher on initialization methods see :numref:`sec_numerical_stability`. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nnet = resnet18(10)\n# Get a list of GPUs\ndevices = d2l.try_all_gpus()\n# Initialize all the parameters of the network\nnet.initialize(init=init.Normal(sigma=0.01), ctx=devices)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = resnet18(10)\n# Get a list of GPUs\ndevices = d2l.try_all_gpus()\n# We will initialize the network inside the training loop\n```\n\n:begin_tab:`mxnet`\nUsing the `split_and_load` function introduced in :numref:`sec_multi_gpu` we can divide a minibatch of data and copy portions to the list of devices provided by the `devices` variable. The network instance *automatically* uses the appropriate GPU to compute the value of the forward propagation. Here we generate 4 observations and split them over the GPUs. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nx = np.random.uniform(size=(4, 1, 28, 28))\nx_shards = gluon.utils.split_and_load(x, devices)\nnet(x_shards[0]), net(x_shards[1])\n```\n\n:begin_tab:`mxnet`\nOnce data passes through the network, the corresponding parameters are initialized *on the device the data passed through*. This means that initialization happens on a per-device basis. Since we picked GPU 0 and GPU 1 for initialization, the network is initialized only there, and not on the CPU. In fact, the parameters do not even exist on the CPU. We can verify this by printing out the parameters and observing any errors that might arise."
    },
    {
      "chunk_id": "6f68931a5bec_1",
      "chapter": "multiple-gpus-concise",
      "heading": "Network Initialization",
      "text": "Since we picked GPU 0 and GPU 1 for initialization, the network is initialized only there, and not on the CPU. In fact, the parameters do not even exist on the CPU. We can verify this by printing out the parameters and observing any errors that might arise. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nweight = net[0].params.get('weight')\n\ntry:\n    weight.data()\nexcept RuntimeError:\n    print('not initialized on cpu')\nweight.data(devices[0])[0], weight.data(devices[1])[0]\n```\n\n:begin_tab:`mxnet`\nNext, let's replace the code to [**evaluate the accuracy**] by one that works (**in parallel across multiple devices**). This serves as a replacement of the `evaluate_accuracy_gpu` function from :numref:`sec_lenet`. The main difference is that we split a minibatch before invoking the network. All else is essentially identical. :end_tab:\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef evaluate_accuracy_gpus(net, data_iter, split_f=d2l.split_batch):\n    \"\"\"Compute the accuracy for a model on a dataset using multiple GPUs.\"\"\"\n    # Query the list of devices\n    devices = list(net.collect_params().values())[0].list_ctx()\n    # No. of correct predictions, no. of predictions\n    metric = d2l.Accumulator(2)\n    for features, labels in data_iter:\n        X_shards, y_shards = split_f(features, labels, devices)\n        # Run in parallel\n        pred_shards = [net(X_shard) for X_shard in X_shards]\n        metric.add(sum(float(d2l.accuracy(pred_shard, y_shard)) for\n                       pred_shard, y_shard in zip(\n                           pred_shards, y_shards)), labels.size)\n    return metric[0] / metric[1]\n```"
    },
    {
      "chunk_id": "a36f9505bfbb_0",
      "chapter": "multiple-gpus-concise",
      "heading": "[**Training**]",
      "text": "As before, the training code needs to perform several basic functions for efficient parallelism:\n\n* Network parameters need to be initialized across all devices. * While iterating over the dataset minibatches are to be divided across all devices. * We compute the loss and its gradient in parallel across devices. * Gradients are aggregated and parameters are updated accordingly. In the end we compute the accuracy (again in parallel) to report the final performance of the network. The training routine is quite similar to implementations in previous chapters, except that we need to split and aggregate data."
    },
    {
      "chunk_id": "a36f9505bfbb_1",
      "chapter": "multiple-gpus-concise",
      "heading": "[**Training**]",
      "text": "In the end we compute the accuracy (again in parallel) to report the final performance of the network. The training routine is quite similar to implementations in previous chapters, except that we need to split and aggregate data. ```{.python .input}\n#@tab mxnet\ndef train(num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    ctx = [d2l.try_gpu(i) for i in range(num_gpus)]\n    net.initialize(init=init.Normal(sigma=0.01), ctx=ctx, force_reinit=True)\n    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n                            {'learning_rate': lr})\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n    timer, num_epochs = d2l.Timer(), 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    for epoch in range(num_epochs):\n        timer.start()\n        for features, labels in train_iter:\n            X_shards, y_shards = d2l.split_batch(features, labels, ctx)\n            with autograd.record():\n                ls = [loss(net(X_shard), y_shard) for X_shard, y_shard\n                      in zip(X_shards, y_shards)]\n            for l in ls:\n                l.backward()\n            trainer.step(batch_size)\n        npx.waitall()\n        timer.stop()\n        animator.add(epoch + 1, (evaluate_accuracy_gpus(net, test_iter),))\n    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n          f'on {str(ctx)}')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(net, num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n    def init_weights(module):\n        if type(module) in [nn.Linear, nn.Conv2d]:\n            nn.init.normal_(module.weight, std=0.01)\n    net.apply(init_weights)\n    # Set the model on multiple GPUs\n    net = nn.DataParallel(net, device_ids=devices)\n    trainer = torch.optim.SGD(net.parameters(), lr)\n    loss = nn.CrossEntropyLoss()\n    timer, num_epochs = d2l.Timer(), 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    for epoch in range(num_epochs):\n        net.train()\n        timer.start()\n        for X, y in train_iter:\n            trainer.zero_grad()\n            X, y = X.to(devices[0]), y.to(devices[0])\n            l = loss(net(X), y)\n            l.backward()\n            trainer.step()\n        timer.stop()\n        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n          f'on {str(devices)}')\n```\n\nLet's see how this works in practice."
    },
    {
      "chunk_id": "a36f9505bfbb_2",
      "chapter": "multiple-gpus-concise",
      "heading": "[**Training**]",
      "text": "As a warm-up we [**train the network on a single GPU.**]\n\n```{.python .input}\n#@tab mxnet\ntrain(num_gpus=1, batch_size=256, lr=0.1)\n```\n\n```{.python .input}\n#@tab pytorch\ntrain(net, num_gpus=1, batch_size=256, lr=0.1)\n```\n\nNext we [**use 2 GPUs for training**]. Compared with LeNet\nevaluated in :numref:`sec_multi_gpu`,\nthe model for ResNet-18 is considerably more complex. This is where parallelization shows its advantage. The time for computation is meaningfully larger than the time for synchronizing parameters. This improves scalability since the overhead for parallelization is less relevant. ```{.python .input}\n#@tab mxnet\ntrain(num_gpus=2, batch_size=512, lr=0.2)\n```\n\n```{.python .input}\n#@tab pytorch\ntrain(net, num_gpus=2, batch_size=512, lr=0.2)\n```"
    },
    {
      "chunk_id": "844bcddb1e54_0",
      "chapter": "multiple-gpus-concise",
      "heading": "Summary",
      "text": ":begin_tab:`mxnet`\n* Gluon provides primitives for model initialization across multiple devices by providing a context list.\n:end_tab:\n* Data is automatically evaluated on the devices where the data can be found.\n* Take care to initialize the networks on each device before trying to access the parameters on that device. Otherwise you will encounter an error.\n* The optimization algorithms automatically aggregate over multiple GPUs."
    },
    {
      "chunk_id": "037a64ce691d_0",
      "chapter": "multiple-gpus-concise",
      "heading": "Exercises",
      "text": ":begin_tab:`mxnet`\n1. This section uses ResNet-18. Try different epochs, batch sizes, and learning rates. Use more GPUs for computation. What happens if you try this with 16 GPUs (e.g., on an AWS p2.16xlarge instance)?\n1. Sometimes, different devices provide different computing power. We could use the GPUs and the CPU at the same time. How should we divide the work? Is it worth the effort? Why? Why not?\n1. What happens if we drop `npx.waitall()`? How would you modify training such that you have an overlap of up to two steps for parallelism?\n:end_tab:\n\n:begin_tab:`pytorch`\n1. This section uses ResNet-18. Try different epochs, batch sizes, and learning rates. Use more GPUs for computation. What happens if you try this with 16 GPUs (e.g., on an AWS p2.16xlarge instance)?\n1. Sometimes, different devices provide different computing power. We could use the GPUs and the CPU at the same time. How should we divide the work? Is it worth the effort? Why? Why not?\n:end_tab:\n\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/365)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1403)\n:end_tab:"
    },
    {
      "chunk_id": "a5b674066780_0",
      "chapter": "multiple-gpus",
      "heading": "multiple-gpus",
      "text": "# Training on Multiple GPUs\n:label:`sec_multi_gpu`\n\nSo far we discussed how to train models efficiently on CPUs and GPUs. We even showed how deep learning frameworks allow one to parallelize computation and communication automatically between them in :numref:`sec_auto_para`. We also showed in :numref:`sec_use_gpu` how to list all the available GPUs on a computer using the `nvidia-smi` command.\nWhat we did *not* discuss is how to actually parallelize deep learning training.\nInstead, we implied in passing that one would somehow split the data across multiple devices and make it work. The present section fills in the details and shows how to train a network in parallel when starting from scratch. Details on how to take advantage of functionality in high-level APIs is relegated to :numref:`sec_multi_gpu_concise`.\nWe assume that you are familiar with minibatch stochastic gradient descent algorithms such as the ones described in :numref:`sec_minibatch_sgd`."
    },
    {
      "chunk_id": "180a054eb9e2_0",
      "chapter": "multiple-gpus",
      "heading": "Splitting the Problem",
      "text": "Let's start with a simple computer vision problem and a slightly archaic network, e.g., with multiple layers of convolutions, pooling, and possibly a few fully connected layers in the end. That is, let's start with a network that looks quite similar to LeNet :cite:`LeCun.Bottou.Bengio.ea.1998` or AlexNet :cite:`Krizhevsky.Sutskever.Hinton.2012`. Given multiple GPUs (2 if it is a desktop server, 4 on an AWS g4dn.12xlarge instance, 8 on a p3.16xlarge, or 16 on a p2.16xlarge), we want to partition training in a manner as to achieve good speedup while simultaneously benefitting from simple and reproducible design choices. Multiple GPUs, after all, increase both *memory* and *computation* ability. In a nutshell, we have the following choices, given a minibatch of training data that we want to classify. First, we could partition the network across multiple GPUs. That is, each GPU takes as input the data flowing into a particular layer, processes data across a number of subsequent layers and then sends the data to the next GPU. This allows us to process data with larger networks when compared with what a single GPU could handle. Besides,\nmemory footprint per GPU can be well controlled (it is a fraction of the total network footprint). However, the interface between layers (and thus GPUs) requires tight synchronization. This can be tricky, in particular if the computational workloads are not properly matched between layers. The problem is exacerbated for large numbers of GPUs. The interface between layers also\nrequires large amounts of data transfer,\nsuch as activations and gradients. This may overwhelm the bandwidth of the GPU buses. Moreover, compute-intensive, yet sequential operations are nontrivial to partition. See e.g., :citet:`Mirhoseini.Pham.Le.ea.2017` for a best effort in this regard. It remains a difficult problem and it is unclear whether it is possible to achieve good (linear) scaling on nontrivial problems."
    },
    {
      "chunk_id": "180a054eb9e2_1",
      "chapter": "multiple-gpus",
      "heading": "Splitting the Problem",
      "text": "See e.g., :citet:`Mirhoseini.Pham.Le.ea.2017` for a best effort in this regard. It remains a difficult problem and it is unclear whether it is possible to achieve good (linear) scaling on nontrivial problems. We do not recommend it unless there is excellent framework or operating system support for chaining together multiple GPUs. Second, we could split the work layerwise. For instance, rather than computing 64 channels on a single GPU we could split up the problem across 4 GPUs, each of which generates data for 16 channels. Likewise, for a fully connected layer we could split the number of output units. :numref:`fig_alexnet_original` (taken from :citet:`Krizhevsky.Sutskever.Hinton.2012`)\nillustrates this design, where this strategy was used to deal with GPUs that had a very small memory footprint (2 GB at the time). This allows for good scaling in terms of computation, provided that the number of channels (or units) is not too small. Besides,\nmultiple GPUs can process increasingly larger networks since the available memory scales linearly. ![Model parallelism in the original AlexNet design due to limited GPU memory.](../img/alexnet-original.svg)\n:label:`fig_alexnet_original`\n\nHowever,\nwe need a *very large* number of synchronization or barrier operations since each layer depends on the results from all the other layers. Moreover, the amount of data that needs to be transferred is potentially even larger than when distributing layers across GPUs. Thus, we do not recommend this approach due to its bandwidth cost and complexity. Last, we could partition data across multiple GPUs. This way all GPUs perform the same type of work, albeit on different observations. Gradients are aggregated across GPUs after each minibatch of training data. This is the simplest approach and it can be applied in any situation. We only need to synchronize after each minibatch. That said, it is highly desirable to start exchanging gradients parameters already while others are still being computed."
    },
    {
      "chunk_id": "180a054eb9e2_2",
      "chapter": "multiple-gpus",
      "heading": "Splitting the Problem",
      "text": "This is the simplest approach and it can be applied in any situation. We only need to synchronize after each minibatch. That said, it is highly desirable to start exchanging gradients parameters already while others are still being computed. Moreover, larger numbers of GPUs lead to larger minibatch sizes, thus increasing training efficiency. However, adding more GPUs does not allow us to train larger models. ![Parallelization on multiple GPUs. From left to right: original problem, network partitioning, layerwise partitioning, data parallelism.](../img/splitting.svg)\n:label:`fig_splitting`\n\n\nA comparison of different ways of parallelization on multiple GPUs is depicted in :numref:`fig_splitting`. By and large, data parallelism is the most convenient way to proceed, provided that we have access to GPUs with sufficiently large memory. See also :cite:`Li.Andersen.Park.ea.2014` for a detailed description of partitioning for distributed training. GPU memory used to be a problem in the early days of deep learning. By now this issue has been resolved for all but the most unusual cases. We focus on data parallelism in what follows."
    },
    {
      "chunk_id": "4143b20e39da_0",
      "chapter": "multiple-gpus",
      "heading": "Data Parallelism",
      "text": "Assume that there are $k$ GPUs on a machine. Given the model to be trained, each GPU will maintain a complete set of model parameters independently though parameter values across the GPUs are identical and synchronized.\nAs an example,\n:numref:`fig_data_parallel` illustrates\ntraining with\ndata parallelism when $k=2$.\n\n\n![Calculation of minibatch stochastic gradient descent using data parallelism on two GPUs.](../img/data-parallel.svg)\n:label:`fig_data_parallel`\n\nIn general, the training proceeds as follows:\n\n* In any iteration of training, given a random minibatch, we split the examples in the batch into $k$ portions and distribute them evenly across the GPUs.\n* Each GPU calculates loss and gradient of the model parameters based on the minibatch subset it was assigned.\n* The local gradients of each of the $k$ GPUs are aggregated to obtain the current minibatch stochastic gradient.\n* The aggregate gradient is re-distributed to each GPU.\n* Each GPU uses this minibatch stochastic gradient to update the complete set of model parameters that it maintains.\n\n\n\n\nNote that in practice we *increase* the minibatch size $k$-fold when training on $k$ GPUs such that each GPU has the same amount of work to do as if we were training on a single GPU only. On a 16-GPU server this can increase the minibatch size considerably and we may have to increase the learning rate accordingly.\nAlso note that batch normalization in :numref:`sec_batch_norm` needs to be adjusted, e.g., by keeping a separate batch normalization coefficient per GPU.\nIn what follows we will use a toy network to illustrate multi-GPU training.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```"
    },
    {
      "chunk_id": "81a09edc2049_0",
      "chapter": "multiple-gpus",
      "heading": "[**A Toy Network**]",
      "text": "We use LeNet as introduced in :numref:`sec_lenet` (with slight modifications). We define it from scratch to illustrate parameter exchange and synchronization in detail."
    },
    {
      "chunk_id": "81a09edc2049_1",
      "chapter": "multiple-gpus",
      "heading": "[**A Toy Network**]",
      "text": "We use LeNet as introduced in :numref:`sec_lenet` (with slight modifications). We define it from scratch to illustrate parameter exchange and synchronization in detail. ```{.python .input}\n#@tab mxnet\n# Initialize model parameters\nscale = 0.01\nW1 = np.random.normal(scale=scale, size=(20, 1, 3, 3))\nb1 = np.zeros(20)\nW2 = np.random.normal(scale=scale, size=(50, 20, 5, 5))\nb2 = np.zeros(50)\nW3 = np.random.normal(scale=scale, size=(800, 128))\nb3 = np.zeros(128)\nW4 = np.random.normal(scale=scale, size=(128, 10))\nb4 = np.zeros(10)\nparams = [W1, b1, W2, b2, W3, b3, W4, b4]\n\n# Define the model\ndef lenet(X, params):\n    h1_conv = npx.convolution(data=X, weight=params[0], bias=params[1],\n                              kernel=(3, 3), num_filter=20)\n    h1_activation = npx.relu(h1_conv)\n    h1 = npx.pooling(data=h1_activation, pool_type='avg', kernel=(2, 2),\n                     stride=(2, 2))\n    h2_conv = npx.convolution(data=h1, weight=params[2], bias=params[3],\n                              kernel=(5, 5), num_filter=50)\n    h2_activation = npx.relu(h2_conv)\n    h2 = npx.pooling(data=h2_activation, pool_type='avg', kernel=(2, 2),\n                     stride=(2, 2))\n    h2 = h2.reshape(h2.shape[0], -1)\n    h3_linear = np.dot(h2, params[4]) + params[5]\n    h3 = npx.relu(h3_linear)\n    y_hat = np.dot(h3, params[6]) + params[7]\n    return y_hat\n\n# Cross-entropy loss function\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\n```\n\n```{.python .input}\n#@tab pytorch\n# Initialize model parameters\nscale = 0.01\nW1 = torch.randn(size=(20, 1, 3, 3)) * scale\nb1 = torch.zeros(20)\nW2 = torch.randn(size=(50, 20, 5, 5)) * scale\nb2 = torch.zeros(50)\nW3 = torch.randn(size=(800, 128)) * scale\nb3 = torch.zeros(128)\nW4 = torch.randn(size=(128, 10)) * scale\nb4 = torch.zeros(10)\nparams = [W1, b1, W2, b2, W3, b3, W4, b4]\n\n# Define the model\ndef lenet(X, params):\n    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n    h1_activation = F.relu(h1_conv)\n    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n    h2_activation = F.relu(h2_conv)\n    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n    h2 = h2.reshape(h2.shape[0], -1)\n    h3_linear = torch.mm(h2, params[4]) + params[5]\n    h3 = F.relu(h3_linear)\n    y_hat = torch.mm(h3, params[6]) + params[7]\n    return y_hat\n\n# Cross-entropy loss function\nloss = nn.CrossEntropyLoss(reduction='none')\n```"
    },
    {
      "chunk_id": "d1c8569e5038_0",
      "chapter": "multiple-gpus",
      "heading": "Data Synchronization",
      "text": "For efficient multi-GPU training we need two basic operations. First we need to have the ability to [**distribute a list of parameters to multiple devices**] and to attach gradients (`get_params`). Without parameters it is impossible to evaluate the network on a GPU. Second, we need the ability to sum parameters across multiple devices, i.e., we need an `allreduce` function. ```{.python .input}\n#@tab mxnet\ndef get_params(params, device):\n    new_params = [p.copyto(device) for p in params]\n    for p in new_params:\n        p.attach_grad()\n    return new_params\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_params(params, device):\n    new_params = [p.to(device) for p in params]\n    for p in new_params:\n        p.requires_grad_()\n    return new_params\n```\n\nLet's try it out by copying the model parameters to one GPU. ```{.python .input}\n#@tab all\nnew_params = get_params(params, d2l.try_gpu(0))\nprint('b1 weight:', new_params[1])\nprint('b1 grad:', new_params[1].grad)\n```\n\nSince we did not perform any computation yet, the gradient with regard to the bias parameter is still zero. Now let's assume that we have a vector distributed across multiple GPUs. The following [**`allreduce` function adds up all vectors and broadcasts the result back to all GPUs**]. Note that for this to work we need to copy the data to the device accumulating the results. ```{.python .input}\n#@tab mxnet\ndef allreduce(data):\n    for i in range(1, len(data)):\n        data[0][:] += data[i].copyto(data[0].ctx)\n    for i in range(1, len(data)):\n        data[0].copyto(data[i])\n```\n\n```{.python .input}\n#@tab pytorch\ndef allreduce(data):\n    for i in range(1, len(data)):\n        data[0][:] += data[i].to(data[0].device)\n    for i in range(1, len(data)):\n        data[i][:] = data[0].to(data[i].device)\n```\n\nLet's test this by creating vectors with different values on different devices and aggregate them."
    },
    {
      "chunk_id": "d1c8569e5038_1",
      "chapter": "multiple-gpus",
      "heading": "Data Synchronization",
      "text": "```{.python .input}\n#@tab mxnet\ndata = [np.ones((1, 2), ctx=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\nprint('before allreduce:\\n', data[0], '\\n', data[1])\nallreduce(data)\nprint('after allreduce:\\n', data[0], '\\n', data[1])\n```\n\n```{.python .input}\n#@tab pytorch\ndata = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\nprint('before allreduce:\\n', data[0], '\\n', data[1])\nallreduce(data)\nprint('after allreduce:\\n', data[0], '\\n', data[1])\n```"
    },
    {
      "chunk_id": "a48c0b8c8402_0",
      "chapter": "multiple-gpus",
      "heading": "Distributing Data",
      "text": "We need a simple utility function to [**distribute a minibatch evenly across multiple GPUs**]. For instance, on two GPUs we would like to have half of the data to be copied to either of the GPUs.\nSince it is more convenient and more concise, we use the built-in function from the deep learning framework to try it out on a $4 \\times 5$ matrix.\n\n```{.python .input}\n#@tab mxnet\ndata = np.arange(20).reshape(4, 5)\ndevices = [npx.gpu(0), npx.gpu(1)]\nsplit = gluon.utils.split_and_load(data, devices)\nprint('input :', data)\nprint('load into', devices)\nprint('output:', split)\n```\n\n```{.python .input}\n#@tab pytorch\ndata = torch.arange(20).reshape(4, 5)\ndevices = [torch.device('cuda:0'), torch.device('cuda:1')]\nsplit = nn.parallel.scatter(data, devices)\nprint('input :', data)\nprint('load into', devices)\nprint('output:', split)\n```\n\nFor later reuse we define a `split_batch` function that splits both data and labels.\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef split_batch(X, y, devices):\n    \"\"\"Split `X` and `y` into multiple devices.\"\"\"\n    assert X.shape[0] == y.shape[0]\n    return (gluon.utils.split_and_load(X, devices),\n            gluon.utils.split_and_load(y, devices))\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef split_batch(X, y, devices):\n    \"\"\"Split `X` and `y` into multiple devices.\"\"\"\n    assert X.shape[0] == y.shape[0]\n    return (nn.parallel.scatter(X, devices),\n            nn.parallel.scatter(y, devices))\n```"
    },
    {
      "chunk_id": "a80d360b43f6_0",
      "chapter": "multiple-gpus",
      "heading": "Training",
      "text": "Now we can implement [**multi-GPU training on a single minibatch**]. Its implementation is primarily based on the data parallelism approach described in this section. We will use the auxiliary functions we just discussed, `allreduce` and `split_and_load`, to synchronize the data among multiple GPUs. Note that we do not need to write any specific code to achieve parallelism. Since the computational graph does not have any dependencies across devices within a minibatch, it is executed in parallel *automatically*."
    },
    {
      "chunk_id": "a80d360b43f6_1",
      "chapter": "multiple-gpus",
      "heading": "Training",
      "text": "Note that we do not need to write any specific code to achieve parallelism. Since the computational graph does not have any dependencies across devices within a minibatch, it is executed in parallel *automatically*. ```{.python .input}\n#@tab mxnet\ndef train_batch(X, y, device_params, devices, lr):\n    X_shards, y_shards = split_batch(X, y, devices)\n    with autograd.record():  # Loss is calculated separately on each GPU\n        ls = [loss(lenet(X_shard, device_W), y_shard)\n              for X_shard, y_shard, device_W in zip(\n                  X_shards, y_shards, device_params)]\n    for l in ls:  # Backpropagation is performed separately on each GPU\n        l.backward()\n    # Sum all gradients from each GPU and broadcast them to all GPUs\n    for i in range(len(device_params[0])):\n        allreduce([device_params[c][i].grad for c in range(len(devices))])\n    # The model parameters are updated separately on each GPU\n    for param in device_params:\n        d2l.sgd(param, lr, X.shape[0])  # Here, we use a full-size batch\n```\n\n```{.python .input}\n#@tab pytorch\ndef train_batch(X, y, device_params, devices, lr):\n    X_shards, y_shards = split_batch(X, y, devices)\n    # Loss is calculated separately on each GPU\n    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n          for X_shard, y_shard, device_W in zip(\n              X_shards, y_shards, device_params)]\n    for l in ls:  # Backpropagation is performed separately on each GPU\n        l.backward()\n    # Sum all gradients from each GPU and broadcast them to all GPUs\n    with torch.no_grad():\n        for i in range(len(device_params[0])):\n            allreduce([device_params[c][i].grad for c in range(len(devices))])\n    # The model parameters are updated separately on each GPU\n    for param in device_params:\n        d2l.sgd(param, lr, X.shape[0]) # Here, we use a full-size batch\n```\n\nNow, we can define [**the training function**]."
    },
    {
      "chunk_id": "a80d360b43f6_2",
      "chapter": "multiple-gpus",
      "heading": "Training",
      "text": "It is slightly different from the ones used in the previous chapters: we need to allocate the GPUs and copy all the model parameters to all the devices. Obviously each batch is processed using the `train_batch` function to deal with multiple GPUs. For convenience (and conciseness of code) we compute the accuracy on a single GPU, though this is *inefficient* since the other GPUs are idle."
    },
    {
      "chunk_id": "a80d360b43f6_3",
      "chapter": "multiple-gpus",
      "heading": "Training",
      "text": "Obviously each batch is processed using the `train_batch` function to deal with multiple GPUs. For convenience (and conciseness of code) we compute the accuracy on a single GPU, though this is *inefficient* since the other GPUs are idle. ```{.python .input}\n#@tab mxnet\ndef train(num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n    # Copy model parameters to `num_gpus` GPUs\n    device_params = [get_params(params, d) for d in devices]\n    num_epochs = 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    timer = d2l.Timer()\n    for epoch in range(num_epochs):\n        timer.start()\n        for X, y in train_iter:\n            # Perform multi-GPU training for a single minibatch\n            train_batch(X, y, device_params, devices, lr)\n            npx.waitall()\n        timer.stop()\n        # Evaluate the model on GPU 0\n        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n          f'on {str(devices)}')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(num_gpus, batch_size, lr):\n    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n    # Copy model parameters to `num_gpus` GPUs\n    device_params = [get_params(params, d) for d in devices]\n    num_epochs = 10\n    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n    timer = d2l.Timer()\n    for epoch in range(num_epochs):\n        timer.start()\n        for X, y in train_iter:\n            # Perform multi-GPU training for a single minibatch\n            train_batch(X, y, device_params, devices, lr)\n            torch.cuda.synchronize()\n        timer.stop()\n        # Evaluate the model on GPU 0\n        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n          f'on {str(devices)}')\n```\n\nLet's see how well this works [**on a single GPU**]."
    },
    {
      "chunk_id": "a80d360b43f6_4",
      "chapter": "multiple-gpus",
      "heading": "Training",
      "text": "We first use a batch size of 256 and a learning rate of 0.2. ```{.python .input}\n#@tab all\ntrain(num_gpus=1, batch_size=256, lr=0.2)\n```\n\nBy keeping the batch size and learning rate unchanged and [**increasing the number of GPUs to 2**], we can see that the test accuracy roughly stays the same compared with\nthe previous experiment. In terms of the optimization algorithms, they are identical. Unfortunately there is no meaningful speedup to be gained here: the model is simply too small; moreover we only have a small dataset, where our slightly unsophisticated approach to implementing multi-GPU training suffered from significant Python overhead. We will encounter more complex models and more sophisticated ways of parallelization going forward. Let's see what happens nonetheless for Fashion-MNIST. ```{.python .input}\n#@tab all\ntrain(num_gpus=2, batch_size=256, lr=0.2)\n```"
    },
    {
      "chunk_id": "c70e8627c909_0",
      "chapter": "multiple-gpus",
      "heading": "Summary",
      "text": "* There are multiple ways to split deep network training over multiple GPUs. We could split them between layers, across layers, or across data. The former two require tightly choreographed data transfers. Data parallelism is the simplest strategy.\n* Data parallel training is straightforward. However, it increases the effective minibatch size to be efficient.\n* In data parallelism, data is split across multiple GPUs, where each GPU executes its own forward and backward operation and subsequently gradients are aggregated and results are broadcast back to the GPUs.\n* We may use slightly increased learning rates for larger minibatches."
    },
    {
      "chunk_id": "3de1c5563ef3_0",
      "chapter": "multiple-gpus",
      "heading": "Exercises",
      "text": "1. When training on $k$ GPUs, change the minibatch size from $b$ to $k \\cdot b$, i.e., scale it up by the number of GPUs.\n1. Compare accuracy for different learning rates. How does it scale with the number of GPUs?\n1. Implement a more efficient `allreduce` function that aggregates different parameters on different GPUs? Why is it more efficient?\n1. Implement multi-GPU test accuracy computation.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/364)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1669)\n:end_tab:"
    },
    {
      "chunk_id": "5e275a86309d_0",
      "chapter": "parameterserver",
      "heading": "parameterserver",
      "text": "# Parameter Servers\n:label:`sec_parameterserver`\n\nAs we move from a single GPU to multiple GPUs and then to multiple servers containing multiple GPUs, possibly all spread out across multiple racks and network switches,\nour algorithms for distributed and parallel training need to become much more sophisticated. Details matter since different interconnects have very different bandwidth (e.g., NVLink can offer up to 100 GB/s across 6 links in an appropriate setting, PCIe 4.0 (16-lane) offers 32 GB/s, while even high speed 100GbE Ethernet only amounts to 10 GB/s). At the same time it is unreasonable to expect that a statistical modeler be an expert in networking and systems.\n\nThe core idea of the parameter server was introduced in :citet:`Smola.Narayanamurthy.2010` in the context of distributed latent variable models. A description of the push and pull semantics then followed in :citet:`Ahmed.Aly.Gonzalez.ea.2012` and a description of the system and an open source library followed in :citet:`Li.Andersen.Park.ea.2014`. In the following we will motivate the components needed for efficiency."
    },
    {
      "chunk_id": "89d648d2981c_0",
      "chapter": "parameterserver",
      "heading": "Data-Parallel Training",
      "text": "Let's review the data parallel training approach to distributed training. We will use this to the exclusion of all others in this section since it is significantly simpler to implement in practice. There are virtually no use cases (besides deep learning on graphs) where any other strategy for parallelism is preferred since GPUs have plenty of memory nowadays. :numref:`fig_parameterserver` describes the variant of data parallelism that we implemented in :numref:`sec_multi_gpu`. The key aspect in it is that the aggregation of gradients occurs on one single GPU (GPU 0) before the updated parameters are rebroadcast to all GPUs. ![Left: single GPU training. Right: a variant of multi-GPU training: (1) we compute loss and gradient, (2) all gradients are aggregated on one GPU, (3) parameter update happens and the parameters are re-distributed to all GPUs.](../img/ps.svg)\n:label:`fig_parameterserver`\n\nIn retrospect, the decision to aggregate on GPU 0 seems rather ad-hoc. After all, we might just as well aggregate on the CPU. In fact, we could even decide to aggregate some of the parameters on one GPU and some others on another. Provided that the optimization algorithm supports this, there is no real reason for why we could not. For instance, if we have four parameter vectors with associated gradients $\\mathbf{g}_1, \\ldots, \\mathbf{g}_4$ we could aggregate the gradients on one GPU for each $\\mathbf{g}_i$ ($i = 1, \\ldots, 4$). This reasoning seems arbitrary and frivolous. After all, the mathematics is the same throughout. However, we are dealing with real physical hardware where different buses have different bandwidth as discussed in :numref:`sec_hardware`. Consider a real 4-way GPU server as described in :numref:`fig_bw_hierarchy`. If it is particularly well connected, it might have a 100 GbE network card. More typical numbers are in the 1--10 GbE range with an effective bandwidth of 100 MB/s to 1 GB/s."
    },
    {
      "chunk_id": "89d648d2981c_1",
      "chapter": "parameterserver",
      "heading": "Data-Parallel Training",
      "text": "Consider a real 4-way GPU server as described in :numref:`fig_bw_hierarchy`. If it is particularly well connected, it might have a 100 GbE network card. More typical numbers are in the 1--10 GbE range with an effective bandwidth of 100 MB/s to 1 GB/s. Since the CPUs have too few PCIe lanes to connect to all GPUs directly (e.g., consumer-grade Intel CPUs have 24 lanes) we need a [multiplexer](https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches). The bandwidth from the CPU on a 16x Gen3 link is 16 GB/s. This is also the speed at which *each* of the GPUs is connected to the switch. This means that it is more effective to communicate between the devices. ![A 4-way GPU server.](../img/bw-hierarchy.svg)\n:label:`fig_bw_hierarchy`\n\nFor the sake of the argument let's assume that the gradients are of 160 MB. In this case it takes 30 ms to send the gradients from all 3 remaining GPUs to the fourth one (each transfer takes 10 ms = 160 MB / 16 GB/s). Adding another 30 ms to transmit the weight vectors back we arrive at a total of 60 ms. If we send all data to the CPU we incur a penalty of 40 ms since *each* of the four GPUs needs to send the data to the CPU, yielding a total of 80 ms. Lastly assume that we are able to split the gradients into 4 parts of 40 MB each. Now we can aggregate each of the parts on a different GPU *simultaneously* since the PCIe switch offers a full-bandwidth operation between all links. Instead of 30 ms this takes 7.5 ms, yielding a total of 15 ms for a synchronization operation. In short, depending on how we synchronize parameters the same operation can take anywhere from 15 ms to 80 ms. :numref:`fig_ps_distributed` depicts the different strategies for exchanging parameters. ![Parameter synchronization strategies.](../img/ps-distributed.svg)\n:label:`fig_ps_distributed`\n\nNote that we have yet another tool at our disposal when it comes to improving performance: in a deep network it takes some time to compute all gradients from the top to the bottom."
    },
    {
      "chunk_id": "89d648d2981c_2",
      "chapter": "parameterserver",
      "heading": "Data-Parallel Training",
      "text": "We can begin synchronizing gradients for some parameter groups even while we are still busy computing them for others. See e.g., :citet:`Sergeev.Del-Balso.2018` for details on how to do this in [Horovod](https://github.com/horovod/horovod)."
    },
    {
      "chunk_id": "ae01977278ab_0",
      "chapter": "parameterserver",
      "heading": "Ring Synchronization",
      "text": "When it comes to synchronization on modern deep learning hardware we often encounter significantly bespoke network connectivity. For instance, the AWS p3.16xlarge and NVIDIA DGX-2 instances share the connectivity structure of :numref:`fig_nvlink`. Each GPU connects to a host CPU via a PCIe link which operates at best at 16 GB/s. Additionally each GPU also has 6 NVLink connections, each of which is capable of transferring 300 Gbit/s bidirectionally. This amounts to around 18 GB/s per link per direction. In short, the aggregate NVLink bandwidth is significantly higher than the PCIe bandwidth. The question is how to use it most efficiently. ![NVLink connectivity on 8  V100 GPU servers (image courtesy of NVIDIA).](../img/nvlink.svg)\n:label:`fig_nvlink`\n\nIt turns out that the optimal synchronization strategy is to decompose the network into two rings and to use them to synchronize data directly :cite:`Wang.Li.Liberty.ea.2018`. :numref:`fig_nvlink_twoloop` illustrates that the network can be decomposed into one ring (1-2-3-4-5-6-7-8-1) with double NVLink bandwidth and into one (1-4-6-3-5-8-2-7-1) with regular bandwidth. Designing an efficient synchronization protocol in this case is nontrivial. ![Decomposition of the NVLink network into two rings.](../img/nvlink-twoloop.svg)\n:label:`fig_nvlink_twoloop`\n\n\nConsider the following thought experiment: given a ring of $n$ computing nodes (or GPUs) we can send gradients from the first to the second node. There it is added to the local gradient and sent on to the third node, and so on. After $n-1$ steps the aggregate gradient can be found in the last-visited node. That is, the time to aggregate gradients grows linearly with the number of nodes. But if we do this the algorithm is quite inefficient. After all, at any time there is only one of the nodes communicating. What if we broke the gradients into $n$ chunks and started synchronizing chunk $i$ starting at node $i$? Since each chunk is of size $1/n$ the total time is now $(n-1)/n \\approx 1$."
    },
    {
      "chunk_id": "ae01977278ab_1",
      "chapter": "parameterserver",
      "heading": "Ring Synchronization",
      "text": "After all, at any time there is only one of the nodes communicating. What if we broke the gradients into $n$ chunks and started synchronizing chunk $i$ starting at node $i$? Since each chunk is of size $1/n$ the total time is now $(n-1)/n \\approx 1$. In other words, the time spent to aggregate gradients *does not grow* as we increase the size of the ring. This is quite an astonishing result. :numref:`fig_ringsync` illustrates the sequence of steps on $n=4$ nodes. ![Ring synchronization across 4 nodes. Each node starts transmitting parts of gradients to its left neighbor until the assembled gradient can be found in its right neighbor.](../img/ringsync.svg)\n:label:`fig_ringsync`\n\nIf we use the same example of synchronizing 160 MB across 8 V100 GPUs we arrive at approximately $2 \\cdot 160 \\textrm{MB} / (3 \\cdot 18 \\textrm{GB/s}) \\approx 6 \\textrm{ms}$. This is better than using the PCIe bus, even though we are now using 8 GPUs. Note that in practice these numbers are a bit worse, since deep learning frameworks often fail to assemble communication into large burst transfers. Note that there is a common misconception that ring synchronization is fundamentally different from other synchronization algorithms. The only difference is that the synchronization path is somewhat more elaborate when compared with a simple tree."
    },
    {
      "chunk_id": "7ffa9500ceaf_0",
      "chapter": "parameterserver",
      "heading": "Multi-Machine Training",
      "text": "Distributed training on multiple machines adds a further challenge: we need to communicate with servers that are only connected across a comparatively lower bandwidth fabric that can be over an order of magnitude slower in some cases. Synchronization across devices is tricky. After all, different machines running training code will have subtly different speed. Hence we need to *synchronize* them if we want to use synchronous distributed optimization. :numref:`fig_ps_multimachine` illustrates how distributed parallel training occurs. 1. A (different) batch of data is read on each machine, split across multiple GPUs and transferred to GPU memory. There predictions and gradients are computed on each GPU batch separately. 2. The gradients from all local GPUs are aggregated on one GPU (or parts of it are aggregated over different GPUs). 3. The gradients are sent to the CPUs. 4. The CPUs send the gradients to a central parameter server which aggregates all the gradients. 5. The aggregate gradients are then used to update the parameters and the updated parameters are broadcast back to the individual CPUs. 6. The information is sent to one (or multiple) GPUs. 7. The updated parameters are spread across all GPUs. ![Multi-machine multi-GPU distributed parallel training.](../img/ps-multimachine.svg)\n:label:`fig_ps_multimachine`\n\nEach of these operations seems rather straightforward. And, indeed, they can be carried out efficiently *within* a single machine. Once we look at multiple machines, though, we can see that the central parameter server becomes the bottleneck. After all, the bandwidth per server is limited, hence for $m$ workers the time it takes to send all gradients to the server is $\\mathcal{O}(m)$. We can break through this barrier by increasing the number of servers to $n$. At this point each server only needs to store $\\mathcal{O}(1/n)$ of the parameters, hence the total time for updates and optimization becomes $\\mathcal{O}(m/n)$."
    },
    {
      "chunk_id": "7ffa9500ceaf_1",
      "chapter": "parameterserver",
      "heading": "Multi-Machine Training",
      "text": "We can break through this barrier by increasing the number of servers to $n$. At this point each server only needs to store $\\mathcal{O}(1/n)$ of the parameters, hence the total time for updates and optimization becomes $\\mathcal{O}(m/n)$. Matching both numbers yields constant scaling regardless of how many workers we are dealing with. In practice we use the *same* machines both as workers and as servers. :numref:`fig_ps_multips` illustrates the design (see also :cite:`Li.Andersen.Park.ea.2014` for details). In particular, ensuring that multiple machines work without unreasonable delays is nontrivial. ![Top: a single parameter server is a bottleneck since its bandwidth is finite. Bottom: multiple parameter servers store parts of the parameters with aggregate bandwidth.](../img/ps-multips.svg)\n:label:`fig_ps_multips`"
    },
    {
      "chunk_id": "59b22ce69ca4_0",
      "chapter": "parameterserver",
      "heading": "Key--Value Stores",
      "text": "Implementing the steps required for distributed multi-GPU training in practice is nontrivial. This is why it pays to use a common abstraction, namely that of a *key--value store* with redefined update semantics. Across many workers and many GPUs the computation for gradient $i$ can be defined as\n\n$$\\mathbf{g}_{i} = \\sum_{k \\in \\textrm{workers}} \\sum_{j \\in \\textrm{GPUs}} \\mathbf{g}_{ijk},$$\n\nwhere $\\mathbf{g}_{ijk}$ is part of gradient $i$ split on GPU $j$ of worker $k$. The key aspect in this operation is that it is a *commutative reduction*, that is, it turns many vectors into one and the order in which the operation is applied does not matter. This is great for our purposes since we do not (need to) have fine grained control over when which gradient is received. Besides, note that this operation is independent among different $i$. This allows us to define the following two operations: *push*, which accumulates gradients, and *pull*, which retrieves aggregate gradients. Since we have many different sets of gradients (after all, we have many layers), we need to index the gradients with a key $i$. This similarity to key--value stores, such as the one introduced in Dynamo\n:cite:`DeCandia.Hastorun.Jampani.ea.2007` is not by coincidence. They, too, satisfy many similar characteristics, in particular when it comes to distributing the parameters across multiple servers. The push and pull operations for key-value stores are described as follows:\n\n* **push(key, value)** sends a particular gradient (the value) from a worker to a common storage. There the value is aggregated, e.g., by summing it up. * **pull(key, value)** retrieves an aggregate value from common storage, e.g., after combining the gradients from all workers."
    },
    {
      "chunk_id": "59b22ce69ca4_1",
      "chapter": "parameterserver",
      "heading": "Key--Value Stores",
      "text": "There the value is aggregated, e.g., by summing it up. * **pull(key, value)** retrieves an aggregate value from common storage, e.g., after combining the gradients from all workers. By hiding all the complexity about synchronization behind a simple push and pull operation we can decouple the concerns of statistical modelers who want to be able to express optimization in simple terms and the system engineers who need to deal with the complexity inherent in distributed synchronization."
    },
    {
      "chunk_id": "7249c6d49288_0",
      "chapter": "parameterserver",
      "heading": "Summary",
      "text": "* Synchronization needs to be highly adaptive to specific network infrastructure and connectivity within a server. This can make a significant difference to the time it takes to synchronize.\n* Ring-synchronization can be optimal for p3 and DGX-2 servers. For others possibly not so much.\n* A hierarchical synchronization strategy works well when adding multiple parameter servers for increased bandwidth."
    },
    {
      "chunk_id": "f994c1ef01c7_0",
      "chapter": "parameterserver",
      "heading": "Exercises",
      "text": "1. Can you increase the ring synchronization even further? Hint: you can send messages in both directions.\n1. Is it possible to allow asynchronous communication (while computation is still ongoing)? How does it affect performance?\n1. What if we lost a server during a long-running computation? How can we design a *fault tolerance* mechanism to avoid restarting the computation fully?\n\n\n[Discussions](https://discuss.d2l.ai/t/366)"
    },
    {
      "chunk_id": "afeb4513714e_0",
      "chapter": "anchor",
      "heading": "anchor",
      "text": "# Anchor Boxes\n:label:`sec_anchor`\n\n\nObject detection algorithms usually\nsample a large number of regions in the input image, determine whether these regions contain\nobjects of interest, and adjust the boundaries\nof the regions so as to predict the\n*ground-truth bounding boxes*\nof the objects more accurately.\nDifferent models may adopt\ndifferent region sampling schemes. \nHere we introduce one of such methods:\nit generates multiple bounding boxes with varying scales and aspect ratios centered on each pixel. \nThese bounding boxes are called *anchor boxes*.\nWe will design an object detection model\nbased on anchor boxes in :numref:`sec_ssd`.\n\nFirst, let's modify the printing accuracy\njust for more concise outputs.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, image, np, npx\n\nnp.set_printoptions(2)  # Simplify printing accuracy\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n\ntorch.set_printoptions(2)  # Simplify printing accuracy\n```"
    },
    {
      "chunk_id": "ae350fcab192_0",
      "chapter": "anchor",
      "heading": "Generating Multiple Anchor Boxes",
      "text": "Suppose that the input image has a height of $h$ and width of $w$. We generate anchor boxes with different shapes centered on each pixel of the image. Let the *scale* be $s\\in (0, 1]$ and\nthe *aspect ratio* (ratio of width to height) is $r > 0$. Then [**the width and height of the anchor box are $ws\\sqrt{r}$ and $hs/\\sqrt{r}$, respectively.**]\nNote that when the center position is given, an anchor box with known width and height is determined. To generate multiple anchor boxes with different shapes,\nlet's set a series of scales\n$s_1,\\ldots, s_n$ and \na series of aspect ratios $r_1,\\ldots, r_m$. When using all the combinations of these scales and aspect ratios with each pixel as the center,\nthe input image will have a total of $whnm$ anchor boxes. Although these anchor boxes may cover all the\nground-truth bounding boxes, the computational complexity is easily too high. In practice,\nwe can only (**consider those combinations\ncontaining**) $s_1$ or $r_1$:\n\n(**$$(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).$$**)\n\nThat is to say, the number of anchor boxes centered on the same pixel is $n+m-1$. For the entire input image, we will generate a total of $wh(n+m-1)$ anchor boxes. The above method of generating anchor boxes is implemented in the following `multibox_prior` function. We specify the input image, a list of scales, and a list of aspect ratios, then this function will return all the anchor boxes. ```{.python .input}\n#@tab mxnet\n#@save\ndef multibox_prior(data, sizes, ratios):\n    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n    in_height, in_width = data.shape[-2:]\n    device, num_sizes, num_ratios = data.ctx, len(sizes), len(ratios)\n    boxes_per_pixel = (num_sizes + num_ratios - 1)\n    size_tensor = d2l.tensor(sizes, ctx=device)\n    ratio_tensor = d2l.tensor(ratios, ctx=device)\n    # Offsets are required to move the anchor to the center of a pixel."
    },
    {
      "chunk_id": "ae350fcab192_1",
      "chapter": "anchor",
      "heading": "Generating Multiple Anchor Boxes",
      "text": "Since\n    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n    offset_h, offset_w = 0.5, 0.5\n    steps_h = 1.0 / in_height  # Scaled steps in y-axis\n    steps_w = 1.0 / in_width  # Scaled steps in x-axis\n\n    # Generate all center points for the anchor boxes\n    center_h = (d2l.arange(in_height, ctx=device) + offset_h) * steps_h\n    center_w = (d2l.arange(in_width, ctx=device) + offset_w) * steps_w\n    shift_x, shift_y = d2l.meshgrid(center_w, center_h)\n    shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)\n\n    # Generate `boxes_per_pixel` number of heights and widths that are later\n    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n    w = np.concatenate((size_tensor * np.sqrt(ratio_tensor[0]),\n                        sizes[0] * np.sqrt(ratio_tensor[1:]))) \\\n                        * in_height / in_width  # Handle rectangular inputs\n    h = np.concatenate((size_tensor / np.sqrt(ratio_tensor[0]),\n                        sizes[0] / np.sqrt(ratio_tensor[1:])))\n    # Divide by 2 to get half height and half width\n    anchor_manipulations = np.tile(np.stack((-w, -h, w, h)).T,\n                                   (in_height * in_width, 1)) / 2\n\n    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n    out_grid = d2l.stack([shift_x, shift_y, shift_x, shift_y],\n                         axis=1).repeat(boxes_per_pixel, axis=0)\n    output = out_grid + anchor_manipulations\n    return np.expand_dims(output, axis=0)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef multibox_prior(data, sizes, ratios):\n    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n    in_height, in_width = data.shape[-2:]\n    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n    boxes_per_pixel = (num_sizes + num_ratios - 1)\n    size_tensor = d2l.tensor(sizes, device=device)\n    ratio_tensor = d2l.tensor(ratios, device=device)\n    # Offsets are required to move the anchor to the center of a pixel."
    },
    {
      "chunk_id": "ae350fcab192_2",
      "chapter": "anchor",
      "heading": "Generating Multiple Anchor Boxes",
      "text": "Since\n    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n    offset_h, offset_w = 0.5, 0.5\n    steps_h = 1.0 / in_height  # Scaled steps in y axis\n    steps_w = 1.0 / in_width  # Scaled steps in x axis\n\n    # Generate all center points for the anchor boxes\n    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n\n    # Generate `boxes_per_pixel` number of heights and widths that are later\n    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n                   * in_height / in_width  # Handle rectangular inputs\n    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\n    # Divide by 2 to get half height and half width\n    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n                                        in_height * in_width, 1) / 2\n\n    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n    output = out_grid + anchor_manipulations\n    return output.unsqueeze(0)\n```\n\nWe can see that [**the shape of the returned anchor box variable `Y`**] is\n(batch size, number of anchor boxes, 4)."
    },
    {
      "chunk_id": "ae350fcab192_3",
      "chapter": "anchor",
      "heading": "Generating Multiple Anchor Boxes",
      "text": "```{.python .input}\n#@tab mxnet\nimg = image.imread('../img/catdog.jpg').asnumpy()\nh, w = img.shape[:2]\n\nprint(h, w)\nX = np.random.uniform(size=(1, 3, h, w))  # Construct input data\nY = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\nY.shape\n```\n\n```{.python .input}\n#@tab pytorch\nimg = d2l.plt.imread('../img/catdog.jpg')\nh, w = img.shape[:2]\n\nprint(h, w)\nX = torch.rand(size=(1, 3, h, w))  # Construct input data\nY = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\nY.shape\n```\n\nAfter changing the shape of the anchor box variable `Y` to (image height, image width, number of anchor boxes centered on the same pixel, 4),\nwe can obtain all the anchor boxes centered on a specified pixel position. In the following,\nwe [**access the first anchor box centered on (250, 250)**]. It has four elements: the $(x, y)$-axis coordinates at the upper-left corner and the $(x, y)$-axis coordinates at the lower-right corner of the anchor box. The coordinate values of both axes\nare divided by the width and height of the image, respectively. ```{.python .input}\n#@tab all\nboxes = Y.reshape(h, w, 5, 4)\nboxes[250, 250, 0, :]\n```\n\nIn order to [**show all the anchor boxes centered on one pixel in the image**],\nwe define the following `show_bboxes` function to draw multiple bounding boxes on the image."
    },
    {
      "chunk_id": "ae350fcab192_4",
      "chapter": "anchor",
      "heading": "Generating Multiple Anchor Boxes",
      "text": "```{.python .input}\n#@tab all\n#@save\ndef show_bboxes(axes, bboxes, labels=None, colors=None):\n    \"\"\"Show bounding boxes.\"\"\"\n\n    def make_list(obj, default_values=None):\n        if obj is None:\n            obj = default_values\n        elif not isinstance(obj, (list, tuple)):\n            obj = [obj]\n        return obj\n\n    labels = make_list(labels)\n    colors = make_list(colors, ['b', 'g', 'r', 'm', 'c'])\n    for i, bbox in enumerate(bboxes):\n        color = colors[i % len(colors)]\n        rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)\n        axes.add_patch(rect)\n        if labels and len(labels) > i:\n            text_color = 'k' if color == 'w' else 'w'\n            axes.text(rect.xy[0], rect.xy[1], labels[i],\n                      va='center', ha='center', fontsize=9, color=text_color,\n                      bbox=dict(facecolor=color, lw=0))\n```\n\nAs we just saw, the coordinate values of the $x$ and $y$ axes in the variable `boxes` have been divided by the width and height of the image, respectively. When drawing anchor boxes,\nwe need to restore their original coordinate values;\nthus, we define variable `bbox_scale` below. Now, we can draw all the anchor boxes centered on (250, 250) in the image. As you can see, the blue anchor box with a scale of 0.75 and an aspect ratio of 1 well\nsurrounds the dog in the image. ```{.python .input}\n#@tab all\nd2l.set_figsize()\nbbox_scale = d2l.tensor((w, h, w, h))\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,\n            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',\n             's=0.75, r=0.5'])\n```"
    },
    {
      "chunk_id": "26b55217d7ee_0",
      "chapter": "anchor",
      "heading": "[**Intersection over Union (IoU)**]",
      "text": "We just mentioned that an anchor box \"well\" surrounds the dog in the image. If the ground-truth bounding box of the object is known, how can \"well\" here be quantified? Intuitively, we can measure the similarity between\nthe anchor box and the ground-truth bounding box. We know that the *Jaccard index* can measure the similarity between two sets. Given sets $\\mathcal{A}$ and $\\mathcal{B}$, their Jaccard index is the size of their intersection divided by the size of their union:\n\n$$J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.$$\n\n\nIn fact, we can consider the pixel area of any bounding box as a set of pixels. In this way, we can measure the similarity of the two bounding boxes by the Jaccard index of their pixel sets. For two bounding boxes, we usually refer their Jaccard index as *intersection over union* (*IoU*), which is the ratio of their intersection area to their union area, as shown in :numref:`fig_iou`. The range of an IoU is between 0 and 1:\n0 means that two bounding boxes do not overlap at all,\nwhile 1 indicates that the two bounding boxes are equal. ![IoU is the ratio of the intersection area to the union area of two bounding boxes.](../img/iou.svg)\n:label:`fig_iou`\n\nFor the remainder of this section, we will use IoU to measure the similarity between anchor boxes and ground-truth bounding boxes, and between different anchor boxes. Given two lists of anchor or bounding boxes,\nthe following `box_iou` computes their pairwise IoU\nacross these two lists. ```{.python .input}\n#@tab mxnet\n#@save\ndef box_iou(boxes1, boxes2):\n    \"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\n    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n                              (boxes[:, 3] - boxes[:, 1]))\n    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n    # (no. of boxes2, 4), (no. of boxes1,), (no."
    },
    {
      "chunk_id": "26b55217d7ee_1",
      "chapter": "anchor",
      "heading": "[**Intersection over Union (IoU)**]",
      "text": "of boxes1, 4),\n    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\n    areas1 = box_area(boxes1)\n    areas2 = box_area(boxes2)\n    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n    # boxes1, no. of boxes2, 2)\n    inter_upperlefts = np.maximum(boxes1[:, None, :2], boxes2[:, :2])\n    inter_lowerrights = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])\n    inters = (inter_lowerrights - inter_upperlefts).clip(min=0)\n    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\n    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n    union_areas = areas1[:, None] + areas2 - inter_areas\n    return inter_areas / union_areas\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef box_iou(boxes1, boxes2):\n    \"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\n    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n                              (boxes[:, 3] - boxes[:, 1]))\n    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\n    areas1 = box_area(boxes1)\n    areas2 = box_area(boxes2)\n    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n    # boxes1, no. of boxes2, 2)\n    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\n    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n    union_areas = areas1[:, None] + areas2 - inter_areas\n    return inter_areas / union_areas\n```"
    },
    {
      "chunk_id": "b2da30141dc0_0",
      "chapter": "anchor",
      "heading": "Labeling Anchor Boxes in Training Data",
      "text": ":label:`subsec_labeling-anchor-boxes`\n\n\nIn a training dataset,\nwe consider each anchor box as a training example.\nIn order to train an object detection model,\nwe need *class* and *offset* labels for each anchor box,\nwhere the former is\nthe class of the object relevant to the anchor box\nand the latter is the offset\nof the ground-truth bounding box relative to the anchor box.\nDuring the prediction,\nfor each image\nwe generate multiple anchor boxes,\npredict classes and offsets for all the anchor boxes,\nadjust their positions according to the predicted offsets to obtain the predicted bounding boxes,\nand finally only output those \npredicted bounding boxes that satisfy certain criteria.\n\n\nAs we know, an object detection training set\ncomes with labels for\nlocations of *ground-truth bounding boxes*\nand classes of their surrounded objects.\nTo label any generated *anchor box*,\nwe refer to the labeled\nlocation and class of its *assigned* ground-truth bounding box that is closest to the anchor box.\nIn the following,\nwe describe an algorithm for assigning\nclosest ground-truth bounding boxes to anchor boxes."
    },
    {
      "chunk_id": "738d2ae0de66_0",
      "chapter": "anchor",
      "heading": "[**Assigning Ground-Truth Bounding Boxes to Anchor Boxes**]",
      "text": "Given an image,\nsuppose that the anchor boxes are $A_1, A_2, \\ldots, A_{n_a}$ and the ground-truth bounding boxes are $B_1, B_2, \\ldots, B_{n_b}$, where $n_a \\geq n_b$. Let's define a matrix $\\mathbf{X} \\in \\mathbb{R}^{n_a \\times n_b}$, whose element $x_{ij}$ in the $i^\\textrm{th}$ row and $j^\\textrm{th}$ column is the IoU of the anchor box $A_i$ and the ground-truth bounding box $B_j$. The algorithm consists of the following steps:\n\n1. Find the largest element in matrix $\\mathbf{X}$ and denote its row and column indices as $i_1$ and $j_1$, respectively. Then the ground-truth bounding box $B_{j_1}$ is assigned to the anchor box $A_{i_1}$. This is quite intuitive because $A_{i_1}$ and $B_{j_1}$ are the closest among all the pairs of anchor boxes and ground-truth bounding boxes. After the first assignment, discard all the elements in the ${i_1}^\\textrm{th}$ row and the ${j_1}^\\textrm{th}$ column in matrix $\\mathbf{X}$. 1. Find the largest of the remaining elements in matrix $\\mathbf{X}$ and denote its row and column indices as $i_2$ and $j_2$, respectively. We assign ground-truth bounding box $B_{j_2}$ to anchor box $A_{i_2}$ and discard all the elements in the ${i_2}^\\textrm{th}$ row and the ${j_2}^\\textrm{th}$ column in matrix $\\mathbf{X}$. 1. At this point, elements in two rows and two columns in  matrix $\\mathbf{X}$ have been discarded. We proceed until all elements in $n_b$ columns in matrix $\\mathbf{X}$ are discarded. At this time, we have assigned a ground-truth bounding box to each of $n_b$ anchor boxes. 1. Only traverse through the remaining $n_a - n_b$ anchor boxes. For example, given any anchor box $A_i$, find the ground-truth bounding box $B_j$ with the largest IoU with $A_i$ throughout the $i^\\textrm{th}$ row of matrix $\\mathbf{X}$, and assign $B_j$ to $A_i$ only if this IoU is greater than a predefined threshold. Let's illustrate the above algorithm using a concrete\nexample."
    },
    {
      "chunk_id": "738d2ae0de66_1",
      "chapter": "anchor",
      "heading": "[**Assigning Ground-Truth Bounding Boxes to Anchor Boxes**]",
      "text": "Let's illustrate the above algorithm using a concrete\nexample. As shown in :numref:`fig_anchor_label` (left), assuming that the maximum value in matrix $\\mathbf{X}$ is $x_{23}$, we assign the ground-truth bounding box $B_3$ to the anchor box $A_2$. Then, we discard all the elements in row 2 and column 3 of the matrix, find the largest $x_{71}$ in the remaining  elements (shaded area), and assign the ground-truth bounding box $B_1$ to the anchor box $A_7$. Next, as shown in :numref:`fig_anchor_label` (middle), discard all the elements in row 7 and column 1 of the matrix, find the largest $x_{54}$ in the remaining  elements (shaded area), and assign the ground-truth bounding box $B_4$ to the anchor box $A_5$. Finally, as shown in :numref:`fig_anchor_label` (right), discard all the elements in row 5 and column 4 of the matrix, find the largest $x_{92}$ in the remaining elements (shaded area), and assign the ground-truth bounding box $B_2$ to the anchor box $A_9$. After that, we only need to traverse through\nthe remaining anchor boxes $A_1, A_3, A_4, A_6, A_8$ and determine whether to assign them ground-truth bounding boxes according to the threshold. ![Assigning ground-truth bounding boxes to anchor boxes.](../img/anchor-label.svg)\n:label:`fig_anchor_label`\n\nThis algorithm is implemented in the following `assign_anchor_to_bbox` function."
    },
    {
      "chunk_id": "738d2ae0de66_2",
      "chapter": "anchor",
      "heading": "[**Assigning Ground-Truth Bounding Boxes to Anchor Boxes**]",
      "text": "![Assigning ground-truth bounding boxes to anchor boxes.](../img/anchor-label.svg)\n:label:`fig_anchor_label`\n\nThis algorithm is implemented in the following `assign_anchor_to_bbox` function. ```{.python .input}\n#@tab mxnet\n#@save\ndef assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n    # box i and the ground-truth bounding box j\n    jaccard = box_iou(anchors, ground_truth)\n    # Initialize the tensor to hold the assigned ground-truth bounding box for\n    # each anchor\n    anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32, ctx=device)\n    # Assign ground-truth bounding boxes according to the threshold\n    max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1)\n    anc_i = np.nonzero(max_ious >= iou_threshold)[0]\n    box_j = indices[max_ious >= iou_threshold]\n    anchors_bbox_map[anc_i] = box_j\n    col_discard = np.full((num_anchors,), -1)\n    row_discard = np.full((num_gt_boxes,), -1)\n    for _ in range(num_gt_boxes):\n        max_idx = np.argmax(jaccard)  # Find the largest IoU\n        box_idx = (max_idx % num_gt_boxes).astype('int32')\n        anc_idx = (max_idx / num_gt_boxes).astype('int32')\n        anchors_bbox_map[anc_idx] = box_idx\n        jaccard[:, box_idx] = col_discard\n        jaccard[anc_idx, :] = row_discard\n    return anchors_bbox_map\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n    # box i and the ground-truth bounding box j\n    jaccard = box_iou(anchors, ground_truth)\n    # Initialize the tensor to hold the assigned ground-truth bounding box for\n    # each anchor\n    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n                                  device=device)\n    # Assign ground-truth bounding boxes according to the threshold\n    max_ious, indices = torch.max(jaccard, dim=1)\n    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\n    box_j = indices[max_ious >= iou_threshold]\n    anchors_bbox_map[anc_i] = box_j\n    col_discard = torch.full((num_anchors,), -1)\n    row_discard = torch.full((num_gt_boxes,), -1)\n    for _ in range(num_gt_boxes):\n        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n        box_idx = (max_idx % num_gt_boxes).long()\n        anc_idx = (max_idx / num_gt_boxes).long()\n        anchors_bbox_map[anc_idx] = box_idx\n        jaccard[:, box_idx] = col_discard\n        jaccard[anc_idx, :] = row_discard\n    return anchors_bbox_map\n```"
    },
    {
      "chunk_id": "89af5766c4d7_0",
      "chapter": "anchor",
      "heading": "Labeling Classes and Offsets",
      "text": "Now we can label the class and offset for each anchor box. Suppose that an anchor box $A$ is assigned\na ground-truth bounding box $B$. On the one hand,\nthe class of the anchor box $A$ will be\nlabeled as that of $B$. On the other hand,\nthe offset of the anchor box $A$ \nwill be labeled according to the \nrelative position between\nthe central coordinates of $B$ and $A$\ntogether with the relative size between\nthese two boxes. Given varying\npositions and sizes of different boxes in the dataset,\nwe can apply transformations\nto those relative positions and sizes\nthat may lead to \nmore uniformly distributed offsets\nthat are easier to fit. Here we describe a common transformation. [**Given the central coordinates of $A$ and $B$ as $(x_a, y_a)$ and $(x_b, y_b)$, \ntheir widths as $w_a$ and $w_b$, \nand their heights as $h_a$ and $h_b$, respectively. We may label the offset of $A$ as\n\n$$\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x},\n\\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y},\n\\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w},\n\\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),$$\n**]\nwhere default values of the constants are $\\mu_x = \\mu_y = \\mu_w = \\mu_h = 0, \\sigma_x=\\sigma_y=0.1$, and $\\sigma_w=\\sigma_h=0.2$. This transformation is implemented below in the `offset_boxes` function. ```{.python .input}\n#@tab all\n#@save\ndef offset_boxes(anchors, assigned_bb, eps=1e-6):\n    \"\"\"Transform for anchor box offsets.\"\"\"\n    c_anc = d2l.box_corner_to_center(anchors)\n    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)\n    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\n    offset_wh = 5 * d2l.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\n    offset = d2l.concat([offset_xy, offset_wh], axis=1)\n    return offset\n```\n\nIf an anchor box is not assigned a ground-truth bounding box, we just label the class of the anchor box as \"background\". Anchor boxes whose classes are background are often referred to as *negative* anchor boxes,\nand the rest are called *positive* anchor boxes."
    },
    {
      "chunk_id": "89af5766c4d7_1",
      "chapter": "anchor",
      "heading": "Labeling Classes and Offsets",
      "text": "Anchor boxes whose classes are background are often referred to as *negative* anchor boxes,\nand the rest are called *positive* anchor boxes. We implement the following `multibox_target` function\nto [**label classes and offsets for anchor boxes**] (the `anchors` argument) using ground-truth bounding boxes (the `labels` argument). This function sets the background class to zero and increments the integer index of a new class by one. ```{.python .input}\n#@tab mxnet\n#@save\ndef multibox_target(anchors, labels):\n    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n    batch_offset, batch_mask, batch_class_labels = [], [], []\n    device, num_anchors = anchors.ctx, anchors.shape[0]\n    for i in range(batch_size):\n        label = labels[i, :, :]\n        anchors_bbox_map = assign_anchor_to_bbox(\n            label[:, 1:], anchors, device)\n        bbox_mask = np.tile((np.expand_dims((anchors_bbox_map >= 0),\n                                            axis=-1)), (1, 4)).astype('int32')\n        # Initialize class labels and assigned bounding box coordinates with\n        # zeros\n        class_labels = d2l.zeros(num_anchors, dtype=np.int32, ctx=device)\n        assigned_bb = d2l.zeros((num_anchors, 4), dtype=np.float32,\n                                ctx=device)\n        # Label classes of anchor boxes using their assigned ground-truth\n        # bounding boxes."
    },
    {
      "chunk_id": "89af5766c4d7_2",
      "chapter": "anchor",
      "heading": "Labeling Classes and Offsets",
      "text": "If an anchor box is not assigned any, we label its\n        # class as background (the value remains zero)\n        indices_true = np.nonzero(anchors_bbox_map >= 0)[0]\n        bb_idx = anchors_bbox_map[indices_true]\n        class_labels[indices_true] = label[bb_idx, 0].astype('int32') + 1\n        assigned_bb[indices_true] = label[bb_idx, 1:]\n        # Offset transformation\n        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n        batch_offset.append(offset.reshape(-1))\n        batch_mask.append(bbox_mask.reshape(-1))\n        batch_class_labels.append(class_labels)\n    bbox_offset = d2l.stack(batch_offset)\n    bbox_mask = d2l.stack(batch_mask)\n    class_labels = d2l.stack(batch_class_labels)\n    return (bbox_offset, bbox_mask, class_labels)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef multibox_target(anchors, labels):\n    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n    batch_offset, batch_mask, batch_class_labels = [], [], []\n    device, num_anchors = anchors.device, anchors.shape[0]\n    for i in range(batch_size):\n        label = labels[i, :, :]\n        anchors_bbox_map = assign_anchor_to_bbox(\n            label[:, 1:], anchors, device)\n        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n            1, 4)\n        # Initialize class labels and assigned bounding box coordinates with\n        # zeros\n        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n                                   device=device)\n        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n                                  device=device)\n        # Label classes of anchor boxes using their assigned ground-truth\n        # bounding boxes."
    },
    {
      "chunk_id": "89af5766c4d7_3",
      "chapter": "anchor",
      "heading": "Labeling Classes and Offsets",
      "text": "If an anchor box is not assigned any, we label its\n        # class as background (the value remains zero)\n        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n        bb_idx = anchors_bbox_map[indices_true]\n        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n        assigned_bb[indices_true] = label[bb_idx, 1:]\n        # Offset transformation\n        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n        batch_offset.append(offset.reshape(-1))\n        batch_mask.append(bbox_mask.reshape(-1))\n        batch_class_labels.append(class_labels)\n    bbox_offset = torch.stack(batch_offset)\n    bbox_mask = torch.stack(batch_mask)\n    class_labels = torch.stack(batch_class_labels)\n    return (bbox_offset, bbox_mask, class_labels)\n```"
    },
    {
      "chunk_id": "fa9dd7a17214_0",
      "chapter": "anchor",
      "heading": "An Example",
      "text": "Let's illustrate anchor box labeling\nvia a concrete example. We define ground-truth bounding boxes for the dog and cat in the loaded image,\nwhere the first element is the class (0 for dog and 1 for cat) and the remaining four elements are the\n$(x, y)$-axis coordinates\nat the upper-left corner and the lower-right corner\n(range is between 0 and 1). We also construct five anchor boxes to be labeled\nusing the coordinates of\nthe upper-left corner and the lower-right corner:\n$A_0, \\ldots, A_4$ (the index starts from 0). Then we [**plot these ground-truth bounding boxes \nand anchor boxes \nin the image.**]\n\n```{.python .input}\n#@tab all\nground_truth = d2l.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n                         [1, 0.55, 0.2, 0.9, 0.88]])\nanchors = d2l.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n                    [0.57, 0.3, 0.92, 0.9]])\n\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')\nshow_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);\n```\n\nUsing the `multibox_target` function defined above,\nwe can [**label classes and offsets\nof these anchor boxes based on\nthe ground-truth bounding boxes**] for the dog and cat. In this example, indices of\nthe background, dog, and cat classes\nare 0, 1, and 2, respectively. Below we add an dimension for examples of anchor boxes and ground-truth bounding boxes. ```{.python .input}\n#@tab mxnet\nlabels = multibox_target(np.expand_dims(anchors, axis=0),\n                         np.expand_dims(ground_truth, axis=0))\n```\n\n```{.python .input}\n#@tab pytorch\nlabels = multibox_target(anchors.unsqueeze(dim=0),\n                         ground_truth.unsqueeze(dim=0))\n```\n\nThere are three items in the returned result, all of which are in the tensor format. The third item contains the labeled classes of the input anchor boxes. Let's analyze the returned class labels below based on\nanchor box and ground-truth bounding box positions in the image."
    },
    {
      "chunk_id": "fa9dd7a17214_1",
      "chapter": "anchor",
      "heading": "An Example",
      "text": "The third item contains the labeled classes of the input anchor boxes. Let's analyze the returned class labels below based on\nanchor box and ground-truth bounding box positions in the image. First, among all the pairs of anchor boxes\nand ground-truth bounding boxes,\nthe IoU of the anchor box $A_4$ and the ground-truth bounding box of the cat is the largest. Thus, the class of $A_4$ is labeled as the cat. Taking out \npairs containing $A_4$ or the ground-truth bounding box of the cat, among the rest \nthe pair of the anchor box $A_1$ and the ground-truth bounding box of the dog has the largest IoU. So the class of $A_1$ is labeled as the dog. Next, we need to traverse through the remaining three unlabeled anchor boxes: $A_0$, $A_2$, and $A_3$. For $A_0$,\nthe class of the ground-truth bounding box with the largest IoU is the dog,\nbut the IoU is below the predefined threshold (0.5),\nso the class is labeled as background;\nfor $A_2$,\nthe class of the ground-truth bounding box with the largest IoU is the cat and the IoU exceeds the threshold, so the class is labeled as the cat;\nfor $A_3$,\nthe class of the ground-truth bounding box with the largest IoU is the cat, but the value is below the threshold, so the class is labeled as background. ```{.python .input}\n#@tab all\nlabels[2]\n```\n\nThe second returned item is a mask variable of the shape (batch size, four times the number of anchor boxes). Every four elements in the mask variable \ncorrespond to the four offset values of each anchor box. Since we do not care about background detection,\noffsets of this negative class should not affect the objective function. Through elementwise multiplications, zeros in the mask variable will filter out negative class offsets before calculating the objective function. ```{.python .input}\n#@tab all\nlabels[1]\n```\n\nThe first returned item contains the four offset values labeled for each anchor box. Note that the offsets of negative-class anchor boxes are labeled as zeros. ```{.python .input}\n#@tab all\nlabels[0]\n```"
    },
    {
      "chunk_id": "1fc381c3db92_0",
      "chapter": "anchor",
      "heading": "Predicting Bounding Boxes with Non-Maximum Suppression",
      "text": ":label:`subsec_predicting-bounding-boxes-nms`\n\nDuring prediction,\nwe generate multiple anchor boxes for the image and predict classes and offsets for each of them. A *predicted bounding box*\nis thus obtained according to \nan anchor box with its predicted offset. Below we implement the `offset_inverse` function\nthat takes in anchors and\noffset predictions as inputs and [**applies inverse offset transformations to\nreturn the predicted bounding box coordinates**]. ```{.python .input}\n#@tab all\n#@save\ndef offset_inverse(anchors, offset_preds):\n    \"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\n    anc = d2l.box_corner_to_center(anchors)\n    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]\n    pred_bbox_wh = d2l.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]\n    pred_bbox = d2l.concat((pred_bbox_xy, pred_bbox_wh), axis=1)\n    predicted_bbox = d2l.box_center_to_corner(pred_bbox)\n    return predicted_bbox\n```\n\nWhen there are many anchor boxes,\nmany similar (with significant overlap)\npredicted bounding boxes \ncan be potentially output for surrounding the same object. To simplify the output,\nwe can merge similar predicted bounding boxes\nthat belong to the same object\nby using *non-maximum suppression* (NMS). Here is how non-maximum suppression works. For a predicted bounding box $B$,\nthe object detection model calculates the predicted likelihood\nfor each class. Denoting by $p$ the largest predicted likelihood,\nthe class corresponding to this probability is the predicted class for $B$. Specifically, we refer to $p$ as the *confidence* (score) of the predicted bounding box $B$. On the same image,\nall the predicted non-background bounding boxes \nare sorted by confidence in descending order\nto generate a list $L$. Then we manipulate the sorted list $L$ in the following steps:\n\n1."
    },
    {
      "chunk_id": "1fc381c3db92_1",
      "chapter": "anchor",
      "heading": "Predicting Bounding Boxes with Non-Maximum Suppression",
      "text": "On the same image,\nall the predicted non-background bounding boxes \nare sorted by confidence in descending order\nto generate a list $L$. Then we manipulate the sorted list $L$ in the following steps:\n\n1. Select the predicted bounding box $B_1$ with the highest confidence from $L$ as a basis and remove all non-basis predicted bounding boxes whose IoU with $B_1$ exceeds a predefined threshold $\\epsilon$ from $L$. At this point, $L$ keeps the predicted bounding box with the highest confidence but drops others that are too similar to it. In a nutshell, those with *non-maximum* confidence scores are *suppressed*. 1. Select the predicted bounding box $B_2$ with the second highest confidence from $L$ as another basis and remove all non-basis predicted bounding boxes whose IoU with $B_2$ exceeds $\\epsilon$ from $L$. 1. Repeat the above process until all the predicted bounding boxes in $L$ have been used as a basis. At this time, the IoU of any pair of predicted bounding boxes in $L$ is below the threshold $\\epsilon$; thus, no pair is too similar with each other. 1. Output all the predicted bounding boxes in the list $L$."
    },
    {
      "chunk_id": "1fc381c3db92_2",
      "chapter": "anchor",
      "heading": "Predicting Bounding Boxes with Non-Maximum Suppression",
      "text": "At this time, the IoU of any pair of predicted bounding boxes in $L$ is below the threshold $\\epsilon$; thus, no pair is too similar with each other. 1. Output all the predicted bounding boxes in the list $L$. [**The following `nms` function sorts confidence scores in descending order and returns their indices.**]\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef nms(boxes, scores, iou_threshold):\n    \"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\n    B = scores.argsort()[::-1]\n    keep = []  # Indices of predicted bounding boxes that will be kept\n    while B.size > 0:\n        i = B[0]\n        keep.append(i)\n        if B.size == 1: break\n        iou = box_iou(boxes[i, :].reshape(-1, 4),\n                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\n        inds = np.nonzero(iou <= iou_threshold)[0]\n        B = B[inds + 1]\n    return np.array(keep, dtype=np.int32, ctx=boxes.ctx)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef nms(boxes, scores, iou_threshold):\n    \"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\n    B = torch.argsort(scores, dim=-1, descending=True)\n    keep = []  # Indices of predicted bounding boxes that will be kept\n    while B.numel() > 0:\n        i = B[0]\n        keep.append(i)\n        if B.numel() == 1: break\n        iou = box_iou(boxes[i, :].reshape(-1, 4),\n                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\n        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)\n        B = B[inds + 1]\n    return d2l.tensor(keep, device=boxes.device)\n```\n\nWe define the following `multibox_detection`\nto [**apply non-maximum suppression\nto predicting bounding boxes**]. Do not worry if you find the implementation\na bit complicated: we will show how it works\nwith a concrete example right after the implementation."
    },
    {
      "chunk_id": "1fc381c3db92_3",
      "chapter": "anchor",
      "heading": "Predicting Bounding Boxes with Non-Maximum Suppression",
      "text": "Do not worry if you find the implementation\na bit complicated: we will show how it works\nwith a concrete example right after the implementation. ```{.python .input}\n#@tab mxnet\n#@save\ndef multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n                       pos_threshold=0.009999999):\n    \"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\n    device, batch_size = cls_probs.ctx, cls_probs.shape[0]\n    anchors = np.squeeze(anchors, axis=0)\n    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n    out = []\n    for i in range(batch_size):\n        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n        conf, class_id = np.max(cls_prob[1:], 0), np.argmax(cls_prob[1:], 0)\n        predicted_bb = offset_inverse(anchors, offset_pred)\n        keep = nms(predicted_bb, conf, nms_threshold)\n        # Find all non-`keep` indices and set the class to background\n        all_idx = np.arange(num_anchors, dtype=np.int32, ctx=device)\n        combined = d2l.concat((keep, all_idx))\n        unique, counts = np.unique(combined, return_counts=True)\n        non_keep = unique[counts == 1]\n        all_id_sorted = d2l.concat((keep, non_keep))\n        class_id[non_keep] = -1\n        class_id = class_id[all_id_sorted].astype('float32')\n        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n        # Here `pos_threshold` is a threshold for positive (non-background)\n        # predictions\n        below_min_idx = (conf < pos_threshold)\n        class_id[below_min_idx] = -1\n        conf[below_min_idx] = 1 - conf[below_min_idx]\n        pred_info = d2l.concat((np.expand_dims(class_id, axis=1),\n                                np.expand_dims(conf, axis=1),\n                                predicted_bb), axis=1)\n        out.append(pred_info)\n    return d2l.stack(out)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n                       pos_threshold=0.009999999):\n    \"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\n    device, batch_size = cls_probs.device, cls_probs.shape[0]\n    anchors = anchors.squeeze(0)\n    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n    out = []\n    for i in range(batch_size):\n        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n        conf, class_id = torch.max(cls_prob[1:], 0)\n        predicted_bb = offset_inverse(anchors, offset_pred)\n        keep = nms(predicted_bb, conf, nms_threshold)\n        # Find all non-`keep` indices and set the class to background\n        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)\n        combined = torch.cat((keep, all_idx))\n        uniques, counts = combined.unique(return_counts=True)\n        non_keep = uniques[counts == 1]\n        all_id_sorted = torch.cat((keep, non_keep))\n        class_id[non_keep] = -1\n        class_id = class_id[all_id_sorted]\n        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n        # Here `pos_threshold` is a threshold for positive (non-background)\n        # predictions\n        below_min_idx = (conf < pos_threshold)\n        class_id[below_min_idx] = -1\n        conf[below_min_idx] = 1 - conf[below_min_idx]\n        pred_info = torch.cat((class_id.unsqueeze(1),\n                               conf.unsqueeze(1),\n                               predicted_bb), dim=1)\n        out.append(pred_info)\n    return d2l.stack(out)\n```\n\nNow let's [**apply the above implementations\nto a concrete example with four anchor boxes**]."
    },
    {
      "chunk_id": "1fc381c3db92_4",
      "chapter": "anchor",
      "heading": "Predicting Bounding Boxes with Non-Maximum Suppression",
      "text": "For simplicity, we assume that the\npredicted offsets are all zeros. This means that the predicted bounding boxes are anchor boxes. For each class among the background, dog, and cat,\nwe also define its predicted likelihood. ```{.python .input}\n#@tab all\nanchors = d2l.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\noffset_preds = d2l.tensor([0] * d2l.size(anchors))\ncls_probs = d2l.tensor([[0] * 4,  # Predicted background likelihood \n                      [0.9, 0.8, 0.7, 0.1],  # Predicted dog likelihood \n                      [0.1, 0.2, 0.3, 0.9]])  # Predicted cat likelihood\n```\n\nWe can [**plot these predicted bounding boxes with their confidence on the image.**]\n\n```{.python .input}\n#@tab all\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, anchors * bbox_scale,\n            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])\n```\n\nNow we can invoke the `multibox_detection` function\nto perform non-maximum suppression,\nwhere the threshold is set to 0.5. Note that we add\na dimension for examples in the tensor input. We can see that [**the shape of the returned result**] is\n(batch size, number of anchor boxes, 6). The six elements in the innermost dimension\ngives the output information for the same predicted bounding box. The first element is the predicted class index, which starts from 0 (0 is dog and 1 is cat). The value -1 indicates background or removal in non-maximum suppression. The second element is the confidence of the predicted bounding box. The remaining four elements are the $(x, y)$-axis coordinates of the upper-left corner and \nthe lower-right corner of the predicted bounding box, respectively (range is between 0 and 1)."
    },
    {
      "chunk_id": "1fc381c3db92_5",
      "chapter": "anchor",
      "heading": "Predicting Bounding Boxes with Non-Maximum Suppression",
      "text": "The second element is the confidence of the predicted bounding box. The remaining four elements are the $(x, y)$-axis coordinates of the upper-left corner and \nthe lower-right corner of the predicted bounding box, respectively (range is between 0 and 1). ```{.python .input}\n#@tab mxnet\noutput = multibox_detection(np.expand_dims(cls_probs, axis=0),\n                            np.expand_dims(offset_preds, axis=0),\n                            np.expand_dims(anchors, axis=0),\n                            nms_threshold=0.5)\noutput\n```\n\n```{.python .input}\n#@tab pytorch\noutput = multibox_detection(cls_probs.unsqueeze(dim=0),\n                            offset_preds.unsqueeze(dim=0),\n                            anchors.unsqueeze(dim=0),\n                            nms_threshold=0.5)\noutput\n```\n\nAfter removing those predicted bounding boxes\nof class -1, \nwe can [**output the final predicted bounding box\nkept by non-maximum suppression**]. ```{.python .input}\n#@tab all\nfig = d2l.plt.imshow(img)\nfor i in d2l.numpy(output[0]):\n    if i[0] == -1:\n        continue\n    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])\n    show_bboxes(fig.axes, [d2l.tensor(i[2:]) * bbox_scale], label)\n```\n\nIn practice, we can remove predicted bounding boxes with lower confidence even before performing non-maximum suppression, thereby reducing computation in this algorithm. We may also post-process the output of non-maximum suppression, for example, by only keeping\nresults with higher confidence\nin the final output."
    },
    {
      "chunk_id": "903d3e4219ce_0",
      "chapter": "anchor",
      "heading": "Summary",
      "text": "* We generate anchor boxes with different shapes centered on each pixel of the image.\n* Intersection over union (IoU), also known as Jaccard index, measures the similarity of two bounding boxes. It is the ratio of their intersection area to their union area.\n* In a training set, we need two types of labels for each anchor box. One is the class of the object relevant to the anchor box and the other is the offset of the ground-truth bounding box relative to the anchor box.\n* During prediction, we can use non-maximum suppression (NMS) to remove similar predicted bounding boxes, thereby simplifying the output."
    },
    {
      "chunk_id": "ba8af9e1320a_0",
      "chapter": "anchor",
      "heading": "Exercises",
      "text": "1. Change values of `sizes` and `ratios` in the `multibox_prior` function. What are the changes to the generated anchor boxes?\n1. Construct and visualize two bounding boxes with an IoU of 0.5. How do they overlap with each other?\n1. Modify the variable `anchors` in :numref:`subsec_labeling-anchor-boxes` and :numref:`subsec_predicting-bounding-boxes-nms`. How do the results change?\n1. Non-maximum suppression is a greedy algorithm that suppresses predicted bounding boxes by *removing* them. Is it possible that some of these removed ones are actually useful? How can this algorithm be modified to suppress *softly*? You may refer to Soft-NMS :cite:`Bodla.Singh.Chellappa.ea.2017`.\n1. Rather than being hand-crafted, can non-maximum suppression be learned?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/370)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1603)\n:end_tab:"
    },
    {
      "chunk_id": "023622fe734e_0",
      "chapter": "bounding-box",
      "heading": "bounding-box",
      "text": "# Object Detection and Bounding Boxes\n:label:`sec_bbox`\n\n\nIn earlier sections (e.g., :numref:`sec_alexnet`--:numref:`sec_googlenet`),\nwe introduced various models for image classification.\nIn image classification tasks,\nwe assume that there is only *one*\nmajor object\nin the image and we only focus on how to \nrecognize its category.\nHowever, there are often *multiple* objects\nin the image of interest.\nWe not only want to know their categories, but also their specific positions in the image.\nIn computer vision, we refer to such tasks as *object detection* (or *object recognition*).\n\nObject detection has been\nwidely applied in many fields.\nFor example, self-driving needs to plan \ntraveling routes\nby detecting the positions\nof vehicles, pedestrians, roads, and obstacles in the captured video images.\nBesides,\nrobots may use this technique\nto detect and localize objects of interest\nthroughout its navigation of an environment.\nMoreover,\nsecurity systems\nmay need to detect abnormal objects, such as intruders or bombs.\n\nIn the next few sections, we will introduce \nseveral deep learning methods for object detection.\nWe will begin with an introduction\nto *positions* (or *locations*) of objects.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import image, npx, np\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\nWe will load the sample image to be used in this section. We can see that there is a dog on the left side of the image and a cat on the right.\nThey are the two major objects in this image.\n\n```{.python .input}\n#@tab mxnet\nd2l.set_figsize()\nimg = image.imread('../img/catdog.jpg').asnumpy()\nd2l.plt.imshow(img);\n```\n\n```{.python .input}\n#@tab pytorch, tensorflow\nd2l.set_figsize()\nimg = d2l.plt.imread('../img/catdog.jpg')\nd2l.plt.imshow(img);\n```"
    },
    {
      "chunk_id": "7621af610c9f_0",
      "chapter": "bounding-box",
      "heading": "Bounding Boxes",
      "text": "In object detection,\nwe usually use a *bounding box* to describe the spatial location of an object. The bounding box is rectangular, which is determined by the $x$ and $y$ coordinates of the upper-left corner of the rectangle and the such coordinates of the lower-right corner. Another commonly used bounding box representation is the $(x, y)$-axis\ncoordinates of the bounding box center, and the width and height of the box. [**Here we define functions to convert between**] these (**two\nrepresentations**):\n`box_corner_to_center` converts from the two-corner\nrepresentation to the center-width-height presentation,\nand `box_center_to_corner` vice versa. The input argument `boxes` should be a two-dimensional tensor of\nshape ($n$, 4), where $n$ is the number of bounding boxes. ```{.python .input}\n#@tab all\n#@save\ndef box_corner_to_center(boxes):\n    \"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    cx = (x1 + x2) / 2\n    cy = (y1 + y2) / 2\n    w = x2 - x1\n    h = y2 - y1\n    boxes = d2l.stack((cx, cy, w, h), axis=-1)\n    return boxes\n\n#@save\ndef box_center_to_corner(boxes):\n    \"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\n    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    x1 = cx - 0.5 * w\n    y1 = cy - 0.5 * h\n    x2 = cx + 0.5 * w\n    y2 = cy + 0.5 * h\n    boxes = d2l.stack((x1, y1, x2, y2), axis=-1)\n    return boxes\n```\n\nWe will [**define the bounding boxes of the dog and the cat in the image**] based\non the coordinate information. The origin of the coordinates in the image\nis the upper-left corner of the image, and to the right and down are the\npositive directions of the $x$ and $y$ axes, respectively. ```{.python .input}\n#@tab all\n# Here `bbox` is the abbreviation for bounding box\ndog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]\n```\n\nWe can verify the correctness of the two\nbounding box conversion functions by converting twice."
    },
    {
      "chunk_id": "7621af610c9f_1",
      "chapter": "bounding-box",
      "heading": "Bounding Boxes",
      "text": "```{.python .input}\n#@tab all\nboxes = d2l.tensor((dog_bbox, cat_bbox))\nbox_center_to_corner(box_corner_to_center(boxes)) == boxes\n```\n\nLet's [**draw the bounding boxes in the image**] to check if they are accurate. Before drawing, we will define a helper function `bbox_to_rect`. It represents the bounding box in the bounding box format of the  `matplotlib` package. ```{.python .input}\n#@tab all\n#@save\ndef bbox_to_rect(bbox, color):\n    \"\"\"Convert bounding box to matplotlib format.\"\"\"\n    # Convert the bounding box (upper-left x, upper-left y, lower-right x,\n    # lower-right y) format to the matplotlib format: ((upper-left x,\n    # upper-left y), width, height)\n    return d2l.plt.Rectangle(\n        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n        fill=False, edgecolor=color, linewidth=2)\n```\n\nAfter adding the bounding boxes on the image,\nwe can see that the main outline of the two objects are basically inside the two boxes. ```{.python .input}\n#@tab all\nfig = d2l.plt.imshow(img)\nfig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))\nfig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));\n```"
    },
    {
      "chunk_id": "6d53914559f8_0",
      "chapter": "bounding-box",
      "heading": "Summary",
      "text": "* Object detection not only recognizes all the objects of interest in the image, but also their positions. The position is generally represented by a rectangular bounding box.\n* We can convert between two commonly used bounding box representations."
    },
    {
      "chunk_id": "1ed61b59a94a_0",
      "chapter": "bounding-box",
      "heading": "Exercises",
      "text": "1. Find another image and try to label a bounding box that contains the object. Compare labeling bounding boxes and categories: which usually takes longer?\n1. Why is the innermost dimension of the input argument `boxes` of `box_corner_to_center` and `box_center_to_corner` always 4?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/369)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1527)\n:end_tab:"
    },
    {
      "chunk_id": "ad9058dbedc1_0",
      "chapter": "fcn",
      "heading": "fcn",
      "text": "# Fully Convolutional Networks\n:label:`sec_fcn`\n\nAs discussed in :numref:`sec_semantic_segmentation`,\nsemantic segmentation\nclassifies images in pixel level.\nA fully convolutional network (FCN)\nuses a convolutional neural network to\ntransform image pixels to pixel classes :cite:`Long.Shelhamer.Darrell.2015`.\nUnlike the CNNs that we encountered earlier\nfor image classification \nor object detection,\na fully convolutional network\ntransforms \nthe height and width of intermediate feature maps\nback to those of the input image:\nthis is achieved by\nthe transposed convolutional layer\nintroduced in :numref:`sec_transposed_conv`.\nAs a result,\nthe classification output\nand the input image \nhave a one-to-one correspondence \nin pixel level:\nthe channel dimension at any output pixel \nholds the classification results\nfor the input pixel at the same spatial position.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, image, init, np, npx\nfrom mxnet.gluon import nn\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import functional as F\n```"
    },
    {
      "chunk_id": "152fc413808e_0",
      "chapter": "fcn",
      "heading": "The Model",
      "text": "Here we describe the basic design of the fully convolutional network model. As shown in :numref:`fig_fcn`,\nthis model first uses a CNN to extract image features,\nthen transforms the number of channels into\nthe number of classes\nvia a $1\\times 1$ convolutional layer,\nand finally transforms the height and width of\nthe feature maps\nto those\nof the input image via\nthe transposed convolution introduced in :numref:`sec_transposed_conv`. As a result,\nthe model output has the same height and width as the input image,\nwhere the output channel contains the predicted classes\nfor the input pixel at the same spatial position. ![Fully convolutional network.](../img/fcn.svg)\n:label:`fig_fcn`\n\nBelow, we [**use a ResNet-18 model pretrained on the ImageNet dataset to extract image features**]\nand denote the model instance as `pretrained_net`. The last few layers of this model\ninclude a global average pooling layer\nand a fully connected layer:\nthey are not needed\nin the fully convolutional network. ```{.python .input}\n#@tab mxnet\npretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)\npretrained_net.features[-3:], pretrained_net.output\n```\n\n```{.python .input}\n#@tab pytorch\npretrained_net = torchvision.models.resnet18(pretrained=True)\nlist(pretrained_net.children())[-3:]\n```\n\nNext, we [**create the fully convolutional network instance `net`**]. It copies all the pretrained layers in the ResNet-18\nexcept for the final global average pooling layer\nand the fully connected layer that are closest\nto the output. ```{.python .input}\n#@tab mxnet\nnet = nn.HybridSequential()\nfor layer in pretrained_net.features[:-2]:\n    net.add(layer)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = nn.Sequential(*list(pretrained_net.children())[:-2])\n```\n\nGiven an input with height and width of 320 and 480 respectively,\nthe forward propagation of `net`\nreduces the input height and width to 1/32 of the original, namely 10 and 15."
    },
    {
      "chunk_id": "152fc413808e_1",
      "chapter": "fcn",
      "heading": "The Model",
      "text": "```{.python .input}\n#@tab mxnet\nX = np.random.uniform(size=(1, 3, 320, 480))\nnet(X).shape\n```\n\n```{.python .input}\n#@tab pytorch\nX = torch.rand(size=(1, 3, 320, 480))\nnet(X).shape\n```\n\nNext, we [**use a $1\\times 1$ convolutional layer to transform the number of output channels into the number of classes (21) of the Pascal VOC2012 dataset.**]\nFinally, we need to (**increase the height and width of the feature maps by 32 times**) to change them back to the height and width of the input image. Recall how to calculate \nthe output shape of a convolutional layer in :numref:`sec_padding`. Since $(320-64+16\\times2+32)/32=10$ and $(480-64+16\\times2+32)/32=15$, we construct a transposed convolutional layer with stride of $32$, \nsetting\nthe height and width of the kernel\nto $64$, the padding to $16$. In general,\nwe can see that\nfor stride $s$,\npadding $s/2$ (assuming $s/2$ is an integer),\nand the height and width of the kernel $2s$, \nthe transposed convolution will increase\nthe height and width of the input by $s$ times. ```{.python .input}\n#@tab mxnet\nnum_classes = 21\nnet.add(nn.Conv2D(num_classes, kernel_size=1),\n        nn.Conv2DTranspose(\n            num_classes, kernel_size=64, padding=16, strides=32))\n```\n\n```{.python .input}\n#@tab pytorch\nnum_classes = 21\nnet.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\nnet.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n                                    kernel_size=64, padding=16, stride=32))\n```"
    },
    {
      "chunk_id": "750834b26fb0_0",
      "chapter": "fcn",
      "heading": "[**Initializing Transposed Convolutional Layers**]",
      "text": "We already know that\ntransposed convolutional layers can increase\nthe height and width of\nfeature maps. In image processing, we may need to scale up\nan image, i.e., *upsampling*. *Bilinear interpolation*\nis one of the commonly used upsampling techniques. It is also often used for initializing transposed convolutional layers. To explain bilinear interpolation,\nsay that \ngiven an input image\nwe want to \ncalculate each pixel \nof the upsampled output image. In order to calculate the pixel of the output image\nat coordinate $(x, y)$, \nfirst map $(x, y)$ to coordinate $(x', y')$ on the input image, for example, according to the ratio of the input size to the output size. Note that the mapped $x'$ and $y'$ are real numbers. Then, find the four pixels closest to coordinate\n$(x', y')$ on the input image. Finally, the pixel of the output image at coordinate $(x, y)$ is calculated based on these four closest pixels\non the input image and their relative distance from $(x', y')$. Upsampling of bilinear interpolation\ncan be implemented by the transposed convolutional layer \nwith the kernel constructed by the following `bilinear_kernel` function. Due to space limitations, we only provide the implementation of the `bilinear_kernel` function below\nwithout discussions on its algorithm design."
    },
    {
      "chunk_id": "750834b26fb0_1",
      "chapter": "fcn",
      "heading": "[**Initializing Transposed Convolutional Layers**]",
      "text": "Due to space limitations, we only provide the implementation of the `bilinear_kernel` function below\nwithout discussions on its algorithm design. ```{.python .input}\n#@tab mxnet\ndef bilinear_kernel(in_channels, out_channels, kernel_size):\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = (np.arange(kernel_size).reshape(-1, 1),\n          np.arange(kernel_size).reshape(1, -1))\n    filt = (1 - np.abs(og[0] - center) / factor) * \\\n           (1 - np.abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size))\n    weight[range(in_channels), range(out_channels), :, :] = filt\n    return np.array(weight)\n```\n\n```{.python .input}\n#@tab pytorch\ndef bilinear_kernel(in_channels, out_channels, kernel_size):\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = (torch.arange(kernel_size).reshape(-1, 1),\n          torch.arange(kernel_size).reshape(1, -1))\n    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n           (1 - torch.abs(og[1] - center) / factor)\n    weight = torch.zeros((in_channels, out_channels,\n                          kernel_size, kernel_size))\n    weight[range(in_channels), range(out_channels), :, :] = filt\n    return weight\n```\n\nLet's [**experiment with upsampling of bilinear interpolation**] \nthat is implemented by a transposed convolutional layer. We construct a transposed convolutional layer that \ndoubles the height and weight,\nand initialize its kernel with the `bilinear_kernel` function."
    },
    {
      "chunk_id": "750834b26fb0_2",
      "chapter": "fcn",
      "heading": "[**Initializing Transposed Convolutional Layers**]",
      "text": "We construct a transposed convolutional layer that \ndoubles the height and weight,\nand initialize its kernel with the `bilinear_kernel` function. ```{.python .input}\n#@tab mxnet\nconv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)\nconv_trans.initialize(init.Constant(bilinear_kernel(3, 3, 4)))\n```\n\n```{.python .input}\n#@tab pytorch\nconv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n                                bias=False)\nconv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));\n```\n\nRead the image `X` and assign the upsampling output to `Y`. In order to print the image, we need to adjust the position of the channel dimension. ```{.python .input}\n#@tab mxnet\nimg = image.imread('../img/catdog.jpg')\nX = np.expand_dims(img.astype('float32').transpose(2, 0, 1), axis=0) / 255\nY = conv_trans(X)\nout_img = Y[0].transpose(1, 2, 0)\n```\n\n```{.python .input}\n#@tab pytorch\nimg = torchvision.transforms.ToTensor()(d2l.Image.open('../img/catdog.jpg'))\nX = img.unsqueeze(0)\nY = conv_trans(X)\nout_img = Y[0].permute(1, 2, 0).detach()\n```\n\nAs we can see, the transposed convolutional layer increases both the height and width of the image by a factor of two. Except for the different scales in coordinates,\nthe image scaled up by bilinear interpolation and the original image printed in :numref:`sec_bbox` look the same. ```{.python .input}\n#@tab mxnet\nd2l.set_figsize()\nprint('input image shape:', img.shape)\nd2l.plt.imshow(img.asnumpy());\nprint('output image shape:', out_img.shape)\nd2l.plt.imshow(out_img.asnumpy());\n```\n\n```{.python .input}\n#@tab pytorch\nd2l.set_figsize()\nprint('input image shape:', img.permute(1, 2, 0).shape)\nd2l.plt.imshow(img.permute(1, 2, 0));\nprint('output image shape:', out_img.shape)\nd2l.plt.imshow(out_img);\n```\n\nIn a fully convolutional network, we [**initialize the transposed convolutional layer with upsampling of bilinear interpolation."
    },
    {
      "chunk_id": "750834b26fb0_3",
      "chapter": "fcn",
      "heading": "[**Initializing Transposed Convolutional Layers**]",
      "text": "For the $1\\times 1$ convolutional layer, we use Xavier initialization.**]\n\n```{.python .input}\n#@tab mxnet\nW = bilinear_kernel(num_classes, num_classes, 64)\nnet[-1].initialize(init.Constant(W))\nnet[-2].initialize(init=init.Xavier())\n```\n\n```{.python .input}\n#@tab pytorch\nW = bilinear_kernel(num_classes, num_classes, 64)\nnet.transpose_conv.weight.data.copy_(W);\n```"
    },
    {
      "chunk_id": "bec37c733a32_0",
      "chapter": "fcn",
      "heading": "[**Reading the Dataset**]",
      "text": "We read\nthe semantic segmentation dataset\nas introduced in :numref:`sec_semantic_segmentation`. \nThe output image shape of random cropping is\nspecified as $320\\times 480$: both the height and width are divisible by $32$.\n\n```{.python .input}\n#@tab all\nbatch_size, crop_size = 32, (320, 480)\ntrain_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)\n```"
    },
    {
      "chunk_id": "b9b8af447102_0",
      "chapter": "fcn",
      "heading": "[**Training**]",
      "text": "Now we can train our constructed\nfully convolutional network. \nThe loss function and accuracy calculation here\nare not essentially different from those in image classification of earlier chapters. \nBecause we use the output channel of the\ntransposed convolutional layer to\npredict the class for each pixel,\nthe channel dimension is specified in the loss calculation.\nIn addition, the accuracy is calculated\nbased on correctness\nof the predicted class for all the pixels.\n\n```{.python .input}\n#@tab mxnet\nnum_epochs, lr, wd, devices = 5, 0.1, 1e-3, d2l.try_all_gpus()\nloss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)\nnet.collect_params().reset_ctx(devices)\ntrainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'learning_rate': lr, 'wd': wd})\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```\n\n```{.python .input}\n#@tab pytorch\ndef loss(inputs, targets):\n    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n\nnum_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()\ntrainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```"
    },
    {
      "chunk_id": "347c86d3f5f8_0",
      "chapter": "fcn",
      "heading": "[**Prediction**]",
      "text": "When predicting, we need to standardize the input image\nin each channel and transform the image into the four-dimensional input format required by the CNN. ```{.python .input}\n#@tab mxnet\ndef predict(img):\n    X = test_iter._dataset.normalize_image(img)\n    X = np.expand_dims(X.transpose(2, 0, 1), axis=0)\n    pred = net(X.as_in_ctx(devices[0])).argmax(axis=1)\n    return pred.reshape(pred.shape[1], pred.shape[2])\n```\n\n```{.python .input}\n#@tab pytorch\ndef predict(img):\n    X = test_iter.dataset.normalize_image(img).unsqueeze(0)\n    pred = net(X.to(devices[0])).argmax(dim=1)\n    return pred.reshape(pred.shape[1], pred.shape[2])\n```\n\nTo [**visualize the predicted class**] of each pixel, we map the predicted class back to its label color in the dataset. ```{.python .input}\n#@tab mxnet\ndef label2image(pred):\n    colormap = np.array(d2l.VOC_COLORMAP, ctx=devices[0], dtype='uint8')\n    X = pred.astype('int32')\n    return colormap[X, :]\n```\n\n```{.python .input}\n#@tab pytorch\ndef label2image(pred):\n    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])\n    X = pred.long()\n    return colormap[X, :]\n```\n\nImages in the test dataset vary in size and shape. Since the model uses a transposed convolutional layer with stride of 32,\nwhen the height or width of an input image is indivisible by 32,\nthe output height or width of the\ntransposed convolutional layer will deviate from the shape of the input image. In order to address this issue,\nwe can crop multiple rectangular areas with height and width that are integer multiples of 32 in the image,\nand perform forward propagation\non the pixels in these areas separately. Note that\nthe union of these rectangular areas needs to completely cover the input image. When a pixel is covered by multiple rectangular areas,\nthe average of the transposed convolution outputs\nin separate areas for this same pixel\ncan be input to\nthe softmax operation\nto predict the class."
    },
    {
      "chunk_id": "347c86d3f5f8_1",
      "chapter": "fcn",
      "heading": "[**Prediction**]",
      "text": "When a pixel is covered by multiple rectangular areas,\nthe average of the transposed convolution outputs\nin separate areas for this same pixel\ncan be input to\nthe softmax operation\nto predict the class. For simplicity, we only read a few larger test images,\nand crop a $320\\times480$ area for prediction starting from the upper-left corner of an image. For these test images, we\nprint their cropped areas,\nprediction results,\nand ground-truth row by row. ```{.python .input}\n#@tab mxnet\nvoc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\ntest_images, test_labels = d2l.read_voc_images(voc_dir, False)\nn, imgs = 4, []\nfor i in range(n):\n    crop_rect = (0, 0, 480, 320)\n    X = image.fixed_crop(test_images[i], *crop_rect)\n    pred = label2image(predict(X))\n    imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)]\nd2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);\n```\n\n```{.python .input}\n#@tab pytorch\nvoc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\ntest_images, test_labels = d2l.read_voc_images(voc_dir, False)\nn, imgs = 4, []\nfor i in range(n):\n    crop_rect = (0, 0, 320, 480)\n    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)\n    pred = label2image(predict(X))\n    imgs += [X.permute(1,2,0), pred.cpu(),\n             torchvision.transforms.functional.crop(\n                 test_labels[i], *crop_rect).permute(1,2,0)]\nd2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);\n```"
    },
    {
      "chunk_id": "e9c6103ea3d0_0",
      "chapter": "fcn",
      "heading": "Summary",
      "text": "* The fully convolutional network first uses a CNN to extract image features, then transforms the number of channels into the number of classes via a $1\\times 1$ convolutional layer, and finally transforms the height and width of the feature maps to those of the input image via the transposed convolution.\n* In a fully convolutional network, we can use upsampling of bilinear interpolation to initialize the transposed convolutional layer."
    },
    {
      "chunk_id": "3b114137e908_0",
      "chapter": "fcn",
      "heading": "Exercises",
      "text": "1. If we use Xavier initialization for the transposed convolutional layer in the experiment, how does the result change?\n1. Can you further improve the accuracy of the model by tuning the hyperparameters?\n1. Predict the classes of all pixels in test images.\n1. The original fully convolutional network paper also uses outputs of some intermediate CNN layers :cite:`Long.Shelhamer.Darrell.2015`. Try to implement this idea.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/377)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1582)\n:end_tab:"
    },
    {
      "chunk_id": "3e70d16fd22b_0",
      "chapter": "fine-tuning",
      "heading": "fine-tuning",
      "text": "# Fine-Tuning\n:label:`sec_fine_tuning`\n\nIn earlier chapters, we discussed how to train models on the Fashion-MNIST training dataset with only 60000 images. We also described ImageNet, the most widely used large-scale image dataset in academia, which has more than 10 million images and 1000 objects. However, the size of the dataset that we usually encounter is between those of the two datasets.\n\n\nSuppose that we want to recognize different types of chairs from images, and then recommend purchase links to users. \nOne possible method is to first identify\n100 common chairs,\ntake 1000 images of different angles for each chair, \nand then train a classification model on the collected image dataset.\nAlthough this chair dataset may be larger than the Fashion-MNIST dataset,\nthe number of examples is still less than one-tenth of \nthat in ImageNet.\nThis may lead to overfitting of complicated models \nthat are suitable for ImageNet on this chair dataset.\nBesides, due to the limited amount of training examples,\nthe accuracy of the trained model\nmay not meet practical requirements.\n\n\nIn order to address the above problems,\nan obvious solution is to collect more data.\nHowever, collecting and labeling data can take a lot of time and money.\nFor example, in order to collect the ImageNet dataset, researchers have spent millions of dollars from research funding.\nAlthough the current data collection cost has been significantly reduced, this cost still cannot be ignored.\n\n\nAnother solution is to apply *transfer learning* to transfer the knowledge learned from the *source dataset* to the *target dataset*.\nFor example, although most of the images in the ImageNet dataset have nothing to do with chairs, the model trained on this dataset may extract more general image features, which can help identify edges, textures, shapes, and object composition.\nThese similar features may\nalso be effective for recognizing chairs."
    },
    {
      "chunk_id": "c08387022a0d_0",
      "chapter": "fine-tuning",
      "heading": "Steps",
      "text": "In this section, we will introduce a common technique in transfer learning: *fine-tuning*. As shown in :numref:`fig_finetune`, fine-tuning consists of the following four steps:\n\n\n1. Pretrain a neural network model, i.e., the *source model*, on a source dataset (e.g., the ImageNet dataset).\n1. Create a new neural network model, i.e., the *target model*. This copies all model designs and their parameters on the source model except the output layer. We assume that these model parameters contain the knowledge learned from the source dataset and this knowledge will also be applicable to the target dataset. We also assume that the output layer of the source model is closely related to the labels of the source dataset; thus it is not used in the target model.\n1. Add an output layer to the target model, whose number of outputs is the number of categories in the target dataset. Then randomly initialize the model parameters of this layer.\n1. Train the target model on the target dataset, such as a chair dataset. The output layer will be trained from scratch, while the parameters of all the other layers are fine-tuned based on the parameters of the source model.\n\n![Fine tuning.](../img/finetune.svg)\n:label:`fig_finetune`\n\nWhen target datasets are much smaller than source datasets, fine-tuning helps to improve models' generalization ability."
    },
    {
      "chunk_id": "dacad97a38ae_0",
      "chapter": "fine-tuning",
      "heading": "Hot Dog Recognition",
      "text": "Let's demonstrate fine-tuning via a concrete case:\nhot dog recognition. \nWe will fine-tune a ResNet model on a small dataset,\nwhich was pretrained on the ImageNet dataset.\nThis small dataset consists of\nthousands of images with and without hot dogs.\nWe will use the fine-tuned model to recognize \nhot dogs from images.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, init, np, npx\nfrom mxnet.gluon import nn\nimport os\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom torch import nn\nimport torch\nimport torchvision\nimport os\n```"
    },
    {
      "chunk_id": "0483c454a816_0",
      "chapter": "fine-tuning",
      "heading": "Reading the Dataset",
      "text": "[**The hot dog dataset we use was taken from online images**]. This dataset consists of\n1400 positive-class images containing hot dogs,\nand as many negative-class images containing other foods. 1000 images of both classes are used for training and the rest are for testing. After unzipping the downloaded dataset,\nwe obtain two folders `hotdog/train` and `hotdog/test`. Both folders have `hotdog` and `not-hotdog` subfolders, either of which contains images of\nthe corresponding class. ```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip', \n                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')\n\ndata_dir = d2l.download_extract('hotdog')\n```\n\nWe create two instances to read all the image files in the training and testing datasets, respectively. ```{.python .input}\n#@tab mxnet\ntrain_imgs = gluon.data.vision.ImageFolderDataset(\n    os.path.join(data_dir, 'train'))\ntest_imgs = gluon.data.vision.ImageFolderDataset(\n    os.path.join(data_dir, 'test'))\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))\ntest_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))\n```\n\nThe first 8 positive examples and the last 8 negative images are shown below. As you can see, [**the images vary in size and aspect ratio**]. ```{.python .input}\n#@tab all\nhotdogs = [train_imgs[i][0] for i in range(8)]\nnot_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]\nd2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);\n```\n\nDuring training, we first crop a random area of random size and random aspect ratio from the image,\nand then scale this area\nto a $224 \\times 224$ input image. During testing, we scale both the height and width of an image to 256 pixels, and then crop a central $224 \\times 224$ area as input. In addition, \nfor the three RGB (red, green, and blue) color channels\nwe *standardize* their values channel by channel."
    },
    {
      "chunk_id": "0483c454a816_1",
      "chapter": "fine-tuning",
      "heading": "Reading the Dataset",
      "text": "During testing, we scale both the height and width of an image to 256 pixels, and then crop a central $224 \\times 224$ area as input. In addition, \nfor the three RGB (red, green, and blue) color channels\nwe *standardize* their values channel by channel. Concretely,\nthe mean value of a channel is subtracted from each value of that channel and then the result is divided by the standard deviation of that channel. [~~Data augmentations~~]\n\n```{.python .input}\n#@tab mxnet\n# Specify the means and standard deviations of the three RGB channels to\n# standardize each channel\nnormalize = gluon.data.vision.transforms.Normalize(\n    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\ntrain_augs = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.RandomResizedCrop(224),\n    gluon.data.vision.transforms.RandomFlipLeftRight(),\n    gluon.data.vision.transforms.ToTensor(),\n    normalize])\n\ntest_augs = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.Resize(256),\n    gluon.data.vision.transforms.CenterCrop(224),\n    gluon.data.vision.transforms.ToTensor(),\n    normalize])\n```\n\n```{.python .input}\n#@tab pytorch\n# Specify the means and standard deviations of the three RGB channels to\n# standardize each channel\nnormalize = torchvision.transforms.Normalize(\n    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\ntrain_augs = torchvision.transforms.Compose([\n    torchvision.transforms.RandomResizedCrop(224),\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.ToTensor(),\n    normalize])\n\ntest_augs = torchvision.transforms.Compose([\n    torchvision.transforms.Resize([256, 256]),\n    torchvision.transforms.CenterCrop(224),\n    torchvision.transforms.ToTensor(),\n    normalize])\n```"
    },
    {
      "chunk_id": "c0b8d41459e8_0",
      "chapter": "fine-tuning",
      "heading": "[**Defining and Initializing the Model**]",
      "text": "We use ResNet-18, which was pretrained on the ImageNet dataset, as the source model. Here, we specify `pretrained=True` to automatically download the pretrained model parameters. If this model is used for the first time,\nInternet connection is required for download. ```{.python .input}\n#@tab mxnet\npretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)\n```\n\n```{.python .input}\n#@tab pytorch\npretrained_net = torchvision.models.resnet18(pretrained=True)\n```\n\n:begin_tab:`mxnet`\nThe pretrained source model instance contains two member variables: `features` and `output`. The former contains all layers of the model except the output layer, and the latter is the output layer of the model. The main purpose of this division is to facilitate the fine-tuning of model parameters of all layers but the output layer. The member variable `output` of source model is shown below. :end_tab:\n\n:begin_tab:`pytorch`\nThe pretrained source model instance contains a number of feature layers and an output layer `fc`. The main purpose of this division is to facilitate the fine-tuning of model parameters of all layers but the output layer. The member variable `fc` of source model is given below. :end_tab:\n\n```{.python .input}\n#@tab mxnet\npretrained_net.output\n```\n\n```{.python .input}\n#@tab pytorch\npretrained_net.fc\n```\n\nAs a fully connected layer, it transforms ResNet's final global average pooling outputs into 1000 class outputs of the ImageNet dataset. We then construct a new neural network as the target model. It is defined in the same way as the pretrained source model except that\nits number of outputs in the final layer\nis set to\nthe number of classes in the target dataset (rather than 1000). In the code below, the model parameters before the output layer of the target model instance `finetune_net` are initialized to model parameters of the corresponding layers from the source model. Since these model parameters were obtained via pretraining on ImageNet, \nthey are effective."
    },
    {
      "chunk_id": "c0b8d41459e8_1",
      "chapter": "fine-tuning",
      "heading": "[**Defining and Initializing the Model**]",
      "text": "Since these model parameters were obtained via pretraining on ImageNet, \nthey are effective. Therefore, we can only use \na small learning rate to *fine-tune* such pretrained parameters. In contrast, model parameters in the output layer are randomly initialized and generally require a larger learning rate to be learned from scratch. Letting the base learning rate be $\\eta$, a learning rate of $10\\eta$ will be used to iterate the model parameters in the output layer. ```{.python .input}\n#@tab mxnet\nfinetune_net = gluon.model_zoo.vision.resnet18_v2(classes=2)\nfinetune_net.features = pretrained_net.features\nfinetune_net.output.initialize(init.Xavier())\n# The model parameters in the output layer will be iterated using a learning\n# rate ten times greater\nfinetune_net.output.collect_params().setattr('lr_mult', 10)\n```\n\n```{.python .input}\n#@tab pytorch\nfinetune_net = torchvision.models.resnet18(pretrained=True)\nfinetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)\nnn.init.xavier_uniform_(finetune_net.fc.weight);\n```"
    },
    {
      "chunk_id": "9df5ed54bc0a_0",
      "chapter": "fine-tuning",
      "heading": "[**Fine-Tuning the Model**]",
      "text": "First, we define a training function `train_fine_tuning` that uses fine-tuning so it can be called multiple times."
    },
    {
      "chunk_id": "9df5ed54bc0a_1",
      "chapter": "fine-tuning",
      "heading": "[**Fine-Tuning the Model**]",
      "text": "First, we define a training function `train_fine_tuning` that uses fine-tuning so it can be called multiple times. ```{.python .input}\n#@tab mxnet\ndef train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5):\n    train_iter = gluon.data.DataLoader(\n        train_imgs.transform_first(train_augs), batch_size, shuffle=True)\n    test_iter = gluon.data.DataLoader(\n        test_imgs.transform_first(test_augs), batch_size)\n    devices = d2l.try_all_gpus()\n    net.collect_params().reset_ctx(devices)\n    net.hybridize()\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n    trainer = gluon.Trainer(net.collect_params(), 'sgd', {\n        'learning_rate': learning_rate, 'wd': 0.001})\n    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n                   devices)\n```\n\n```{.python .input}\n#@tab pytorch\n# If `param_group=True`, the model parameters in the output layer will be\n# updated using a learning rate ten times greater\ndef train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,\n                      param_group=True):\n    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n        os.path.join(data_dir, 'train'), transform=train_augs),\n        batch_size=batch_size, shuffle=True)\n    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n        os.path.join(data_dir, 'test'), transform=test_augs),\n        batch_size=batch_size)\n    devices = d2l.try_all_gpus()\n    loss = nn.CrossEntropyLoss(reduction=\"none\")\n    if param_group:\n        params_1x = [param for name, param in net.named_parameters()\n             if name not in [\"fc.weight\", \"fc.bias\"]]\n        trainer = torch.optim.SGD([{'params': params_1x},\n                                   {'params': net.fc.parameters(),\n                                    'lr': learning_rate * 10}],\n                                lr=learning_rate, weight_decay=0.001)\n    else:\n        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n                                  weight_decay=0.001)    \n    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n                   devices)\n```\n\nWe [**set the base learning rate to a small value**]\nin order to *fine-tune* the model parameters obtained via pretraining."
    },
    {
      "chunk_id": "9df5ed54bc0a_2",
      "chapter": "fine-tuning",
      "heading": "[**Fine-Tuning the Model**]",
      "text": "Based on the previous settings, we will train the output layer parameters of the target model from scratch using a learning rate ten times greater. ```{.python .input}\n#@tab mxnet\ntrain_fine_tuning(finetune_net, 0.01)\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_fine_tuning(finetune_net, 5e-5)\n```\n\n[**For comparison,**] we define an identical model, but (**initialize all of its model parameters to random values**). Since the entire model needs to be trained from scratch, we can use a larger learning rate. ```{.python .input}\n#@tab mxnet\nscratch_net = gluon.model_zoo.vision.resnet18_v2(classes=2)\nscratch_net.initialize(init=init.Xavier())\ntrain_fine_tuning(scratch_net, 0.1)\n```\n\n```{.python .input}\n#@tab pytorch\nscratch_net = torchvision.models.resnet18()\nscratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)\ntrain_fine_tuning(scratch_net, 5e-4, param_group=False)\n```\n\nAs we can see, the fine-tuned model tends to perform better for the same epoch\nbecause its initial parameter values are more effective."
    },
    {
      "chunk_id": "8448c299b9bc_0",
      "chapter": "fine-tuning",
      "heading": "Summary",
      "text": "* Transfer learning transfers knowledge learned from the source dataset to the target dataset. Fine-tuning is a common technique for transfer learning.\n* The target model copies all model designs with their parameters from the source model except the output layer, and fine-tunes these parameters based on the target dataset. In contrast, the output layer of the target model needs to be trained from scratch.\n* Generally, fine-tuning parameters uses a smaller learning rate, while training the output layer from scratch can use a larger learning rate."
    },
    {
      "chunk_id": "112283de1138_0",
      "chapter": "fine-tuning",
      "heading": "Exercises",
      "text": "1. Keep increasing the learning rate of `finetune_net`. How does the accuracy of the model change?\n2. Further adjust hyperparameters of `finetune_net` and `scratch_net` in the comparative experiment. Do they still differ in accuracy?\n3. Set the parameters before the output layer of `finetune_net` to those of the source model and do *not* update them during training. How does the accuracy of the model change? You can use the following code.\n\n```{.python .input}\n#@tab mxnet\nfinetune_net.features.collect_params().setattr('grad_req', 'null')\n```\n\n```{.python .input}\n#@tab pytorch\nfor param in finetune_net.parameters():\n    param.requires_grad = False\n```\n\n4. In fact, there is a \"hotdog\" class in the `ImageNet` dataset. Its corresponding weight parameter in the output layer can be obtained via the following code. How can we leverage this weight parameter?\n\n```{.python .input}\n#@tab mxnet\nweight = pretrained_net.output.weight\nhotdog_w = np.split(weight.data(), 1000, axis=0)[713]\nhotdog_w.shape\n```\n\n```{.python .input}\n#@tab pytorch\nweight = pretrained_net.fc.weight\nhotdog_w = torch.split(weight.data, 1, dim=0)[934]\nhotdog_w.shape\n```\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/368)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1439)\n:end_tab:"
    },
    {
      "chunk_id": "7b6fdcb1e4d2_0",
      "chapter": "image-augmentation",
      "heading": "image-augmentation",
      "text": "# Image Augmentation\n:label:`sec_image_augmentation`\n\nIn :numref:`sec_alexnet`, \nwe mentioned that large datasets \nare a prerequisite\nfor the success of\ndeep neural networks\nin various applications.\n*Image augmentation* \ngenerates similar but distinct training examples\nafter a series of random changes to the training images, thereby expanding the size of the training set.\nAlternatively,\nimage augmentation can be motivated\nby the fact that \nrandom tweaks of training examples \nallow models to rely less on\ncertain attributes, thereby improving their generalization ability.\nFor example, we can crop an image in different ways to make the object of interest appear in different positions, thereby reducing the dependence of a model on the position of the object. \nWe can also adjust factors such as brightness and color to reduce a model's sensitivity to color.\nIt is probably true\nthat image augmentation was indispensable\nfor the success of AlexNet at that time.\nIn this section we will discuss this widely used technique in computer vision.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, image, init, np, npx\nfrom mxnet.gluon import nn\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nfrom torch import nn\n```"
    },
    {
      "chunk_id": "cd7ede1ebced_0",
      "chapter": "image-augmentation",
      "heading": "Common Image Augmentation Methods",
      "text": "In our investigation of common image augmentation methods, we will use the following $400\\times 500$ image an example.\n\n```{.python .input}\n#@tab mxnet\nd2l.set_figsize()\nimg = image.imread('../img/cat1.jpg')\nd2l.plt.imshow(img.asnumpy());\n```\n\n```{.python .input}\n#@tab pytorch\nd2l.set_figsize()\nimg = d2l.Image.open('../img/cat1.jpg')\nd2l.plt.imshow(img);\n```\n\nMost image augmentation methods have a certain degree of randomness. To make it easier for us to observe the effect of image augmentation, next we define an auxiliary function `apply`. This function runs the image augmentation method `aug` multiple times on the input image `img` and shows all the results.\n\n```{.python .input}\n#@tab all\ndef apply(img, aug, num_rows=2, num_cols=4, scale=1.5):\n    Y = [aug(img) for _ in range(num_rows * num_cols)]\n    d2l.show_images(Y, num_rows, num_cols, scale=scale)\n```"
    },
    {
      "chunk_id": "37ef25ff1478_0",
      "chapter": "image-augmentation",
      "heading": "Flipping and Cropping",
      "text": ":begin_tab:`mxnet`\n[**Flipping the image left and right**] usually does not change the category of the object. This is one of the earliest and most widely used methods of image augmentation. Next, we use the `transforms` module to create the `RandomFlipLeftRight` instance, which flips\nan image left and right with a 50% chance. :end_tab:\n\n:begin_tab:`pytorch`\n[**Flipping the image left and right**] usually does not change the category of the object. This is one of the earliest and most widely used methods of image augmentation. Next, we use the `transforms` module to create the `RandomHorizontalFlip` instance, which flips\nan image left and right with a 50% chance. :end_tab:\n\n```{.python .input}\n#@tab mxnet\napply(img, gluon.data.vision.transforms.RandomFlipLeftRight())\n```\n\n```{.python .input}\n#@tab pytorch\napply(img, torchvision.transforms.RandomHorizontalFlip())\n```\n\n:begin_tab:`mxnet`\n[**Flipping up and down**] is not as common as flipping left and right. But at least for this example image, flipping up and down does not hinder recognition. Next, we create a `RandomFlipTopBottom` instance to flip\nan image up and down with a 50% chance. :end_tab:\n\n:begin_tab:`pytorch`\n[**Flipping up and down**] is not as common as flipping left and right. But at least for this example image, flipping up and down does not hinder recognition. Next, we create a `RandomVerticalFlip` instance to flip\nan image up and down with a 50% chance. :end_tab:\n\n```{.python .input}\n#@tab mxnet\napply(img, gluon.data.vision.transforms.RandomFlipTopBottom())\n```\n\n```{.python .input}\n#@tab pytorch\napply(img, torchvision.transforms.RandomVerticalFlip())\n```\n\nIn the example image we used, the cat is in the middle of the image, but this may not be the case in general. In :numref:`sec_pooling`, we explained that the pooling layer can reduce the sensitivity of a convolutional layer to the target position."
    },
    {
      "chunk_id": "37ef25ff1478_1",
      "chapter": "image-augmentation",
      "heading": "Flipping and Cropping",
      "text": "In :numref:`sec_pooling`, we explained that the pooling layer can reduce the sensitivity of a convolutional layer to the target position. In addition, we can also randomly crop the image to make objects appear in different positions in the image at different scales, which can also reduce the sensitivity of a model to the target position. In the code below, we [**randomly crop**] an area with an area of $10\\% \\sim 100\\%$ of the original area each time, and the ratio of width to height of this area is randomly selected from $0.5 \\sim 2$. Then, the width and height of the region are both scaled to 200 pixels. Unless otherwise specified, the random number between $a$ and $b$ in this section refers to a continuous value obtained by random and uniform sampling from the interval $[a, b]$. ```{.python .input}\n#@tab mxnet\nshape_aug = gluon.data.vision.transforms.RandomResizedCrop(\n    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))\napply(img, shape_aug)\n```\n\n```{.python .input}\n#@tab pytorch\nshape_aug = torchvision.transforms.RandomResizedCrop(\n    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))\napply(img, shape_aug)\n```"
    },
    {
      "chunk_id": "e5d5ea3eee64_0",
      "chapter": "image-augmentation",
      "heading": "Changing Colors",
      "text": "Another augmentation method is changing colors. We can change four aspects of the image color: brightness, contrast, saturation, and hue. In the example below, we [**randomly change the brightness**] of the image to a value between 50% ($1-0.5$) and 150% ($1+0.5$) of the original image.\n\n```{.python .input}\n#@tab mxnet\napply(img, gluon.data.vision.transforms.RandomBrightness(0.5))\n```\n\n```{.python .input}\n#@tab pytorch\napply(img, torchvision.transforms.ColorJitter(\n    brightness=0.5, contrast=0, saturation=0, hue=0))\n```\n\nSimilarly, we can [**randomly change the hue**] of the image.\n\n```{.python .input}\n#@tab mxnet\napply(img, gluon.data.vision.transforms.RandomHue(0.5))\n```\n\n```{.python .input}\n#@tab pytorch\napply(img, torchvision.transforms.ColorJitter(\n    brightness=0, contrast=0, saturation=0, hue=0.5))\n```\n\nWe can also create a `RandomColorJitter` instance and set how to [**randomly change the `brightness`, `contrast`, `saturation`, and `hue` of the image at the same time**].\n\n```{.python .input}\n#@tab mxnet\ncolor_aug = gluon.data.vision.transforms.RandomColorJitter(\n    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\napply(img, color_aug)\n```\n\n```{.python .input}\n#@tab pytorch\ncolor_aug = torchvision.transforms.ColorJitter(\n    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\napply(img, color_aug)\n```"
    },
    {
      "chunk_id": "1ea39ae5581e_0",
      "chapter": "image-augmentation",
      "heading": "Combining Multiple Image Augmentation Methods",
      "text": "In practice, we will [**combine multiple image augmentation methods**]. \nFor example,\nwe can combine the different image augmentation methods defined above and apply them to each image via a `Compose` instance.\n\n```{.python .input}\n#@tab mxnet\naugs = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.RandomFlipLeftRight(), color_aug, shape_aug])\napply(img, augs)\n```\n\n```{.python .input}\n#@tab pytorch\naugs = torchvision.transforms.Compose([\n    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])\napply(img, augs)\n```"
    },
    {
      "chunk_id": "4b58f1552684_0",
      "chapter": "image-augmentation",
      "heading": "[**Training with Image Augmentation**]",
      "text": "Let's train a model with image augmentation. Here we use the CIFAR-10 dataset instead of the Fashion-MNIST dataset that we used before. This is because the position and size of the objects in the Fashion-MNIST dataset have been normalized, while the color and size of the objects in the CIFAR-10 dataset have more significant differences. The first 32 training images in the CIFAR-10 dataset are shown below. ```{.python .input}\n#@tab mxnet\nd2l.show_images(gluon.data.vision.CIFAR10(\n    train=True)[:32][0], 4, 8, scale=0.8);\n```\n\n```{.python .input}\n#@tab pytorch\nall_images = torchvision.datasets.CIFAR10(train=True, root=\"../data\",\n                                          download=True)\nd2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);\n```\n\nIn order to obtain definitive results during prediction, we usually only apply image augmentation to training examples, and do not use image augmentation with random operations during prediction. [**Here we only use the simplest random left-right flipping method**]. In addition, we use a `ToTensor` instance to convert a minibatch of images into the format required by the deep learning framework, i.e., \n32-bit floating point numbers between 0 and 1 with the shape of (batch size, number of channels, height, width). ```{.python .input}\n#@tab mxnet\ntrain_augs = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.RandomFlipLeftRight(),\n    gluon.data.vision.transforms.ToTensor()])\n\ntest_augs = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.ToTensor()])\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_augs = torchvision.transforms.Compose([\n     torchvision.transforms.RandomHorizontalFlip(),\n     torchvision.transforms.ToTensor()])\n\ntest_augs = torchvision.transforms.Compose([\n     torchvision.transforms.ToTensor()])\n```\n\n:begin_tab:`mxnet`\nNext, we define an auxiliary function to facilitate reading the image and\napplying image augmentation."
    },
    {
      "chunk_id": "4b58f1552684_1",
      "chapter": "image-augmentation",
      "heading": "[**Training with Image Augmentation**]",
      "text": "The `transform_first` function provided by Gluon's\ndatasets applies image augmentation to the first element of each training\nexample (image and label), i.e., the image. For\na detailed introduction to `DataLoader`, please refer to :numref:`sec_fashion_mnist`. :end_tab:\n\n:begin_tab:`pytorch`\nNext, we [**define an auxiliary function to facilitate reading the image and\napplying image augmentation**]. The `transform` argument provided by PyTorch's\ndataset applies augmentation to transform the images. For\na detailed introduction to `DataLoader`, please refer to :numref:`sec_fashion_mnist`. :end_tab:\n\n```{.python .input}\n#@tab mxnet\ndef load_cifar10(is_train, augs, batch_size):\n    return gluon.data.DataLoader(\n        gluon.data.vision.CIFAR10(train=is_train).transform_first(augs),\n        batch_size=batch_size, shuffle=is_train,\n        num_workers=d2l.get_dataloader_workers())\n```\n\n```{.python .input}\n#@tab pytorch\ndef load_cifar10(is_train, augs, batch_size):\n    dataset = torchvision.datasets.CIFAR10(root=\"../data\", train=is_train,\n                                           transform=augs, download=True)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                    shuffle=is_train, num_workers=d2l.get_dataloader_workers())\n    return dataloader\n```"
    },
    {
      "chunk_id": "492e8b2ce6b3_0",
      "chapter": "image-augmentation",
      "heading": "Multi-GPU Training",
      "text": "We train the ResNet-18 model from\n:numref:`sec_resnet` on the\nCIFAR-10 dataset. Recall the introduction to\nmulti-GPU training in :numref:`sec_multi_gpu_concise`. In the following,\n[**we define a function to train and evaluate the model using multiple GPUs**]."
    },
    {
      "chunk_id": "492e8b2ce6b3_1",
      "chapter": "image-augmentation",
      "heading": "Multi-GPU Training",
      "text": "Recall the introduction to\nmulti-GPU training in :numref:`sec_multi_gpu_concise`. In the following,\n[**we define a function to train and evaluate the model using multiple GPUs**]. ```{.python .input}\n#@tab mxnet\n#@save\ndef train_batch_ch13(net, features, labels, loss, trainer, devices,\n                     split_f=d2l.split_batch):\n    \"\"\"Train for a minibatch with multiple GPUs (defined in Chapter 13).\"\"\"\n    X_shards, y_shards = split_f(features, labels, devices)\n    with autograd.record():\n        pred_shards = [net(X_shard) for X_shard in X_shards]\n        ls = [loss(pred_shard, y_shard) for pred_shard, y_shard\n              in zip(pred_shards, y_shards)]\n    for l in ls:\n        l.backward()\n    # The `True` flag allows parameters with stale gradients, which is useful\n    # later (e.g., in fine-tuning BERT)\n    trainer.step(labels.shape[0], ignore_stale_grad=True)\n    train_loss_sum = sum([float(l.sum()) for l in ls])\n    train_acc_sum = sum(d2l.accuracy(pred_shard, y_shard)\n                        for pred_shard, y_shard in zip(pred_shards, y_shards))\n    return train_loss_sum, train_acc_sum\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef train_batch_ch13(net, X, y, loss, trainer, devices):\n    \"\"\"Train for a minibatch with multiple GPUs (defined in Chapter 13).\"\"\"\n    if isinstance(X, list):\n        # Required for BERT fine-tuning (to be covered later)\n        X = [x.to(devices[0]) for x in X]\n    else:\n        X = X.to(devices[0])\n    y = y.to(devices[0])\n    net.train()\n    trainer.zero_grad()\n    pred = net(X)\n    l = loss(pred, y)\n    l.sum().backward()\n    trainer.step()\n    train_loss_sum = l.sum()\n    train_acc_sum = d2l.accuracy(pred, y)\n    return train_loss_sum, train_acc_sum\n```\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n               devices=d2l.try_all_gpus(), split_f=d2l.split_batch):\n    \"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\n    timer, num_batches = d2l.Timer(), len(train_iter)\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n                            legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        # Sum of training loss, sum of training accuracy, no."
    },
    {
      "chunk_id": "492e8b2ce6b3_2",
      "chapter": "image-augmentation",
      "heading": "Multi-GPU Training",
      "text": "of examples,\n        # no. of predictions\n        metric = d2l.Accumulator(4)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = train_batch_ch13(\n                net, features, labels, loss, trainer, devices, split_f)\n            metric.add(l, acc, labels.shape[0], labels.size)\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[2], metric[1] / metric[3],\n                              None))\n        test_acc = d2l.evaluate_accuracy_gpus(net, test_iter, split_f)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n          f'{str(devices)}')\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n               devices=d2l.try_all_gpus()):\n    \"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\n    timer, num_batches = d2l.Timer(), len(train_iter)\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n                            legend=['train loss', 'train acc', 'test acc'])\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    for epoch in range(num_epochs):\n        # Sum of training loss, sum of training accuracy, no. of examples,\n        # no."
    },
    {
      "chunk_id": "492e8b2ce6b3_3",
      "chapter": "image-augmentation",
      "heading": "Multi-GPU Training",
      "text": "of examples,\n        # no. of predictions\n        metric = d2l.Accumulator(4)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = train_batch_ch13(\n                net, features, labels, loss, trainer, devices)\n            metric.add(l, acc, labels.shape[0], labels.numel())\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[2], metric[1] / metric[3],\n                              None))\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n          f'{str(devices)}')\n```\n\nNow we can [**define the `train_with_data_aug` function to train the model with image augmentation**]. This function gets all available GPUs, \nuses Adam as the optimization algorithm,\napplies image augmentation to the training dataset,\nand finally calls the `train_ch13` function just defined to train and evaluate the model."
    },
    {
      "chunk_id": "492e8b2ce6b3_4",
      "chapter": "image-augmentation",
      "heading": "Multi-GPU Training",
      "text": "This function gets all available GPUs, \nuses Adam as the optimization algorithm,\napplies image augmentation to the training dataset,\nand finally calls the `train_ch13` function just defined to train and evaluate the model. ```{.python .input}\n#@tab mxnet\nbatch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10)\nnet.initialize(init=init.Xavier(), ctx=devices)\n\ndef train_with_data_aug(train_augs, test_augs, net, lr=0.001):\n    train_iter = load_cifar10(True, train_augs, batch_size)\n    test_iter = load_cifar10(False, test_augs, batch_size)\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n    trainer = gluon.Trainer(net.collect_params(), 'adam',\n                            {'learning_rate': lr})\n    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)\n```\n\n```{.python .input}\n#@tab pytorch\nbatch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)\nnet.apply(d2l.init_cnn)\n\ndef train_with_data_aug(train_augs, test_augs, net, lr=0.001):\n    train_iter = load_cifar10(True, train_augs, batch_size)\n    test_iter = load_cifar10(False, test_augs, batch_size)\n    loss = nn.CrossEntropyLoss(reduction=\"none\")\n    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n    net(next(iter(train_iter))[0])\n    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)\n```\n\nLet's [**train the model**] using image augmentation based on random left-right flipping. ```{.python .input}\n#@tab all\ntrain_with_data_aug(train_augs, test_augs, net)\n```"
    },
    {
      "chunk_id": "0343f68f7e4e_0",
      "chapter": "image-augmentation",
      "heading": "Summary",
      "text": "* Image augmentation generates random images based on existing training data to improve the generalization ability of models.\n* In order to obtain definitive results during prediction, we usually only apply image augmentation to training examples, and do not use image augmentation with random operations during prediction.\n* Deep learning frameworks provide many different image augmentation methods, which can be applied simultaneously."
    },
    {
      "chunk_id": "f18b8b460299_0",
      "chapter": "image-augmentation",
      "heading": "Exercises",
      "text": "1. Train the model without using image augmentation: `train_with_data_aug(test_augs, test_augs)`. Compare training and testing accuracy when using and not using image augmentation. Can this comparative experiment support the argument that image augmentation can mitigate overfitting? Why?\n1. Combine multiple different image augmentation methods in model training on the CIFAR-10 dataset. Does it improve test accuracy? \n1. Refer to the online documentation of the deep learning framework. What other image augmentation methods does it also provide?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/367)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1404)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Computer Vision\n:label:`chap_cv`\n\nWhether it is medical diagnosis, self-driving vehicles, camera monitoring, or smart filters, many applications in the field of computer vision are closely related to our current and future lives. \nIn recent years, deep learning has been\nthe transformative power for advancing the performance of computer vision systems.\nIt can be said that the most advanced computer vision applications are almost inseparable from deep learning.\nIn view of this, this chapter will focus on the field of computer vision, and investigate methods and applications that have recently been influential in academia and industry.\n\n\nIn :numref:`chap_cnn` and :numref:`chap_modern_cnn`, we studied various convolutional neural networks that are\ncommonly used in computer vision, and applied them\nto simple image classification tasks. \nAt the beginning of this chapter, we will describe\ntwo methods that \nmay improve model generalization, namely *image augmentation* and *fine-tuning*,\nand apply them to image classification. \nSince deep neural networks can effectively represent images in multiple levels, \nsuch layerwise representations have been successfully \nused in various computer vision tasks such as *object detection*, *semantic segmentation*, and *style transfer*. \nFollowing the key idea of leveraging layerwise representations in computer vision,\nwe will begin with major components and techniques for object detection. Next, we will show how to use *fully convolutional networks* for semantic segmentation of images. Then we will explain how to use style transfer techniques to generate images like the cover of this book.\nIn the end, we conclude this chapter\nby applying the materials of this chapter and several previous chapters on two popular computer vision benchmark datasets.\n\n```toc\n:maxdepth: 2\n\nimage-augmentation\nfine-tuning\nbounding-box\nanchor\nmultiscale-object-detection\nobject-detection-dataset\nssd\nrcnn\nsemantic-segmentation-and-dataset\ntransposed-conv\nfcn\nneural-style\nkaggle-cifar10\nkaggle-dog\n```"
    },
    {
      "chunk_id": "8954884ad371_0",
      "chapter": "kaggle-cifar10",
      "heading": "kaggle-cifar10",
      "text": "# Image Classification (CIFAR-10) on Kaggle\n:label:`sec_kaggle_cifar10`\n\nSo far, we have been using high-level APIs of deep learning frameworks to directly obtain image datasets in tensor format.\nHowever, custom image datasets\noften come in the form of image files.\nIn this section, we will start from\nraw image files,\nand organize, read, then transform them\ninto tensor format step by step.\n\nWe experimented with the CIFAR-10 dataset in :numref:`sec_image_augmentation`,\nwhich is an important dataset in computer vision.\nIn this section,\nwe will apply the knowledge we learned\nin previous sections\nto practice the Kaggle competition of\nCIFAR-10 image classification.\n(**The web address of the competition is https://www.kaggle.com/c/cifar-10**)\n\n:numref:`fig_kaggle_cifar10` shows the information on the competition's webpage.\nIn order to submit the results,\nyou need to register a Kaggle account.\n\n![CIFAR-10 image classification competition webpage information. The competition dataset can be obtained by clicking the \"Data\" tab.](../img/kaggle-cifar10.png)\n:width:`600px`\n:label:`fig_kaggle_cifar10`\n\n```{.python .input}\n#@tab mxnet\nimport collections\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import gluon, init, npx\nfrom mxnet.gluon import nn\nimport os\nimport pandas as pd\nimport shutil\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nimport collections\nfrom d2l import torch as d2l\nimport math\nimport torch\nimport torchvision\nfrom torch import nn\nimport os\nimport pandas as pd\nimport shutil\n```"
    },
    {
      "chunk_id": "bb0ea2ada03e_0",
      "chapter": "kaggle-cifar10",
      "heading": "Obtaining and Organizing the Dataset",
      "text": "The competition dataset is divided into\na training set and a test set,\nwhich contain 50000 and 300000 images, respectively.\nIn the test set,\n10000 images will be used for evaluation,\nwhile the remaining 290000 images will not\nbe evaluated:\nthey are included just\nto make it hard\nto cheat with\n*manually* labeled results of the test set.\nThe images in this dataset\nare all png color (RGB channels) image files,\nwhose height and width are both 32 pixels.\nThe images cover a total of 10 categories, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks.\nThe upper-left corner of :numref:`fig_kaggle_cifar10` shows some images of airplanes, cars, and birds in the dataset."
    },
    {
      "chunk_id": "cb75acb63926_0",
      "chapter": "kaggle-cifar10",
      "heading": "Downloading the Dataset",
      "text": "After logging in to Kaggle, we can click the \"Data\" tab on the CIFAR-10 image classification competition webpage shown in :numref:`fig_kaggle_cifar10` and download the dataset by clicking the \"Download All\" button.\nAfter unzipping the downloaded file in `../data`, and unzipping `train.7z` and `test.7z` inside it, you will find the entire dataset in the following paths:\n\n* `../data/cifar-10/train/[1-50000].png`\n* `../data/cifar-10/test/[1-300000].png`\n* `../data/cifar-10/trainLabels.csv`\n* `../data/cifar-10/sampleSubmission.csv`\n\nwhere the `train` and `test` directories contain the training and testing images, respectively, `trainLabels.csv` provides labels for the training images, and `sample_submission.csv` is a sample submission file.\n\nTo make it easier to get started, [**we provide a small-scale sample of the dataset that\ncontains the first 1000 training images and 5 random testing images.**]\nTo use the full dataset of the Kaggle competition, you need to set the following `demo` variable to `False`.\n\n```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',\n                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')\n\n# If you use the full dataset downloaded for the Kaggle competition, set\n# `demo` to False\ndemo = True\n\nif demo:\n    data_dir = d2l.download_extract('cifar10_tiny')\nelse:\n    data_dir = '../data/cifar-10/'\n```"
    },
    {
      "chunk_id": "d37c69ab4a25_0",
      "chapter": "kaggle-cifar10",
      "heading": "[**Organizing the Dataset**]",
      "text": "We need to organize datasets to facilitate model training and testing. Let's first read the labels from the csv file. The following function returns a dictionary that maps\nthe non-extension part of the filename to its label. ```{.python .input}\n#@tab all\n#@save\ndef read_csv_labels(fname):\n    \"\"\"Read `fname` to return a filename to label dictionary.\"\"\"\n    with open(fname, 'r') as f:\n        # Skip the file header line (column name)\n        lines = f.readlines()[1:]\n    tokens = [l.rstrip().split(',') for l in lines]\n    return dict(((name, label) for name, label in tokens))\n\nlabels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\nprint('# training examples:', len(labels))\nprint('# classes:', len(set(labels.values())))\n```\n\nNext, we define the `reorg_train_valid` function to [**split the validation set out of the original training set.**]\nThe argument `valid_ratio` in this function is the ratio of the number of examples in the validation set to the number of examples in the original training set. More concretely,\nlet $n$ be the number of images of the class with the least examples, and $r$ be the ratio. The validation set will split out\n$\\max(\\lfloor nr\\rfloor,1)$ images for each class. Let's use `valid_ratio=0.1` as an example. Since the original training set has 50000 images,\nthere will be 45000 images used for training in the path `train_valid_test/train`,\nwhile the other 5000 images will be split out\nas validation set in the path `train_valid_test/valid`. After organizing the dataset, images of the same class will be placed under the same folder."
    },
    {
      "chunk_id": "d37c69ab4a25_1",
      "chapter": "kaggle-cifar10",
      "heading": "[**Organizing the Dataset**]",
      "text": "After organizing the dataset, images of the same class will be placed under the same folder. ```{.python .input}\n#@tab all\n#@save\ndef copyfile(filename, target_dir):\n    \"\"\"Copy a file into a target directory.\"\"\"\n    os.makedirs(target_dir, exist_ok=True)\n    shutil.copy(filename, target_dir)\n\n#@save\ndef reorg_train_valid(data_dir, labels, valid_ratio):\n    \"\"\"Split the validation set out of the original training set.\"\"\"\n    # The number of examples of the class that has the fewest examples in the\n    # training dataset\n    n = collections.Counter(labels.values()).most_common()[-1][1]\n    # The number of examples per class for the validation set\n    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n    label_count = {}\n    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n        label = labels[train_file.split('.')[0]]\n        fname = os.path.join(data_dir, 'train', train_file)\n        copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n                                     'train_valid', label))\n        if label not in label_count or label_count[label] < n_valid_per_label:\n            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n                                         'valid', label))\n            label_count[label] = label_count.get(label, 0) + 1\n        else:\n            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n                                         'train', label))\n    return n_valid_per_label\n```\n\nThe `reorg_test` function below [**organizes the testing set for data loading during prediction.**]\n\n```{.python .input}\n#@tab all\n#@save\ndef reorg_test(data_dir):\n    \"\"\"Organize the testing set for data loading during prediction.\"\"\"\n    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n        copyfile(os.path.join(data_dir, 'test', test_file),\n                 os.path.join(data_dir, 'train_valid_test', 'test',\n                              'unknown'))\n```\n\nFinally, we use a function to [**invoke**]\nthe `read_csv_labels`, `reorg_train_valid`, and `reorg_test` (**functions defined above.**)\n\n```{.python .input}\n#@tab all\ndef reorg_cifar10_data(data_dir, valid_ratio):\n    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n    reorg_train_valid(data_dir, labels, valid_ratio)\n    reorg_test(data_dir)\n```\n\nHere we only set the batch size to 32 for the small-scale sample of the dataset."
    },
    {
      "chunk_id": "d37c69ab4a25_2",
      "chapter": "kaggle-cifar10",
      "heading": "[**Organizing the Dataset**]",
      "text": "When training and testing\nthe complete dataset of the Kaggle competition,\n`batch_size` should be set to a larger integer, such as 128. We split out 10% of the training examples as the validation set for tuning hyperparameters. ```{.python .input}\n#@tab all\nbatch_size = 32 if demo else 128\nvalid_ratio = 0.1\nreorg_cifar10_data(data_dir, valid_ratio)\n```"
    },
    {
      "chunk_id": "31a1a3e3fc94_0",
      "chapter": "kaggle-cifar10",
      "heading": "[**Image Augmentation**]",
      "text": "We use image augmentation to address overfitting. For example, images can be flipped horizontally at random during training. We can also perform standardization for the three RGB channels of color images. Below lists some of these operations that you can tweak."
    },
    {
      "chunk_id": "31a1a3e3fc94_1",
      "chapter": "kaggle-cifar10",
      "heading": "[**Image Augmentation**]",
      "text": "For example, images can be flipped horizontally at random during training. We can also perform standardization for the three RGB channels of color images. Below lists some of these operations that you can tweak. ```{.python .input}\n#@tab mxnet\ntransform_train = gluon.data.vision.transforms.Compose([\n    # Scale the image up to a square of 40 pixels in both height and width\n    gluon.data.vision.transforms.Resize(40),\n    # Randomly crop a square image of 40 pixels in both height and width to\n    # produce a small square of 0.64 to 1 times the area of the original\n    # image, and then scale it to a square of 32 pixels in both height and\n    # width\n    gluon.data.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n                                                   ratio=(1.0, 1.0)),\n    gluon.data.vision.transforms.RandomFlipLeftRight(),\n    gluon.data.vision.transforms.ToTensor(),\n    # Standardize each channel of the image\n    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n                                           [0.2023, 0.1994, 0.2010])])\n```\n\n```{.python .input}\n#@tab pytorch\ntransform_train = torchvision.transforms.Compose([\n    # Scale the image up to a square of 40 pixels in both height and width\n    torchvision.transforms.Resize(40),\n    # Randomly crop a square image of 40 pixels in both height and width to\n    # produce a small square of 0.64 to 1 times the area of the original\n    # image, and then scale it to a square of 32 pixels in both height and\n    # width\n    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n                                                   ratio=(1.0, 1.0)),\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.ToTensor(),\n    # Standardize each channel of the image\n    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n                                     [0.2023, 0.1994, 0.2010])])\n```\n\nDuring testing,\nwe only perform standardization on images\nso as to\nremove randomness in the evaluation results."
    },
    {
      "chunk_id": "31a1a3e3fc94_2",
      "chapter": "kaggle-cifar10",
      "heading": "[**Image Augmentation**]",
      "text": "```{.python .input}\n#@tab mxnet\ntransform_test = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.ToTensor(),\n    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n                                           [0.2023, 0.1994, 0.2010])])\n```\n\n```{.python .input}\n#@tab pytorch\ntransform_test = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n                                     [0.2023, 0.1994, 0.2010])])\n```"
    },
    {
      "chunk_id": "ca2babcc6df6_0",
      "chapter": "kaggle-cifar10",
      "heading": "Reading the Dataset",
      "text": "Next, we [**read the organized dataset consisting of raw image files**]. Each example includes an image and a label. ```{.python .input}\n#@tab mxnet\ntrain_ds, valid_ds, train_valid_ds, test_ds = [\n    gluon.data.vision.ImageFolderDataset(\n        os.path.join(data_dir, 'train_valid_test', folder))\n    for folder in ['train', 'valid', 'train_valid', 'test']]\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n    os.path.join(data_dir, 'train_valid_test', folder),\n    transform=transform_train) for folder in ['train', 'train_valid']]\n\nvalid_ds, test_ds = [torchvision.datasets.ImageFolder(\n    os.path.join(data_dir, 'train_valid_test', folder),\n    transform=transform_test) for folder in ['valid', 'test']]\n```\n\nDuring training,\nwe need to [**specify all the image augmentation operations defined above**]. When the validation set\nis used for model evaluation during hyperparameter tuning,\nno randomness from image augmentation should be introduced. Before final prediction,\nwe train the model on the combined training set and validation set to make full use of all the labeled data."
    },
    {
      "chunk_id": "ca2babcc6df6_1",
      "chapter": "kaggle-cifar10",
      "heading": "Reading the Dataset",
      "text": "Before final prediction,\nwe train the model on the combined training set and validation set to make full use of all the labeled data. ```{.python .input}\n#@tab mxnet\ntrain_iter, train_valid_iter = [gluon.data.DataLoader(\n    dataset.transform_first(transform_train), batch_size, shuffle=True,\n    last_batch='discard') for dataset in (train_ds, train_valid_ds)]\n\nvalid_iter = gluon.data.DataLoader(\n    valid_ds.transform_first(transform_test), batch_size, shuffle=False,\n    last_batch='discard')\n\ntest_iter = gluon.data.DataLoader(\n    test_ds.transform_first(transform_test), batch_size, shuffle=False,\n    last_batch='keep')\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_iter, train_valid_iter = [torch.utils.data.DataLoader(\n    dataset, batch_size, shuffle=True, drop_last=True)\n    for dataset in (train_ds, train_valid_ds)]\n\nvalid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n                                         drop_last=True)\n\ntest_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n                                        drop_last=False)\n```"
    },
    {
      "chunk_id": "b5d845880ca5_0",
      "chapter": "kaggle-cifar10",
      "heading": "Defining the [**Model**]",
      "text": ":begin_tab:`mxnet`\nHere, we build the residual blocks based on the `HybridBlock` class, which is\nslightly different from the implementation described in\n:numref:`sec_resnet`. This is for improving computational efficiency. :end_tab:\n\n```{.python .input}\n#@tab mxnet\nclass Residual(nn.HybridBlock):\n    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n        super(Residual, self).__init__(**kwargs)\n        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n                               strides=strides)\n        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\n                                   strides=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm()\n        self.bn2 = nn.BatchNorm()\n\n    def hybrid_forward(self, F, X):\n        Y = F.npx.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        return F.npx.relu(Y + X)\n```\n\n:begin_tab:`mxnet`\nNext, we define the ResNet-18 model. :end_tab:\n\n```{.python .input}\n#@tab mxnet\ndef resnet18(num_classes):\n    net = nn.HybridSequential()\n    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n            nn.BatchNorm(), nn.Activation('relu'))\n\n    def resnet_block(num_channels, num_residuals, first_block=False):\n        blk = nn.HybridSequential()\n        for i in range(num_residuals):\n            if i == 0 and not first_block:\n                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n            else:\n                blk.add(Residual(num_channels))\n        return blk\n\n    net.add(resnet_block(64, 2, first_block=True),\n            resnet_block(128, 2),\n            resnet_block(256, 2),\n            resnet_block(512, 2))\n    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n    return net\n```\n\n:begin_tab:`mxnet`\nWe use Xavier initialization described in :numref:`subsec_xavier` before training begins."
    },
    {
      "chunk_id": "b5d845880ca5_1",
      "chapter": "kaggle-cifar10",
      "heading": "Defining the [**Model**]",
      "text": ":end_tab:\n\n:begin_tab:`pytorch`\nWe define the ResNet-18 model described in\n:numref:`sec_resnet`. :end_tab:\n\n```{.python .input}\n#@tab mxnet\ndef get_net(devices):\n    num_classes = 10\n    net = resnet18(num_classes)\n    net.initialize(ctx=devices, init=init.Xavier())\n    return net\n\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_net():\n    num_classes = 10\n    net = d2l.resnet18(num_classes, 3)\n    return net\n\nloss = nn.CrossEntropyLoss(reduction=\"none\")\n```"
    },
    {
      "chunk_id": "1e1f5d38e2fb_0",
      "chapter": "kaggle-cifar10",
      "heading": "Defining the [**Training Function**]",
      "text": "We will select models and tune hyperparameters according to the model's performance on the validation set. In the following, we define the model training function `train`."
    },
    {
      "chunk_id": "1e1f5d38e2fb_1",
      "chapter": "kaggle-cifar10",
      "heading": "Defining the [**Training Function**]",
      "text": "We will select models and tune hyperparameters according to the model's performance on the validation set. In the following, we define the model training function `train`. ```{.python .input}\n#@tab mxnet\ndef train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n          lr_decay):\n    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n    num_batches, timer = len(train_iter), d2l.Timer()\n    legend = ['train loss', 'train acc']\n    if valid_iter is not None:\n        legend.append('valid acc')\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=legend)\n    for epoch in range(num_epochs):\n        metric = d2l.Accumulator(3)\n        if epoch > 0 and epoch % lr_period == 0:\n            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = d2l.train_batch_ch13(\n                net, features, labels.astype('float32'), loss, trainer,\n                devices, d2l.split_batch)\n            metric.add(l, acc, labels.shape[0])\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[2], metric[1] / metric[2],\n                              None))\n        if valid_iter is not None:\n            valid_acc = d2l.evaluate_accuracy_gpus(net, valid_iter,\n                                                   d2l.split_batch)\n            animator.add(epoch + 1, (None, None, valid_acc))\n    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n                f'train acc {metric[1] / metric[2]:.3f}')\n    if valid_iter is not None:\n        measures += f', valid acc {valid_acc:.3f}'\n    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n          f' examples/sec on {str(devices)}')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n          lr_decay):\n    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n                              weight_decay=wd)\n    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n    num_batches, timer = len(train_iter), d2l.Timer()\n    legend = ['train loss', 'train acc']\n    if valid_iter is not None:\n        legend.append('valid acc')\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=legend)\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    for epoch in range(num_epochs):\n        net.train()\n        metric = d2l.Accumulator(3)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = d2l.train_batch_ch13(net, features, labels,\n                                          loss, trainer, devices)\n            metric.add(l, acc, labels.shape[0])\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[2], metric[1] / metric[2],\n                              None))\n        if valid_iter is not None:\n            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)\n            animator.add(epoch + 1, (None, None, valid_acc))\n        scheduler.step()\n    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n                f'train acc {metric[1] / metric[2]:.3f}')\n    if valid_iter is not None:\n        measures += f', valid acc {valid_acc:.3f}'\n    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n          f' examples/sec on {str(devices)}')\n```"
    },
    {
      "chunk_id": "05c14e761f0b_0",
      "chapter": "kaggle-cifar10",
      "heading": "[**Training and Validating the Model**]",
      "text": "Now, we can train and validate the model.\nAll the following hyperparameters can be tuned.\nFor example, we can increase the number of epochs.\nWhen `lr_period` and `lr_decay` are set to 4 and 0.9, respectively, the learning rate of the optimization algorithm will be multiplied by 0.9 after every 4 epochs. Just for ease of demonstration,\nwe only train 20 epochs here.\n\n```{.python .input}\n#@tab mxnet\ndevices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 0.02, 5e-4\nlr_period, lr_decay, net = 4, 0.9, get_net(devices)\nnet.hybridize()\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n```\n\n```{.python .input}\n#@tab pytorch\ndevices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4\nlr_period, lr_decay, net = 4, 0.9, get_net()\nnet(next(iter(train_iter))[0])\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n```"
    },
    {
      "chunk_id": "59688fc6b050_0",
      "chapter": "kaggle-cifar10",
      "heading": "[**Classifying the Testing Set**] and Submitting Results on Kaggle",
      "text": "After obtaining a promising model with hyperparameters,\nwe use all the labeled data (including the validation set) to retrain the model and classify the testing set.\n\n```{.python .input}\n#@tab mxnet\nnet, preds = get_net(devices), []\nnet.hybridize()\ntrain(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n\nfor X, _ in test_iter:\n    y_hat = net(X.as_in_ctx(devices[0]))\n    preds.extend(y_hat.argmax(axis=1).astype(int).asnumpy())\nsorted_ids = list(range(1, len(test_ds) + 1))\nsorted_ids.sort(key=lambda x: str(x))\ndf = pd.DataFrame({'id': sorted_ids, 'label': preds})\ndf['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\ndf.to_csv('submission.csv', index=False)\n```\n\n```{.python .input}\n#@tab pytorch\nnet, preds = get_net(), []\nnet(next(iter(train_valid_iter))[0])\ntrain(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n\nfor X, _ in test_iter:\n    y_hat = net(X.to(devices[0]))\n    preds.extend(y_hat.argmax(dim=1).type(torch.int32).cpu().numpy())\nsorted_ids = list(range(1, len(test_ds) + 1))\nsorted_ids.sort(key=lambda x: str(x))\ndf = pd.DataFrame({'id': sorted_ids, 'label': preds})\ndf['label'] = df['label'].apply(lambda x: train_valid_ds.classes[x])\ndf.to_csv('submission.csv', index=False)\n```\n\nThe above code\nwill generate a `submission.csv` file,\nwhose format\nmeets the requirement of the Kaggle competition.\nThe method\nfor submitting results to Kaggle\nis similar to that in :numref:`sec_kaggle_house`."
    },
    {
      "chunk_id": "0744059cb800_0",
      "chapter": "kaggle-cifar10",
      "heading": "Summary",
      "text": "* We can read datasets containing raw image files after organizing them into the required format.\n\n:begin_tab:`mxnet`\n* We can use convolutional neural networks, image augmentation, and hybrid programing in an image classification competition.\n:end_tab:\n\n:begin_tab:`pytorch`\n* We can use convolutional neural networks and image augmentation in an image classification competition.\n:end_tab:"
    },
    {
      "chunk_id": "c6efe959172f_0",
      "chapter": "kaggle-cifar10",
      "heading": "Exercises",
      "text": "1. Use the complete CIFAR-10 dataset for this Kaggle competition. Set hyperparameters as `batch_size = 128`, `num_epochs = 100`, `lr = 0.1`, `lr_period = 50`, and `lr_decay = 0.1`.  See what accuracy and ranking you can achieve in this competition. Can you further improve them?\n1. What accuracy can you get when not using image augmentation?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/379)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1479)\n:end_tab:"
    },
    {
      "chunk_id": "1b7e53bd824b_0",
      "chapter": "kaggle-dog",
      "heading": "kaggle-dog",
      "text": "# Dog Breed Identification (ImageNet Dogs) on Kaggle\n\nIn this section, we will practice\nthe dog breed identification problem on\nKaggle. (**The web address of this competition is https://www.kaggle.com/c/dog-breed-identification**)\n\nIn this competition,\n120 different breeds of dogs will be recognized.\nIn fact,\nthe dataset for this competition is\na subset of the ImageNet dataset.\nUnlike the images in the CIFAR-10 dataset in :numref:`sec_kaggle_cifar10`,\nthe images in the ImageNet dataset are both higher and wider in varying dimensions.\n:numref:`fig_kaggle_dog` shows the information on the competition's webpage. You need a Kaggle account\nto submit your results.\n\n\n![The dog breed identification competition website. The competition dataset can be obtained by clicking the \"Data\" tab.](../img/kaggle-dog.jpg)\n:width:`400px`\n:label:`fig_kaggle_dog`\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, npx\nfrom mxnet.gluon import nn\nimport os\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nfrom torch import nn\nimport os\n```"
    },
    {
      "chunk_id": "1a9ca543719e_0",
      "chapter": "kaggle-dog",
      "heading": "Obtaining and Organizing the Dataset",
      "text": "The competition dataset is divided into a training set and a test set, which contain 10222 and 10357 JPEG images\nof three RGB (color) channels, respectively.\nAmong the training dataset,\nthere are 120 breeds of dogs\nsuch as Labradors, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers."
    },
    {
      "chunk_id": "3bc4872a5b59_0",
      "chapter": "kaggle-dog",
      "heading": "Downloading the Dataset",
      "text": "After logging into Kaggle,\nyou can click on the \"Data\" tab on the\ncompetition webpage shown in :numref:`fig_kaggle_dog` and download the dataset by clicking the \"Download All\" button.\nAfter unzipping the downloaded file in `../data`, you will find the entire dataset in the following paths:\n\n* ../data/dog-breed-identification/labels.csv\n* ../data/dog-breed-identification/sample_submission.csv\n* ../data/dog-breed-identification/train\n* ../data/dog-breed-identification/test\n\nYou may have noticed that the above structure is\nsimilar to that of the CIFAR-10 competition in :numref:`sec_kaggle_cifar10`, where folders `train/` and `test/` contain training and testing dog images, respectively, and `labels.csv` contains\nthe labels for the training images.\nSimilarly, to make it easier to get started, [**we provide a small sample of the dataset**] mentioned above: `train_valid_test_tiny.zip`.\nIf you are going to use the full dataset for the Kaggle competition, you need to change the `demo` variable below to `False`.\n\n```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',\n                            '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')\n\n# If you use the full dataset downloaded for the Kaggle competition, change\n# the variable below to `False`\ndemo = True\nif demo:\n    data_dir = d2l.download_extract('dog_tiny')\nelse:\n    data_dir = os.path.join('..', 'data', 'dog-breed-identification')\n```"
    },
    {
      "chunk_id": "189a6e0cd623_0",
      "chapter": "kaggle-dog",
      "heading": "[**Organizing the Dataset**]",
      "text": "We can organize the dataset similarly to what we did in :numref:`sec_kaggle_cifar10`, namely splitting out\na validation set from the original training set, and moving images into subfolders grouped by labels.\n\nThe `reorg_dog_data` function below reads\nthe training data labels, splits out the validation set, and organizes the training set.\n\n```{.python .input}\n#@tab all\ndef reorg_dog_data(data_dir, valid_ratio):\n    labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))\n    d2l.reorg_train_valid(data_dir, labels, valid_ratio)\n    d2l.reorg_test(data_dir)\n\n\nbatch_size = 32 if demo else 128\nvalid_ratio = 0.1\nreorg_dog_data(data_dir, valid_ratio)\n```"
    },
    {
      "chunk_id": "e59e9486355a_0",
      "chapter": "kaggle-dog",
      "heading": "[**Image Augmentation**]",
      "text": "Recall that this dog breed dataset\nis a subset of the ImageNet dataset,\nwhose images\nare larger than those of the CIFAR-10 dataset\nin :numref:`sec_kaggle_cifar10`. The following\nlists a few image augmentation operations\nthat might be useful for relatively larger images. ```{.python .input}\n#@tab mxnet\ntransform_train = gluon.data.vision.transforms.Compose([\n    # Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n    # the original area and height-to-width ratio between 3/4 and 4/3. Then,\n    # scale the image to create a new 224 x 224 image\n    gluon.data.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n                                                   ratio=(3.0/4.0, 4.0/3.0)),\n    gluon.data.vision.transforms.RandomFlipLeftRight(),\n    # Randomly change the brightness, contrast, and saturation\n    gluon.data.vision.transforms.RandomColorJitter(brightness=0.4,\n                                                   contrast=0.4,\n                                                   saturation=0.4),\n    # Add random noise\n    gluon.data.vision.transforms.RandomLighting(0.1),\n    gluon.data.vision.transforms.ToTensor(),\n    # Standardize each channel of the image\n    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],\n                                           [0.229, 0.224, 0.225])])\n```\n\n```{.python .input}\n#@tab pytorch\ntransform_train = torchvision.transforms.Compose([\n    # Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n    # the original area and height-to-width ratio between 3/4 and 4/3."
    },
    {
      "chunk_id": "e59e9486355a_1",
      "chapter": "kaggle-dog",
      "heading": "[**Image Augmentation**]",
      "text": "Then,\n    # scale the image to create a new 224 x 224 image\n    torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n                                             ratio=(3.0/4.0, 4.0/3.0)),\n    torchvision.transforms.RandomHorizontalFlip(),\n    # Randomly change the brightness, contrast, and saturation\n    torchvision.transforms.ColorJitter(brightness=0.4,\n                                       contrast=0.4,\n                                       saturation=0.4),\n    # Add random noise\n    torchvision.transforms.ToTensor(),\n    # Standardize each channel of the image\n    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n                                     [0.229, 0.224, 0.225])])\n```\n\nDuring prediction,\nwe only use image preprocessing operations\nwithout randomness. ```{.python .input}\n#@tab mxnet\ntransform_test = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.Resize(256),\n    # Crop a 224 x 224 square area from the center of the image\n    gluon.data.vision.transforms.CenterCrop(224),\n    gluon.data.vision.transforms.ToTensor(),\n    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],\n                                           [0.229, 0.224, 0.225])])\n```\n\n```{.python .input}\n#@tab pytorch\ntransform_test = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(256),\n    # Crop a 224 x 224 square area from the center of the image\n    torchvision.transforms.CenterCrop(224),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n                                     [0.229, 0.224, 0.225])])\n```"
    },
    {
      "chunk_id": "4fe4ddf9c8e0_0",
      "chapter": "kaggle-dog",
      "heading": "[**Reading the Dataset**]",
      "text": "As in :numref:`sec_kaggle_cifar10`,\nwe can read the organized dataset\nconsisting of raw image files.\n\n```{.python .input}\n#@tab mxnet\ntrain_ds, valid_ds, train_valid_ds, test_ds = [\n    gluon.data.vision.ImageFolderDataset(\n        os.path.join(data_dir, 'train_valid_test', folder))\n    for folder in ('train', 'valid', 'train_valid', 'test')]\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n    os.path.join(data_dir, 'train_valid_test', folder),\n    transform=transform_train) for folder in ['train', 'train_valid']]\n\nvalid_ds, test_ds = [torchvision.datasets.ImageFolder(\n    os.path.join(data_dir, 'train_valid_test', folder),\n    transform=transform_test) for folder in ['valid', 'test']]\n```\n\nBelow we create data iterator instances\nthe same way\nas in :numref:`sec_kaggle_cifar10`.\n\n```{.python .input}\n#@tab mxnet\ntrain_iter, train_valid_iter = [gluon.data.DataLoader(\n    dataset.transform_first(transform_train), batch_size, shuffle=True,\n    last_batch='discard') for dataset in (train_ds, train_valid_ds)]\n\nvalid_iter = gluon.data.DataLoader(\n    valid_ds.transform_first(transform_test), batch_size, shuffle=False,\n    last_batch='discard')\n\ntest_iter = gluon.data.DataLoader(\n    test_ds.transform_first(transform_test), batch_size, shuffle=False,\n    last_batch='keep')\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_iter, train_valid_iter = [torch.utils.data.DataLoader(\n    dataset, batch_size, shuffle=True, drop_last=True)\n    for dataset in (train_ds, train_valid_ds)]\n\nvalid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n                                         drop_last=True)\n\ntest_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n                                        drop_last=False)\n```"
    },
    {
      "chunk_id": "e1515063135f_0",
      "chapter": "kaggle-dog",
      "heading": "[**Fine-Tuning a Pretrained Model**]",
      "text": "Again,\nthe dataset for this competition is a subset of the ImageNet dataset. Therefore, we can use the approach discussed in\n:numref:`sec_fine_tuning`\nto select a model pretrained on the\nfull ImageNet dataset and use it to extract image features to be fed into a\ncustom small-scale output network. High-level APIs of deep learning frameworks\nprovide a wide range of models\npretrained on the ImageNet dataset. Here, we choose\na pretrained ResNet-34 model,\nwhere we simply reuse\nthe input of this model's output layer\n(i.e., the extracted\nfeatures). Then we can replace the original output layer with a small custom\noutput network that can be trained,\nsuch as stacking two\nfully connected layers. Different from the experiment in\n:numref:`sec_fine_tuning`,\nthe following does\nnot retrain the pretrained model used for feature\nextraction. This reduces training time and\nmemory for storing gradients. Recall that we\nstandardized images using\nthe means and standard deviations of the three RGB channels for the full ImageNet dataset. In fact,\nthis is also consistent with the standardization operation\nby the pretrained model on ImageNet."
    },
    {
      "chunk_id": "e1515063135f_1",
      "chapter": "kaggle-dog",
      "heading": "[**Fine-Tuning a Pretrained Model**]",
      "text": "Recall that we\nstandardized images using\nthe means and standard deviations of the three RGB channels for the full ImageNet dataset. In fact,\nthis is also consistent with the standardization operation\nby the pretrained model on ImageNet. ```{.python .input}\n#@tab mxnet\ndef get_net(devices):\n    finetune_net = gluon.model_zoo.vision.resnet34_v2(pretrained=True)\n    # Define a new output network\n    finetune_net.output_new = nn.HybridSequential(prefix='')\n    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n    # There are 120 output categories\n    finetune_net.output_new.add(nn.Dense(120))\n    # Initialize the output network\n    finetune_net.output_new.initialize(init.Xavier(), ctx=devices)\n    # Distribute the model parameters to the CPUs or GPUs used for computation\n    finetune_net.collect_params().reset_ctx(devices)\n    return finetune_net\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_net(devices):\n    finetune_net = nn.Sequential()\n    finetune_net.features = torchvision.models.resnet34(pretrained=True)\n    # Define a new output network (there are 120 output categories)\n    finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),\n                                            nn.ReLU(),\n                                            nn.Linear(256, 120))\n    # Move the model to devices\n    finetune_net = finetune_net.to(devices[0])\n    # Freeze parameters of feature layers\n    for param in finetune_net.features.parameters():\n        param.requires_grad = False\n    return finetune_net\n```\n\nBefore [**calculating the loss**],\nwe first obtain the input of the pretrained model's output layer, i.e., the extracted feature. Then we use this feature as input for our small custom output network to calculate the loss."
    },
    {
      "chunk_id": "e1515063135f_2",
      "chapter": "kaggle-dog",
      "heading": "[**Fine-Tuning a Pretrained Model**]",
      "text": "Then we use this feature as input for our small custom output network to calculate the loss. ```{.python .input}\n#@tab mxnet\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\n\ndef evaluate_loss(data_iter, net, devices):\n    l_sum, n = 0.0, 0\n    for features, labels in data_iter:\n        X_shards, y_shards = d2l.split_batch(features, labels, devices)\n        output_features = [net.features(X_shard) for X_shard in X_shards]\n        outputs = [net.output_new(feature) for feature in output_features]\n        ls = [loss(output, y_shard).sum() for output, y_shard\n              in zip(outputs, y_shards)]\n        l_sum += sum([float(l.sum()) for l in ls])\n        n += labels.size\n    return l_sum / n\n```\n\n```{.python .input}\n#@tab pytorch\nloss = nn.CrossEntropyLoss(reduction='none')\n\ndef evaluate_loss(data_iter, net, devices):\n    l_sum, n = 0.0, 0\n    for features, labels in data_iter:\n        features, labels = features.to(devices[0]), labels.to(devices[0])\n        outputs = net(features)\n        l = loss(outputs, labels)\n        l_sum += l.sum()\n        n += labels.numel()\n    return l_sum / n\n```"
    },
    {
      "chunk_id": "9192ea68f5c7_0",
      "chapter": "kaggle-dog",
      "heading": "Defining [**the Training Function**]",
      "text": "We will select the model and tune hyperparameters according to the model's performance on the validation set. The model training function `train` only\niterates parameters of the small custom output network."
    },
    {
      "chunk_id": "9192ea68f5c7_1",
      "chapter": "kaggle-dog",
      "heading": "Defining [**the Training Function**]",
      "text": "We will select the model and tune hyperparameters according to the model's performance on the validation set. The model training function `train` only\niterates parameters of the small custom output network. ```{.python .input}\n#@tab mxnet\ndef train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n          lr_decay):\n    # Only train the small custom output network\n    trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',\n                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n    num_batches, timer = len(train_iter), d2l.Timer()\n    legend = ['train loss']\n    if valid_iter is not None:\n        legend.append('valid loss')\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=legend)\n    for epoch in range(num_epochs):\n        metric = d2l.Accumulator(2)\n        if epoch > 0 and epoch % lr_period == 0:\n            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            X_shards, y_shards = d2l.split_batch(features, labels, devices)\n            output_features = [net.features(X_shard) for X_shard in X_shards]\n            with autograd.record():\n                outputs = [net.output_new(feature)\n                           for feature in output_features]\n                ls = [loss(output, y_shard).sum() for output, y_shard\n                      in zip(outputs, y_shards)]\n            for l in ls:\n                l.backward()\n            trainer.step(batch_size)\n            metric.add(sum([float(l.sum()) for l in ls]), labels.shape[0])\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[1], None))\n        if valid_iter is not None:\n            valid_loss = evaluate_loss(valid_iter, net, devices)\n            animator.add(epoch + 1, (None, valid_loss))\n    measures = f'train loss {metric[0] / metric[1]:.3f}'\n    if valid_iter is not None:\n        measures += f', valid loss {valid_loss:.3f}'\n    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n          f' examples/sec on {str(devices)}')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n          lr_decay):\n    # Only train the small custom output network\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    trainer = torch.optim.SGD((param for param in net.parameters()\n                               if param.requires_grad), lr=lr,\n                              momentum=0.9, weight_decay=wd)\n    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n    num_batches, timer = len(train_iter), d2l.Timer()\n    legend = ['train loss']\n    if valid_iter is not None:\n        legend.append('valid loss')\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                            legend=legend)\n    for epoch in range(num_epochs):\n        metric = d2l.Accumulator(2)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            features, labels = features.to(devices[0]), labels.to(devices[0])\n            trainer.zero_grad()\n            output = net(features)\n            l = loss(output, labels).sum()\n            l.backward()\n            trainer.step()\n            metric.add(l, labels.shape[0])\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[1], None))\n        measures = f'train loss {metric[0] / metric[1]:.3f}'\n        if valid_iter is not None:\n            valid_loss = evaluate_loss(valid_iter, net, devices)\n            animator.add(epoch + 1, (None, valid_loss.detach().cpu()))\n        scheduler.step()\n    if valid_iter is not None:\n        measures += f', valid loss {valid_loss:.3f}'\n    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n          f' examples/sec on {str(devices)}')\n```"
    },
    {
      "chunk_id": "28c18a384bd2_0",
      "chapter": "kaggle-dog",
      "heading": "[**Training and Validating the Model**]",
      "text": "Now we can train and validate the model.\nThe following hyperparameters are all tunable.\nFor example, the number of epochs can be increased. Because `lr_period` and `lr_decay` are set to 2 and 0.9, respectively, the learning rate of the optimization algorithm will be multiplied by 0.9 after every 2 epochs.\n\n```{.python .input}\n#@tab mxnet\ndevices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 5e-3, 1e-4\nlr_period, lr_decay, net = 2, 0.9, get_net(devices)\nnet.hybridize()\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n```\n\n```{.python .input}\n#@tab pytorch\ndevices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 1e-4, 1e-4\nlr_period, lr_decay, net = 2, 0.9, get_net(devices)\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n```"
    },
    {
      "chunk_id": "4a467d1920f7_0",
      "chapter": "kaggle-dog",
      "heading": "[**Classifying the Testing Set**] and Submitting Results on Kaggle",
      "text": "Similar to the final step in :numref:`sec_kaggle_cifar10`,\nin the end all the labeled data (including the validation set) are used for training the model and classifying the testing set.\nWe will use the trained custom output network\nfor classification.\n\n```{.python .input}\n#@tab mxnet\nnet = get_net(devices)\nnet.hybridize()\ntrain(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n\npreds = []\nfor data, label in test_iter:\n    output_features = net.features(data.as_in_ctx(devices[0]))\n    output = npx.softmax(net.output_new(output_features))\n    preds.extend(output.asnumpy())\nids = sorted(os.listdir(\n    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\nwith open('submission.csv', 'w') as f:\n    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n    for i, output in zip(ids, preds):\n        f.write(i.split('.')[0] + ',' + ','.join(\n            [str(num) for num in output]) + '\\n')\n```\n\n```{.python .input}\n#@tab pytorch\nnet = get_net(devices)\ntrain(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n      lr_decay)\n\npreds = []\nfor data, label in test_iter:\n    output = torch.nn.functional.softmax(net(data.to(devices[0])), dim=1)\n    preds.extend(output.cpu().detach().numpy())\nids = sorted(os.listdir(\n    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\nwith open('submission.csv', 'w') as f:\n    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n    for i, output in zip(ids, preds):\n        f.write(i.split('.')[0] + ',' + ','.join(\n            [str(num) for num in output]) + '\\n')\n```\n\nThe above code\nwill generate a `submission.csv` file\nto be submitted\nto Kaggle in the same way described in :numref:`sec_kaggle_house`."
    },
    {
      "chunk_id": "ca9b5fb6f765_0",
      "chapter": "kaggle-dog",
      "heading": "Summary",
      "text": "* Images in the ImageNet dataset are larger (with varying dimensions) than CIFAR-10 images. We may modify image augmentation operations for tasks on a different dataset.\n* To classify a subset of the ImageNet dataset, we can leverage pre-trained models on the full ImageNet dataset to extract features and only train a custom small-scale output network. This will lead to less computational time and memory cost."
    },
    {
      "chunk_id": "e963f2650604_0",
      "chapter": "kaggle-dog",
      "heading": "Exercises",
      "text": "1. When using the full Kaggle competition dataset, what results can you achieve when you increase `batch_size` (batch size) and `num_epochs` (number of epochs) while setting some other hyperparameters as `lr = 0.01`, `lr_period = 10`, and `lr_decay = 0.1`?\n1. Do you get better results if you use a deeper pretrained model? How do you tune hyperparameters? Can you further improve the results?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/380)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1481)\n:end_tab:"
    },
    {
      "chunk_id": "c271f70fac5f_0",
      "chapter": "multiscale-object-detection",
      "heading": "multiscale-object-detection",
      "text": "# Multiscale Object Detection\n:label:`sec_multiscale-object-detection`\n\n\nIn :numref:`sec_anchor`,\nwe generated multiple anchor boxes centered on each pixel of an input image. \nEssentially these anchor boxes \nrepresent samples of\ndifferent regions of the image.\nHowever, \nwe may end up with too many anchor boxes to compute\nif they are generated for *every* pixel.\nThink of a $561 \\times 728$ input image.\nIf five anchor boxes \nwith varying shapes\nare generated for each pixel as their center,\nover two million anchor boxes ($561 \\times 728 \\times 5$) need to be labeled and predicted on the image."
    },
    {
      "chunk_id": "252e593a5f07_0",
      "chapter": "multiscale-object-detection",
      "heading": "Multiscale Anchor Boxes",
      "text": ":label:`subsec_multiscale-anchor-boxes`\n\nYou may realize that\nit is not difficult to reduce anchor boxes on an image. For instance, we can just \nuniformly sample a small portion of pixels\nfrom the input image\nto generate anchor boxes centered on them. In addition, \nat different scales\nwe can generate different numbers of anchor boxes\nof different sizes. Intuitively,\nsmaller objects are more likely\nto appear on an image than larger ones. As an example,\n$1 \\times 1$, $1 \\times 2$, and $2 \\times 2$ objects \ncan appear on a $2 \\times 2$ image\nin 4, 2, and 1 possible ways, respectively. Therefore, when using smaller anchor boxes to detect smaller objects, we can sample more regions,\nwhile for larger objects we can sample fewer regions. To demonstrate how to generate anchor boxes\nat multiple scales, let's read an image. Its height and width are 561 and 728 pixels, respectively. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import image, np, npx\n\nnpx.set_np()\n\nimg = image.imread('../img/catdog.jpg')\nh, w = img.shape[:2]\nh, w\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n\nimg = d2l.plt.imread('../img/catdog.jpg')\nh, w = img.shape[:2]\nh, w\n```\n\nRecall that in :numref:`sec_conv_layer`\nwe call a two-dimensional array output of \na convolutional layer a feature map. By defining the feature map shape,\nwe can determine centers of uniformly sampled anchor boxes  on any image. The `display_anchors` function is defined below. [**We generate anchor boxes (`anchors`) on the feature map (`fmap`) with each unit (pixel) as the anchor box center.**]\nSince the $(x, y)$-axis coordinate values\nin the anchor boxes (`anchors`) have been divided by the width and height of the feature map (`fmap`),\nthese values are between 0 and 1,\nwhich indicate the relative positions of\nanchor boxes in the feature map."
    },
    {
      "chunk_id": "252e593a5f07_1",
      "chapter": "multiscale-object-detection",
      "heading": "Multiscale Anchor Boxes",
      "text": "Since centers of the anchor boxes (`anchors`)\nare spread over all units on the feature map (`fmap`),\nthese centers must be *uniformly* distributed\non any input image\nin terms of their relative spatial positions. More concretely,\ngiven the width and height of the feature map `fmap_w` and `fmap_h`, respectively,\nthe following function will *uniformly* sample\npixels in `fmap_h` rows and `fmap_w` columns\non any input image. Centered on these uniformly sampled pixels,\nanchor boxes of scale `s` (assuming the length of the list `s` is 1) and different aspect ratios (`ratios`)\nwill be generated. ```{.python .input}\n#@tab mxnet\ndef display_anchors(fmap_w, fmap_h, s):\n    d2l.set_figsize()\n    # Values on the first two dimensions do not affect the output\n    fmap = np.zeros((1, 10, fmap_h, fmap_w))\n    anchors = npx.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\n    bbox_scale = np.array((w, h, w, h))\n    d2l.show_bboxes(d2l.plt.imshow(img.asnumpy()).axes,\n                    anchors[0] * bbox_scale)\n```\n\n```{.python .input}\n#@tab pytorch\ndef display_anchors(fmap_w, fmap_h, s):\n    d2l.set_figsize()\n    # Values on the first two dimensions do not affect the output\n    fmap = d2l.zeros((1, 10, fmap_h, fmap_w))\n    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\n    bbox_scale = d2l.tensor((w, h, w, h))\n    d2l.show_bboxes(d2l.plt.imshow(img).axes,\n                    anchors[0] * bbox_scale)\n```\n\nFirst, let's [**consider\ndetection of small objects**]. In order to make it easier to distinguish when displayed, the anchor boxes with different centers here do not overlap:\nthe anchor box scale is set to 0.15\nand the height and width of the feature map are set to 4. We can see\nthat the centers of the anchor boxes in 4 rows and 4 columns on the image are uniformly distributed. ```{.python .input}\n#@tab all\ndisplay_anchors(fmap_w=4, fmap_h=4, s=[0.15])\n```\n\nWe move on to [**reduce the height and width of the feature map by half and use larger anchor boxes to detect larger objects**]."
    },
    {
      "chunk_id": "252e593a5f07_2",
      "chapter": "multiscale-object-detection",
      "heading": "Multiscale Anchor Boxes",
      "text": "```{.python .input}\n#@tab all\ndisplay_anchors(fmap_w=4, fmap_h=4, s=[0.15])\n```\n\nWe move on to [**reduce the height and width of the feature map by half and use larger anchor boxes to detect larger objects**]. When the scale is set to 0.4, \nsome anchor boxes will overlap with each other. ```{.python .input}\n#@tab all\ndisplay_anchors(fmap_w=2, fmap_h=2, s=[0.4])\n```\n\nFinally, we [**further reduce the height and width of the feature map by half and increase the anchor box scale to 0.8**]. Now the center of the anchor box is the center of the image. ```{.python .input}\n#@tab all\ndisplay_anchors(fmap_w=1, fmap_h=1, s=[0.8])\n```"
    },
    {
      "chunk_id": "12ff717fd78c_0",
      "chapter": "multiscale-object-detection",
      "heading": "Multiscale Detection",
      "text": "Since we have generated multiscale anchor boxes,\nwe will use them to detect objects of various sizes\nat different scales. In the following\nwe introduce a CNN-based multiscale object detection\nmethod that we will implement\nin :numref:`sec_ssd`. At some scale,\nsay that we have $c$ feature maps of shape $h \\times w$. Using the method in :numref:`subsec_multiscale-anchor-boxes`,\nwe generate $hw$ sets of anchor boxes,\nwhere each set has $a$ anchor boxes with the same center. For example, \nat the first scale in the experiments in :numref:`subsec_multiscale-anchor-boxes`,\ngiven ten (number of channels) $4 \\times 4$ feature maps,\nwe generated 16 sets of anchor boxes,\nwhere each set contains 3 anchor boxes with the same center. Next, each anchor box is labeled with\nthe class and offset based on ground-truth bounding boxes. At the current scale, the object detection model needs to predict the classes and offsets of $hw$ sets of anchor boxes on the input image, where different sets have different centers. Assume that the $c$ feature maps here\nare the intermediate outputs obtained\nby the CNN forward propagation based on the input image. Since there are $hw$ different spatial positions on each feature map,\nthe same spatial position can be \nthought of as having $c$ units. According to the\ndefinition of receptive field in :numref:`sec_conv_layer`,\nthese $c$ units at the same spatial position\nof the feature maps\nhave the same receptive field on the input image:\nthey represent the input image information\nin the same receptive field. Therefore, we can transform the $c$ units\nof the feature maps at the same spatial position\ninto the\nclasses and offsets of the $a$ anchor boxes\ngenerated using this spatial position. In essence,\nwe use the information of the input image in a certain receptive field\nto predict the classes and offsets of the anchor boxes\nthat are\nclose to that receptive field\non the input image."
    },
    {
      "chunk_id": "12ff717fd78c_1",
      "chapter": "multiscale-object-detection",
      "heading": "Multiscale Detection",
      "text": "In essence,\nwe use the information of the input image in a certain receptive field\nto predict the classes and offsets of the anchor boxes\nthat are\nclose to that receptive field\non the input image. When the feature maps at different layers\nhave varying-size receptive fields on the input image, they can be used to detect objects of different sizes. For example, we can design a neural network where\nunits of feature maps that are closer to the output layer\nhave wider receptive fields,\nso they can detect larger objects from the input image. In a nutshell, we can leverage\nlayerwise representations of images at multiple levels\nby deep neural networks\nfor multiscale object detection. We will show how this works through a concrete example\nin :numref:`sec_ssd`."
    },
    {
      "chunk_id": "e3d92f1a01e8_0",
      "chapter": "multiscale-object-detection",
      "heading": "Summary",
      "text": "* At multiple scales, we can generate anchor boxes with different sizes to detect objects with different sizes.\n* By defining the shape of feature maps, we can determine centers of uniformly sampled anchor boxes on any image.\n* We use the information of the input image in a certain receptive field to predict the classes and offsets of the anchor boxes that are close to that receptive field on the input image.\n* Through deep learning, we can leverage its layerwise representations of images at multiple levels for multiscale object detection."
    },
    {
      "chunk_id": "dffcf97f09d2_0",
      "chapter": "multiscale-object-detection",
      "heading": "Exercises",
      "text": "1. According to our discussions in :numref:`sec_alexnet`, deep neural networks learn hierarchical features with increasing levels of abstraction for images. In multiscale object detection, do feature maps at different scales correspond to different levels of abstraction? Why or why not?\n1. At the first scale (`fmap_w=4, fmap_h=4`) in the experiments in :numref:`subsec_multiscale-anchor-boxes`, generate uniformly distributed anchor boxes that may overlap.\n1. Given a feature map variable with shape $1 \\times c \\times h \\times w$, where $c$, $h$, and $w$ are the number of channels, height, and width of the feature maps, respectively. How can you transform this variable into the classes and offsets of anchor boxes? What is the shape of the output?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/371)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1607)\n:end_tab:"
    },
    {
      "chunk_id": "cee1b6e8a8db_0",
      "chapter": "neural-style",
      "heading": "neural-style",
      "text": "# Neural Style Transfer\n\nIf you are a photography enthusiast,\nyou may be familiar with the filter.\nIt can change the color style of photos\nso that landscape photos become sharper\nor portrait photos have whitened skins.\nHowever,\none filter usually only changes\none aspect of the photo.\nTo apply an ideal style\nto a photo,\nyou probably need to\ntry many different filter combinations.\nThis process is\nas complex as tuning the hyperparameters of a model.\n\n\n\nIn this section, we will\nleverage layerwise representations of a CNN\nto automatically apply the style of one image\nto another image, i.e., *style transfer* :cite:`Gatys.Ecker.Bethge.2016`.\nThis task needs two input images:\none is the *content image* and\nthe other is the *style image*.\nWe will use neural networks\nto modify the content image\nto make it close to the style image in style.\nFor example,\nthe content image in :numref:`fig_style_transfer` is a landscape photo taken by us\nin Mount Rainier National Park in the suburbs of Seattle, while the style image is an oil painting\nwith the theme of autumn oak trees.\nIn the output synthesized image,\nthe oil brush strokes of the style image\nare applied, leading to more vivid colors,\nwhile preserving the main shape of the objects\nin the content image.\n\n![Given content and style images, style transfer outputs a synthesized image.](../img/style-transfer.svg)\n:label:`fig_style_transfer`"
    },
    {
      "chunk_id": "77e25fbe1aa1_0",
      "chapter": "neural-style",
      "heading": "Method",
      "text": ":numref:`fig_style_transfer_model` illustrates\nthe CNN-based style transfer method with a simplified example.\nFirst, we initialize the synthesized image,\nfor example, into the content image.\nThis synthesized image is the only variable that needs to be updated during the style transfer process,\ni.e., the model parameters to be updated during training.\nThen we choose a pretrained CNN\nto extract image features and freeze its\nmodel parameters during training.\nThis deep CNN uses multiple layers\nto extract\nhierarchical features for images.\nWe can choose the output of some of these layers as content features or style features.\nTake :numref:`fig_style_transfer_model` as an example.\nThe pretrained neural network here has 3 convolutional layers,\nwhere the second layer outputs the content features,\nand the first and third layers output the style features.\n\n![CNN-based style transfer process. Solid lines show the direction of forward propagation and dotted lines show backward propagation. ](../img/neural-style.svg)\n:label:`fig_style_transfer_model`\n\nNext, we calculate the loss function of style transfer through forward propagation (direction of solid arrows), and update the model parameters (the synthesized image for output) through backpropagation (direction of dashed arrows).\nThe loss function commonly used in style transfer consists of three parts:\n(i) *content loss* makes the synthesized image and the content image close in content features;\n(ii) *style loss* makes the synthesized image and style image close in style features;\nand (iii) *total variation loss* helps to reduce the noise in the synthesized image.\nFinally, when the model training is over, we output the model parameters of the style transfer to generate\nthe final synthesized image.\n\n\n\nIn the following,\nwe will explain the technical details of style transfer via a concrete experiment."
    },
    {
      "chunk_id": "4294f19c8f38_0",
      "chapter": "neural-style",
      "heading": "[**Reading the Content and Style Images**]",
      "text": "First, we read the content and style images.\nFrom their printed coordinate axes,\nwe can tell that these images have different sizes.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, image, init, np, npx\nfrom mxnet.gluon import nn\n\nnpx.set_np()\n\nd2l.set_figsize()\ncontent_img = image.imread('../img/rainier.jpg')\nd2l.plt.imshow(content_img.asnumpy());\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nfrom torch import nn\n\nd2l.set_figsize()\ncontent_img = d2l.Image.open('../img/rainier.jpg')\nd2l.plt.imshow(content_img);\n```\n\n```{.python .input}\n#@tab mxnet\nstyle_img = image.imread('../img/autumn-oak.jpg')\nd2l.plt.imshow(style_img.asnumpy());\n```\n\n```{.python .input}\n#@tab pytorch\nstyle_img = d2l.Image.open('../img/autumn-oak.jpg')\nd2l.plt.imshow(style_img);\n```"
    },
    {
      "chunk_id": "91d945c3d62f_0",
      "chapter": "neural-style",
      "heading": "[**Preprocessing and Postprocessing**]",
      "text": "Below, we define two functions for preprocessing and postprocessing images.\nThe `preprocess` function standardizes\neach of the three RGB channels of the input image and transforms the results into the CNN input format.\nThe `postprocess` function restores the pixel values in the output image to their original values before standardization.\nSince the image printing function requires that each pixel has a floating point value from 0 to 1,\nwe replace any value smaller than 0 or greater than 1 with 0 or 1, respectively.\n\n```{.python .input}\n#@tab mxnet\nrgb_mean = np.array([0.485, 0.456, 0.406])\nrgb_std = np.array([0.229, 0.224, 0.225])\n\ndef preprocess(img, image_shape):\n    img = image.imresize(img, *image_shape)\n    img = (img.astype('float32') / 255 - rgb_mean) / rgb_std\n    return np.expand_dims(img.transpose(2, 0, 1), axis=0)\n\ndef postprocess(img):\n    img = img[0].as_in_ctx(rgb_std.ctx)\n    return (img.transpose(1, 2, 0) * rgb_std + rgb_mean).clip(0, 1)\n```\n\n```{.python .input}\n#@tab pytorch\nrgb_mean = torch.tensor([0.485, 0.456, 0.406])\nrgb_std = torch.tensor([0.229, 0.224, 0.225])\n\ndef preprocess(img, image_shape):\n    transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize(image_shape),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n    return transforms(img).unsqueeze(0)\n\ndef postprocess(img):\n    img = img[0].to(rgb_std.device)\n    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))\n```"
    },
    {
      "chunk_id": "809f1d75746d_0",
      "chapter": "neural-style",
      "heading": "[**Extracting Features**]",
      "text": "We use the VGG-19 model pretrained on the ImageNet dataset to extract image features :cite:`Gatys.Ecker.Bethge.2016`. ```{.python .input}\n#@tab mxnet\npretrained_net = gluon.model_zoo.vision.vgg19(pretrained=True)\n```\n\n```{.python .input}\n#@tab pytorch\npretrained_net = torchvision.models.vgg19(pretrained=True)\n```\n\nIn order to extract the content features and style features of the image, we can select the output of certain layers in the VGG network. Generally speaking, the closer to the input layer, the easier to extract details of the image, and vice versa, the easier to extract the global information of the image. In order to avoid excessively\nretaining the details of the content image in the synthesized image,\nwe choose a VGG layer that is closer to the output as the *content layer* to output the content features of the image. We also select the output of different VGG layers for extracting local and global style features. These layers are also called *style layers*. As mentioned in :numref:`sec_vgg`,\nthe VGG network uses 5 convolutional blocks. In the experiment, we choose the last convolutional layer of the fourth convolutional block as the content layer, and the first convolutional layer of each convolutional block as the style layer. The indices of these layers can be obtained by printing the `pretrained_net` instance. ```{.python .input}\n#@tab all\nstyle_layers, content_layers = [0, 5, 10, 19, 28], [25]\n```\n\nWhen extracting features using VGG layers,\nwe only need to use all those\nfrom the input layer to the content layer or style layer that is closest to the output layer. Let's construct a new network instance `net`, which only retains all the VGG layers to be\nused for feature extraction."
    },
    {
      "chunk_id": "809f1d75746d_1",
      "chapter": "neural-style",
      "heading": "[**Extracting Features**]",
      "text": "Let's construct a new network instance `net`, which only retains all the VGG layers to be\nused for feature extraction. ```{.python .input}\n#@tab mxnet\nnet = nn.Sequential()\nfor i in range(max(content_layers + style_layers) + 1):\n    net.add(pretrained_net.features[i])\n```\n\n```{.python .input}\n#@tab pytorch\nnet = nn.Sequential(*[pretrained_net.features[i] for i in\n                      range(max(content_layers + style_layers) + 1)])\n```\n\nGiven the input `X`, if we simply invoke\nthe forward propagation `net(X)`, we can only get the output of the last layer. Since we also need the outputs of intermediate layers,\nwe need to perform layer-by-layer computation and keep\nthe content and style layer outputs. ```{.python .input}\n#@tab all\ndef extract_features(X, content_layers, style_layers):\n    contents = []\n    styles = []\n    for i in range(len(net)):\n        X = net[i](X)\n        if i in style_layers:\n            styles.append(X)\n        if i in content_layers:\n            contents.append(X)\n    return contents, styles\n```\n\nTwo functions are defined below:\nthe `get_contents` function extracts content features from the content image,\nand the `get_styles` function extracts style features from the style image. Since there is no need to update the model parameters of the pretrained VGG during training,\nwe can extract the content and the style features\neven before the training starts. Since the synthesized image\nis a set of model parameters to be updated\nfor style transfer,\nwe can only extract the content and style features of the synthesized image by calling the `extract_features` function during training."
    },
    {
      "chunk_id": "809f1d75746d_2",
      "chapter": "neural-style",
      "heading": "[**Extracting Features**]",
      "text": "Since the synthesized image\nis a set of model parameters to be updated\nfor style transfer,\nwe can only extract the content and style features of the synthesized image by calling the `extract_features` function during training. ```{.python .input}\n#@tab mxnet\ndef get_contents(image_shape, device):\n    content_X = preprocess(content_img, image_shape).copyto(device)\n    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n    return content_X, contents_Y\n\ndef get_styles(image_shape, device):\n    style_X = preprocess(style_img, image_shape).copyto(device)\n    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n    return style_X, styles_Y\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_contents(image_shape, device):\n    content_X = preprocess(content_img, image_shape).to(device)\n    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n    return content_X, contents_Y\n\ndef get_styles(image_shape, device):\n    style_X = preprocess(style_img, image_shape).to(device)\n    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n    return style_X, styles_Y\n```"
    },
    {
      "chunk_id": "9b2a717c4b31_0",
      "chapter": "neural-style",
      "heading": "[**Defining the Loss Function**]",
      "text": "Now we will describe the loss function for style transfer. The loss function consists of\nthe content loss, style loss, and total variation loss."
    },
    {
      "chunk_id": "c3b0b969ce72_0",
      "chapter": "neural-style",
      "heading": "Content Loss",
      "text": "Similar to the loss function in linear regression,\nthe content loss measures the difference\nin content features\nbetween the synthesized image and the content image via\nthe squared loss function.\nThe two inputs of the squared loss function\nare both\noutputs of the content layer computed by the `extract_features` function.\n\n```{.python .input}\n#@tab mxnet\ndef content_loss(Y_hat, Y):\n    return np.square(Y_hat - Y).mean()\n```\n\n```{.python .input}\n#@tab pytorch\ndef content_loss(Y_hat, Y):\n    # We detach the target content from the tree used to dynamically compute\n    # the gradient: this is a stated value, not a variable. Otherwise the loss\n    # will throw an error.\n    return torch.square(Y_hat - Y.detach()).mean()\n```"
    },
    {
      "chunk_id": "383377f160fe_0",
      "chapter": "neural-style",
      "heading": "Style Loss",
      "text": "Style loss, similar to content loss,\nalso uses the squared loss function to measure the difference in style between the synthesized image and the style image. To express the style output of any style layer,\nwe first use the `extract_features` function to\ncompute the style layer output. Suppose that the output has\n1 example, $c$ channels,\nheight $h$, and width $w$,\nwe can transform this output into\nmatrix $\\mathbf{X}$ with $c$ rows and $hw$ columns. This matrix can be thought of as\nthe concatenation of\n$c$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_c$,\neach of which has a length of $hw$. Here, vector $\\mathbf{x}_i$ represents the style feature of channel $i$. In the *Gram matrix* of these vectors $\\mathbf{X}\\mathbf{X}^\\top \\in \\mathbb{R}^{c \\times c}$, element $x_{ij}$ in row $i$ and column $j$ is the dot product of vectors $\\mathbf{x}_i$ and $\\mathbf{x}_j$. It represents the correlation of the style features of channels $i$ and $j$. We use this Gram matrix to represent the style output of any style layer. Note that when the value of $hw$ is larger,\nit likely leads to larger values in the Gram matrix. Note also that the height and width of the Gram matrix are both the number of channels $c$. To allow style loss not to be affected\nby these values,\nthe `gram` function below divides\nthe Gram matrix by the number of its elements, i.e., $chw$. ```{.python .input}\n#@tab all\ndef gram(X):\n    num_channels, n = X.shape[1], d2l.size(X) // X.shape[1]\n    X = d2l.reshape(X, (num_channels, n))\n    return d2l.matmul(X, X.T) / (num_channels * n)\n```\n\nObviously,\nthe two Gram matrix inputs of the squared loss function for style loss are based on\nthe style layer outputs for\nthe synthesized image and the style image. It is assumed here that the Gram matrix `gram_Y` based on the style image has been precomputed."
    },
    {
      "chunk_id": "383377f160fe_1",
      "chapter": "neural-style",
      "heading": "Style Loss",
      "text": "It is assumed here that the Gram matrix `gram_Y` based on the style image has been precomputed. ```{.python .input}\n#@tab mxnet\ndef style_loss(Y_hat, gram_Y):\n    return np.square(gram(Y_hat) - gram_Y).mean()\n```\n\n```{.python .input}\n#@tab pytorch\ndef style_loss(Y_hat, gram_Y):\n    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()\n```"
    },
    {
      "chunk_id": "6337d9123a3f_0",
      "chapter": "neural-style",
      "heading": "Total Variation Loss",
      "text": "Sometimes, the learned synthesized image\nhas a lot of high-frequency noise,\ni.e., particularly bright or dark pixels.\nOne common noise reduction method is\n*total variation denoising*.\nDenote by $x_{i, j}$ the pixel value at coordinate $(i, j)$.\nReducing total variation loss\n\n$$\\sum_{i, j} \\left|x_{i, j} - x_{i+1, j}\\right| + \\left|x_{i, j} - x_{i, j+1}\\right|$$\n\nmakes values of neighboring pixels on the synthesized image closer.\n\n```{.python .input}\n#@tab all\ndef tv_loss(Y_hat):\n    return 0.5 * (d2l.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n                  d2l.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())\n```"
    },
    {
      "chunk_id": "c5524ce844ad_0",
      "chapter": "neural-style",
      "heading": "Loss Function",
      "text": "[**The loss function of style transfer is the weighted sum of content loss, style loss, and total variation loss**].\nBy adjusting these weight hyperparameters,\nwe can balance among\ncontent retention,\nstyle transfer,\nand noise reduction on the synthesized image.\n\n```{.python .input}\n#@tab all\ncontent_weight, style_weight, tv_weight = 1, 1e4, 10\n\ndef compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n    # Calculate the content, style, and total variance losses respectively\n    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n        contents_Y_hat, contents_Y)]\n    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n        styles_Y_hat, styles_Y_gram)]\n    tv_l = tv_loss(X) * tv_weight\n    # Add up all the losses\n    l = sum(styles_l + contents_l + [tv_l])\n    return contents_l, styles_l, tv_l, l\n```"
    },
    {
      "chunk_id": "5650644a7eeb_0",
      "chapter": "neural-style",
      "heading": "[**Initializing the Synthesized Image**]",
      "text": "In style transfer,\nthe synthesized image is the only variable that needs to be updated during training.\nThus, we can define a simple model, `SynthesizedImage`, and treat the synthesized image as the model parameters.\nIn this model, forward propagation just returns the model parameters.\n\n```{.python .input}\n#@tab mxnet\nclass SynthesizedImage(nn.Block):\n    def __init__(self, img_shape, **kwargs):\n        super(SynthesizedImage, self).__init__(**kwargs)\n        self.weight = self.params.get('weight', shape=img_shape)\n\n    def forward(self):\n        return self.weight.data()\n```\n\n```{.python .input}\n#@tab pytorch\nclass SynthesizedImage(nn.Module):\n    def __init__(self, img_shape, **kwargs):\n        super(SynthesizedImage, self).__init__(**kwargs)\n        self.weight = nn.Parameter(torch.rand(*img_shape))\n\n    def forward(self):\n        return self.weight\n```\n\nNext, we define the `get_inits` function.\nThis function creates a synthesized image model instance and initializes it to the image `X`.\nGram matrices for the style image at various style layers, `styles_Y_gram`, are computed prior to training.\n\n```{.python .input}\n#@tab mxnet\ndef get_inits(X, device, lr, styles_Y):\n    gen_img = SynthesizedImage(X.shape)\n    gen_img.initialize(init.Constant(X), ctx=device, force_reinit=True)\n    trainer = gluon.Trainer(gen_img.collect_params(), 'adam',\n                            {'learning_rate': lr})\n    styles_Y_gram = [gram(Y) for Y in styles_Y]\n    return gen_img(), styles_Y_gram, trainer\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_inits(X, device, lr, styles_Y):\n    gen_img = SynthesizedImage(X.shape).to(device)\n    gen_img.weight.data.copy_(X.data)\n    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n    styles_Y_gram = [gram(Y) for Y in styles_Y]\n    return gen_img(), styles_Y_gram, trainer\n```"
    },
    {
      "chunk_id": "3c50dab657f8_0",
      "chapter": "neural-style",
      "heading": "[**Training**]",
      "text": "When training the model for style transfer,\nwe continuously extract\ncontent features and style features of the synthesized image, and calculate the loss function. Below defines the training loop."
    },
    {
      "chunk_id": "3c50dab657f8_1",
      "chapter": "neural-style",
      "heading": "[**Training**]",
      "text": "When training the model for style transfer,\nwe continuously extract\ncontent features and style features of the synthesized image, and calculate the loss function. Below defines the training loop. ```{.python .input}\n#@tab mxnet\ndef train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[10, num_epochs], ylim=[0, 20],\n                            legend=['content', 'style', 'TV'],\n                            ncols=2, figsize=(7, 2.5))\n    for epoch in range(num_epochs):\n        with autograd.record():\n            contents_Y_hat, styles_Y_hat = extract_features(\n                X, content_layers, style_layers)\n            contents_l, styles_l, tv_l, l = compute_loss(\n                X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n        l.backward()\n        trainer.step(1)\n        if (epoch + 1) % lr_decay_epoch == 0:\n            trainer.set_learning_rate(trainer.learning_rate * 0.8)\n        if (epoch + 1) % 10 == 0:\n            animator.axes[1].imshow(postprocess(X).asnumpy())\n            animator.add(epoch + 1, [float(sum(contents_l)),\n                                     float(sum(styles_l)), float(tv_l)])\n    return X\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[10, num_epochs],\n                            legend=['content', 'style', 'TV'],\n                            ncols=2, figsize=(7, 2.5))\n    for epoch in range(num_epochs):\n        trainer.zero_grad()\n        contents_Y_hat, styles_Y_hat = extract_features(\n            X, content_layers, style_layers)\n        contents_l, styles_l, tv_l, l = compute_loss(\n            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n        l.backward()\n        trainer.step()\n        scheduler.step()\n        if (epoch + 1) % 10 == 0:\n            animator.axes[1].imshow(postprocess(X))\n            animator.add(epoch + 1, [float(sum(contents_l)),\n                                     float(sum(styles_l)), float(tv_l)])\n    return X\n```\n\nNow we [**start to train the model**]."
    },
    {
      "chunk_id": "3c50dab657f8_2",
      "chapter": "neural-style",
      "heading": "[**Training**]",
      "text": "We rescale the height and width of the content and style images to 300 by 450 pixels. We use the content image to initialize the synthesized image. ```{.python .input}\n#@tab mxnet\ndevice, image_shape = d2l.try_gpu(), (450, 300)\nnet.collect_params().reset_ctx(device)\ncontent_X, contents_Y = get_contents(image_shape, device)\n_, styles_Y = get_styles(image_shape, device)\noutput = train(content_X, contents_Y, styles_Y, device, 0.9, 500, 50)\n```\n\n```{.python .input}\n#@tab pytorch\ndevice, image_shape = d2l.try_gpu(), (300, 450)  # PIL Image (h, w)\nnet = net.to(device)\ncontent_X, contents_Y = get_contents(image_shape, device)\n_, styles_Y = get_styles(image_shape, device)\noutput = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)\n```\n\nWe can see that the synthesized image\nretains the scenery and objects of the content image,\nand transfers the color of the style image\nat the same time. For example,\nthe synthesized image has blocks of color like\nthose in the style image. Some of these blocks even have the subtle texture of brush strokes."
    },
    {
      "chunk_id": "074f83dbd5ff_0",
      "chapter": "neural-style",
      "heading": "Summary",
      "text": "* The loss function commonly used in style transfer consists of three parts: (i) content loss makes the synthesized image and the content image close in content features; (ii) style loss makes the synthesized image and style image close in style features; and (iii) total variation loss helps to reduce the noise in the synthesized image.\n* We can use a pretrained CNN to extract image features and minimize the loss function to continuously update the synthesized image as model parameters during training.\n* We use Gram matrices to represent the style outputs from the style layers."
    },
    {
      "chunk_id": "7c30938acf43_0",
      "chapter": "neural-style",
      "heading": "Exercises",
      "text": "1. How does the output change when you select different content and style layers?\n1. Adjust the weight hyperparameters in the loss function. Does the output retain more content or have less noise?\n1. Use different content and style images. Can you create more interesting synthesized images?\n1. Can we apply style transfer for text? Hint: you may refer to the survey paper by :citet:`10.1145/3544903.3544906`.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/378)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1476)\n:end_tab:"
    },
    {
      "chunk_id": "993adb1de1e7_0",
      "chapter": "object-detection-dataset",
      "heading": "object-detection-dataset",
      "text": "# The Object Detection Dataset\n:label:`sec_object-detection-dataset`\n\nThere is no small dataset such as MNIST and Fashion-MNIST in the field of object detection.\nIn order to quickly demonstrate object detection models,\n[**we collected and labeled a small dataset**].\nFirst, we took photos of free bananas from our office\nand generated\n1000 banana images with different rotations and sizes.\nThen we placed each banana image\nat a random position on some background image.\nIn the end, we labeled bounding boxes for those bananas on the images."
    },
    {
      "chunk_id": "0ee8417315fa_0",
      "chapter": "object-detection-dataset",
      "heading": "[**Downloading the Dataset**]",
      "text": "The banana detection dataset with all the image and\ncsv label files can be downloaded directly from the Internet.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, image, np, npx\nimport os\nimport pandas as pd\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nimport os\nimport pandas as pd\n```\n\n```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['banana-detection'] = (\n    d2l.DATA_URL + 'banana-detection.zip',\n    '5de26c8fce5ccdea9f91267273464dc968d20d72')\n```"
    },
    {
      "chunk_id": "2e8c6dd0922a_0",
      "chapter": "object-detection-dataset",
      "heading": "Reading the Dataset",
      "text": "We are going to [**read the banana detection dataset**] in the `read_data_bananas`\nfunction below. The dataset includes a csv file for\nobject class labels and\nground-truth bounding box coordinates\nat the upper-left and lower-right corners."
    },
    {
      "chunk_id": "2e8c6dd0922a_1",
      "chapter": "object-detection-dataset",
      "heading": "Reading the Dataset",
      "text": "We are going to [**read the banana detection dataset**] in the `read_data_bananas`\nfunction below. The dataset includes a csv file for\nobject class labels and\nground-truth bounding box coordinates\nat the upper-left and lower-right corners. ```{.python .input}\n#@tab mxnet\n#@save\ndef read_data_bananas(is_train=True):\n    \"\"\"Read the banana detection dataset images and labels.\"\"\"\n    data_dir = d2l.download_extract('banana-detection')\n    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train\n                             else 'bananas_val', 'label.csv')\n    csv_data = pd.read_csv(csv_fname)\n    csv_data = csv_data.set_index('img_name')\n    images, targets = [], []\n    for img_name, target in csv_data.iterrows():\n        images.append(image.imread(\n            os.path.join(data_dir, 'bananas_train' if is_train else\n                         'bananas_val', 'images', f'{img_name}')))\n        # Here `target` contains (class, upper-left x, upper-left y,\n        # lower-right x, lower-right y), where all the images have the same\n        # banana class (index 0)\n        targets.append(list(target))\n    return images, np.expand_dims(np.array(targets), 1) / 256\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef read_data_bananas(is_train=True):\n    \"\"\"Read the banana detection dataset images and labels.\"\"\"\n    data_dir = d2l.download_extract('banana-detection')\n    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train\n                             else 'bananas_val', 'label.csv')\n    csv_data = pd.read_csv(csv_fname)\n    csv_data = csv_data.set_index('img_name')\n    images, targets = [], []\n    for img_name, target in csv_data.iterrows():\n        images.append(torchvision.io.read_image(\n            os.path.join(data_dir, 'bananas_train' if is_train else\n                         'bananas_val', 'images', f'{img_name}')))\n        # Here `target` contains (class, upper-left x, upper-left y,\n        # lower-right x, lower-right y), where all the images have the same\n        # banana class (index 0)\n        targets.append(list(target))\n    return images, torch.tensor(targets).unsqueeze(1) / 256\n```\n\nBy using the `read_data_bananas` function to read images and labels,\nthe following `BananasDataset` class\nwill allow us to [**create a customized `Dataset` instance**]\nfor loading the banana detection dataset."
    },
    {
      "chunk_id": "2e8c6dd0922a_2",
      "chapter": "object-detection-dataset",
      "heading": "Reading the Dataset",
      "text": "```{.python .input}\n#@tab mxnet\n#@save\nclass BananasDataset(gluon.data.Dataset):\n    \"\"\"A customized dataset to load the banana detection dataset.\"\"\"\n    def __init__(self, is_train):\n        self.features, self.labels = read_data_bananas(is_train)\n        print('read ' + str(len(self.features)) + (f' training examples' if\n              is_train else f' validation examples'))\n\n    def __getitem__(self, idx):\n        return (self.features[idx].astype('float32').transpose(2, 0, 1),\n                self.labels[idx])\n\n    def __len__(self):\n        return len(self.features)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass BananasDataset(torch.utils.data.Dataset):\n    \"\"\"A customized dataset to load the banana detection dataset.\"\"\"\n    def __init__(self, is_train):\n        self.features, self.labels = read_data_bananas(is_train)\n        print('read ' + str(len(self.features)) + (f' training examples' if\n              is_train else f' validation examples'))\n\n    def __getitem__(self, idx):\n        return (self.features[idx].float(), self.labels[idx])\n\n    def __len__(self):\n        return len(self.features)\n```\n\nFinally, we define\nthe `load_data_bananas` function to [**return two\ndata iterator instances for both the training and test sets.**]\nFor the test dataset,\nthere is no need to read it in random order."
    },
    {
      "chunk_id": "2e8c6dd0922a_3",
      "chapter": "object-detection-dataset",
      "heading": "Reading the Dataset",
      "text": "```{.python .input}\n#@tab mxnet\n#@save\ndef load_data_bananas(batch_size):\n    \"\"\"Load the banana detection dataset.\"\"\"\n    train_iter = gluon.data.DataLoader(BananasDataset(is_train=True),\n                                       batch_size, shuffle=True)\n    val_iter = gluon.data.DataLoader(BananasDataset(is_train=False),\n                                     batch_size)\n    return train_iter, val_iter\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef load_data_bananas(batch_size):\n    \"\"\"Load the banana detection dataset.\"\"\"\n    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),\n                                             batch_size, shuffle=True)\n    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),\n                                           batch_size)\n    return train_iter, val_iter\n```\n\nLet's [**read a minibatch and print the shapes of\nboth images and labels**] in this minibatch. The shape of the image minibatch,\n(batch size, number of channels, height, width),\nlooks familiar:\nit is the same as in our earlier image classification tasks. The shape of the label minibatch is\n(batch size, $m$, 5),\nwhere $m$ is the largest possible number of bounding boxes\nthat any image has in the dataset. Although computation in minibatches is more efficient,\nit requires that all the image examples\ncontain the same number of bounding boxes to form a minibatch via concatenation. In general,\nimages may have a varying number of bounding boxes;\nthus,\nimages with fewer than $m$ bounding boxes\nwill be padded with illegal bounding boxes\nuntil $m$ is reached. Then\nthe label of each bounding box is represented by an array of length 5. The first element in the array is the class of the object in the bounding box,\nwhere -1 indicates an illegal bounding box for padding. The remaining four elements of the array are\nthe ($x$, $y$)-coordinate values\nof the upper-left corner and the lower-right corner\nof the bounding box (the range is between 0 and 1)."
    },
    {
      "chunk_id": "2e8c6dd0922a_4",
      "chapter": "object-detection-dataset",
      "heading": "Reading the Dataset",
      "text": "The remaining four elements of the array are\nthe ($x$, $y$)-coordinate values\nof the upper-left corner and the lower-right corner\nof the bounding box (the range is between 0 and 1). For the banana dataset,\nsince there is only one bounding box on each image,\nwe have $m=1$. ```{.python .input}\n#@tab all\nbatch_size, edge_size = 32, 256\ntrain_iter, _ = load_data_bananas(batch_size)\nbatch = next(iter(train_iter))\nbatch[0].shape, batch[1].shape\n```"
    },
    {
      "chunk_id": "3aec9c3f5a21_0",
      "chapter": "object-detection-dataset",
      "heading": "[**Demonstration**]",
      "text": "Let's demonstrate ten images with their labeled ground-truth bounding boxes.\nWe can see that the rotations, sizes, and positions of bananas vary across all these images.\nOf course, this is just a simple artificial dataset.\nIn practice, real-world datasets are usually much more complicated.\n\n```{.python .input}\n#@tab mxnet\nimgs = (batch[0][:10].transpose(0, 2, 3, 1)) / 255\naxes = d2l.show_images(imgs, 2, 5, scale=2)\nfor ax, label in zip(axes, batch[1][:10]):\n    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])\n```\n\n```{.python .input}\n#@tab pytorch\nimgs = (batch[0][:10].permute(0, 2, 3, 1)) / 255\naxes = d2l.show_images(imgs, 2, 5, scale=2)\nfor ax, label in zip(axes, batch[1][:10]):\n    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])\n```"
    },
    {
      "chunk_id": "5b8ab4eddb4e_0",
      "chapter": "object-detection-dataset",
      "heading": "Summary",
      "text": "* The banana detection dataset we collected can be used to demonstrate object detection models.\n* The data loading for object detection is similar to that for image classification. However, in object detection the labels also contain information of ground-truth bounding boxes, which is missing in image classification."
    },
    {
      "chunk_id": "cde70c909d4f_0",
      "chapter": "object-detection-dataset",
      "heading": "Exercises",
      "text": "1. Demonstrate other images with ground-truth bounding boxes in the banana detection dataset. How do they differ with respect to bounding boxes and objects?\n1. Say that we want to apply data augmentation, such as random cropping, to object detection. How can it be different from that in image classification? Hint: what if a cropped image only contains a small portion of an object?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/372)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1608)\n:end_tab:"
    },
    {
      "chunk_id": "d2f7c6382fa2_0",
      "chapter": "rcnn",
      "heading": "rcnn",
      "text": "# Region-based CNNs (R-CNNs)\n:label:`sec_rcnn`\n\nBesides single shot multibox detection\ndescribed in :numref:`sec_ssd`,\nregion-based CNNs or regions with CNN features (R-CNNs)\nare also among many pioneering\napproaches of\napplying\ndeep learning to object detection\n:cite:`Girshick.Donahue.Darrell.ea.2014`.\nIn this section, we will introduce\nthe R-CNN and its series of improvements: the fast R-CNN\n:cite:`Girshick.2015`, the faster R-CNN :cite:`Ren.He.Girshick.ea.2015`, and the mask R-CNN\n:cite:`He.Gkioxari.Dollar.ea.2017`.\nDue to limited space, we will only\nfocus on the design of these models."
    },
    {
      "chunk_id": "579e2d8eb956_0",
      "chapter": "rcnn",
      "heading": "R-CNNs",
      "text": "The *R-CNN* first extracts\nmany (e.g., 2000) *region proposals*\nfrom the input image\n(e.g., anchor boxes can also be considered\nas region proposals),\nlabeling their classes and bounding boxes (e.g., offsets).\n:cite:`Girshick.Donahue.Darrell.ea.2014`\nThen a CNN is used to\nperform forward propagation on each region proposal\nto extract its features.\nNext, features of each region proposal\nare used for\npredicting the class and bounding box\nof this region proposal.\n\n\n![The R-CNN model.](../img/r-cnn.svg)\n:label:`fig_r-cnn`\n\n:numref:`fig_r-cnn` shows the R-CNN model. More concretely, the R-CNN consists of the following four steps:\n\n1. Perform *selective search* to extract multiple high-quality region proposals on the input image :cite:`Uijlings.Van-De-Sande.Gevers.ea.2013`. These proposed regions are usually selected at multiple scales with different shapes and sizes. Each region proposal will be labeled with a class and a ground-truth bounding box.\n1. Choose a pretrained CNN and truncate it before the output layer. Resize each region proposal to the input size required by the network, and output the extracted features for the region proposal through forward propagation.\n1. Take the extracted features and labeled class of each region proposal as an example. Train multiple support vector machines to classify objects, where each support vector machine individually determines whether the example contains a specific class.\n1. Take the extracted features and labeled bounding box of each region proposal as an example. Train a linear regression model to predict the ground-truth bounding box.\n\n\nAlthough the R-CNN model uses pretrained CNNs to effectively extract image features,\nit is slow.\nImagine that we select\nthousands of region proposals from a single input image:\nthis requires thousands of\nCNN forward propagations to perform object detection.\nThis massive\ncomputing load makes it infeasible to\nwidely use R-CNNs in real-world applications."
    },
    {
      "chunk_id": "35d426bd8f79_0",
      "chapter": "rcnn",
      "heading": "Fast R-CNN",
      "text": "The main performance bottleneck of\nan R-CNN lies in\nthe independent CNN forward propagation\nfor each region proposal,\nwithout sharing computation. Since these regions usually have\noverlaps,\nindependent feature extractions lead to\nmuch repeated computation. One of the major improvements of\nthe *fast R-CNN* from the\nR-CNN is that\nthe CNN forward propagation\nis only performed on\nthe entire image :cite:`Girshick.2015`. ![The fast R-CNN model.](../img/fast-rcnn.svg)\n:label:`fig_fast_r-cnn`\n\n:numref:`fig_fast_r-cnn` describes the fast R-CNN model. Its major computations are as follows:\n\n\n1. Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extraction is the entire image, rather than individual region proposals. Moreover, this CNN is trainable. Given an input image, let the shape of the CNN output be $1 \\times c \\times h_1  \\times w_1$. 1. Suppose that selective search generates $n$ region proposals. These region proposals (of different shapes) mark regions of interest (of different shapes) on the CNN output. Then these regions of interest further extract features of the same shape (say height $h_2$ and width $w_2$ are specified) in order to be easily concatenated. To achieve this, the fast R-CNN introduces the *region of interest (RoI) pooling* layer: the CNN output and region proposals are input into this layer, outputting concatenated features of shape $n \\times c \\times h_2 \\times w_2$ that are further extracted for all the region proposals. 1. Using a fully connected layer, transform the concatenated features into an output of shape $n \\times d$, where $d$ depends on the model design. 1. Predict the class and bounding box for each of the $n$ region proposals. More concretely, in class and bounding box prediction, transform the fully connected layer output into an output of shape $n \\times q$ ($q$ is the number of classes) and an output of shape $n \\times 4$, respectively. The class prediction uses softmax regression."
    },
    {
      "chunk_id": "35d426bd8f79_1",
      "chapter": "rcnn",
      "heading": "Fast R-CNN",
      "text": "The class prediction uses softmax regression. The region of interest pooling layer proposed in the fast R-CNN is different from the pooling layer introduced in :numref:`sec_pooling`. In the pooling layer,\nwe indirectly control the output shape\nby specifying sizes of\nthe pooling window, padding, and stride. In contrast,\nwe can directly specify the output shape\nin the region of interest pooling layer. For example, let's specify\nthe output height and width\nfor each region as $h_2$ and $w_2$, respectively. For any region of interest window\nof shape $h \\times w$,\nthis window is divided into a $h_2 \\times w_2$ grid\nof subwindows,\nwhere the shape of each subwindow is approximately\n$(h/h_2) \\times (w/w_2)$. In practice,\nthe height and width of any subwindow shall be rounded up, and the largest element shall be used as output of the subwindow. Therefore, the region of interest pooling layer can extract features of the same shape\neven when regions of interest have different shapes. As an illustrative example,\nin :numref:`fig_roi`,\nthe upper-left $3\\times 3$ region of interest\nis selected on a $4 \\times 4$ input. For this region of interest,\nwe use a $2\\times 2$ region of interest pooling layer to obtain\na $2\\times 2$ output. Note that\neach of the four divided subwindows\ncontains elements\n0, 1, 4, and 5 (5 is the maximum);\n2 and 6 (6 is the maximum);\n8 and 9 (9 is the maximum);\nand 10. ![A $2\\times 2$ region of interest pooling layer.](../img/roi.svg)\n:label:`fig_roi`\n\nBelow we demonstrate the computation of the region of interest pooling layer. Suppose that the height and width of the CNN-extracted features `X` are both 4, and there is only a single channel."
    },
    {
      "chunk_id": "35d426bd8f79_2",
      "chapter": "rcnn",
      "heading": "Fast R-CNN",
      "text": "Suppose that the height and width of the CNN-extracted features `X` are both 4, and there is only a single channel. ```{.python .input}\n#@tab mxnet\nfrom mxnet import np, npx\n\nnpx.set_np()\n\nX = np.arange(16).reshape(1, 1, 4, 4)\nX\n```\n\n```{.python .input}\n#@tab pytorch\nimport torch\nimport torchvision\n\nX = torch.arange(16.).reshape(1, 1, 4, 4)\nX\n```\n\nLet's further suppose\nthat  the height and width of the input image are both 40 pixels and that selective search generates two region proposals on this image. Each region proposal\nis expressed as five elements:\nits object class followed by the $(x, y)$-coordinates of its upper-left and lower-right corners. ```{.python .input}\n#@tab mxnet\nrois = np.array([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])\n```\n\n```{.python .input}\n#@tab pytorch\nrois = torch.Tensor([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])\n```\n\nBecause the height and width of `X` are $1/10$ of the height and width of the input image,\nthe coordinates of the two region proposals\nare multiplied by 0.1 according to the specified `spatial_scale` argument. Then the two regions of interest are marked on `X` as `X[:, :, 0:3, 0:3]` and `X[:, :, 1:4, 0:4]`, respectively. Finally in the $2\\times 2$ region of interest pooling,\neach region of interest is divided\ninto a grid of sub-windows to\nfurther extract features of the same shape $2\\times 2$. ```{.python .input}\n#@tab mxnet\nnpx.roi_pooling(X, rois, pooled_size=(2, 2), spatial_scale=0.1)\n```\n\n```{.python .input}\n#@tab pytorch\ntorchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1)\n```"
    },
    {
      "chunk_id": "2e544fc8196d_0",
      "chapter": "rcnn",
      "heading": "Faster R-CNN",
      "text": "To be more accurate in object detection,\nthe fast R-CNN model\nusually has to generate\na lot of region proposals in selective search. To reduce region proposals\nwithout loss of accuracy,\nthe *faster R-CNN*\nproposes to replace selective search with a *region proposal network* :cite:`Ren.He.Girshick.ea.2015`. ![The faster R-CNN model.](../img/faster-rcnn.svg)\n:label:`fig_faster_r-cnn`\n\n\n:numref:`fig_faster_r-cnn` shows the faster R-CNN model. Compared with the fast R-CNN,\nthe faster R-CNN only changes\nthe region proposal method\nfrom selective search to a region proposal network. The rest of the model remain\nunchanged. The region proposal network\nworks in the following steps:\n\n1. Use a $3\\times 3$ convolutional layer with padding of 1 to transform the CNN output to a new output with $c$ channels. In this way, each unit along the spatial dimensions of the CNN-extracted feature maps gets a new feature vector of length $c$. 1. Centered on each pixel of the feature maps, generate multiple anchor boxes of different scales and aspect ratios and label them. 1. Using the length-$c$ feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box. 1. Consider those predicted bounding boxes whose  predicted classes are objects. Remove overlapped results using non-maximum suppression. The remaining  predicted bounding boxes for objects are the region proposals required by the region of interest pooling layer. It is worth noting that,\nas part of the faster R-CNN model,\nthe region\nproposal network is jointly trained\nwith the rest of the model. In other words, the objective function of\nthe faster R-CNN includes\nnot only the class and bounding box prediction\nin object detection,\nbut also the binary class and bounding box prediction\nof anchor boxes in the region proposal network."
    },
    {
      "chunk_id": "2e544fc8196d_1",
      "chapter": "rcnn",
      "heading": "Faster R-CNN",
      "text": "In other words, the objective function of\nthe faster R-CNN includes\nnot only the class and bounding box prediction\nin object detection,\nbut also the binary class and bounding box prediction\nof anchor boxes in the region proposal network. As a result of the end-to-end training,\nthe region proposal network learns\nhow to generate high-quality region proposals,\nso as to stay accurate in object detection\nwith a reduced number of region proposals\nthat are learned from data."
    },
    {
      "chunk_id": "a7b40bf5af6b_0",
      "chapter": "rcnn",
      "heading": "Mask R-CNN",
      "text": "In the training dataset,\nif pixel-level positions of object\nare also labeled on images,\nthe *mask R-CNN* can effectively leverage\nsuch detailed labels\nto further improve the accuracy of object detection :cite:`He.Gkioxari.Dollar.ea.2017`.\n\n\n![The mask R-CNN model.](../img/mask-rcnn.svg)\n:label:`fig_mask_r-cnn`\n\nAs shown in :numref:`fig_mask_r-cnn`,\nthe mask R-CNN\nis modified based on the faster R-CNN.\nSpecifically,\nthe mask R-CNN replaces the\nregion of interest pooling layer with the\n*region of interest (RoI) alignment* layer.\nThis region of interest alignment layer\nuses bilinear interpolation\nto preserve the spatial information on the feature maps, which is more suitable for pixel-level prediction.\nThe output of this layer\ncontains feature maps of the same shape\nfor all the regions of interest.\nThey are used\nto predict\nnot only the class and bounding box for each region of interest,\nbut also the pixel-level position of the object through an additional fully convolutional network.\nMore details on using a fully convolutional network to predict pixel-level semantics of an image\nwill be provided\nin subsequent sections of this chapter."
    },
    {
      "chunk_id": "f0eaa1f25f60_0",
      "chapter": "rcnn",
      "heading": "Summary",
      "text": "* The R-CNN extracts many region proposals from the input image, uses a CNN to perform forward propagation on each region proposal to extract its features, then uses these features to predict the class and bounding box of this region proposal.\n* One of the major improvements of  the fast R-CNN from the R-CNN is that the CNN forward propagation is only performed on  the entire image. It also introduces the region of interest pooling layer, so that features of the same shape can be further extracted for regions of interest that have different shapes.\n* The faster R-CNN replaces the selective search used in the fast R-CNN with a jointly trained region proposal network, so that the former can stay accurate in object detection with a reduced number of region proposals.\n* Based on the faster R-CNN, the mask R-CNN additionally introduces a fully convolutional network, so as to leverage pixel-level labels to further improve the accuracy of object detection."
    },
    {
      "chunk_id": "f4d05f82596f_0",
      "chapter": "rcnn",
      "heading": "Exercises",
      "text": "1. Can we frame object detection as a single regression problem, such as predicting bounding boxes and class probabilities? You may refer to the design of the YOLO model :cite:`Redmon.Divvala.Girshick.ea.2016`.\n1. Compare single shot multibox detection with the methods introduced in this section. What are their major differences? You may refer to Figure 2 of :citet:`Zhao.Zheng.Xu.ea.2019`.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/374)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1409)\n:end_tab:"
    },
    {
      "chunk_id": "9a8c228850bb_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "semantic-segmentation-and-dataset",
      "text": "# Semantic Segmentation and the Dataset\n:label:`sec_semantic_segmentation`\n\nWhen discussing object detection tasks\nin :numref:`sec_bbox`--:numref:`sec_rcnn`,\nrectangular bounding boxes\nare used to label and predict objects in images.\nThis section will discuss the problem of *semantic segmentation*,\nwhich focuses on how to divide an image into regions belonging to different semantic classes.\nDifferent from object detection,\nsemantic segmentation\nrecognizes and understands\nwhat are in images in pixel level:\nits labeling and prediction of semantic regions are\nin pixel level.\n:numref:`fig_segmentation` shows the labels\nof the dog, cat, and background of the image in semantic segmentation.\nCompared with in object detection,\nthe pixel-level borders labeled\nin semantic segmentation are obviously more fine-grained.\n\n\n![Labels of the dog, cat, and background of the image in semantic segmentation.](../img/segmentation.svg)\n:label:`fig_segmentation`"
    },
    {
      "chunk_id": "33edaffa1bc5_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "Image Segmentation and Instance Segmentation",
      "text": "There are also two important tasks\nin the field of computer vision that are similar to semantic segmentation,\nnamely image segmentation and instance segmentation.\nWe will briefly\ndistinguish them from semantic segmentation as follows.\n\n* *Image segmentation* divides an image into several constituent regions. The methods for this type of problem usually make use of the correlation between pixels in the image. It does not need label information about image pixels during training, and it cannot guarantee that the segmented regions will have the semantics that we hope to obtain during prediction. Taking the image in :numref:`fig_segmentation` as input, image segmentation may divide the dog into two regions: one covers the mouth and eyes which are mainly black, and the other covers the rest of the body which is mainly yellow.\n* *Instance segmentation* is also called *simultaneous detection and segmentation*. It studies how to recognize the pixel-level regions of each object instance in an image. Different from semantic segmentation, instance segmentation needs to distinguish not only semantics, but also different object instances. For example, if there are two dogs in the image, instance segmentation needs to distinguish which of the two dogs a pixel belongs to."
    },
    {
      "chunk_id": "299a3a084b9b_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "The Pascal VOC2012 Semantic Segmentation Dataset",
      "text": "[**On of the most important semantic segmentation dataset\nis [Pascal VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).**]\nIn the following,\nwe will take a look at this dataset. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, image, np, npx\nimport os\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nimport os\n```\n\nThe tar file of the dataset is about 2 GB,\nso it may take a while to download the file. The extracted dataset is located at `../data/VOCdevkit/VOC2012`. ```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',\n                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')\n\nvoc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n```\n\nAfter entering the path `../data/VOCdevkit/VOC2012`,\nwe can see the different components of the dataset. The `ImageSets/Segmentation` path contains text files\nthat specify training and test samples,\nwhile the `JPEGImages` and `SegmentationClass` paths\nstore the input image and label for each example, respectively. The label here is also in the image format,\nwith the same size\nas its labeled input image. Besides,\npixels with the same color in any label image belong to the same semantic class. The following defines the `read_voc_images` function to [**read all the input images and labels into the memory**]."
    },
    {
      "chunk_id": "299a3a084b9b_1",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "The Pascal VOC2012 Semantic Segmentation Dataset",
      "text": "Besides,\npixels with the same color in any label image belong to the same semantic class. The following defines the `read_voc_images` function to [**read all the input images and labels into the memory**]. ```{.python .input}\n#@tab mxnet\n#@save\ndef read_voc_images(voc_dir, is_train=True):\n    \"\"\"Read all VOC feature and label images.\"\"\"\n    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n                             'train.txt' if is_train else 'val.txt')\n    with open(txt_fname, 'r') as f:\n        images = f.read().split()\n    features, labels = [], []\n    for i, fname in enumerate(images):\n        features.append(image.imread(os.path.join(\n            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n        labels.append(image.imread(os.path.join(\n            voc_dir, 'SegmentationClass', f'{fname}.png')))\n    return features, labels\n\ntrain_features, train_labels = read_voc_images(voc_dir, True)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef read_voc_images(voc_dir, is_train=True):\n    \"\"\"Read all VOC feature and label images.\"\"\"\n    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n                             'train.txt' if is_train else 'val.txt')\n    mode = torchvision.io.image.ImageReadMode.RGB\n    with open(txt_fname, 'r') as f:\n        images = f.read().split()\n    features, labels = [], []\n    for i, fname in enumerate(images):\n        features.append(torchvision.io.read_image(os.path.join(\n            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n        labels.append(torchvision.io.read_image(os.path.join(\n            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))\n    return features, labels\n\ntrain_features, train_labels = read_voc_images(voc_dir, True)\n```\n\nWe [**draw the first five input images and their labels**]. In the label images, white and black represent borders and  background, respectively, while the other colors correspond to different classes."
    },
    {
      "chunk_id": "299a3a084b9b_2",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "The Pascal VOC2012 Semantic Segmentation Dataset",
      "text": "In the label images, white and black represent borders and  background, respectively, while the other colors correspond to different classes. ```{.python .input}\n#@tab mxnet\nn = 5\nimgs = train_features[:n] + train_labels[:n]\nd2l.show_images(imgs, 2, n);\n```\n\n```{.python .input}\n#@tab pytorch\nn = 5\nimgs = train_features[:n] + train_labels[:n]\nimgs = [img.permute(1,2,0) for img in imgs]\nd2l.show_images(imgs, 2, n);\n```\n\nNext, we [**enumerate\nthe RGB color values and class names**]\nfor all the labels in this dataset. ```{.python .input}\n#@tab all\n#@save\nVOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n                [0, 64, 128]]\n\n#@save\nVOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n```\n\nWith the two constants defined above,\nwe can conveniently\n[**find the class index for each pixel in a label**]. We define the `voc_colormap2label` function\nto build the mapping from the above RGB color values\nto class indices,\nand the `voc_label_indices` function\nto map any RGB values to their class indices in this Pascal VOC2012 dataset."
    },
    {
      "chunk_id": "299a3a084b9b_3",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "The Pascal VOC2012 Semantic Segmentation Dataset",
      "text": "We define the `voc_colormap2label` function\nto build the mapping from the above RGB color values\nto class indices,\nand the `voc_label_indices` function\nto map any RGB values to their class indices in this Pascal VOC2012 dataset. ```{.python .input}\n#@tab mxnet\n#@save\ndef voc_colormap2label():\n    \"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\n    colormap2label = np.zeros(256 ** 3)\n    for i, colormap in enumerate(VOC_COLORMAP):\n        colormap2label[\n            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n    return colormap2label\n\n#@save\ndef voc_label_indices(colormap, colormap2label):\n    \"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\n    colormap = colormap.astype(np.int32)\n    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n           + colormap[:, :, 2])\n    return colormap2label[idx]\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef voc_colormap2label():\n    \"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\n    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n    for i, colormap in enumerate(VOC_COLORMAP):\n        colormap2label[\n            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n    return colormap2label\n\n#@save\ndef voc_label_indices(colormap, colormap2label):\n    \"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\n    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n           + colormap[:, :, 2])\n    return colormap2label[idx]\n```\n\n[**For example**], in the first example image,\nthe class index for the front part of the airplane is 1,\nwhile the background index is 0. ```{.python .input}\n#@tab all\ny = voc_label_indices(train_labels[0], voc_colormap2label())\ny[105:115, 130:140], VOC_CLASSES[1]\n```"
    },
    {
      "chunk_id": "97cbd5294284_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "Data Preprocessing",
      "text": "In previous experiments\nsuch as in :numref:`sec_alexnet`--:numref:`sec_googlenet`,\nimages are rescaled\nto fit the model's required input shape.\nHowever, in semantic segmentation,\ndoing so\nrequires rescaling the predicted pixel classes\nback to the original shape of the input image.\nSuch rescaling may be inaccurate,\nespecially for segmented regions with different classes. To avoid this issue,\nwe crop the image to a *fixed* shape instead of rescaling. Specifically, [**using random cropping from image augmentation, we crop the same area of\nthe input image and the label**].\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef voc_rand_crop(feature, label, height, width):\n    \"\"\"Randomly crop both feature and label images.\"\"\"\n    feature, rect = image.random_crop(feature, (width, height))\n    label = image.fixed_crop(label, *rect)\n    return feature, label\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef voc_rand_crop(feature, label, height, width):\n    \"\"\"Randomly crop both feature and label images.\"\"\"\n    rect = torchvision.transforms.RandomCrop.get_params(\n        feature, (height, width))\n    feature = torchvision.transforms.functional.crop(feature, *rect)\n    label = torchvision.transforms.functional.crop(label, *rect)\n    return feature, label\n```\n\n```{.python .input}\n#@tab mxnet\nimgs = []\nfor _ in range(n):\n    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\nd2l.show_images(imgs[::2] + imgs[1::2], 2, n);\n```\n\n```{.python .input}\n#@tab pytorch\nimgs = []\nfor _ in range(n):\n    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\n\nimgs = [img.permute(1, 2, 0) for img in imgs]\nd2l.show_images(imgs[::2] + imgs[1::2], 2, n);\n```"
    },
    {
      "chunk_id": "527b066b81da_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "[**Custom Semantic Segmentation Dataset Class**]",
      "text": "We define a custom semantic segmentation dataset class `VOCSegDataset` by inheriting the `Dataset` class provided by high-level APIs. By implementing the `__getitem__` function,\nwe can arbitrarily access the input image indexed as `idx` in the dataset and the class index of each pixel in this image. Since some images in the dataset\nhave a smaller size\nthan the output size of random cropping,\nthese examples are filtered out\nby a custom `filter` function. In addition, we also\ndefine the `normalize_image` function to\nstandardize the values of the three RGB channels of input images."
    },
    {
      "chunk_id": "527b066b81da_1",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "[**Custom Semantic Segmentation Dataset Class**]",
      "text": "In addition, we also\ndefine the `normalize_image` function to\nstandardize the values of the three RGB channels of input images. ```{.python .input}\n#@tab mxnet\n#@save\nclass VOCSegDataset(gluon.data.Dataset):\n    \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n    def __init__(self, is_train, crop_size, voc_dir):\n        self.rgb_mean = np.array([0.485, 0.456, 0.406])\n        self.rgb_std = np.array([0.229, 0.224, 0.225])\n        self.crop_size = crop_size\n        features, labels = read_voc_images(voc_dir, is_train=is_train)\n        self.features = [self.normalize_image(feature)\n                         for feature in self.filter(features)]\n        self.labels = self.filter(labels)\n        self.colormap2label = voc_colormap2label()\n        print('read ' + str(len(self.features)) + ' examples')\n\n    def normalize_image(self, img):\n        return (img.astype('float32') / 255 - self.rgb_mean) / self.rgb_std\n\n    def filter(self, imgs):\n        return [img for img in imgs if (\n            img.shape[0] >= self.crop_size[0] and\n            img.shape[1] >= self.crop_size[1])]\n\n    def __getitem__(self, idx):\n        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n                                       *self.crop_size)\n        return (feature.transpose(2, 0, 1),\n                voc_label_indices(label, self.colormap2label))\n\n    def __len__(self):\n        return len(self.features)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass VOCSegDataset(torch.utils.data.Dataset):\n    \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n\n    def __init__(self, is_train, crop_size, voc_dir):\n        self.transform = torchvision.transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        self.crop_size = crop_size\n        features, labels = read_voc_images(voc_dir, is_train=is_train)\n        self.features = [self.normalize_image(feature)\n                         for feature in self.filter(features)]\n        self.labels = self.filter(labels)\n        self.colormap2label = voc_colormap2label()\n        print('read ' + str(len(self.features)) + ' examples')\n\n    def normalize_image(self, img):\n        return self.transform(img.float() / 255)\n\n    def filter(self, imgs):\n        return [img for img in imgs if (\n            img.shape[1] >= self.crop_size[0] and\n            img.shape[2] >= self.crop_size[1])]\n\n    def __getitem__(self, idx):\n        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n                                       *self.crop_size)\n        return (feature, voc_label_indices(label, self.colormap2label))\n\n    def __len__(self):\n        return len(self.features)\n```"
    },
    {
      "chunk_id": "75dce0231ea9_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "[**Reading the Dataset**]",
      "text": "We use the custom `VOCSegDatase`t class to\ncreate instances of the training set and test set, respectively.\nSuppose that\nwe specify that the output shape of randomly cropped images is $320\\times 480$.\nBelow we can view the number of examples\nthat are retained in the training set and test set.\n\n```{.python .input}\n#@tab all\ncrop_size = (320, 480)\nvoc_train = VOCSegDataset(True, crop_size, voc_dir)\nvoc_test = VOCSegDataset(False, crop_size, voc_dir)\n```\n\nSetting the batch size to 64,\nwe define the data iterator for the training set.\nLet's print the shape of the first minibatch.\nDifferent from in image classification or object detection, labels here are three-dimensional tensors.\n\n```{.python .input}\n#@tab mxnet\nbatch_size = 64\ntrain_iter = gluon.data.DataLoader(voc_train, batch_size, shuffle=True,\n                                   last_batch='discard',\n                                   num_workers=d2l.get_dataloader_workers())\nfor X, Y in train_iter:\n    print(X.shape)\n    print(Y.shape)\n    break\n```\n\n```{.python .input}\n#@tab pytorch\nbatch_size = 64\ntrain_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,\n                                    drop_last=True,\n                                    num_workers=d2l.get_dataloader_workers())\nfor X, Y in train_iter:\n    print(X.shape)\n    print(Y.shape)\n    break\n```"
    },
    {
      "chunk_id": "1bab54aa730e_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "[**Putting It All Together**]",
      "text": "Finally, we define the following `load_data_voc` function\nto download and read the Pascal VOC2012 semantic segmentation dataset.\nIt returns data iterators for both the training and test datasets.\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef load_data_voc(batch_size, crop_size):\n    \"\"\"Load the VOC semantic segmentation dataset.\"\"\"\n    voc_dir = d2l.download_extract('voc2012', os.path.join(\n        'VOCdevkit', 'VOC2012'))\n    num_workers = d2l.get_dataloader_workers()\n    train_iter = gluon.data.DataLoader(\n        VOCSegDataset(True, crop_size, voc_dir), batch_size,\n        shuffle=True, last_batch='discard', num_workers=num_workers)\n    test_iter = gluon.data.DataLoader(\n        VOCSegDataset(False, crop_size, voc_dir), batch_size,\n        last_batch='discard', num_workers=num_workers)\n    return train_iter, test_iter\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef load_data_voc(batch_size, crop_size):\n    \"\"\"Load the VOC semantic segmentation dataset.\"\"\"\n    voc_dir = d2l.download_extract('voc2012', os.path.join(\n        'VOCdevkit', 'VOC2012'))\n    num_workers = d2l.get_dataloader_workers()\n    train_iter = torch.utils.data.DataLoader(\n        VOCSegDataset(True, crop_size, voc_dir), batch_size,\n        shuffle=True, drop_last=True, num_workers=num_workers)\n    test_iter = torch.utils.data.DataLoader(\n        VOCSegDataset(False, crop_size, voc_dir), batch_size,\n        drop_last=True, num_workers=num_workers)\n    return train_iter, test_iter\n```"
    },
    {
      "chunk_id": "29e0dcacddc1_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "Summary",
      "text": "* Semantic segmentation recognizes and understands what are in an image in pixel level by dividing the image into regions belonging to different semantic classes.\n* One of the most important semantic segmentation dataset is Pascal VOC2012.\n* In semantic segmentation, since the input image and  label correspond one-to-one on the pixel, the input image is randomly cropped to a fixed shape rather than rescaled."
    },
    {
      "chunk_id": "42267d47e8f0_0",
      "chapter": "semantic-segmentation-and-dataset",
      "heading": "Exercises",
      "text": "1. How can semantic segmentation be applied in autonomous vehicles and medical image diagnostics? Can you think of other applications?\n1. Recall the descriptions of data augmentation in :numref:`sec_image_augmentation`. Which of the image augmentation methods used in image classification would be infeasible to be applied in semantic segmentation?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/375)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1480)\n:end_tab:"
    },
    {
      "chunk_id": "1b1465d453ed_0",
      "chapter": "ssd",
      "heading": "ssd",
      "text": "# Single Shot Multibox Detection\n:label:`sec_ssd`\n\nIn :numref:`sec_bbox`--:numref:`sec_object-detection-dataset`,\nwe introduced bounding boxes, anchor boxes,\nmultiscale object detection, and the dataset for object detection.\nNow we are ready to use such background\nknowledge to design an object detection model:\nsingle shot multibox detection\n(SSD) :cite:`Liu.Anguelov.Erhan.ea.2016`.\nThis model is simple, fast, and widely used.\nAlthough this is just one of vast amounts of\nobject detection models,\nsome of the design principles\nand implementation details in this section\nare also applicable to other models."
    },
    {
      "chunk_id": "ebc2c7e0d62a_0",
      "chapter": "ssd",
      "heading": "Model",
      "text": ":numref:`fig_ssd` provides an overview of\nthe design of single-shot multibox detection.\nThis model mainly consists of\na base network\nfollowed by\nseveral multiscale feature map blocks.\nThe base network\nis for extracting features from the input image,\nso it can use a deep CNN.\nFor example,\nthe original single-shot multibox detection paper\nadopts a VGG network truncated before the\nclassification layer :cite:`Liu.Anguelov.Erhan.ea.2016`,\nwhile ResNet has also been commonly used.\nThrough our design\nwe can make the base network output\nlarger feature maps\nso as to generate more anchor boxes\nfor detecting smaller objects.\nSubsequently,\neach multiscale feature map block\nreduces (e.g., by half)\nthe height and width of the feature maps\nfrom the previous block,\nand enables each unit\nof the feature maps\nto increase its receptive field on the input image.\n\n\nRecall the design\nof multiscale object detection\nthrough layerwise representations of images by\ndeep neural networks\nin :numref:`sec_multiscale-object-detection`.\nSince\nmultiscale feature maps closer to the top of :numref:`fig_ssd`\nare smaller but have larger receptive fields,\nthey are suitable for detecting\nfewer but larger objects.\n\nIn a nutshell,\nvia its base network and several multiscale feature map blocks,\nsingle-shot multibox detection\ngenerates a varying number of anchor boxes with different sizes,\nand detects varying-size objects\nby predicting classes and offsets\nof these anchor boxes (thus the bounding boxes);\nthus, this is a multiscale object detection model.\n\n\n![As a multiscale object detection model, single-shot multibox detection mainly consists of a base network followed by several multiscale feature map blocks.](../img/ssd.svg)\n:label:`fig_ssd`\n\n\nIn the following,\nwe will describe the implementation details\nof different blocks in :numref:`fig_ssd`. To begin with, we discuss how to implement\nthe class and bounding box prediction."
    },
    {
      "chunk_id": "0ccbac43c5b0_0",
      "chapter": "ssd",
      "heading": "[**Class Prediction Layer**]",
      "text": "Let the number of object classes be $q$. Then anchor boxes have $q+1$ classes,\nwhere class 0 is background. At some scale,\nsuppose that the height and width of feature maps\nare $h$ and $w$, respectively. When $a$ anchor boxes\nare generated with\neach spatial position of these feature maps as their center,\na total of $hwa$ anchor boxes need to be classified. This often makes classification with fully connected layers infeasible due to likely\nheavy parametrization costs. Recall how we used channels of\nconvolutional layers\nto predict classes in :numref:`sec_nin`. Single-shot multibox detection uses the\nsame technique to reduce model complexity. Specifically,\nthe class prediction layer uses a convolutional layer\nwithout altering width or height of feature maps. In this way,\nthere can be a one-to-one correspondence\nbetween outputs and inputs\nat the same spatial dimensions (width and height)\nof feature maps. More concretely,\nchannels of the output feature maps\nat any spatial position ($x$, $y$)\nrepresent class predictions\nfor all the anchor boxes centered on\n($x$, $y$) of the input feature maps. To produce valid predictions,\nthere must be $a(q+1)$ output channels,\nwhere for the same spatial position\nthe output channel with index $i(q+1) + j$\nrepresents the prediction of\nthe class $j$ ($0 \\leq j \\leq q$)\nfor the anchor box $i$ ($0 \\leq i < a$). Below we define such a class prediction layer,\nspecifying $a$ and $q$ via arguments `num_anchors` and `num_classes`, respectively. This layer uses a $3\\times3$ convolutional layer with a\npadding of 1. The width and height of the input and output of this\nconvolutional layer remain unchanged."
    },
    {
      "chunk_id": "0ccbac43c5b0_1",
      "chapter": "ssd",
      "heading": "[**Class Prediction Layer**]",
      "text": "This layer uses a $3\\times3$ convolutional layer with a\npadding of 1. The width and height of the input and output of this\nconvolutional layer remain unchanged. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, image, init, np, npx\nfrom mxnet.gluon import nn\n\nnpx.set_np()\n\ndef cls_predictor(num_anchors, num_classes):\n    return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3,\n                     padding=1)\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import functional as F\n\ndef cls_predictor(num_inputs, num_anchors, num_classes):\n    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\n                     kernel_size=3, padding=1)\n```"
    },
    {
      "chunk_id": "7de4a3d2e39b_0",
      "chapter": "ssd",
      "heading": "(**Bounding Box Prediction Layer**)",
      "text": "The design of the bounding box prediction layer is similar to that of the class prediction layer.\nThe only difference lies in the number of outputs for each anchor box:\nhere we need to predict four offsets rather than $q+1$ classes.\n\n```{.python .input}\n#@tab mxnet\ndef bbox_predictor(num_anchors):\n    return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1)\n```\n\n```{.python .input}\n#@tab pytorch\ndef bbox_predictor(num_inputs, num_anchors):\n    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)\n```"
    },
    {
      "chunk_id": "479c5d8349a7_0",
      "chapter": "ssd",
      "heading": "[**Concatenating Predictions for Multiple Scales**]",
      "text": "As we mentioned, single-shot multibox detection\nuses multiscale feature maps to generate anchor boxes and predict their classes and offsets. At different scales,\nthe shapes of feature maps\nor the numbers of anchor boxes centered on the same unit\nmay vary. Therefore,\nshapes of the prediction outputs\nat different scales may vary. In the following example,\nwe construct feature maps at two different scales,\n`Y1` and `Y2`,\nfor the same minibatch,\nwhere the height and width of `Y2`\nare half of those of `Y1`. Let's take class prediction as an example. Suppose that\n5 and 3 anchor boxes\nare generated for every unit in `Y1` and `Y2`, respectively. Suppose further that\nthe number of object classes is 10. For feature maps `Y1` and `Y2`\nthe numbers of channels in the class prediction outputs\nare $5\\times(10+1)=55$ and $3\\times(10+1)=33$, respectively,\nwhere either output shape is\n(batch size, number of channels, height, width). ```{.python .input}\n#@tab mxnet\ndef forward(x, block):\n    block.initialize()\n    return block(x)\n\nY1 = forward(np.zeros((2, 8, 20, 20)), cls_predictor(5, 10))\nY2 = forward(np.zeros((2, 16, 10, 10)), cls_predictor(3, 10))\nY1.shape, Y2.shape\n```\n\n```{.python .input}\n#@tab pytorch\ndef forward(x, block):\n    return block(x)\n\nY1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\nY2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\nY1.shape, Y2.shape\n```\n\nAs we can see, except for the batch size dimension,\nthe other three dimensions all have different sizes. To concatenate these two prediction outputs for more efficient computation,\nwe will transform these tensors into a more consistent format. Note that\nthe channel dimension holds the predictions for\nanchor boxes with the same center. We first move this dimension to the innermost. Since the batch size remains the same for different scales,\nwe can transform the prediction output\ninto a two-dimensional tensor\nwith shape (batch size, height $\\times$ width $\\times$ number of channels)."
    },
    {
      "chunk_id": "479c5d8349a7_1",
      "chapter": "ssd",
      "heading": "[**Concatenating Predictions for Multiple Scales**]",
      "text": "We first move this dimension to the innermost. Since the batch size remains the same for different scales,\nwe can transform the prediction output\ninto a two-dimensional tensor\nwith shape (batch size, height $\\times$ width $\\times$ number of channels). Then we can concatenate\nsuch outputs at different scales\nalong dimension 1. ```{.python .input}\n#@tab mxnet\ndef flatten_pred(pred):\n    return npx.batch_flatten(pred.transpose(0, 2, 3, 1))\n\ndef concat_preds(preds):\n    return np.concatenate([flatten_pred(p) for p in preds], axis=1)\n```\n\n```{.python .input}\n#@tab pytorch\ndef flatten_pred(pred):\n    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\n\ndef concat_preds(preds):\n    return torch.cat([flatten_pred(p) for p in preds], dim=1)\n```\n\nIn this way,\neven though `Y1` and `Y2` have different sizes\nin channels, heights, and widths,\nwe can still concatenate these two prediction outputs at two different scales for the same minibatch. ```{.python .input}\n#@tab all\nconcat_preds([Y1, Y2]).shape\n```"
    },
    {
      "chunk_id": "fa3eda642624_0",
      "chapter": "ssd",
      "heading": "[**Downsampling Block**]",
      "text": "In order to detect objects at multiple scales,\nwe define the following downsampling block `down_sample_blk` that\nhalves the height and width of input feature maps.\nIn fact,\nthis block applies the design of VGG blocks\nin :numref:`subsec_vgg-blocks`.\nMore concretely,\neach downsampling block consists of\ntwo $3\\times3$ convolutional layers with padding of 1\nfollowed by a $2\\times2$ max-pooling layer with stride of 2.\nAs we know, $3\\times3$ convolutional layers with padding of 1 do not change the shape of feature maps.\nHowever, the subsequent $2\\times2$ max-pooling  reduces the height and width of input feature maps by half.\nFor both input and output feature maps of this downsampling block,\nbecause $1\\times 2+(3-1)+(3-1)=6$,\neach unit in the output\nhas a $6\\times6$ receptive field on the input.\nTherefore, the downsampling block enlarges the receptive field of each unit in its output feature maps.\n\n```{.python .input}\n#@tab mxnet\ndef down_sample_blk(num_channels):\n    blk = nn.Sequential()\n    for _ in range(2):\n        blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1),\n                nn.BatchNorm(in_channels=num_channels),\n                nn.Activation('relu'))\n    blk.add(nn.MaxPool2D(2))\n    return blk\n```\n\n```{.python .input}\n#@tab pytorch\ndef down_sample_blk(in_channels, out_channels):\n    blk = []\n    for _ in range(2):\n        blk.append(nn.Conv2d(in_channels, out_channels,\n                             kernel_size=3, padding=1))\n        blk.append(nn.BatchNorm2d(out_channels))\n        blk.append(nn.ReLU())\n        in_channels = out_channels\n    blk.append(nn.MaxPool2d(2))\n    return nn.Sequential(*blk)\n```\n\nIn the following example, our constructed downsampling block changes the number of input channels and halves the height and width of the input feature maps.\n\n```{.python .input}\n#@tab mxnet\nforward(np.zeros((2, 3, 20, 20)), down_sample_blk(10)).shape\n```\n\n```{.python .input}\n#@tab pytorch\nforward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape\n```"
    },
    {
      "chunk_id": "09779ada39e1_0",
      "chapter": "ssd",
      "heading": "[**Base Network Block**]",
      "text": "The base network block is used to extract features from input images.\nFor simplicity,\nwe construct a small base network\nconsisting of three downsampling blocks\nthat double the number of channels at each block.\nGiven a $256\\times256$ input image,\nthis base network block outputs $32 \\times 32$ feature maps ($256/2^3=32$).\n\n```{.python .input}\n#@tab mxnet\ndef base_net():\n    blk = nn.Sequential()\n    for num_filters in [16, 32, 64]:\n        blk.add(down_sample_blk(num_filters))\n    return blk\n\nforward(np.zeros((2, 3, 256, 256)), base_net()).shape\n```\n\n```{.python .input}\n#@tab pytorch\ndef base_net():\n    blk = []\n    num_filters = [3, 16, 32, 64]\n    for i in range(len(num_filters) - 1):\n        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n    return nn.Sequential(*blk)\n\nforward(torch.zeros((2, 3, 256, 256)), base_net()).shape\n```"
    },
    {
      "chunk_id": "1864574fce1d_0",
      "chapter": "ssd",
      "heading": "The Complete Model",
      "text": "[**The complete\nsingle shot multibox detection model\nconsists of five blocks.**]\nThe feature maps produced by each block\nare used for both\n(i) generating anchor boxes\nand (ii) predicting classes and offsets of these anchor boxes. Among these five blocks,\nthe first one\nis the base network block,\nthe second to the fourth are\ndownsampling blocks,\nand the last block\nuses global max-pooling\nto reduce both the height and width to 1. Technically,\nthe second to the fifth blocks\nare all\nthose\nmultiscale feature map blocks\nin :numref:`fig_ssd`. ```{.python .input}\n#@tab mxnet\ndef get_blk(i):\n    if i == 0:\n        blk = base_net()\n    elif i == 4:\n        blk = nn.GlobalMaxPool2D()\n    else:\n        blk = down_sample_blk(128)\n    return blk\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_blk(i):\n    if i == 0:\n        blk = base_net()\n    elif i == 1:\n        blk = down_sample_blk(64, 128)\n    elif i == 4:\n        blk = nn.AdaptiveMaxPool2d((1,1))\n    else:\n        blk = down_sample_blk(128, 128)\n    return blk\n```\n\nNow we [**define the forward propagation**]\nfor each block. Different from\nin image classification tasks,\noutputs here include\n(i) CNN feature maps `Y`,\n(ii) anchor boxes generated using `Y` at the current scale,\nand (iii) classes and offsets predicted (based on `Y`)\nfor these anchor boxes."
    },
    {
      "chunk_id": "1864574fce1d_1",
      "chapter": "ssd",
      "heading": "The Complete Model",
      "text": "Different from\nin image classification tasks,\noutputs here include\n(i) CNN feature maps `Y`,\n(ii) anchor boxes generated using `Y` at the current scale,\nand (iii) classes and offsets predicted (based on `Y`)\nfor these anchor boxes. ```{.python .input}\n#@tab mxnet\ndef blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n    Y = blk(X)\n    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n    cls_preds = cls_predictor(Y)\n    bbox_preds = bbox_predictor(Y)\n    return (Y, anchors, cls_preds, bbox_preds)\n```\n\n```{.python .input}\n#@tab pytorch\ndef blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n    Y = blk(X)\n    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n    cls_preds = cls_predictor(Y)\n    bbox_preds = bbox_predictor(Y)\n    return (Y, anchors, cls_preds, bbox_preds)\n```\n\nRecall that\nin :numref:`fig_ssd`\na multiscale feature map block\nthat is closer to the top\nis for detecting larger objects;\nthus, it needs to generate larger anchor boxes. In the above forward propagation,\nat each multiscale feature map block\nwe pass in a list of two scale values\nvia the `sizes` argument\nof the invoked `multibox_prior` function (described in :numref:`sec_anchor`). In the following,\nthe interval between 0.2 and 1.05\nis split evenly\ninto five sections to determine the\nsmaller scale values at the five blocks: 0.2, 0.37, 0.54, 0.71, and 0.88. Then their larger scale values\nare given by\n$\\sqrt{0.2 \\times 0.37} = 0.272$, $\\sqrt{0.37 \\times 0.54} = 0.447$, and so on. [~~Hyperparameters for each block~~]\n\n```{.python .input}\n#@tab all\nsizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n         [0.88, 0.961]]\nratios = [[1, 2, 0.5]] * 5\nnum_anchors = len(sizes[0]) + len(ratios[0]) - 1\n```\n\nNow we can [**define the complete model**] `TinySSD` as follows."
    },
    {
      "chunk_id": "1864574fce1d_2",
      "chapter": "ssd",
      "heading": "The Complete Model",
      "text": "```{.python .input}\n#@tab mxnet\nclass TinySSD(nn.Block):\n    def __init__(self, num_classes, **kwargs):\n        super(TinySSD, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        for i in range(5):\n            # Equivalent to the assignment statement `self.blk_i = get_blk(i)`\n            setattr(self, f'blk_{i}', get_blk(i))\n            setattr(self, f'cls_{i}', cls_predictor(num_anchors, num_classes))\n            setattr(self, f'bbox_{i}', bbox_predictor(num_anchors))\n\n    def forward(self, X):\n        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n        for i in range(5):\n            # Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`\n            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n        anchors = np.concatenate(anchors, axis=1)\n        cls_preds = concat_preds(cls_preds)\n        cls_preds = cls_preds.reshape(\n            cls_preds.shape[0], -1, self.num_classes + 1)\n        bbox_preds = concat_preds(bbox_preds)\n        return anchors, cls_preds, bbox_preds\n```\n\n```{.python .input}\n#@tab pytorch\nclass TinySSD(nn.Module):\n    def __init__(self, num_classes, **kwargs):\n        super(TinySSD, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        idx_to_in_channels = [64, 128, 128, 128, 128]\n        for i in range(5):\n            # Equivalent to the assignment statement `self.blk_i = get_blk(i)`\n            setattr(self, f'blk_{i}', get_blk(i))\n            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\n                                                    num_anchors, num_classes))\n            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\n                                                      num_anchors))\n\n    def forward(self, X):\n        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n        for i in range(5):\n            # Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`\n            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n        anchors = torch.cat(anchors, dim=1)\n        cls_preds = concat_preds(cls_preds)\n        cls_preds = cls_preds.reshape(\n            cls_preds.shape[0], -1, self.num_classes + 1)\n        bbox_preds = concat_preds(bbox_preds)\n        return anchors, cls_preds, bbox_preds\n```\n\nWe [**create a model instance\nand use it to perform forward propagation**]\non a minibatch of $256 \\times 256$ images `X`."
    },
    {
      "chunk_id": "1864574fce1d_3",
      "chapter": "ssd",
      "heading": "The Complete Model",
      "text": "As shown earlier in this section,\nthe first block outputs $32 \\times 32$ feature maps. Recall that\nthe second to fourth downsampling blocks\nhalve the height and width\nand the fifth block uses global pooling. Since 4 anchor boxes\nare generated for each unit along spatial dimensions\nof feature maps,\nat all the five scales\na total of $(32^2 + 16^2 + 8^2 + 4^2 + 1)\\times 4 = 5444$ anchor boxes are generated for each image. ```{.python .input}\n#@tab mxnet\nnet = TinySSD(num_classes=1)\nnet.initialize()\nX = np.zeros((32, 3, 256, 256))\nanchors, cls_preds, bbox_preds = net(X)\n\nprint('output anchors:', anchors.shape)\nprint('output class preds:', cls_preds.shape)\nprint('output bbox preds:', bbox_preds.shape)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = TinySSD(num_classes=1)\nX = torch.zeros((32, 3, 256, 256))\nanchors, cls_preds, bbox_preds = net(X)\n\nprint('output anchors:', anchors.shape)\nprint('output class preds:', cls_preds.shape)\nprint('output bbox preds:', bbox_preds.shape)\n```"
    },
    {
      "chunk_id": "9ea205fccfe5_0",
      "chapter": "ssd",
      "heading": "Training",
      "text": "Now we will explain\nhow to train the single shot multibox detection model\nfor object detection."
    },
    {
      "chunk_id": "5e6b262ad2e9_0",
      "chapter": "ssd",
      "heading": "Reading the Dataset and Initializing the Model",
      "text": "To begin with,\nlet's [**read\nthe banana detection dataset**]\ndescribed in :numref:`sec_object-detection-dataset`.\n\n```{.python .input}\n#@tab all\nbatch_size = 32\ntrain_iter, _ = d2l.load_data_bananas(batch_size)\n```\n\nThere is only one class in the banana detection dataset. After defining the model,\nwe need to (**initialize its parameters and define\nthe optimization algorithm**).\n\n```{.python .input}\n#@tab mxnet\ndevice, net = d2l.try_gpu(), TinySSD(num_classes=1)\nnet.initialize(init=init.Xavier(), ctx=device)\ntrainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'learning_rate': 0.2, 'wd': 5e-4})\n```\n\n```{.python .input}\n#@tab pytorch\ndevice, net = d2l.try_gpu(), TinySSD(num_classes=1)\ntrainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)\n```"
    },
    {
      "chunk_id": "1387fc42710f_0",
      "chapter": "ssd",
      "heading": "[**Defining Loss and Evaluation Functions**]",
      "text": "Object detection has two types of losses. The first loss concerns classes of anchor boxes:\nits computation\ncan simply reuse\nthe cross-entropy loss function\nthat we used for image classification. The second loss\nconcerns offsets of positive (non-background) anchor boxes:\nthis is a regression problem. For this regression problem,\nhowever,\nhere we do not use the squared loss\ndescribed in :numref:`subsec_normal_distribution_and_squared_loss`. Instead,\nwe use the $\\ell_1$ norm loss,\nthe absolute value of the difference between\nthe prediction and the ground-truth. The mask variable `bbox_masks` filters out\nnegative anchor boxes and illegal (padded)\nanchor boxes in the loss calculation. In the end, we sum up\nthe anchor box class loss\nand the anchor box offset loss\nto obtain the loss function for the model. ```{.python .input}\n#@tab mxnet\ncls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\nbbox_loss = gluon.loss.L1Loss()\n\ndef calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n    cls = cls_loss(cls_preds, cls_labels)\n    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks)\n    return cls + bbox\n```\n\n```{.python .input}\n#@tab pytorch\ncls_loss = nn.CrossEntropyLoss(reduction='none')\nbbox_loss = nn.L1Loss(reduction='none')\n\ndef calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n    cls = cls_loss(cls_preds.reshape(-1, num_classes),\n                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n    bbox = bbox_loss(bbox_preds * bbox_masks,\n                     bbox_labels * bbox_masks).mean(dim=1)\n    return cls + bbox\n```\n\nWe can use accuracy to evaluate the classification results. Due to the used $\\ell_1$ norm loss for the offsets,\nwe use the *mean absolute error* to evaluate the\npredicted bounding boxes. These prediction results are obtained\nfrom the generated anchor boxes and the\npredicted offsets for them."
    },
    {
      "chunk_id": "1387fc42710f_1",
      "chapter": "ssd",
      "heading": "[**Defining Loss and Evaluation Functions**]",
      "text": "Due to the used $\\ell_1$ norm loss for the offsets,\nwe use the *mean absolute error* to evaluate the\npredicted bounding boxes. These prediction results are obtained\nfrom the generated anchor boxes and the\npredicted offsets for them. ```{.python .input}\n#@tab mxnet\ndef cls_eval(cls_preds, cls_labels):\n    # Because the class prediction results are on the final dimension,\n    # `argmax` needs to specify this dimension\n    return float((cls_preds.argmax(axis=-1).astype(\n        cls_labels.dtype) == cls_labels).sum())\n\ndef bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n    return float((np.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())\n```\n\n```{.python .input}\n#@tab pytorch\ndef cls_eval(cls_preds, cls_labels):\n    # Because the class prediction results are on the final dimension,\n    # `argmax` needs to specify this dimension\n    return float((cls_preds.argmax(dim=-1).type(\n        cls_labels.dtype) == cls_labels).sum())\n\ndef bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())\n```"
    },
    {
      "chunk_id": "4bf9c84ed2fe_0",
      "chapter": "ssd",
      "heading": "[**Training the Model**]",
      "text": "When training the model,\nwe need to generate multiscale anchor boxes (`anchors`)\nand predict their classes (`cls_preds`) and offsets (`bbox_preds`) in the forward propagation. Then we label the classes (`cls_labels`) and offsets (`bbox_labels`) of such generated anchor boxes\nbased on the label information `Y`. Finally, we calculate the loss function\nusing the predicted and labeled values\nof the classes and offsets. For concise implementations,\nevaluation of the test dataset is omitted here. ```{.python .input}\n#@tab mxnet\nnum_epochs, timer = 20, d2l.Timer()\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                        legend=['class error', 'bbox mae'])\nfor epoch in range(num_epochs):\n    # Sum of training accuracy, no. of examples in sum of training accuracy,\n    # Sum of absolute error, no."
    },
    {
      "chunk_id": "4bf9c84ed2fe_1",
      "chapter": "ssd",
      "heading": "[**Training the Model**]",
      "text": "of examples in sum of training accuracy,\n    # Sum of absolute error, no. of examples in sum of absolute error\n    metric = d2l.Accumulator(4)\n    for features, target in train_iter:\n        timer.start()\n        X = features.as_in_ctx(device)\n        Y = target.as_in_ctx(device)\n        with autograd.record():\n            # Generate multiscale anchor boxes and predict their classes and\n            # offsets\n            anchors, cls_preds, bbox_preds = net(X)\n            # Label the classes and offsets of these anchor boxes\n            bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors,\n                                                                      Y)\n            # Calculate the loss function using the predicted and labeled\n            # values of the classes and offsets\n            l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n                          bbox_masks)\n        l.backward()\n        trainer.step(batch_size)\n        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.size,\n                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n                   bbox_labels.size)\n    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\n    animator.add(epoch + 1, (cls_err, bbox_mae))\nprint(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\nprint(f'{len(train_iter._dataset) / timer.stop():.1f} examples/sec on '\n      f'{str(device)}')\n```\n\n```{.python .input}\n#@tab pytorch\nnum_epochs, timer = 20, d2l.Timer()\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                        legend=['class error', 'bbox mae'])\nnet = net.to(device)\nfor epoch in range(num_epochs):\n    # Sum of training accuracy, no. of examples in sum of training accuracy,\n    # Sum of absolute error, no."
    },
    {
      "chunk_id": "4bf9c84ed2fe_2",
      "chapter": "ssd",
      "heading": "[**Training the Model**]",
      "text": "of examples in sum of training accuracy,\n    # Sum of absolute error, no. of examples in sum of absolute error\n    metric = d2l.Accumulator(4)\n    net.train()\n    for features, target in train_iter:\n        timer.start()\n        trainer.zero_grad()\n        X, Y = features.to(device), target.to(device)\n        # Generate multiscale anchor boxes and predict their classes and\n        # offsets\n        anchors, cls_preds, bbox_preds = net(X)\n        # Label the classes and offsets of these anchor boxes\n        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)\n        # Calculate the loss function using the predicted and labeled values\n        # of the classes and offsets\n        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n                      bbox_masks)\n        l.mean().backward()\n        trainer.step()\n        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),\n                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n                   bbox_labels.numel())\n    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\n    animator.add(epoch + 1, (cls_err, bbox_mae))\nprint(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\nprint(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '\n      f'{str(device)}')\n```"
    },
    {
      "chunk_id": "ff3c66671d01_0",
      "chapter": "ssd",
      "heading": "[**Prediction**]",
      "text": "During prediction,\nthe goal is to detect all the objects of interest\non the image. Below\nwe read and resize a test image,\nconverting it to\na four-dimensional tensor that is\nrequired by convolutional layers. ```{.python .input}\n#@tab mxnet\nimg = image.imread('../img/banana.jpg')\nfeature = image.imresize(img, 256, 256).astype('float32')\nX = np.expand_dims(feature.transpose(2, 0, 1), axis=0)\n```\n\n```{.python .input}\n#@tab pytorch\nX = torchvision.io.read_image('../img/banana.jpg').unsqueeze(0).float()\nimg = X.squeeze(0).permute(1, 2, 0).long()\n```\n\nUsing the `multibox_detection` function below,\nthe predicted bounding boxes\nare obtained\nfrom the anchor boxes and their predicted offsets. Then non-maximum suppression is used\nto remove similar predicted bounding boxes. ```{.python .input}\n#@tab mxnet\ndef predict(X):\n    anchors, cls_preds, bbox_preds = net(X.as_in_ctx(device))\n    cls_probs = npx.softmax(cls_preds).transpose(0, 2, 1)\n    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n    return output[0, idx]\n\noutput = predict(X)\n```\n\n```{.python .input}\n#@tab pytorch\ndef predict(X):\n    net.eval()\n    anchors, cls_preds, bbox_preds = net(X.to(device))\n    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)\n    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n    return output[0, idx]\n\noutput = predict(X)\n```\n\nFinally, we [**display\nall the predicted bounding boxes with\nconfidence 0.9 or above**]\nas output."
    },
    {
      "chunk_id": "ff3c66671d01_1",
      "chapter": "ssd",
      "heading": "[**Prediction**]",
      "text": "```{.python .input}\n#@tab mxnet\ndef display(img, output, threshold):\n    d2l.set_figsize((5, 5))\n    fig = d2l.plt.imshow(img.asnumpy())\n    for row in output:\n        score = float(row[1])\n        if score < threshold:\n            continue\n        h, w = img.shape[:2]\n        bbox = [row[2:6] * np.array((w, h, w, h), ctx=row.ctx)]\n        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n\ndisplay(img, output, threshold=0.9)\n```\n\n```{.python .input}\n#@tab pytorch\ndef display(img, output, threshold):\n    d2l.set_figsize((5, 5))\n    fig = d2l.plt.imshow(img)\n    for row in output:\n        score = float(row[1])\n        if score < threshold:\n            continue\n        h, w = img.shape[:2]\n        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]\n        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n\ndisplay(img, output.cpu(), threshold=0.9)\n```"
    },
    {
      "chunk_id": "174b75e2a3fd_0",
      "chapter": "ssd",
      "heading": "Summary",
      "text": "* Single shot multibox detection is a multiscale object detection model. Via its base network and several multiscale feature map blocks, single-shot multibox detection generates a varying number of anchor boxes with different sizes, and detects varying-size objects by predicting classes and offsets of these anchor boxes (thus the bounding boxes).\n* When training the single-shot multibox detection model, the loss function is calculated based on the predicted and labeled values of the anchor box classes and offsets."
    },
    {
      "chunk_id": "74bedf598f5e_0",
      "chapter": "ssd",
      "heading": "Exercises",
      "text": "1. Can you improve the single-shot multibox detection by improving the loss function? For example, replace $\\ell_1$ norm loss with smooth $\\ell_1$ norm loss for the predicted offsets. This loss function uses a square function around zero for smoothness, which is controlled by the hyperparameter $\\sigma$:\n\n$$\nf(x) =\n    \\begin{cases}\n    (\\sigma x)^2/2,& \\textrm{if }|x| < 1/\\sigma^2\\\\\n    |x|-0.5/\\sigma^2,& \\textrm{otherwise}\n    \\end{cases}\n$$\n\nWhen $\\sigma$ is very large, this loss is similar to the $\\ell_1$ norm loss. When its value is smaller, the loss function is smoother. ```{.python .input}\n#@tab mxnet\nsigmas = [10, 1, 0.5]\nlines = ['-', '--', '-.']\nx = np.arange(-2, 2, 0.1)\nd2l.set_figsize()\n\nfor l, s in zip(lines, sigmas):\n    y = npx.smooth_l1(x, scalar=s)\n    d2l.plt.plot(x.asnumpy(), y.asnumpy(), l, label='sigma=%.1f' % s)\nd2l.plt.legend();\n```\n\n```{.python .input}\n#@tab pytorch\ndef smooth_l1(data, scalar):\n    out = []\n    for i in data:\n        if abs(i) < 1 / (scalar ** 2):\n            out.append(((scalar * i) ** 2) / 2)\n        else:\n            out.append(abs(i) - 0.5 / (scalar ** 2))\n    return torch.tensor(out)\n\nsigmas = [10, 1, 0.5]\nlines = ['-', '--', '-.']\nx = torch.arange(-2, 2, 0.1)\nd2l.set_figsize()\n\nfor l, s in zip(lines, sigmas):\n    y = smooth_l1(x, scalar=s)\n    d2l.plt.plot(x, y, l, label='sigma=%.1f' % s)\nd2l.plt.legend();\n```\n\nBesides, in the experiment we used cross-entropy loss for class prediction:\ndenoting by $p_j$ the predicted probability for the ground-truth class $j$, the cross-entropy loss is $-\\log p_j$. We can also use the focal loss\n:cite:`Lin.Goyal.Girshick.ea.2017`: given hyperparameters $\\gamma > 0$\nand $\\alpha > 0$, this loss is defined as:\n\n$$ - \\alpha (1-p_j)^{\\gamma} \\log p_j.$$\n\nAs we can see, increasing $\\gamma$\ncan effectively reduce the relative loss\nfor well-classified examples (e.g., $p_j > 0.5$)\nso the training\ncan focus more on those difficult examples that are misclassified."
    },
    {
      "chunk_id": "74bedf598f5e_1",
      "chapter": "ssd",
      "heading": "Exercises",
      "text": "```{.python .input}\n#@tab mxnet\ndef focal_loss(gamma, x):\n    return -(1 - x) ** gamma * np.log(x)\n\nx = np.arange(0.01, 1, 0.01)\nfor l, gamma in zip(lines, [0, 1, 5]):\n    y = d2l.plt.plot(x.asnumpy(), focal_loss(gamma, x).asnumpy(), l,\n                     label='gamma=%.1f' % gamma)\nd2l.plt.legend();\n```\n\n```{.python .input}\n#@tab pytorch\ndef focal_loss(gamma, x):\n    return -(1 - x) ** gamma * torch.log(x)\n\nx = torch.arange(0.01, 1, 0.01)\nfor l, gamma in zip(lines, [0, 1, 5]):\n    y = d2l.plt.plot(x, focal_loss(gamma, x), l, label='gamma=%.1f' % gamma)\nd2l.plt.legend();\n```\n\n2. Due to space limitations, we have omitted some implementation details of the single shot multibox detection model in this section. Can you further improve the model in the following aspects:\n    1. When an object is much smaller compared with the image, the model could resize the input image bigger. 1. There are typically a vast number of negative anchor boxes. To make the class distribution more balanced, we could downsample negative anchor boxes. 1. In the loss function, assign different weight hyperparameters to the class loss and the offset loss. 1. Use other methods to evaluate the object detection model, such as those in the single shot multibox detection paper :cite:`Liu.Anguelov.Erhan.ea.2016`. :begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/373)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1604)\n:end_tab:"
    },
    {
      "chunk_id": "bde87893051b_0",
      "chapter": "transposed-conv",
      "heading": "transposed-conv",
      "text": "# Transposed Convolution\n:label:`sec_transposed_conv`\n\nThe CNN layers we have seen so far,\nsuch as convolutional layers (:numref:`sec_conv_layer`) and pooling layers (:numref:`sec_pooling`),\ntypically reduce (downsample) the spatial dimensions (height and width) of the input,\nor keep them unchanged.\nIn semantic segmentation\nthat classifies at pixel-level,\nit will be convenient if\nthe spatial dimensions of the\ninput and output are the same.\nFor example,\nthe channel dimension at one output pixel \ncan hold the classification results\nfor the input pixel at the same spatial position.\n\n\nTo achieve this, especially after \nthe spatial dimensions are reduced by CNN layers,\nwe can use another type\nof CNN layers\nthat can increase (upsample) the spatial dimensions\nof intermediate feature maps.\nIn this section,\nwe will introduce \n*transposed convolution*, which is also called *fractionally-strided convolution* :cite:`Dumoulin.Visin.2016`, \nfor reversing downsampling operations\nby the convolution.\n\n```{.python .input}\n#@tab mxnet\nfrom mxnet import np, npx, init\nfrom mxnet.gluon import nn\nfrom d2l import mxnet as d2l\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n```"
    },
    {
      "chunk_id": "0a7e64d207f4_0",
      "chapter": "transposed-conv",
      "heading": "Basic Operation",
      "text": "Ignoring channels for now,\nlet's begin with\nthe basic transposed convolution operation\nwith stride of 1 and no padding. Suppose that\nwe are given a \n$n_h \\times n_w$ input tensor\nand a $k_h \\times k_w$ kernel. Sliding the kernel window with stride of 1\nfor $n_w$ times in each row\nand $n_h$ times in each column\nyields \na total of $n_h n_w$ intermediate results. Each intermediate result is\na $(n_h + k_h - 1) \\times (n_w + k_w - 1)$\ntensor that are initialized as zeros. To compute each intermediate tensor,\neach element in the input tensor\nis multiplied by the kernel\nso that the resulting $k_h \\times k_w$ tensor\nreplaces a portion in\neach intermediate tensor. Note that\nthe position of the replaced portion in each\nintermediate tensor corresponds to the position of the element\nin the input tensor used for the computation. In the end, all the intermediate results\nare summed over to produce the output. As an example,\n:numref:`fig_trans_conv` illustrates\nhow transposed convolution with a $2\\times 2$ kernel is computed for a $2\\times 2$ input tensor. ![Transposed convolution with a $2\\times 2$ kernel. The shaded portions are a portion of an intermediate tensor as well as the input and kernel tensor elements used for the  computation.](../img/trans_conv.svg)\n:label:`fig_trans_conv`\n\n\nWe can (**implement this basic transposed convolution operation**) `trans_conv` for a input matrix `X` and a kernel matrix `K`. ```{.python .input}\n#@tab all\ndef trans_conv(X, K):\n    h, w = K.shape\n    Y = d2l.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Y[i: i + h, j: j + w] += X[i, j] * K\n    return Y\n```\n\nIn contrast to the regular convolution (in :numref:`sec_conv_layer`) that *reduces* input elements\nvia the kernel,\nthe transposed convolution\n*broadcasts* input elements \nvia the kernel, thereby\nproducing an output\nthat is larger than the input."
    },
    {
      "chunk_id": "0a7e64d207f4_1",
      "chapter": "transposed-conv",
      "heading": "Basic Operation",
      "text": "We can construct the input tensor `X` and the kernel tensor `K` from :numref:`fig_trans_conv` to [**validate the output of the above implementation**] of the basic two-dimensional transposed convolution operation. ```{.python .input}\n#@tab all\nX = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\nK = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\ntrans_conv(X, K)\n```\n\nAlternatively,\nwhen the input `X` and kernel `K` are both\nfour-dimensional tensors,\nwe can [**use high-level APIs to obtain the same results**]. ```{.python .input}\n#@tab mxnet\nX, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\ntconv = nn.Conv2DTranspose(1, kernel_size=2)\ntconv.initialize(init.Constant(K))\ntconv(X)\n```\n\n```{.python .input}\n#@tab pytorch\nX, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\ntconv.weight.data = K\ntconv(X)\n```"
    },
    {
      "chunk_id": "fb7a15bc1f0b_0",
      "chapter": "transposed-conv",
      "heading": "[**Padding, Strides, and Multiple Channels**]",
      "text": "Different from in the regular convolution\nwhere padding is applied to input,\nit is applied to output\nin the transposed convolution. For example,\nwhen specifying the padding number\non either side of the height and width \nas 1,\nthe first and last rows and columns\nwill be removed from the transposed convolution output. ```{.python .input}\n#@tab mxnet\ntconv = nn.Conv2DTranspose(1, kernel_size=2, padding=1)\ntconv.initialize(init.Constant(K))\ntconv(X)\n```\n\n```{.python .input}\n#@tab pytorch\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\ntconv.weight.data = K\ntconv(X)\n```\n\nIn the transposed convolution,\nstrides are specified for intermediate results (thus output), not for input. Using the same input and kernel tensors\nfrom :numref:`fig_trans_conv`,\nchanging the stride from 1 to 2\nincreases both the height and weight\nof intermediate tensors, hence the output tensor\nin :numref:`fig_trans_conv_stride2`. ![Transposed convolution with a $2\\times 2$ kernel with stride of 2. The shaded portions are a portion of an intermediate tensor as well as the input and kernel tensor elements used for the  computation.](../img/trans_conv_stride2.svg)\n:label:`fig_trans_conv_stride2`\n\n\n\nThe following code snippet can validate the transposed convolution output for stride of 2 in :numref:`fig_trans_conv_stride2`. ```{.python .input}\n#@tab mxnet\ntconv = nn.Conv2DTranspose(1, kernel_size=2, strides=2)\ntconv.initialize(init.Constant(K))\ntconv(X)\n```\n\n```{.python .input}\n#@tab pytorch\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\ntconv.weight.data = K\ntconv(X)\n```\n\nFor multiple input and output channels,\nthe transposed convolution\nworks in the same way as the regular convolution. Suppose that\nthe input has $c_i$ channels,\nand that the transposed convolution\nassigns a $k_h\\times k_w$ kernel tensor\nto each input channel. When multiple output channels \nare specified,\nwe will have a $c_i\\times k_h\\times k_w$ kernel for each output channel."
    },
    {
      "chunk_id": "fb7a15bc1f0b_1",
      "chapter": "transposed-conv",
      "heading": "[**Padding, Strides, and Multiple Channels**]",
      "text": "When multiple output channels \nare specified,\nwe will have a $c_i\\times k_h\\times k_w$ kernel for each output channel. As in all, if we feed $\\mathsf{X}$ into a convolutional layer $f$ to output $\\mathsf{Y}=f(\\mathsf{X})$ and create a transposed convolutional layer $g$ with the same hyperparameters as $f$ except \nfor the number of output channels \nbeing the number of channels in $\\mathsf{X}$,\nthen $g(Y)$ will have the same shape as $\\mathsf{X}$. This can be illustrated in the following example. ```{.python .input}\n#@tab mxnet\nX = np.random.uniform(size=(1, 10, 16, 16))\nconv = nn.Conv2D(20, kernel_size=5, padding=2, strides=3)\ntconv = nn.Conv2DTranspose(10, kernel_size=5, padding=2, strides=3)\nconv.initialize()\ntconv.initialize()\ntconv(conv(X)).shape == X.shape\n```\n\n```{.python .input}\n#@tab pytorch\nX = torch.rand(size=(1, 10, 16, 16))\nconv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\ntconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\ntconv(conv(X)).shape == X.shape\n```"
    },
    {
      "chunk_id": "c358bc82cc91_0",
      "chapter": "transposed-conv",
      "heading": "[**Connection to Matrix Transposition**]",
      "text": ":label:`subsec-connection-to-mat-transposition`\n\nThe transposed convolution is named after\nthe matrix transposition. To explain,\nlet's first\nsee how to implement convolutions\nusing matrix multiplications. In the example below, we define a $3\\times 3$ input `X` and a $2\\times 2$ convolution kernel `K`, and then use the `corr2d` function to compute the convolution output `Y`. ```{.python .input}\n#@tab all\nX = d2l.arange(9.0).reshape(3, 3)\nK = d2l.tensor([[1.0, 2.0], [3.0, 4.0]])\nY = d2l.corr2d(X, K)\nY\n```\n\nNext, we rewrite the convolution kernel `K` as\na sparse weight matrix `W`\ncontaining a lot of zeros. The shape of the weight matrix is ($4$, $9$),\nwhere the non-zero elements come from\nthe convolution kernel `K`. ```{.python .input}\n#@tab all\ndef kernel2matrix(K):\n    k, W = d2l.zeros(5), d2l.zeros((4, 9))\n    k[:2], k[3:5] = K[0, :], K[1, :]\n    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n    return W\n\nW = kernel2matrix(K)\nW\n```\n\nConcatenate the input `X` row by row to get a vector of length 9. Then the matrix multiplication of `W` and the vectorized `X` gives a vector of length 4. After reshaping it, we can obtain the same result `Y`\nfrom the original convolution operation above:\nwe just implemented convolutions using matrix multiplications. ```{.python .input}\n#@tab all\nY == d2l.matmul(W, d2l.reshape(X, -1)).reshape(2, 2)\n```\n\nLikewise, we can implement transposed convolutions using\nmatrix multiplications. In the following example,\nwe take the $2 \\times 2$ output `Y` from the above\nregular convolution\nas input to the transposed convolution. To implement this operation by multiplying matrices,\nwe only need to transpose the weight matrix `W`\nwith the new shape $(9, 4)$. ```{.python .input}\n#@tab all\nZ = trans_conv(Y, K)\nZ == d2l.matmul(W.T, d2l.reshape(Y, -1)).reshape(3, 3)\n```\n\nConsider implementing the convolution\nby multiplying matrices."
    },
    {
      "chunk_id": "c358bc82cc91_1",
      "chapter": "transposed-conv",
      "heading": "[**Connection to Matrix Transposition**]",
      "text": "```{.python .input}\n#@tab all\nZ = trans_conv(Y, K)\nZ == d2l.matmul(W.T, d2l.reshape(Y, -1)).reshape(3, 3)\n```\n\nConsider implementing the convolution\nby multiplying matrices. Given an input vector $\\mathbf{x}$\nand a weight matrix $\\mathbf{W}$,\nthe forward propagation function of the convolution\ncan be implemented\nby multiplying its input with the weight matrix\nand outputting a vector \n$\\mathbf{y}=\\mathbf{W}\\mathbf{x}$. Since backpropagation\nfollows the chain rule\nand $\\nabla_{\\mathbf{x}}\\mathbf{y}=\\mathbf{W}^\\top$,\nthe backpropagation function of the convolution\ncan be implemented\nby multiplying its input with the \ntransposed weight matrix $\\mathbf{W}^\\top$. Therefore, \nthe transposed convolutional layer\ncan just exchange the forward propagation function\nand the backpropagation function of the convolutional layer:\nits forward propagation \nand backpropagation functions\nmultiply their input vector with \n$\\mathbf{W}^\\top$ and $\\mathbf{W}$, respectively."
    },
    {
      "chunk_id": "090c9ea6ef6c_0",
      "chapter": "transposed-conv",
      "heading": "Summary",
      "text": "* In contrast to the regular convolution that reduces input elements via the kernel, the transposed convolution broadcasts input elements via the kernel, thereby producing an output that is larger than the input.\n* If we feed $\\mathsf{X}$ into a convolutional layer $f$ to output $\\mathsf{Y}=f(\\mathsf{X})$ and create a transposed convolutional layer $g$ with the same hyperparameters as $f$ except for the number of output channels being the number of channels in $\\mathsf{X}$, then $g(Y)$ will have the same shape as $\\mathsf{X}$.\n* We can implement convolutions using matrix multiplications. The transposed convolutional layer can just exchange the forward propagation function and the backpropagation function of the convolutional layer."
    },
    {
      "chunk_id": "c34a46c09a54_0",
      "chapter": "transposed-conv",
      "heading": "Exercises",
      "text": "1. In :numref:`subsec-connection-to-mat-transposition`, the convolution input `X` and the transposed convolution output `Z` have the same shape. Do they have the same value? Why?\n1. Is it efficient to use matrix multiplications to implement convolutions? Why?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/376)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1450)\n:end_tab:"
    },
    {
      "chunk_id": "f2582c85e316_0",
      "chapter": "alexnet",
      "heading": "alexnet",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Deep Convolutional Neural Networks (AlexNet)\n:label:`sec_alexnet`\n\n\nAlthough CNNs were well known\nin the computer vision and machine learning communities\nfollowing the introduction of LeNet :cite:`LeCun.Jackel.Bottou.ea.1995`,\nthey did not immediately dominate the field. Although LeNet achieved good results on early small datasets,\nthe performance and feasibility of training CNNs\non larger, more realistic datasets had yet to be established. In fact, for much of the intervening time between the early 1990s\nand the watershed results of 2012 :cite:`Krizhevsky.Sutskever.Hinton.2012`,\nneural networks were often surpassed by other machine learning methods,\nsuch as kernel methods :cite:`Scholkopf.Smola.2002`, ensemble methods :cite:`Freund.Schapire.ea.1996`,\nand structured estimation :cite:`Taskar.Guestrin.Koller.2004`. For computer vision, this comparison is perhaps not entirely accurate. That is, although the inputs to convolutional networks\nconsist of raw or lightly-processed (e.g., by centering) pixel values, practitioners would never feed raw pixels into traditional models. Instead, typical computer vision pipelines\nconsisted of manually engineering feature extraction pipelines, such as SIFT :cite:`Lowe.2004`, SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`, and bags of visual words :cite:`Sivic.Zisserman.2003`. Rather than *learning* the features, the features were *crafted*. Most of the progress came from having more clever ideas for feature extraction on the one hand and deep insight into geometry :cite:`Hartley.Zisserman.2000` on the other. The learning algorithm was often considered an afterthought. Although some neural network accelerators were available in the 1990s,\nthey were not yet sufficiently powerful to make\ndeep multichannel, multilayer CNNs\nwith a large number of parameters."
    },
    {
      "chunk_id": "f2582c85e316_1",
      "chapter": "alexnet",
      "heading": "alexnet",
      "text": "The learning algorithm was often considered an afterthought. Although some neural network accelerators were available in the 1990s,\nthey were not yet sufficiently powerful to make\ndeep multichannel, multilayer CNNs\nwith a large number of parameters. For instance, NVIDIA's GeForce 256 from 1999\nwas able to process at most 480 million floating-point operations, such as additions and multiplications, per second (MFLOPS), without any meaningful\nprogramming framework for operations beyond games. Today's accelerators are able to perform in excess of 1000 TFLOPs per device. Moreover, datasets were still relatively small: OCR on 60,000 low-resolution $28 \\times 28$ pixel images was considered a highly challenging task. Added to these obstacles, key tricks for training neural networks\nincluding parameter initialization heuristics :cite:`Glorot.Bengio.2010`,\nclever variants of stochastic gradient descent :cite:`Kingma.Ba.2014`,\nnon-squashing activation functions :cite:`Nair.Hinton.2010`,\nand effective regularization techniques :cite:`Srivastava.Hinton.Krizhevsky.ea.2014` were still missing. Thus, rather than training *end-to-end* (pixel to classification) systems,\nclassical pipelines looked more like this:\n\n1. Obtain an interesting dataset. In the early days, these datasets required expensive sensors. For instance, the [Apple QuickTake 100](https://en.wikipedia.org/wiki/Apple_QuickTake) of 1994 sported a whopping 0.3 megapixel (VGA) resolution, capable of storing up to 8 images, all for the price of \\$1000. 1. Preprocess the dataset with hand-crafted features based on some knowledge of optics, geometry, other analytic tools, and occasionally on the serendipitous discoveries by lucky graduate students. 1. Feed the data through a standard set of feature extractors such as the SIFT (scale-invariant feature transform) :cite:`Lowe.2004`, the SURF (speeded up robust features) :cite:`Bay.Tuytelaars.Van-Gool.2006`, or any number of other hand-tuned pipelines. OpenCV still provides SIFT extractors to this day! 1."
    },
    {
      "chunk_id": "f2582c85e316_2",
      "chapter": "alexnet",
      "heading": "alexnet",
      "text": "OpenCV still provides SIFT extractors to this day! 1. Dump the resulting representations into your favorite classifier, likely a linear model or kernel method, to train a classifier. If you spoke to machine learning researchers,\nthey would reply that machine learning was both important and beautiful. Elegant theories proved the properties of various classifiers :cite:`boucheron2005theory` and convex\noptimization :cite:`Boyd.Vandenberghe.2004` had become the mainstay for obtaining them. The field of machine learning was thriving, rigorous, and eminently useful. However,\nif you spoke to a computer vision researcher,\nyou would hear a very different story. The dirty truth of image recognition, they would tell you,\nis that features, geometry :cite:`Hartley.Zisserman.2000,hartley2009global`, and engineering,\nrather than novel learning algorithms, drove progress. Computer vision researchers justifiably believed\nthat a slightly bigger or cleaner dataset\nor a slightly improved feature-extraction pipeline\nmattered far more to the final accuracy than any learning algorithm. ```{.python .input  n=2}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, init, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input  n=3}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input  n=4}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "51068f45e303_0",
      "chapter": "alexnet",
      "heading": "Representation Learning",
      "text": "Another way to cast the state of affairs is that\nthe most important part of the pipeline was the representation. And up until 2012 the representation was calculated mostly mechanically. In fact, engineering a new set of feature functions, improving results, and writing up the method\nall featured prominently in papers. SIFT :cite:`Lowe.2004`,\nSURF :cite:`Bay.Tuytelaars.Van-Gool.2006`,\nHOG (histograms of oriented gradient) :cite:`Dalal.Triggs.2005`,\nbags of visual words :cite:`Sivic.Zisserman.2003`,\nand similar feature extractors ruled the roost. Another group of researchers,\nincluding Yann LeCun, Geoff Hinton, Yoshua Bengio,\nAndrew Ng, Shun-ichi Amari, and Juergen Schmidhuber,\nhad different plans. They believed that features themselves ought to be learned. Moreover, they believed that to be reasonably complex,\nthe features ought to be hierarchically composed\nwith multiple jointly learned layers, each with learnable parameters. In the case of an image, the lowest layers might come\nto detect edges, colors, and textures, by analogy with how the visual system in animals\nprocesses its input. In particular, the automatic design of visual features such as those obtained\nby sparse coding :cite:`olshausen1996emergence` remained an open challenge until the advent of modern CNNs. It was not until :citet:`Dean.Corrado.Monga.ea.2012,le2013building` that the idea of generating features\nfrom image data automatically gained significant traction. The first modern CNN :cite:`Krizhevsky.Sutskever.Hinton.2012`, named\n*AlexNet* after one of its inventors, Alex Krizhevsky, is largely an evolutionary improvement\nover LeNet. It achieved excellent performance in the 2012 ImageNet challenge. ![Image filters learned by the first layer of AlexNet. Reproduction courtesy of :citet:`Krizhevsky.Sutskever.Hinton.2012`.](../img/filters.png)\n:width:`400px`\n:label:`fig_filters`\n\nInterestingly, in the lowest layers of the network,\nthe model learned feature extractors that resembled some traditional filters."
    },
    {
      "chunk_id": "51068f45e303_1",
      "chapter": "alexnet",
      "heading": "Representation Learning",
      "text": "Reproduction courtesy of :citet:`Krizhevsky.Sutskever.Hinton.2012`.](../img/filters.png)\n:width:`400px`\n:label:`fig_filters`\n\nInterestingly, in the lowest layers of the network,\nthe model learned feature extractors that resembled some traditional filters. :numref:`fig_filters`\nshows lower-level image descriptors. Higher layers in the network might build upon these representations\nto represent larger structures, like eyes, noses, blades of grass, and so on. Even higher layers might represent whole objects\nlike people, airplanes, dogs, or frisbees. Ultimately, the final hidden state learns a compact representation\nof the image that summarizes its contents\nsuch that data belonging to different categories can be easily separated. AlexNet (2012) and its precursor LeNet (1995) share many architectural elements. This begs the question: why did it take so long? A key difference was that, over the previous two decades, the amount of data and the computing power available had increased significantly. As such AlexNet was much larger: it was trained on much more data, and on much faster GPUs compared to the CPUs available in 1995."
    },
    {
      "chunk_id": "91909e25f4e0_0",
      "chapter": "alexnet",
      "heading": "Missing Ingredient: Data",
      "text": "Deep models with many layers require large amounts of data\nin order to enter the regime\nwhere they significantly outperform traditional methods\nbased on convex optimizations (e.g., linear and kernel methods).\nHowever, given the limited storage capacity of computers,\nthe relative expense of (imaging) sensors,\nand the comparatively tighter research budgets in the 1990s,\nmost research relied on tiny datasets.\nNumerous papers relied on the UCI collection of datasets,\nmany of which contained only hundreds or (a few) thousands of images\ncaptured in low resolution and often with an artificially clean background.\n\nIn 2009, the ImageNet dataset was released :cite:`Deng.Dong.Socher.ea.2009`,\nchallenging researchers to learn models from 1 million examples,\n1000 each from 1000 distinct categories of objects. The categories themselves\nwere based on the most popular noun nodes in WordNet :cite:`Miller.1995`.\nThe ImageNet team used Google Image Search to prefilter large candidate sets\nfor each category and employed\nthe Amazon Mechanical Turk crowdsourcing pipeline\nto confirm for each image whether it belonged to the associated category.\nThis scale was unprecedented, exceeding others by over an order of magnitude\n(e.g., CIFAR-100 has 60,000 images). Another aspect was that the images were at\nrelatively high resolution of $224 \\times 224$ pixels, unlike the 80 million-sized\nTinyImages dataset :cite:`Torralba.Fergus.Freeman.2008`, consisting of $32 \\times 32$ pixel thumbnails.\nThis allowed for the formation of higher-level features.\nThe associated competition, dubbed the ImageNet Large Scale Visual Recognition\nChallenge :cite:`russakovsky2015imagenet`,\npushed computer vision and machine learning research forward,\nchallenging researchers to identify which models performed best\nat a greater scale than academics had previously considered. The largest vision datasets, such as LAION-5B\n:cite:`schuhmann2022laion` contain billions of images with additional metadata."
    },
    {
      "chunk_id": "7ff9af824977_0",
      "chapter": "alexnet",
      "heading": "Missing Ingredient: Hardware",
      "text": "Deep learning models are voracious consumers of compute cycles. Training can take hundreds of epochs, and each iteration\nrequires passing data through many layers of computationally expensive\nlinear algebra operations. This is one of the main reasons why in the 1990s and early 2000s,\nsimple algorithms based on the more-efficiently optimized\nconvex objectives were preferred. *Graphical processing units* (GPUs) proved to be a game changer\nin making deep learning feasible. These chips had earlier been developed for accelerating\ngraphics processing to benefit computer games. In particular, they were optimized for high throughput $4 \\times 4$\nmatrix--vector products, which are needed for many computer graphics tasks. Fortunately, the math is strikingly similar\nto that required for calculating convolutional layers. Around that time, NVIDIA and ATI had begun optimizing GPUs\nfor general computing operations :cite:`Fernando.2004`,\ngoing as far as to market them as *general-purpose GPUs* (GPGPUs). To provide some intuition, consider the cores of a modern microprocessor\n(CPU). Each of the cores is fairly powerful running at a high clock frequency\nand sporting large caches (up to several megabytes of L3). Each core is well-suited to executing a wide range of instructions,\nwith branch predictors, a deep pipeline, specialized execution units,\nspeculative execution,\nand many other bells and whistles\nthat enable it to run a large variety of programs with sophisticated control flow. This apparent strength, however, is also its Achilles heel:\ngeneral-purpose cores are very expensive to build. They excel at general-purpose\ncode with lots of control flow. This requires lots of chip area, not just for the\nactual ALU (arithmetic logical unit) where computation happens, but also for\nall the aforementioned bells and whistles, plus\nmemory interfaces, caching logic between cores,\nhigh-speed interconnects, and so on. CPUs are\ncomparatively bad at any single task when compared with dedicated hardware."
    },
    {
      "chunk_id": "7ff9af824977_1",
      "chapter": "alexnet",
      "heading": "Missing Ingredient: Hardware",
      "text": "CPUs are\ncomparatively bad at any single task when compared with dedicated hardware. Modern laptops have 4--8 cores,\nand even high-end servers rarely exceed 64 cores per socket,\nsimply because it is not cost-effective. By comparison, GPUs can consist of thousands of small processing elements (NIVIDA's latest Ampere chips have up to 6912 CUDA cores), often grouped into larger groups (NVIDIA calls them warps). The details differ somewhat between NVIDIA, AMD, ARM and other chip vendors. While each core is relatively weak,\nrunning at about 1GHz clock frequency,\nit is the total number of such cores that makes GPUs orders of magnitude faster than CPUs. For instance, NVIDIA's recent Ampere A100 GPU offers over 300 TFLOPs per chip for specialized 16-bit precision (BFLOAT16) matrix-matrix multiplications, and up to 20 TFLOPs for more general-purpose floating point operations (FP32). At the same time, floating point performance of CPUs rarely exceeds 1 TFLOPs. For instance, Amazon's Graviton 3  reaches 2 TFLOPs peak performance for 16-bit precision operations, a number similar to the GPU performance of Apple's M1 processor. There are many reasons why GPUs are much faster than CPUs in terms of FLOPs. First, power consumption tends to grow *quadratically* with clock frequency. Hence, for the power budget of a CPU core that runs four times faster (a typical number),\nyou can use 16 GPU cores at $\\frac{1}{4}$ the speed,\nwhich yields $16 \\times \\frac{1}{4} = 4$ times the performance. Second, GPU cores are much simpler\n(in fact, for a long time they were not even *able*\nto execute general-purpose code),\nwhich makes them more energy efficient. For instance, (i) they tend not to support speculative evaluation, (ii) it typically is not possible to program each processing element individually, and (iii) the caches per core tend to be much smaller. Last, many operations in deep learning require high memory bandwidth. Again, GPUs shine here with buses that are at least 10 times as wide as many CPUs. Back to 2012."
    },
    {
      "chunk_id": "7ff9af824977_2",
      "chapter": "alexnet",
      "heading": "Missing Ingredient: Hardware",
      "text": "Last, many operations in deep learning require high memory bandwidth. Again, GPUs shine here with buses that are at least 10 times as wide as many CPUs. Back to 2012. A major breakthrough came\nwhen Alex Krizhevsky and Ilya Sutskever\nimplemented a deep CNN\nthat could run on GPUs. They realized that the computational bottlenecks in CNNs,\nconvolutions and matrix multiplications,\nare all operations that could be parallelized in hardware. Using two NVIDIA GTX 580s with 3GB of memory, either of which was capable of 1.5 TFLOPs (still a challenge for most CPUs a decade later),\nthey implemented fast convolutions. The [cuda-convnet](https://code.google.com/archive/p/cuda-convnet/) code\nwas good enough that for several years\nit was the industry standard and powered\nthe first couple of years of the deep learning boom."
    },
    {
      "chunk_id": "b1874e6190b4_0",
      "chapter": "alexnet",
      "heading": "AlexNet",
      "text": "AlexNet, which employed an 8-layer CNN,\nwon the ImageNet Large Scale Visual Recognition Challenge 2012\nby a large margin :cite:`Russakovsky.Deng.Huang.ea.2013`.\nThis network showed, for the first time,\nthat the features obtained by learning can transcend manually-designed features, breaking the previous paradigm in computer vision.\n\nThe architectures of AlexNet and LeNet are strikingly similar,\nas :numref:`fig_alexnet` illustrates.\nNote that we provide a slightly streamlined version of AlexNet\nremoving some of the design quirks that were needed in 2012\nto make the model fit on two small GPUs.\n\n![From LeNet (left) to AlexNet (right).](../img/alexnet.svg)\n:label:`fig_alexnet`\n\nThere are also significant differences between AlexNet and LeNet.\nFirst, AlexNet is much deeper than the comparatively small LeNet-5.\nAlexNet consists of eight layers: five convolutional layers,\ntwo fully connected hidden layers, and one fully connected output layer.\nSecond, AlexNet used the ReLU instead of the sigmoid\nas its activation function. Let's delve into the details below."
    },
    {
      "chunk_id": "210458adc421_0",
      "chapter": "alexnet",
      "heading": "Architecture",
      "text": "In AlexNet's first layer, the convolution window shape is $11\\times11$.\nSince the images in ImageNet are eight times taller and wider\nthan the MNIST images,\nobjects in ImageNet data tend to occupy more pixels with more visual detail.\nConsequently, a larger convolution window is needed to capture the object.\nThe convolution window shape in the second layer\nis reduced to $5\\times5$, followed by $3\\times3$.\nIn addition, after the first, second, and fifth convolutional layers,\nthe network adds max-pooling layers\nwith a window shape of $3\\times3$ and a stride of 2.\nMoreover, AlexNet has ten times more convolution channels than LeNet.\n\nAfter the final convolutional layer, there are two huge fully connected layers\nwith 4096 outputs.\nThese layers require nearly 1GB model parameters.\nBecause of the limited memory in early GPUs,\nthe original AlexNet used a dual data stream design,\nso that each of their two GPUs could be responsible\nfor storing and computing only its half of the model.\nFortunately, GPU memory is comparatively abundant now,\nso we rarely need to break up models across GPUs these days\n(our version of the AlexNet model deviates\nfrom the original paper in this aspect)."
    },
    {
      "chunk_id": "936111b4877f_0",
      "chapter": "alexnet",
      "heading": "Activation Functions",
      "text": "Furthermore, AlexNet changed the sigmoid activation function to a simpler ReLU activation function. On the one hand, the computation of the ReLU activation function is simpler. For example, it does not have the exponentiation operation found in the sigmoid activation function.\n On the other hand, the ReLU activation function makes model training easier when using different parameter initialization methods. This is because, when the output of the sigmoid activation function is very close to 0 or 1, the gradient of these regions is almost 0, so that backpropagation cannot continue to update some of the model parameters. By contrast, the gradient of the ReLU activation function in the positive interval is always 1 (:numref:`subsec_activation-functions`). Therefore, if the model parameters are not properly initialized, the sigmoid function may obtain a gradient of almost 0 in the positive interval, meaning that the model cannot be effectively trained."
    },
    {
      "chunk_id": "5eb7d6d36bc8_0",
      "chapter": "alexnet",
      "heading": "Capacity Control and Preprocessing",
      "text": "AlexNet controls the model complexity of the fully connected layer\nby dropout (:numref:`sec_dropout`),\nwhile LeNet only uses weight decay. To augment the data even further, the training loop of AlexNet\nadded a great deal of image augmentation,\nsuch as flipping, clipping, and color changes. This makes the model more robust and the larger sample size effectively reduces overfitting. See :citet:`Buslaev.Iglovikov.Khvedchenya.ea.2020` for an in-depth review of such preprocessing steps."
    },
    {
      "chunk_id": "5eb7d6d36bc8_1",
      "chapter": "alexnet",
      "heading": "Capacity Control and Preprocessing",
      "text": "This makes the model more robust and the larger sample size effectively reduces overfitting. See :citet:`Buslaev.Iglovikov.Khvedchenya.ea.2020` for an in-depth review of such preprocessing steps. ```{.python .input  n=5}\n%%tab pytorch, mxnet, tensorflow\nclass AlexNet(d2l.Classifier):\n    def __init__(self, lr=0.1, num_classes=10):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Sequential()\n            self.net.add(\n                nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n                nn.MaxPool2D(pool_size=3, strides=2),\n                nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n                nn.MaxPool2D(pool_size=3, strides=2),\n                nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n                nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n                nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n                nn.MaxPool2D(pool_size=3, strides=2),\n                nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n                nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n                nn.Dense(num_classes))\n            self.net.initialize(init.Xavier())\n        if tab.selected('pytorch'):\n            self.net = nn.Sequential(\n                nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n                nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n                nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n                nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n                nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n                nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n                nn.LazyLinear(num_classes))\n            self.net.apply(d2l.init_cnn)\n        if tab.selected('tensorflow'):\n            self.net = tf.keras.models.Sequential([\n                tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4,\n                                       activation='relu'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n                tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same',\n                                       activation='relu'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n                tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n                                       activation='relu'),\n                tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same',\n                                       activation='relu'),\n                tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same',\n                                       activation='relu'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n                tf.keras.layers.Flatten(),\n                tf.keras.layers.Dense(4096, activation='relu'),\n                tf.keras.layers.Dropout(0.5),\n                tf.keras.layers.Dense(4096, activation='relu'),\n                tf.keras.layers.Dropout(0.5),\n                tf.keras.layers.Dense(num_classes)])\n```\n\n```{.python .input}\n%%tab jax\nclass AlexNet(d2l.Classifier):\n    lr: float = 0.1\n    num_classes: int = 10\n    training: bool = True\n\n    def setup(self):\n        self.net = nn.Sequential([\n            nn.Conv(features=96, kernel_size=(11, 11), strides=4, padding=1),\n            nn.relu,\n            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2)),\n            nn.Conv(features=256, kernel_size=(5, 5)),\n            nn.relu,\n            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2)),\n            nn.Conv(features=384, kernel_size=(3, 3)), nn.relu,\n            nn.Conv(features=384, kernel_size=(3, 3)), nn.relu,\n            nn.Conv(features=256, kernel_size=(3, 3)), nn.relu,\n            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2)),\n            lambda x: x.reshape((x.shape[0], -1)),  # flatten\n            nn.Dense(features=4096),\n            nn.relu,\n            nn.Dropout(0.5, deterministic=not self.training),\n            nn.Dense(features=4096),\n            nn.relu,\n            nn.Dropout(0.5, deterministic=not self.training),\n            nn.Dense(features=self.num_classes)\n        ])\n```\n\nWe [**construct a single-channel data example**] with both height and width of 224 (**to observe the output shape of each layer**)."
    },
    {
      "chunk_id": "5eb7d6d36bc8_2",
      "chapter": "alexnet",
      "heading": "Capacity Control and Preprocessing",
      "text": "It matches the AlexNet architecture in :numref:`fig_alexnet`. ```{.python .input  n=6}\n%%tab pytorch, mxnet\nAlexNet().layer_summary((1, 1, 224, 224))\n```\n\n```{.python .input  n=7}\n%%tab tensorflow\nAlexNet().layer_summary((1, 224, 224, 1))\n```\n\n```{.python .input}\n%%tab jax\nAlexNet(training=False).layer_summary((1, 224, 224, 1))\n```"
    },
    {
      "chunk_id": "4e472464496a_0",
      "chapter": "alexnet",
      "heading": "Training",
      "text": "Although AlexNet was trained on ImageNet in :citet:`Krizhevsky.Sutskever.Hinton.2012`,\nwe use Fashion-MNIST here\nsince training an ImageNet model to convergence could take hours or days\neven on a modern GPU.\nOne of the problems with applying AlexNet directly on [**Fashion-MNIST**]\nis that its (**images have lower resolution**) ($28 \\times 28$ pixels)\n(**than ImageNet images.**)\nTo make things work, (**we upsample them to $224 \\times 224$**).\nThis is generally not a smart practice, as it simply increases the computational\ncomplexity without adding information. Nonetheless, we do it here to be faithful to the AlexNet architecture.\nWe perform this resizing with the `resize` argument in the `d2l.FashionMNIST` constructor.\n\nNow, we can [**start training AlexNet.**]\nCompared to LeNet in :numref:`sec_lenet`,\nthe main change here is the use of a smaller learning rate\nand much slower training due to the deeper and wider network,\nthe higher image resolution, and the more costly convolutions.\n\n```{.python .input  n=8}\n%%tab pytorch, mxnet, jax\nmodel = AlexNet(lr=0.01)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ntrainer.fit(model, data)\n```\n\n```{.python .input  n=9}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\nwith d2l.try_gpu():\n    model = AlexNet(lr=0.01)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "d0ce88619467_0",
      "chapter": "alexnet",
      "heading": "Discussion",
      "text": "AlexNet's structure bears a striking resemblance to LeNet, with a number of critical improvements, both for accuracy (dropout) and for ease of training (ReLU). What is equally striking is the amount of progress that has been made in terms of deep learning tooling. What was several months of work in 2012 can now be accomplished in a dozen lines of code using any modern framework.\n\nReviewing the architecture, we see that AlexNet has an Achilles heel when it comes to efficiency: the last two hidden layers require matrices of size $6400 \\times 4096$ and $4096 \\times 4096$, respectively. This corresponds to 164 MB of memory and 81 MFLOPs of computation, both of which are a nontrivial outlay, especially on smaller devices, such as mobile phones. This is one of the reasons why AlexNet has been surpassed by much more effective architectures that we will cover in the following sections. Nonetheless, it is a key step from shallow to deep networks that are used nowadays. Note that even though the number of parameters exceeds by far the amount of training data in our experiments (the last two layers have more than 40 million parameters, trained on a datasets of 60 thousand images), there is hardly any overfitting: training and validation loss are virtually identical throughout training. This is due to the improved regularization, such as dropout, inherent in modern deep network designs.\n\nAlthough it seems that there are only a few more lines in AlexNet's implementation than in LeNet's, it took the academic community many years to embrace this conceptual change and take advantage of its excellent experimental results. This was also due to the lack of efficient computational tools. At the time neither DistBelief :cite:`Dean.Corrado.Monga.ea.2012` nor Caffe :cite:`Jia.Shelhamer.Donahue.ea.2014` existed, and Theano :cite:`Bergstra.Breuleux.Bastien.ea.2010` still lacked many distinguishing features. It was the availability of TensorFlow :cite:`Abadi.Barham.Chen.ea.2016` that dramatically changed the situation."
    },
    {
      "chunk_id": "463bbbbd76c4_0",
      "chapter": "alexnet",
      "heading": "Exercises",
      "text": "1. Following up on the discussion above, analyze the computational properties of AlexNet.\n    1. Compute the memory footprint for convolutions and fully connected layers, respectively. Which one dominates?\n    1. Calculate the computational cost for the convolutions and the fully connected layers.\n    1. How does the memory (read and write bandwidth, latency, size) affect computation? Is there any difference in its effects for training and inference?\n1. You are a chip designer and need to trade off computation and memory bandwidth. For example, a faster chip requires more power and possibly a larger chip area. More memory bandwidth requires more pins and control logic, thus also more area. How do you optimize?\n1. Why do engineers no longer report performance benchmarks on AlexNet?\n1. Try increasing the number of epochs when training AlexNet. Compared with LeNet, how do the results differ? Why?\n1. AlexNet may be too complex for the Fashion-MNIST dataset, in particular due to the low resolution of the initial images.\n    1. Try simplifying the model to make the training faster, while ensuring that the accuracy does not drop significantly.\n    1. Design a better model that works directly on $28 \\times 28$ images.\n1. Modify the batch size, and observe the changes in throughput (images/s), accuracy, and GPU memory.\n1. Apply dropout and ReLU to LeNet-5. Does it improve? Can you improve things further by preprocessing to take advantage of the invariances inherent in the images?\n1. Can you make AlexNet overfit? Which feature do you need to remove or change to break training?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/75)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/76)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/276)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18001)\n:end_tab:"
    },
    {
      "chunk_id": "ef5cf6106afd_0",
      "chapter": "batch-norm",
      "heading": "batch-norm",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Batch Normalization\n:label:`sec_batch_norm`\n\nTraining deep neural networks is difficult.\nGetting them to converge in a reasonable amount of time can be tricky.\nIn this section, we describe *batch normalization*, a popular and effective technique\nthat consistently accelerates the convergence of deep networks :cite:`Ioffe.Szegedy.2015`.\nTogether with residual blocks---covered later in :numref:`sec_resnet`---batch normalization\nhas made it possible for practitioners to routinely train networks with over 100 layers.\nA secondary (serendipitous) benefit of batch normalization lies in its inherent regularization.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx, init\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom functools import partial\nfrom jax import numpy as jnp\nimport jax\nimport optax\n```"
    },
    {
      "chunk_id": "cbd982759a07_0",
      "chapter": "batch-norm",
      "heading": "Training Deep Networks",
      "text": "When working with data, we often preprocess before training. Choices regarding data preprocessing often make an enormous difference in the final results. Recall our application of MLPs to predicting house prices (:numref:`sec_kaggle_house`). Our first step when working with real data\nwas to standardize our input features to have\nzero mean $\\boldsymbol{\\mu} = 0$ and unit variance $\\boldsymbol{\\Sigma} = \\boldsymbol{1}$ across multiple observations :cite:`friedman1987exploratory`, frequently rescaling the latter so  that the diagonal is unity, i.e., $\\Sigma_{ii} = 1$. Yet another strategy is to rescale vectors to unit length, possibly zero mean *per observation*. This can work well, e.g., for spatial sensor data. These preprocessing techniques and many others, are\nbeneficial for keeping the estimation problem well controlled. For a review of feature selection and extraction see the article of :citet:`guyon2008feature`, for example. Standardizing vectors also has the nice side-effect of constraining the function complexity of functions that act upon it. For instance, the celebrated radius-margin bound :cite:`Vapnik95` in support vector machines and the Perceptron Convergence Theorem :cite:`Novikoff62` rely on inputs of bounded norm. Intuitively, this standardization plays nicely with our optimizers\nsince it puts the parameters *a priori* on a similar scale. As such, it is only natural to ask whether a corresponding normalization step *inside* a deep network\nmight not be beneficial. While this is not quite the reasoning that led to the invention of batch normalization :cite:`Ioffe.Szegedy.2015`, it is a useful way of understanding it and its cousin, layer normalization :cite:`Ba.Kiros.Hinton.2016`, within a unified framework."
    },
    {
      "chunk_id": "cbd982759a07_1",
      "chapter": "batch-norm",
      "heading": "Training Deep Networks",
      "text": "While this is not quite the reasoning that led to the invention of batch normalization :cite:`Ioffe.Szegedy.2015`, it is a useful way of understanding it and its cousin, layer normalization :cite:`Ba.Kiros.Hinton.2016`, within a unified framework. Second, for a typical MLP or CNN, as we train,\nthe variables \nin intermediate layers (e.g., affine transformation outputs in MLP)\nmay take values with widely varying magnitudes:\nwhether along the layers from input to output, across units in the same layer,\nand over time due to our updates to the model parameters. The inventors of batch normalization postulated informally\nthat this drift in the distribution of such variables could hamper the convergence of the network. Intuitively, we might conjecture that if one\nlayer has variable activations that are 100 times that of another layer,\nthis might necessitate compensatory adjustments in the learning rates. Adaptive solvers\nsuch as AdaGrad :cite:`Duchi.Hazan.Singer.2011`, Adam :cite:`Kingma.Ba.2014`, Yogi :cite:`Zaheer.Reddi.Sachan.ea.2018`, or Distributed Shampoo :cite:`anil2020scalable` aim to address this from the viewpoint of optimization, e.g., by adding aspects of second-order methods. The alternative is to prevent the problem from occurring, simply by adaptive normalization. Third, deeper networks are complex and tend to be more liable to overfitting. This means that regularization becomes more critical. A common technique for regularization is noise\ninjection. This has been known for a long time, e.g., with regard to noise injection for the\ninputs :cite:`Bishop.1995`. It also forms the basis of dropout in :numref:`sec_dropout`. As it turns out, quite serendipitously, batch normalization conveys all three benefits: preprocessing, numerical stability, and regularization."
    },
    {
      "chunk_id": "cbd982759a07_2",
      "chapter": "batch-norm",
      "heading": "Training Deep Networks",
      "text": "It also forms the basis of dropout in :numref:`sec_dropout`. As it turns out, quite serendipitously, batch normalization conveys all three benefits: preprocessing, numerical stability, and regularization. Batch normalization is applied to individual layers, or optionally, to all of them:\nIn each training iteration,\nwe first normalize the inputs (of batch normalization)\nby subtracting their mean and\ndividing by their standard deviation,\nwhere both are estimated based on the statistics of the current minibatch. Next, we apply a scale coefficient and an offset to recover the lost degrees\nof freedom. It is precisely due to this *normalization* based on *batch* statistics\nthat *batch normalization* derives its name. Note that if we tried to apply batch normalization with minibatches of size 1,\nwe would not be able to learn anything. That is because after subtracting the means,\neach hidden unit would take value 0. As you might guess, since we are devoting a whole section to batch normalization,\nwith large enough minibatches the approach proves effective and stable. One takeaway here is that when applying batch normalization,\nthe choice of batch size is\neven more significant than without batch normalization, or at least,\nsuitable calibration is needed as we might adjust batch size. Denote by $\\mathcal{B}$ a minibatch and let $\\mathbf{x} \\in \\mathcal{B}$ be an input to \nbatch normalization ($\\textrm{BN}$). In this case the batch normalization is defined as follows:\n\n$$\\textrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n:eqlabel:`eq_batchnorm`\n\nIn :eqref:`eq_batchnorm`,\n$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ is the  sample mean\nand $\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}$ is the sample standard deviation of the minibatch $\\mathcal{B}$. After applying standardization,\nthe resulting minibatch\nhas zero mean and unit variance. The choice of unit variance\n(rather than some other magic number) is arbitrary."
    },
    {
      "chunk_id": "cbd982759a07_3",
      "chapter": "batch-norm",
      "heading": "Training Deep Networks",
      "text": "After applying standardization,\nthe resulting minibatch\nhas zero mean and unit variance. The choice of unit variance\n(rather than some other magic number) is arbitrary. We recover this degree of freedom\nby including an elementwise\n*scale parameter* $\\boldsymbol{\\gamma}$ and *shift parameter* $\\boldsymbol{\\beta}$\nthat have the same shape as $\\mathbf{x}$. Both are parameters that\nneed to be learned as part of model training. The variable magnitudes\nfor intermediate layers cannot diverge during training\nsince batch normalization actively centers and rescales them back\nto a given mean and size (via $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$). Practical experience confirms that, as alluded to when discussing feature rescaling, batch normalization seems to allow for more aggressive learning rates. We calculate $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$ in :eqref:`eq_batchnorm` as follows:\n\n$$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B} = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}\n\\textrm{ and }\n\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon.$$\n\nNote that we add a small constant $\\epsilon > 0$\nto the variance estimate\nto ensure that we never attempt division by zero,\neven in cases where the empirical variance estimate might be very small or vanish. The estimates $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$ counteract the scaling issue\nby using noisy estimates of mean and variance. You might think that this noisiness should be a problem. On the contrary, it is actually beneficial. This turns out to be a recurring theme in deep learning. For reasons that are not yet well-characterized theoretically,\nvarious sources of noise in optimization\noften lead to faster training and less overfitting:\nthis variation appears to act as a form of regularization."
    },
    {
      "chunk_id": "cbd982759a07_4",
      "chapter": "batch-norm",
      "heading": "Training Deep Networks",
      "text": "For reasons that are not yet well-characterized theoretically,\nvarious sources of noise in optimization\noften lead to faster training and less overfitting:\nthis variation appears to act as a form of regularization. :citet:`Teye.Azizpour.Smith.2018` and :citet:`Luo.Wang.Shao.ea.2018`\nrelated the properties of batch normalization to Bayesian priors and penalties, respectively. In particular, this sheds some light on the puzzle\nof why batch normalization works best for moderate minibatch sizes in the 50--100 range. This particular size of minibatch seems to inject just the \"right amount\" of noise per layer, both in terms of scale via $\\hat{\\boldsymbol{\\sigma}}$, and in terms of offset via $\\hat{\\boldsymbol{\\mu}}$: a\nlarger minibatch regularizes less due to the more stable estimates, whereas tiny minibatches\ndestroy useful signal due to high variance. Exploring this direction further, considering alternative types\nof preprocessing and filtering may yet lead to other effective types of regularization. Fixing a trained model, you might think\nthat we would prefer using the entire dataset\nto estimate the mean and variance. Once training is complete, why would we want\nthe same image to be classified differently,\ndepending on the batch in which it happens to reside? During training, such exact calculation is infeasible\nbecause the intermediate variables\nfor all data examples\nchange every time we update our model. However, once the model is trained,\nwe can calculate the means and variances\nof each layer's variables based on the entire dataset. Indeed this is standard practice for\nmodels employing batch normalization;\nthus batch normalization layers function differently\nin *training mode* (normalizing by minibatch statistics)\nthan in *prediction mode* (normalizing by dataset statistics). In this form they closely resemble the behavior of dropout regularization of :numref:`sec_dropout`,\nwhere noise is only injected during training."
    },
    {
      "chunk_id": "5cbeb9702cc6_0",
      "chapter": "batch-norm",
      "heading": "Batch Normalization Layers",
      "text": "Batch normalization implementations for fully connected layers\nand convolutional layers are slightly different.\nOne key difference between batch normalization and other layers\nis that because the former operates on a full minibatch at a time,\nwe cannot just ignore the batch dimension\nas we did before when introducing other layers."
    },
    {
      "chunk_id": "dfcb0352efcd_0",
      "chapter": "batch-norm",
      "heading": "Fully Connected Layers",
      "text": "When applying batch normalization to fully connected layers,\n:citet:`Ioffe.Szegedy.2015`, in their original paper inserted batch normalization after the affine transformation\nand *before* the nonlinear activation function. Later applications experimented with\ninserting batch normalization right *after* activation functions.\nDenoting the input to the fully connected layer by $\\mathbf{x}$,\nthe affine transformation\nby $\\mathbf{W}\\mathbf{x} + \\mathbf{b}$ (with the weight parameter $\\mathbf{W}$ and the bias parameter $\\mathbf{b}$),\nand the activation function by $\\phi$,\nwe can express the computation of a batch-normalization-enabled,\nfully connected layer output $\\mathbf{h}$ as follows:\n\n$$\\mathbf{h} = \\phi(\\textrm{BN}(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) ).$$\n\nRecall that mean and variance are computed\non the *same* minibatch\non which the transformation is applied."
    },
    {
      "chunk_id": "ee5693f6b29d_0",
      "chapter": "batch-norm",
      "heading": "Convolutional Layers",
      "text": "Similarly, with convolutional layers,\nwe can apply batch normalization after the convolution\nbut before the nonlinear activation function. The key difference from batch normalization\nin fully connected layers is that we apply the operation on a per-channel basis\n*across all locations*. This is compatible with our assumption of translation\ninvariance that led to convolutions: we assumed that the specific location of a pattern\nwithin an image was not critical for the purpose of understanding.\n\nAssume that our minibatches contain $m$ examples\nand that for each channel,\nthe output of the convolution has height $p$ and width $q$.\nFor convolutional layers, we carry out each batch normalization\nover the $m \\cdot p \\cdot q$ elements per output channel simultaneously.\nThus, we collect the values over all spatial locations\nwhen computing the mean and variance\nand consequently\napply the same mean and variance\nwithin a given channel\nto normalize the value at each spatial location.\nEach channel has its own scale and shift parameters,\nboth of which are scalars."
    },
    {
      "chunk_id": "171a45632b36_0",
      "chapter": "batch-norm",
      "heading": "Layer Normalization",
      "text": ":label:`subsec_layer-normalization-in-bn`\n\nNote that in the context of convolutions the batch normalization is well defined even for\nminibatches of size 1: after all, we have all the locations across an image to average. Consequently,\nmean and variance are well defined, even if it is just within a single observation. This consideration\nled :citet:`Ba.Kiros.Hinton.2016` to introduce the notion of *layer normalization*. It works just like\na batch norm, only that it is applied to one observation at a time. Consequently both the offset and the scaling factor are scalars. For an $n$-dimensional vector $\\mathbf{x}$, layer norms are given by \n\n$$\\mathbf{x} \\rightarrow \\textrm{LN}(\\mathbf{x}) =  \\frac{\\mathbf{x} - \\hat{\\mu}}{\\hat\\sigma},$$\n\nwhere scaling and offset are applied coefficient-wise\nand given by \n\n$$\\hat{\\mu} \\stackrel{\\textrm{def}}{=} \\frac{1}{n} \\sum_{i=1}^n x_i \\textrm{ and }\n\\hat{\\sigma}^2 \\stackrel{\\textrm{def}}{=} \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 + \\epsilon.$$\n\nAs before we add a small offset $\\epsilon > 0$ to prevent division by zero. One of the major benefits of using layer normalization is that it prevents divergence. After all, ignoring $\\epsilon$, the output of the layer normalization is scale independent. That is, we have $\\textrm{LN}(\\mathbf{x}) \\approx \\textrm{LN}(\\alpha \\mathbf{x})$ for any choice of $\\alpha \\neq 0$. This becomes an equality for $|\\alpha| \\to \\infty$ (the approximate equality is due to the offset $\\epsilon$ for the variance). \n\nAnother advantage of the layer normalization is that it does not depend on the minibatch size. It is also independent of whether we are in training or test regime. In other words, it is simply a deterministic transformation that standardizes the activations to a given scale. This can be very beneficial in preventing divergence in optimization. We skip further details and recommend that interested readers consult the original paper."
    },
    {
      "chunk_id": "85aae02ddc4e_0",
      "chapter": "batch-norm",
      "heading": "Batch Normalization During Prediction",
      "text": "As we mentioned earlier, batch normalization typically behaves differently\nin training mode than in prediction mode.\nFirst, the noise in the sample mean and the sample variance\narising from estimating each on minibatches\nis no longer desirable once we have trained the model.\nSecond, we might not have the luxury\nof computing per-batch normalization statistics.\nFor example,\nwe might need to apply our model to make one prediction at a time.\n\nTypically, after training, we use the entire dataset\nto compute stable estimates of the variable statistics\nand then fix them at prediction time.\nHence, batch normalization behaves differently during training than at test time.\nRecall that dropout also exhibits this characteristic."
    },
    {
      "chunk_id": "1b880b4ce57f_0",
      "chapter": "batch-norm",
      "heading": "(**Implementation from Scratch**)",
      "text": "To see how batch normalization works in practice, we implement one from scratch below. ```{.python .input}\n%%tab mxnet\ndef batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    # Use autograd to determine whether we are in training mode\n    if not autograd.is_training():\n        # In prediction mode, use mean and variance obtained by moving average\n        X_hat = (X - moving_mean) / np.sqrt(moving_var + eps)\n    else:\n        assert len(X.shape) in (2, 4)\n        if len(X.shape) == 2:\n            # When using a fully connected layer, calculate the mean and\n            # variance on the feature dimension\n            mean = X.mean(axis=0)\n            var = ((X - mean) ** 2).mean(axis=0)\n        else:\n            # When using a two-dimensional convolutional layer, calculate the\n            # mean and variance on the channel dimension (axis=1)."
    },
    {
      "chunk_id": "1b880b4ce57f_1",
      "chapter": "batch-norm",
      "heading": "(**Implementation from Scratch**)",
      "text": "Here we\n            # need to maintain the shape of X, so that the broadcasting\n            # operation can be carried out later\n            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n        # In training mode, the current mean and variance are used \n        X_hat = (X - mean) / np.sqrt(var + eps)\n        # Update the mean and variance using moving average\n        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n        moving_var = (1.0 - momentum) * moving_var + momentum * var\n    Y = gamma * X_hat + beta  # Scale and shift\n    return Y, moving_mean, moving_var\n```\n\n```{.python .input}\n%%tab pytorch\ndef batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n    # Use is_grad_enabled to determine whether we are in training mode\n    if not torch.is_grad_enabled():\n        # In prediction mode, use mean and variance obtained by moving average\n        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n    else:\n        assert len(X.shape) in (2, 4)\n        if len(X.shape) == 2:\n            # When using a fully connected layer, calculate the mean and\n            # variance on the feature dimension\n            mean = X.mean(dim=0)\n            var = ((X - mean) ** 2).mean(dim=0)\n        else:\n            # When using a two-dimensional convolutional layer, calculate the\n            # mean and variance on the channel dimension (axis=1)."
    },
    {
      "chunk_id": "1b880b4ce57f_2",
      "chapter": "batch-norm",
      "heading": "(**Implementation from Scratch**)",
      "text": "Here we\n            # need to maintain the shape of X, so that the broadcasting\n            # operation can be carried out later\n            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n        # In training mode, the current mean and variance are used \n        X_hat = (X - mean) / torch.sqrt(var + eps)\n        # Update the mean and variance using moving average\n        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n        moving_var = (1.0 - momentum) * moving_var + momentum * var\n    Y = gamma * X_hat + beta  # Scale and shift\n    return Y, moving_mean.data, moving_var.data\n```\n\n```{.python .input}\n%%tab tensorflow\ndef batch_norm(X, gamma, beta, moving_mean, moving_var, eps):\n    # Compute reciprocal of square root of the moving variance elementwise\n    inv = tf.cast(tf.math.rsqrt(moving_var + eps), X.dtype)\n    # Scale and shift\n    inv *= gamma\n    Y = X * inv + (beta - moving_mean * inv)\n    return Y\n```\n\n```{.python .input}\n%%tab jax\ndef batch_norm(X, deterministic, gamma, beta, moving_mean, moving_var, eps,\n               momentum):\n    # Use `deterministic` to determine whether the current mode is training\n    # mode or prediction mode\n    if deterministic:\n        # In prediction mode, use mean and variance obtained by moving average\n        # `linen.Module.variables` have a `value` attribute containing the array\n        X_hat = (X - moving_mean.value) / jnp.sqrt(moving_var.value + eps)\n    else:\n        assert len(X.shape) in (2, 4)\n        if len(X.shape) == 2:\n            # When using a fully connected layer, calculate the mean and\n            # variance on the feature dimension\n            mean = X.mean(axis=0)\n            var = ((X - mean) ** 2).mean(axis=0)\n        else:\n            # When using a two-dimensional convolutional layer, calculate the\n            # mean and variance on the channel dimension (axis=1)."
    },
    {
      "chunk_id": "1b880b4ce57f_3",
      "chapter": "batch-norm",
      "heading": "(**Implementation from Scratch**)",
      "text": "Here we\n            # need to maintain the shape of `X`, so that the broadcasting\n            # operation can be carried out later\n            mean = X.mean(axis=(0, 2, 3), keepdims=True)\n            var = ((X - mean) ** 2).mean(axis=(0, 2, 3), keepdims=True)\n        # In training mode, the current mean and variance are used\n        X_hat = (X - mean) / jnp.sqrt(var + eps)\n        # Update the mean and variance using moving average\n        moving_mean.value = momentum * moving_mean.value + (1.0 - momentum) * mean\n        moving_var.value = momentum * moving_var.value + (1.0 - momentum) * var\n    Y = gamma * X_hat + beta  # Scale and shift\n    return Y\n```\n\nWe can now [**create a proper `BatchNorm` layer.**]\nOur layer will maintain proper parameters\nfor scale `gamma` and shift `beta`,\nboth of which will be updated in the course of training. Additionally, our layer will maintain\nmoving averages of the means and variances\nfor subsequent use during model prediction. Putting aside the algorithmic details,\nnote the design pattern underlying our implementation of the layer. Typically, we define the mathematics in a separate function, say `batch_norm`. We then integrate this functionality into a custom layer,\nwhose code mostly addresses bookkeeping matters,\nsuch as moving data to the right device context,\nallocating and initializing any required variables,\nkeeping track of moving averages (here for mean and variance), and so on. This pattern enables a clean separation of mathematics from boilerplate code. Also note that for the sake of convenience\nwe did not worry about automatically inferring the input shape here;\nthus we need to specify the number of features throughout. By now all modern deep learning frameworks offer automatic detection of size and shape in the\nhigh-level batch normalization APIs (in practice we will use this instead)."
    },
    {
      "chunk_id": "1b880b4ce57f_4",
      "chapter": "batch-norm",
      "heading": "(**Implementation from Scratch**)",
      "text": "By now all modern deep learning frameworks offer automatic detection of size and shape in the\nhigh-level batch normalization APIs (in practice we will use this instead). ```{.python .input}\n%%tab mxnet\nclass BatchNorm(nn.Block):\n    # `num_features`: the number of outputs for a fully connected layer\n    # or the number of output channels for a convolutional layer. `num_dims`:\n    # 2 for a fully connected layer and 4 for a convolutional layer\n    def __init__(self, num_features, num_dims, **kwargs):\n        super().__init__(**kwargs)\n        if num_dims == 2:\n            shape = (1, num_features)\n        else:\n            shape = (1, num_features, 1, 1)\n        # The scale parameter and the shift parameter (model parameters) are\n        # initialized to 1 and 0, respectively\n        self.gamma = self.params.get('gamma', shape=shape, init=init.One())\n        self.beta = self.params.get('beta', shape=shape, init=init.Zero())\n        # The variables that are not model parameters are initialized to 0 and\n        # 1\n        self.moving_mean = np.zeros(shape)\n        self.moving_var = np.ones(shape)\n\n    def forward(self, X):\n        # If `X` is not on the main memory, copy `moving_mean` and\n        # `moving_var` to the device where `X` is located\n        if self.moving_mean.ctx != X.ctx:\n            self.moving_mean = self.moving_mean.copyto(X.ctx)\n            self.moving_var = self.moving_var.copyto(X.ctx)\n        # Save the updated `moving_mean` and `moving_var`\n        Y, self.moving_mean, self.moving_var = batch_norm(\n            X, self.gamma.data(), self.beta.data(), self.moving_mean,\n            self.moving_var, eps=1e-12, momentum=0.1)\n        return Y\n```\n\n```{.python .input}\n%%tab pytorch\nclass BatchNorm(nn.Module):\n    # num_features: the number of outputs for a fully connected layer or the\n    # number of output channels for a convolutional layer."
    },
    {
      "chunk_id": "1b880b4ce57f_5",
      "chapter": "batch-norm",
      "heading": "(**Implementation from Scratch**)",
      "text": "num_dims: 2 for a\n    # fully connected layer and 4 for a convolutional layer\n    def __init__(self, num_features, num_dims):\n        super().__init__()\n        if num_dims == 2:\n            shape = (1, num_features)\n        else:\n            shape = (1, num_features, 1, 1)\n        # The scale parameter and the shift parameter (model parameters) are\n        # initialized to 1 and 0, respectively\n        self.gamma = nn.Parameter(torch.ones(shape))\n        self.beta = nn.Parameter(torch.zeros(shape))\n        # The variables that are not model parameters are initialized to 0 and\n        # 1\n        self.moving_mean = torch.zeros(shape)\n        self.moving_var = torch.ones(shape)\n\n    def forward(self, X):\n        # If X is not on the main memory, copy moving_mean and moving_var to\n        # the device where X is located\n        if self.moving_mean.device != X.device:\n            self.moving_mean = self.moving_mean.to(X.device)\n            self.moving_var = self.moving_var.to(X.device)\n        # Save the updated moving_mean and moving_var\n        Y, self.moving_mean, self.moving_var = batch_norm(\n            X, self.gamma, self.beta, self.moving_mean,\n            self.moving_var, eps=1e-5, momentum=0.1)\n        return Y\n```\n\n```{.python .input}\n%%tab tensorflow\nclass BatchNorm(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(BatchNorm, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        weight_shape = [input_shape[-1], ]\n        # The scale parameter and the shift parameter (model parameters) are\n        # initialized to 1 and 0, respectively\n        self.gamma = self.add_weight(name='gamma', shape=weight_shape,\n            initializer=tf.initializers.ones, trainable=True)\n        self.beta = self.add_weight(name='beta', shape=weight_shape,\n            initializer=tf.initializers.zeros, trainable=True)\n        # The variables that are not model parameters are initialized to 0\n        self.moving_mean = self.add_weight(name='moving_mean',\n            shape=weight_shape, initializer=tf.initializers.zeros,\n            trainable=False)\n        self.moving_variance = self.add_weight(name='moving_variance',\n            shape=weight_shape, initializer=tf.initializers.ones,\n            trainable=False)\n        super(BatchNorm, self).build(input_shape)\n\n    def assign_moving_average(self, variable, value):\n        momentum = 0.1\n        delta = (1.0 - momentum) * variable + momentum * value\n        return variable.assign(delta)\n\n    @tf.function\n    def call(self, inputs, training):\n        if training:\n            axes = list(range(len(inputs.shape) - 1))\n            batch_mean = tf.reduce_mean(inputs, axes, keepdims=True)\n            batch_variance = tf.reduce_mean(tf.math.squared_difference(\n                inputs, tf.stop_gradient(batch_mean)), axes, keepdims=True)\n            batch_mean = tf.squeeze(batch_mean, axes)\n            batch_variance = tf.squeeze(batch_variance, axes)\n            mean_update = self.assign_moving_average(\n                self.moving_mean, batch_mean)\n            variance_update = self.assign_moving_average(\n                self.moving_variance, batch_variance)\n            self.add_update(mean_update)\n            self.add_update(variance_update)\n            mean, variance = batch_mean, batch_variance\n        else:\n            mean, variance = self.moving_mean, self.moving_variance\n        output = batch_norm(inputs, moving_mean=mean, moving_var=variance,\n            beta=self.beta, gamma=self.gamma, eps=1e-5)\n        return output\n```\n\n```{.python .input}\n%%tab jax\nclass BatchNorm(nn.Module):\n    # `num_features`: the number of outputs for a fully connected layer\n    # or the number of output channels for a convolutional layer."
    },
    {
      "chunk_id": "1b880b4ce57f_6",
      "chapter": "batch-norm",
      "heading": "(**Implementation from Scratch**)",
      "text": "# `num_dims`: 2 for a fully connected layer and 4 for a convolutional layer\n    # Use `deterministic` to determine whether the current mode is training\n    # mode or prediction mode\n    num_features: int\n    num_dims: int\n    deterministic: bool = False\n\n    @nn.compact\n    def __call__(self, X):\n        if self.num_dims == 2:\n            shape = (1, self.num_features)\n        else:\n            shape = (1, 1, 1, self.num_features)\n\n        # The scale parameter and the shift parameter (model parameters) are\n        # initialized to 1 and 0, respectively\n        gamma = self.param('gamma', jax.nn.initializers.ones, shape)\n        beta = self.param('beta', jax.nn.initializers.zeros, shape)\n\n        # The variables that are not model parameters are initialized to 0 and\n        # 1. Save them to the 'batch_stats' collection\n        moving_mean = self.variable('batch_stats', 'moving_mean', jnp.zeros, shape)\n        moving_var = self.variable('batch_stats', 'moving_var', jnp.ones, shape)\n        Y = batch_norm(X, self.deterministic, gamma, beta,\n                       moving_mean, moving_var, eps=1e-5, momentum=0.9)\n\n        return Y\n```\n\nWe used `momentum` to govern the aggregation over past mean and variance estimates. This is somewhat of a misnomer as it has nothing whatsoever to do with the *momentum* term of optimization. Nonetheless, it is the commonly adopted name for this term and in deference to API naming convention we use the same variable name in our code."
    },
    {
      "chunk_id": "f6d03159ae73_0",
      "chapter": "batch-norm",
      "heading": "[**LeNet with Batch Normalization**]",
      "text": "To see how to apply `BatchNorm` in context,\nbelow we apply it to a traditional LeNet model (:numref:`sec_lenet`). Recall that batch normalization is applied\nafter the convolutional layers or fully connected layers\nbut before the corresponding activation functions."
    },
    {
      "chunk_id": "f6d03159ae73_1",
      "chapter": "batch-norm",
      "heading": "[**LeNet with Batch Normalization**]",
      "text": "Recall that batch normalization is applied\nafter the convolutional layers or fully connected layers\nbut before the corresponding activation functions. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass BNLeNetScratch(d2l.Classifier):\n    def __init__(self, lr=0.1, num_classes=10):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Sequential()\n            self.net.add(\n                nn.Conv2D(6, kernel_size=5), BatchNorm(6, num_dims=4),\n                nn.Activation('sigmoid'),\n                nn.AvgPool2D(pool_size=2, strides=2),\n                nn.Conv2D(16, kernel_size=5), BatchNorm(16, num_dims=4),\n                nn.Activation('sigmoid'),\n                nn.AvgPool2D(pool_size=2, strides=2), nn.Dense(120),\n                BatchNorm(120, num_dims=2), nn.Activation('sigmoid'),\n                nn.Dense(84), BatchNorm(84, num_dims=2),\n                nn.Activation('sigmoid'), nn.Dense(num_classes))\n            self.initialize()\n        if tab.selected('pytorch'):\n            self.net = nn.Sequential(\n                nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),\n                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n                nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),\n                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n                nn.Flatten(), nn.LazyLinear(120),\n                BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\n                BatchNorm(84, num_dims=2), nn.Sigmoid(),\n                nn.LazyLinear(num_classes))\n        if tab.selected('tensorflow'):\n            self.net = tf.keras.models.Sequential([\n                tf.keras.layers.Conv2D(filters=6, kernel_size=5,\n                                       input_shape=(28, 28, 1)),\n                BatchNorm(), tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n                tf.keras.layers.Conv2D(filters=16, kernel_size=5),\n                BatchNorm(), tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n                tf.keras.layers.Flatten(), tf.keras.layers.Dense(120),\n                BatchNorm(), tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.Dense(84), BatchNorm(),\n                tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.Dense(num_classes)])\n```\n\n```{.python .input}\n%%tab jax\nclass BNLeNetScratch(d2l.Classifier):\n    lr: float = 0.1\n    num_classes: int = 10\n    training: bool = True\n\n    def setup(self):\n        self.net = nn.Sequential([\n            nn.Conv(6, kernel_size=(5, 5)),\n            BatchNorm(6, num_dims=4, deterministic=not self.training),\n            nn.sigmoid,\n            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n            nn.Conv(16, kernel_size=(5, 5)),\n            BatchNorm(16, num_dims=4, deterministic=not self.training),\n            nn.sigmoid,\n            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n            lambda x: x.reshape((x.shape[0], -1)),\n            nn.Dense(120),\n            BatchNorm(120, num_dims=2, deterministic=not self.training),\n            nn.sigmoid,\n            nn.Dense(84),\n            BatchNorm(84, num_dims=2, deterministic=not self.training),\n            nn.sigmoid,\n            nn.Dense(self.num_classes)])\n```\n\n:begin_tab:`jax`\nSince `BatchNorm` layers need to calculate the batch statistics\n(mean and variance), Flax keeps track of the `batch_stats` dictionary, updating\nthem with every minibatch."
    },
    {
      "chunk_id": "f6d03159ae73_2",
      "chapter": "batch-norm",
      "heading": "[**LeNet with Batch Normalization**]",
      "text": "Collections like `batch_stats` can be stored in the\n`TrainState` object (in the `d2l.Trainer` class defined in\n:numref:`oo-design-training`) as an attribute and during the model's forward pass,\nthese should be passed to the `mutable` argument, so that Flax returns the mutated\nvariables. :end_tab:\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.Classifier)  #@save\n@partial(jax.jit, static_argnums=(0, 5))\ndef loss(self, params, X, Y, state, averaged=True):\n    Y_hat, updates = state.apply_fn({'params': params,\n                                     'batch_stats': state.batch_stats},\n                                    *X, mutable=['batch_stats'],\n                                    rngs={'dropout': state.dropout_rng})\n    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n    Y = d2l.reshape(Y, (-1,))\n    fn = optax.softmax_cross_entropy_with_integer_labels\n    return (fn(Y_hat, Y).mean(), updates) if averaged else (fn(Y_hat, Y), updates)\n```\n\nAs before, we will [**train our network on the Fashion-MNIST dataset**]. This code is virtually identical to that when we first trained LeNet. ```{.python .input}\n%%tab mxnet, pytorch, jax\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128)\nmodel = BNLeNetScratch(lr=0.1)\nif tab.selected('pytorch'):\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128)\nwith d2l.try_gpu():\n    model = BNLeNetScratch(lr=0.5)\n    trainer.fit(model, data)\n```\n\nLet's [**have a look at the scale parameter `gamma`\nand the shift parameter `beta`**] learned\nfrom the first batch normalization layer."
    },
    {
      "chunk_id": "f6d03159ae73_3",
      "chapter": "batch-norm",
      "heading": "[**LeNet with Batch Normalization**]",
      "text": "```{.python .input}\n%%tab mxnet\nmodel.net[1].gamma.data().reshape(-1,), model.net[1].beta.data().reshape(-1,)\n```\n\n```{.python .input}\n%%tab pytorch\nmodel.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reshape(model.net.layers[1].gamma, (-1,)), tf.reshape(\n    model.net.layers[1].beta, (-1,))\n```\n\n```{.python .input}\n%%tab jax\ntrainer.state.params['net']['layers_1']['gamma'].reshape((-1,)), \\\ntrainer.state.params['net']['layers_1']['beta'].reshape((-1,))\n```"
    },
    {
      "chunk_id": "2eb29d12835b_0",
      "chapter": "batch-norm",
      "heading": "[**Concise Implementation**]",
      "text": "Compared with the `BatchNorm` class,\nwhich we just defined ourselves,\nwe can use the `BatchNorm` class defined in high-level APIs from the deep learning framework directly. The code looks virtually identical\nto our implementation above, except that we no longer need to provide additional arguments for it to get the dimensions right."
    },
    {
      "chunk_id": "2eb29d12835b_1",
      "chapter": "batch-norm",
      "heading": "[**Concise Implementation**]",
      "text": "The code looks virtually identical\nto our implementation above, except that we no longer need to provide additional arguments for it to get the dimensions right. ```{.python .input}\n%%tab pytorch, tensorflow, mxnet\nclass BNLeNet(d2l.Classifier):\n    def __init__(self, lr=0.1, num_classes=10):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Sequential()\n            self.net.add(\n                nn.Conv2D(6, kernel_size=5), nn.BatchNorm(),\n                nn.Activation('sigmoid'),\n                nn.AvgPool2D(pool_size=2, strides=2),\n                nn.Conv2D(16, kernel_size=5), nn.BatchNorm(),\n                nn.Activation('sigmoid'),\n                nn.AvgPool2D(pool_size=2, strides=2),\n                nn.Dense(120), nn.BatchNorm(), nn.Activation('sigmoid'),\n                nn.Dense(84), nn.BatchNorm(), nn.Activation('sigmoid'),\n                nn.Dense(num_classes))\n            self.initialize()\n        if tab.selected('pytorch'):\n            self.net = nn.Sequential(\n                nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n                nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n                nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n                nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n                nn.Sigmoid(), nn.LazyLinear(num_classes))\n        if tab.selected('tensorflow'):\n            self.net = tf.keras.models.Sequential([\n                tf.keras.layers.Conv2D(filters=6, kernel_size=5,\n                                       input_shape=(28, 28, 1)),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n                tf.keras.layers.Conv2D(filters=16, kernel_size=5),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n                tf.keras.layers.Flatten(), tf.keras.layers.Dense(120),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.Dense(84),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Activation('sigmoid'),\n                tf.keras.layers.Dense(num_classes)])\n```\n\n```{.python .input}\n%%tab jax\nclass BNLeNet(d2l.Classifier):\n    lr: float = 0.1\n    num_classes: int = 10\n    training: bool = True\n\n    def setup(self):\n        self.net = nn.Sequential([\n            nn.Conv(6, kernel_size=(5, 5)),\n            nn.BatchNorm(not self.training),\n            nn.sigmoid,\n            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n            nn.Conv(16, kernel_size=(5, 5)),\n            nn.BatchNorm(not self.training),\n            nn.sigmoid,\n            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n            lambda x: x.reshape((x.shape[0], -1)),\n            nn.Dense(120),\n            nn.BatchNorm(not self.training),\n            nn.sigmoid,\n            nn.Dense(84),\n            nn.BatchNorm(not self.training),\n            nn.sigmoid,\n            nn.Dense(self.num_classes)])\n```\n\nBelow, we [**use the same hyperparameters to train our model.**]\nNote that as usual, the high-level API variant runs much faster\nbecause its code has been compiled to C++ or CUDA\nwhile our custom implementation must be interpreted by Python."
    },
    {
      "chunk_id": "2eb29d12835b_2",
      "chapter": "batch-norm",
      "heading": "[**Concise Implementation**]",
      "text": "```{.python .input}\n%%tab mxnet, pytorch, jax\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128)\nmodel = BNLeNet(lr=0.1)\nif tab.selected('pytorch'):\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128)\nwith d2l.try_gpu():\n    model = BNLeNet(lr=0.5)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "6742285200cf_0",
      "chapter": "batch-norm",
      "heading": "Discussion",
      "text": "Intuitively, batch normalization is thought\nto make the optimization landscape smoother. However, we must be careful to distinguish between\nspeculative intuitions and true explanations\nfor the phenomena that we observe when training deep models. Recall that we do not even know why simpler\ndeep neural networks (MLPs and conventional CNNs)\ngeneralize well in the first place. Even with dropout and weight decay,\nthey remain so flexible that their ability to generalize to unseen data\nlikely needs significantly more refined learning-theoretic generalization guarantees. The original paper proposing batch normalization :cite:`Ioffe.Szegedy.2015`, in addition to introducing a powerful and useful tool,\noffered an explanation for why it works:\nby reducing *internal covariate shift*. Presumably by *internal covariate shift* they\nmeant something like the intuition expressed above---the\nnotion that the distribution of variable values changes\nover the course of training. However, there were two problems with this explanation:\ni) This drift is very different from *covariate shift*,\nrendering the name a misnomer. If anything, it is closer to concept drift. ii) The explanation offers an under-specified intuition\nbut leaves the question of *why precisely this technique works*\nan open question wanting for a rigorous explanation. Throughout this book, we aim to convey the intuitions that practitioners\nuse to guide their development of deep neural networks. However, we believe that it is important\nto separate these guiding intuitions\nfrom established scientific fact. Eventually, when you master this material\nand start writing your own research papers\nyou will want to be clear to delineate\nbetween technical claims and hunches. Following the success of batch normalization,\nits explanation in terms of *internal covariate shift*\nhas repeatedly surfaced in debates in the technical literature\nand broader discourse about how to present machine learning research."
    },
    {
      "chunk_id": "6742285200cf_1",
      "chapter": "batch-norm",
      "heading": "Discussion",
      "text": "Following the success of batch normalization,\nits explanation in terms of *internal covariate shift*\nhas repeatedly surfaced in debates in the technical literature\nand broader discourse about how to present machine learning research. In a memorable speech given while accepting a Test of Time Award\nat the 2017 NeurIPS conference,\nAli Rahimi used *internal covariate shift*\nas a focal point in an argument likening\nthe modern practice of deep learning to alchemy. Subsequently, the example was revisited in detail\nin a position paper outlining\ntroubling trends in machine learning :cite:`Lipton.Steinhardt.2018`. Other authors\nhave proposed alternative explanations for the success of batch normalization,\nsome :cite:`Santurkar.Tsipras.Ilyas.ea.2018`\nclaiming that batch normalization's success comes despite exhibiting behavior\nthat is in some ways opposite to those claimed in the original paper. We note that the *internal covariate shift*\nis no more worthy of criticism than any of\nthousands of similarly vague claims\nmade every year in the technical machine learning literature. Likely, its resonance as a focal point of these debates\nowes to its broad recognizability for the target audience. Batch normalization has proven an indispensable method,\napplied in nearly all deployed image classifiers,\nearning the paper that introduced the technique\ntens of thousands of citations. We conjecture, though, that the guiding principles\nof regularization through noise injection, acceleration through rescaling and lastly preprocessing\nmay well lead to further inventions of layers and techniques in the future. On a more practical note, there are a number of aspects worth remembering about batch normalization:\n\n* During model training, batch normalization continuously adjusts the intermediate output of\n  the network by utilizing the mean and standard deviation of the minibatch, so that the\n  values of the intermediate output in each layer throughout the neural network are more stable."
    },
    {
      "chunk_id": "6742285200cf_2",
      "chapter": "batch-norm",
      "heading": "Discussion",
      "text": "* Batch normalization is slightly different for fully connected layers than for convolutional layers. In fact,\n  for convolutional layers, layer normalization can sometimes be used as an alternative. * Like a dropout layer, batch normalization layers have different behaviors\n  in training mode than in prediction mode. * Batch normalization is useful for regularization and improving convergence in optimization. By contrast,\n  the original motivation of reducing internal covariate shift seems not to be a valid explanation. * For more robust models that are less sensitive to input perturbations, consider removing batch normalization :cite:`wang2022removing`."
    },
    {
      "chunk_id": "e78734f8a97c_0",
      "chapter": "batch-norm",
      "heading": "Exercises",
      "text": "1. Should we remove the bias parameter from the fully connected layer or the convolutional layer before the batch normalization? Why?\n1. Compare the learning rates for LeNet with and without batch normalization.\n    1. Plot the increase in validation accuracy.\n    1. How large can you make the learning rate before the optimization fails in both cases?\n1. Do we need batch normalization in every layer? Experiment with it.\n1. Implement a \"lite\" version of batch normalization that only removes the mean, or alternatively one that\n   only removes the variance. How does it behave?\n1. Fix the parameters `beta` and `gamma`. Observe and analyze the results.\n1. Can you replace dropout by batch normalization? How does the behavior change?\n1. Research ideas: think of other normalization transforms that you can apply:\n    1. Can you apply the probability integral transform?\n    1. Can you use a full-rank covariance estimate? Why should you probably not do that? \n    1. Can you use other compact matrix variants (block-diagonal, low-displacement rank, Monarch, etc.)?\n    1. Does a sparsification compression act as a regularizer?\n    1. Are there other projections (e.g., convex cone, symmetry group-specific transforms) that you can use?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/83)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/84)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/330)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18005)\n:end_tab:"
    },
    {
      "chunk_id": "5004ce5172f4_0",
      "chapter": "cnn-design",
      "heading": "cnn-design",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Designing Convolution Network Architectures\n:label:`sec_cnn-design`\n\nThe previous sections have taken us on a tour of modern network design for computer vision. Common to all the work we covered was that it greatly relied on the intuition of scientists. Many of the architectures are heavily informed by human creativity and to a much lesser extent by systematic exploration of the design space that deep networks offer. Nonetheless, this *network engineering* approach has been tremendously successful. Ever since AlexNet (:numref:`sec_alexnet`)\nbeat conventional computer vision models on ImageNet,\nit has become popular to construct very deep networks\nby stacking blocks of convolutions, all designed according to the same pattern. In particular, $3 \\times 3$ convolutions were \npopularized by VGG networks (:numref:`sec_vgg`). NiN (:numref:`sec_nin`) showed that even $1 \\times 1$ convolutions could \nbe beneficial by adding local nonlinearities. Moreover, NiN solved the problem of aggregating information at the head of a network \nby aggregating across all locations. GoogLeNet (:numref:`sec_googlenet`) added multiple branches of different convolution width, \ncombining the advantages of VGG and NiN in its Inception block. ResNets (:numref:`sec_resnet`) \nchanged the inductive bias towards the identity mapping (from $f(x) = 0$). This allowed for very deep networks. Almost a decade later, the ResNet design is still popular, a testament to its design. Lastly, ResNeXt (:numref:`subsec_resnext`) added grouped convolutions, offering a better trade-off between parameters and computation. A precursor to Transformers for vision, the Squeeze-and-Excitation Networks (SENets) allow for efficient information transfer between locations\n:cite:`Hu.Shen.Sun.2018`. This was accomplished by computing a per-channel global attention function."
    },
    {
      "chunk_id": "5004ce5172f4_1",
      "chapter": "cnn-design",
      "heading": "cnn-design",
      "text": "A precursor to Transformers for vision, the Squeeze-and-Excitation Networks (SENets) allow for efficient information transfer between locations\n:cite:`Hu.Shen.Sun.2018`. This was accomplished by computing a per-channel global attention function. Up to now we have omitted networks obtained via *neural architecture search* (NAS) :cite:`zoph2016neural,liu2018darts`. We chose to do so since their cost is usually enormous, relying on brute-force search, genetic algorithms, reinforcement learning, or some other form of hyperparameter optimization. Given a fixed search space,\nNAS uses a search strategy to automatically select\nan architecture based on the returned performance estimation. The outcome of NAS\nis a single network instance. EfficientNets are a notable outcome of this search :cite:`tan2019efficientnet`. In the following we discuss an idea that is quite different to the quest for the *single best network*. It is computationally relatively inexpensive, it leads to scientific insights on the way, and it is quite effective in terms of the quality of outcomes. Let's review the strategy by :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` to *design network design spaces*. The strategy combines the strength of manual design and NAS. It accomplishes this by operating on *distributions of networks* and optimizing the distributions in a way to obtain good performance for entire families of networks. The outcome of it are *RegNets*, specifically RegNetX and RegNetY, plus a range of guiding principles for the design of performant CNNs. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx, init\nfrom mxnet.gluon import nn\n\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\nfrom d2l import tensorflow as d2l\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\n```"
    },
    {
      "chunk_id": "ccd602c34a23_0",
      "chapter": "cnn-design",
      "heading": "The AnyNet Design Space",
      "text": ":label:`subsec_the-anynet-design-space`\n\nThe description below closely follows the reasoning in :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` with some abbreviations to make it fit in the scope of the book. To begin, we need a template for the family of networks to explore. One of the commonalities of the designs in this chapter is that the networks consist of a *stem*, a *body* and a *head*. The stem performs initial image processing, often through convolutions with a larger window size. The body consists of multiple blocks, carrying out the bulk of the transformations needed to go from raw images to object representations. Lastly, the head converts this into the desired outputs, such as via a softmax regressor for multiclass classification. The body, in turn, consists of multiple stages, operating on the image at decreasing resolutions. In fact, both the stem and each subsequent stage quarter the spatial resolution. Lastly, each stage consists of one or more blocks. This pattern is common to all networks, from VGG to ResNeXt. Indeed, for the design of generic AnyNet networks, :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` used the ResNeXt block of :numref:`fig_resnext_block`. ![The AnyNet design space. The numbers $(\\mathit{c}, \\mathit{r})$ along each arrow indicate the number of channels $c$ and the resolution $\\mathit{r} \\times \\mathit{r}$ of the images at that point. From left to right: generic network structure composed of stem, body, and head; body composed of four stages; detailed structure of a stage; two alternative structures for blocks, one without downsampling and one that halves the resolution in each dimension. Design choices include depth $\\mathit{d_i}$, the number of output channels $\\mathit{c_i}$, the number of groups $\\mathit{g_i}$, and bottleneck ratio $\\mathit{k_i}$ for any stage $\\mathit{i}$.](../img/anynet.svg)\n:label:`fig_anynet_full`\n\nLet's review the structure outlined in :numref:`fig_anynet_full` in detail. As mentioned, an AnyNet consists of a stem, body, and head."
    },
    {
      "chunk_id": "ccd602c34a23_1",
      "chapter": "cnn-design",
      "heading": "The AnyNet Design Space",
      "text": "As mentioned, an AnyNet consists of a stem, body, and head. The stem takes as its input RGB images (3 channels), using a $3 \\times 3$ convolution with a stride of $2$, followed by a batch norm, to halve the resolution from $r \\times r$ to $r/2 \\times r/2$. Moreover, it generates $c_0$ channels that serve as input to the body. Since the network is designed to work well with ImageNet images of shape $224 \\times 224 \\times 3$, the body serves to reduce this to $7 \\times 7 \\times c_4$ through 4 stages (recall that $224 / 2^{1+4} = 7$), each with an eventual stride of $2$. Lastly, the head employs an entirely standard design via global average pooling, similar to NiN (:numref:`sec_nin`), followed by a fully connected layer to emit an $n$-dimensional vector for $n$-class classification. Most of the relevant design decisions are inherent to the body of the network. It proceeds in stages, where each stage is composed of the same type of ResNeXt blocks as we discussed in :numref:`subsec_resnext`. The design there is again entirely generic: we begin with a block that halves the resolution by using a stride of $2$ (the rightmost in :numref:`fig_anynet_full`). To match this, the residual branch of the ResNeXt block needs to pass through a $1 \\times 1$ convolution. This block is followed by a variable number of additional ResNeXt blocks that leave both resolution and the number of channels unchanged. Note that a common design practice is to add a slight bottleneck in the design of convolutional blocks. As such, with bottleneck ratio $k_i \\geq 1$ we afford some number of channels, $c_i/k_i$,  within each block for stage $i$ (as the experiments show, this is not really effective and should be skipped). Lastly, since we are dealing with ResNeXt blocks, we also need to pick the number of groups $g_i$ for grouped convolutions at stage $i$."
    },
    {
      "chunk_id": "ccd602c34a23_2",
      "chapter": "cnn-design",
      "heading": "The AnyNet Design Space",
      "text": "Lastly, since we are dealing with ResNeXt blocks, we also need to pick the number of groups $g_i$ for grouped convolutions at stage $i$. This seemingly generic design space provides us nonetheless with many parameters: we can set the block width (number of channels) $c_0, \\ldots c_4$, the depth (number of blocks) per stage $d_1, \\ldots d_4$, the bottleneck ratios $k_1, \\ldots k_4$, and the group widths (numbers of groups) $g_1, \\ldots g_4$. In total this adds up to 17 parameters, resulting in an unreasonably large number of configurations that would warrant exploring. We need some tools to reduce this huge design space effectively. This is where the conceptual beauty of design spaces comes in. Before we do so, let's implement the generic design first."
    },
    {
      "chunk_id": "ccd602c34a23_3",
      "chapter": "cnn-design",
      "heading": "The AnyNet Design Space",
      "text": "We need some tools to reduce this huge design space effectively. This is where the conceptual beauty of design spaces comes in. Before we do so, let's implement the generic design first. ```{.python .input}\n%%tab mxnet\nclass AnyNet(d2l.Classifier):\n    def stem(self, num_channels):\n        net = nn.Sequential()\n        net.add(nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=2),\n                nn.BatchNorm(), nn.Activation('relu'))\n        return net\n```\n\n```{.python .input}\n%%tab pytorch\nclass AnyNet(d2l.Classifier):\n    def stem(self, num_channels):\n        return nn.Sequential(\n            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\n            nn.LazyBatchNorm2d(), nn.ReLU())\n```\n\n```{.python .input}\n%%tab tensorflow\nclass AnyNet(d2l.Classifier):\n    def stem(self, num_channels):\n        return tf.keras.models.Sequential([\n            tf.keras.layers.Conv2D(num_channels, kernel_size=3, strides=2,\n                                   padding='same'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Activation('relu')])\n```\n\n```{.python .input}\n%%tab jax\nclass AnyNet(d2l.Classifier):\n    arch: tuple\n    stem_channels: int\n    lr: float = 0.1\n    num_classes: int = 10\n    training: bool = True\n\n    def setup(self):\n        self.net = self.create_net()\n\n    def stem(self, num_channels):\n        return nn.Sequential([\n            nn.Conv(num_channels, kernel_size=(3, 3), strides=(2, 2),\n                    padding=(1, 1)),\n            nn.BatchNorm(not self.training),\n            nn.relu\n        ])\n```\n\nEach stage consists of `depth` ResNeXt blocks,\nwhere `num_channels` specifies the block width. Note that the first block halves the height and width of input images."
    },
    {
      "chunk_id": "ccd602c34a23_4",
      "chapter": "cnn-design",
      "heading": "The AnyNet Design Space",
      "text": "Note that the first block halves the height and width of input images. ```{.python .input}\n%%tab mxnet\n@d2l.add_to_class(AnyNet)\ndef stage(self, depth, num_channels, groups, bot_mul):\n    net = nn.Sequential()\n    for i in range(depth):\n        if i == 0:\n            net.add(d2l.ResNeXtBlock(\n                num_channels, groups, bot_mul, use_1x1conv=True, strides=2))\n        else:\n            net.add(d2l.ResNeXtBlock(\n                num_channels, num_channels, groups, bot_mul))\n    return net\n```\n\n```{.python .input}\n%%tab pytorch\n@d2l.add_to_class(AnyNet)\ndef stage(self, depth, num_channels, groups, bot_mul):\n    blk = []\n    for i in range(depth):\n        if i == 0:\n            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n                use_1x1conv=True, strides=2))\n        else:\n            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\n    return nn.Sequential(*blk)\n```\n\n```{.python .input}\n%%tab tensorflow\n@d2l.add_to_class(AnyNet)\ndef stage(self, depth, num_channels, groups, bot_mul):\n    net = tf.keras.models.Sequential()\n    for i in range(depth):\n        if i == 0:\n            net.add(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n                use_1x1conv=True, strides=2))\n        else:\n            net.add(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\n    return net\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(AnyNet)\ndef stage(self, depth, num_channels, groups, bot_mul):\n    blk = []\n    for i in range(depth):\n        if i == 0:\n            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n                use_1x1conv=True, strides=(2, 2), training=self.training))\n        else:\n            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n                                        training=self.training))\n    return nn.Sequential(blk)\n```\n\nPutting the network stem, body, and head together,\nwe complete the implementation of AnyNet."
    },
    {
      "chunk_id": "ccd602c34a23_5",
      "chapter": "cnn-design",
      "heading": "The AnyNet Design Space",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(AnyNet)\ndef __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n    super(AnyNet, self).__init__()\n    self.save_hyperparameters()\n    if tab.selected('mxnet'):\n        self.net = nn.Sequential()\n        self.net.add(self.stem(stem_channels))\n        for i, s in enumerate(arch):\n            self.net.add(self.stage(*s))\n        self.net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n        self.net.initialize(init.Xavier())\n    if tab.selected('pytorch'):\n        self.net = nn.Sequential(self.stem(stem_channels))\n        for i, s in enumerate(arch):\n            self.net.add_module(f'stage{i+1}', self.stage(*s))\n        self.net.add_module('head', nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n            nn.LazyLinear(num_classes)))\n        self.net.apply(d2l.init_cnn)\n    if tab.selected('tensorflow'):\n        self.net = tf.keras.models.Sequential(self.stem(stem_channels))\n        for i, s in enumerate(arch):\n            self.net.add(self.stage(*s))\n        self.net.add(tf.keras.models.Sequential([\n            tf.keras.layers.GlobalAvgPool2D(),\n            tf.keras.layers.Dense(units=num_classes)]))\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(AnyNet)\ndef create_net(self):\n    net = nn.Sequential([self.stem(self.stem_channels)])\n    for i, s in enumerate(self.arch):\n        net.layers.extend([self.stage(*s)])\n    net.layers.extend([nn.Sequential([\n        lambda x: nn.avg_pool(x, window_shape=x.shape[1:3],\n                            strides=x.shape[1:3], padding='valid'),\n        lambda x: x.reshape((x.shape[0], -1)),\n        nn.Dense(self.num_classes)])])\n    return net\n```"
    },
    {
      "chunk_id": "592c365581bd_0",
      "chapter": "cnn-design",
      "heading": "Distributions and Parameters of Design Spaces",
      "text": "As just discussed in :numref:`subsec_the-anynet-design-space`, parameters of a design space are hyperparameters of networks in that design space. Consider the problem of identifying good parameters in the AnyNet design space. We could try finding the *single best* parameter choice for a given amount of computation (e.g., FLOPs and compute time). If we allowed for even only *two* possible choices for each parameter, we would have to explore $2^{17} = 131072$ combinations to find the best solution. This is clearly infeasible because of its exorbitant cost. Even worse, we do not really learn anything from this exercise in terms of how one should design a network. Next time we add, say, an X-stage, or a shift operation, or similar, we would need to start from scratch. Even worse, due to the stochasticity in training (rounding, shuffling, bit errors), no two runs are likely to produce exactly the same results. A better strategy would be to try to determine general guidelines of how the choices of parameters should be related. For instance, the bottleneck ratio, the number of channels, blocks, groups, or their change between layers should ideally be governed by a collection of simple rules. The approach in :citet:`radosavovic2019network` relies on the following four assumptions:\n\n1. We assume that general design principles actually exist, so that many networks satisfying these requirements should offer good performance. Consequently, identifying a *distribution* over networks can be a sensible strategy. In other words, we assume that there are many good needles in the haystack. 1. We need not train networks to convergence before we can assess whether a network is good. Instead, it is sufficient to use the intermediate results as reliable guidance for final accuracy. Using (approximate) proxies to optimize an objective is referred to as multi-fidelity optimization :cite:`forrester2007multi`."
    },
    {
      "chunk_id": "592c365581bd_1",
      "chapter": "cnn-design",
      "heading": "Distributions and Parameters of Design Spaces",
      "text": "Instead, it is sufficient to use the intermediate results as reliable guidance for final accuracy. Using (approximate) proxies to optimize an objective is referred to as multi-fidelity optimization :cite:`forrester2007multi`. Consequently, design optimization is carried out, based on the accuracy achieved after only a few passes through the dataset, reducing the cost significantly. 1. Results obtained at a smaller scale (for smaller networks) generalize to larger ones. Consequently, optimization is carried out for networks that are structurally similar, but with a smaller number of blocks, fewer channels, etc. Only in the end will we need to verify that the so-found networks also offer good performance at scale. 1. Aspects of the design can be approximately factorized so that it is possible to infer their effect on the quality of the outcome somewhat independently. In other words, the optimization problem is moderately easy. These assumptions allow us to test many networks cheaply. In particular, we can *sample* uniformly from the space of configurations and evaluate their performance. Subsequently, we can evaluate the quality of the choice of parameters by reviewing the *distribution* of error/accuracy that can be achieved with said networks. Denote by $F(e)$ the cumulative distribution function (CDF) for errors committed by networks of a given design space, drawn using probability disribution $p$. That is, \n\n$$F(e, p) \\stackrel{\\textrm{def}}{=} P_{\\textrm{net} \\sim p} \\{e(\\textrm{net}) \\leq e\\}.$$\n\nOur goal is now to find a distribution $p$ over *networks* such that most networks have a very low error rate and where the support of $p$ is concise. Of course, this is computationally infeasible to perform accurately."
    },
    {
      "chunk_id": "592c365581bd_2",
      "chapter": "cnn-design",
      "heading": "Distributions and Parameters of Design Spaces",
      "text": "Of course, this is computationally infeasible to perform accurately. We resort to a sample of networks $\\mathcal{Z} \\stackrel{\\textrm{def}}{=} \\{\\textrm{net}_1, \\ldots \\textrm{net}_n\\}$ (with errors $e_1, \\ldots, e_n$, respectively) from $p$ and use the empirical CDF $\\hat{F}(e, \\mathcal{Z})$ instead:\n\n$$\\hat{F}(e, \\mathcal{Z}) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}(e_i \\leq e).$$\n\nWhenever the CDF for one set of choices majorizes (or matches) another CDF it follows that its choice of parameters is superior (or indifferent). Accordingly \n:citet:`Radosavovic.Kosaraju.Girshick.ea.2020` experimented with a shared network bottleneck ratio $k_i = k$ for all stages $i$ of the network. This gets rid of three of the four parameters governing the bottleneck ratio. To assess whether this (negatively) affects the performance one can draw networks from the constrained and from the unconstrained distribution and compare the corresonding CDFs. It turns out that this constraint does not affect the accuracy of the distribution of networks at all, as can be seen in the first panel of :numref:`fig_regnet-fig`. Likewise, we could choose to pick the same group width $g_i = g$ occurring at the various stages of the network. Again, this does not affect performance, as can be seen in the second panel of :numref:`fig_regnet-fig`. Both steps combined reduce the number of free parameters by six. ![Comparing error empirical distribution functions of design spaces. $\\textrm{AnyNet}_\\mathit{A}$ is the original design space; $\\textrm{AnyNet}_\\mathit{B}$ ties the bottleneck ratios, $\\textrm{AnyNet}_\\mathit{C}$ also ties group widths, $\\textrm{AnyNet}_\\mathit{D}$ increases the network depth across stages. From left to right: (i) tying bottleneck ratios has no effect on performance; (ii) tying group widths has no effect on performance; (iii) increasing network widths (channels) across stages improves performance; (iv) increasing network depths across stages improves performance."
    },
    {
      "chunk_id": "592c365581bd_3",
      "chapter": "cnn-design",
      "heading": "Distributions and Parameters of Design Spaces",
      "text": "Figure courtesy of :citet:`Radosavovic.Kosaraju.Girshick.ea.2020`.](../img/regnet-fig.png)\n:label:`fig_regnet-fig`\n\nNext we look for ways to reduce the multitude of potential choices for width and depth of the stages. It is a reasonable assumption that, as we go deeper, the number of channels should increase, i.e., $c_i \\geq c_{i-1}$ ($w_{i+1} \\geq w_i$ per their notation in :numref:`fig_regnet-fig`), yielding \n$\\textrm{AnyNetX}_D$. Likewise, it is equally reasonable to assume that as the stages progress, they should become deeper, i.e., $d_i \\geq d_{i-1}$, yielding $\\textrm{AnyNetX}_E$. This can be experimentally verified in the third and fourth panel of :numref:`fig_regnet-fig`, respectively."
    },
    {
      "chunk_id": "837f1589c4f0_0",
      "chapter": "cnn-design",
      "heading": "RegNet",
      "text": "The resulting $\\textrm{AnyNetX}_E$ design space consists of simple networks\nfollowing easy-to-interpret design principles:\n\n* Share the bottleneck ratio $k_i = k$ for all stages $i$;\n* Share the group width $g_i = g$ for all stages $i$;\n* Increase network width across stages: $c_{i} \\leq c_{i+1}$;\n* Increase network depth across stages: $d_{i} \\leq d_{i+1}$. This leaves us with a final set of choices: how to pick the specific values for the above parameters of the eventual $\\textrm{AnyNetX}_E$ design space. By studying the best-performing networks from the distribution in $\\textrm{AnyNetX}_E$ one can observe the following: the width of the network ideally increases linearly with the block index across the network, i.e., $c_j \\approx c_0 + c_a j$, where $j$ is the block index and slope $c_a > 0$. Given that we get to choose a different block width only per stage, we arrive at a piecewise constant function, engineered to match this dependence. Furthermore, experiments also show that a bottleneck ratio of $k = 1$ performs best, i.e., we are advised not to use bottlenecks at all. We recommend the interested reader reviews further details in the design of specific networks for different amounts of computation by perusing :citet:`Radosavovic.Kosaraju.Girshick.ea.2020`. For instance, an effective 32-layer RegNetX variant is given by $k = 1$ (no bottleneck), $g = 16$ (group width is 16), $c_1 = 32$ and $c_2 = 80$ channels for the first and second stage, respectively, chosen to be $d_1=4$ and $d_2=6$ blocks deep. The astonishing insight from the design is that it still applies, even when investigating networks at a larger scale. Even better, it even holds for Squeeze-and-Excitation (SE) network designs (RegNetY) that have a global channel activation :cite:`Hu.Shen.Sun.2018`."
    },
    {
      "chunk_id": "837f1589c4f0_1",
      "chapter": "cnn-design",
      "heading": "RegNet",
      "text": "Even better, it even holds for Squeeze-and-Excitation (SE) network designs (RegNetY) that have a global channel activation :cite:`Hu.Shen.Sun.2018`. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass RegNetX32(AnyNet):\n    def __init__(self, lr=0.1, num_classes=10):\n        stem_channels, groups, bot_mul = 32, 16, 1\n        depths, channels = (4, 6), (32, 80)\n        super().__init__(\n            ((depths[0], channels[0], groups, bot_mul),\n             (depths[1], channels[1], groups, bot_mul)),\n            stem_channels, lr, num_classes)\n```\n\n```{.python .input}\n%%tab jax\nclass RegNetX32(AnyNet):\n    lr: float = 0.1\n    num_classes: int = 10\n    stem_channels: int = 32\n    arch: tuple = ((4, 32, 16, 1), (6, 80, 16, 1))\n```\n\nWe can see that each RegNetX stage progressively reduces resolution and increases output channels. ```{.python .input}\n%%tab mxnet, pytorch\nRegNetX32().layer_summary((1, 1, 96, 96))\n```\n\n```{.python .input}\n%%tab tensorflow\nRegNetX32().layer_summary((1, 96, 96, 1))\n```\n\n```{.python .input}\n%%tab jax\nRegNetX32(training=False).layer_summary((1, 96, 96, 1))\n```"
    },
    {
      "chunk_id": "cddb0dc478fc_0",
      "chapter": "cnn-design",
      "heading": "Training",
      "text": "Training the 32-layer RegNetX on the Fashion-MNIST dataset is just like before.\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nmodel = RegNetX32(lr=0.05)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nwith d2l.try_gpu():\n    model = RegNetX32(lr=0.01)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "665918efb5b4_0",
      "chapter": "cnn-design",
      "heading": "Discussion",
      "text": "With desirable inductive biases (assumptions or preferences) like locality and translation invariance (:numref:`sec_why-conv`)\nfor vision, CNNs have been the dominant architectures in this area. This remained the case from LeNet up until Transformers (:numref:`sec_transformer`) :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021,touvron2021training` started surpassing CNNs in terms of accuracy. While much of the recent progress in terms of vision Transformers *can* be backported into CNNs :cite:`liu2022convnet`, it is only possible at a higher computational cost. Just as importantly, recent hardware optimizations (NVIDIA Ampere and Hopper) have only widened the gap in favor of Transformers. \n\nIt is worth noting that Transformers have a significantly lower degree of inductive bias towards locality and translation invariance than CNNs. That learned structures prevailed is due, not least, to the availability of large image collections, such as LAION-400m and LAION-5B :cite:`schuhmann2022laion` with up to 5 billion images. Quite surprisingly, some of the more relevant work in this context even includes MLPs :cite:`tolstikhin2021mlp`. \n\nIn sum, vision Transformers (:numref:`sec_vision-transformer`) by now lead in terms of \nstate-of-the-art performance in large-scale image classification, \nshowing that *scalability trumps inductive biases* :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`.\nThis includes pretraining large-scale Transformers (:numref:`sec_large-pretraining-transformers`) with multi-head self-attention (:numref:`sec_multihead-attention`). We invite the readers to dive into these chapters for a much more detailed discussion."
    },
    {
      "chunk_id": "67a3584901fd_0",
      "chapter": "cnn-design",
      "heading": "Exercises",
      "text": "1. Increase the number of stages to four. Can you design a deeper RegNetX that performs better?\n1. De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How does your new model perform?\n1. Implement multiple instances of a \"VioNet\" family by *violating* the design principles of RegNetX. How do they perform? Which of ($d_i$, $c_i$, $g_i$, $b_i$) is the most important factor?\n1. Your goal is to design the \"perfect\" MLP. Can you use the design principles introduced above to find good architectures? Is it possible to extrapolate from small to large networks?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/7462)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/7463)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/8738)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18009)\n:end_tab:"
    },
    {
      "chunk_id": "c34c8a41f4e7_0",
      "chapter": "densenet",
      "heading": "densenet",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Densely Connected Networks (DenseNet)\n:label:`sec_densenet`\n\nResNet significantly changed the view of how to parametrize the functions in deep networks. *DenseNet* (dense convolutional network) is to some extent the logical extension of this :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`.\nDenseNet is characterized by both the connectivity pattern where\neach layer connects to all the preceding layers\nand the concatenation operation (rather than the addition operator in ResNet) to preserve and reuse features\nfrom earlier layers.\nTo understand how to arrive at it, let's take a small detour to mathematics.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\n```"
    },
    {
      "chunk_id": "235007692bc5_0",
      "chapter": "densenet",
      "heading": "From ResNet to DenseNet",
      "text": "Recall the Taylor expansion for functions. At the point $x = 0$ it can be written as\n\n$$f(x) = f(0) + x \\cdot \\left[f'(0) + x \\cdot \\left[\\frac{f''(0)}{2!}  + x \\cdot \\left[\\frac{f'''(0)}{3!}  + \\cdots \\right]\\right]\\right].$$\n\n\nThe key point is that it decomposes a function into terms of increasingly higher order. In a similar vein, ResNet decomposes functions into\n\n$$f(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x}).$$\n\nThat is, ResNet decomposes $f$ into a simple linear term and a more complex\nnonlinear one. What if we wanted to capture (not necessarily add) information beyond two terms? One such solution is DenseNet :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`. ![The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation. ](../img/densenet-block.svg)\n:label:`fig_densenet_block`\n\nAs shown in :numref:`fig_densenet_block`, the key difference between ResNet and DenseNet is that in the latter case outputs are *concatenated* (denoted by $[,]$) rather than added. As a result, we perform a mapping from $\\mathbf{x}$ to its values after applying an increasingly complex sequence of functions:\n\n$$\\mathbf{x} \\to \\left[\n\\mathbf{x},\nf_1(\\mathbf{x}),\nf_2\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right)\\right]\\right), f_3\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right), f_2\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right)\\right]\\right)\\right]\\right), \\ldots\\right].$$\n\nIn the end, all these functions are combined in MLP to reduce the number of features again. In terms of implementation this is quite simple:\nrather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The final layer of such a chain is densely connected to all previous layers. The dense connections are shown in :numref:`fig_densenet`. ![Dense connections in DenseNet."
    },
    {
      "chunk_id": "235007692bc5_1",
      "chapter": "densenet",
      "heading": "From ResNet to DenseNet",
      "text": "The final layer of such a chain is densely connected to all previous layers. The dense connections are shown in :numref:`fig_densenet`. ![Dense connections in DenseNet. Note how the dimensionality increases with depth.](../img/densenet.svg)\n:label:`fig_densenet`\n\nThe main components that comprise a DenseNet are *dense blocks* and *transition layers*. The former define how the inputs and outputs are concatenated, while the latter control the number of channels so that it is not too large, \nsince the expansion $\\mathbf{x} \\to \\left[\\mathbf{x}, f_1(\\mathbf{x}),\nf_2\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right)\\right]\\right), \\ldots \\right]$ can be quite high-dimensional."
    },
    {
      "chunk_id": "4b67670ef8fd_0",
      "chapter": "densenet",
      "heading": "[**Dense Blocks**]",
      "text": "DenseNet uses the modified \"batch normalization, activation, and convolution\"\nstructure of ResNet (see the exercise in :numref:`sec_resnet`). First, we implement this convolution block structure. ```{.python .input}\n%%tab mxnet\ndef conv_block(num_channels):\n    blk = nn.Sequential()\n    blk.add(nn.BatchNorm(),\n            nn.Activation('relu'),\n            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n    return blk\n```\n\n```{.python .input}\n%%tab pytorch\ndef conv_block(num_channels):\n    return nn.Sequential(\n        nn.LazyBatchNorm2d(), nn.ReLU(),\n        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass ConvBlock(tf.keras.layers.Layer):\n    def __init__(self, num_channels):\n        super(ConvBlock, self).__init__()\n        self.bn = tf.keras.layers.BatchNormalization()\n        self.relu = tf.keras.layers.ReLU()\n        self.conv = tf.keras.layers.Conv2D(\n            filters=num_channels, kernel_size=(3, 3), padding='same')\n\n        self.listLayers = [self.bn, self.relu, self.conv]\n\n    def call(self, x):\n        y = x\n        for layer in self.listLayers.layers:\n            y = layer(y)\n        y = tf.keras.layers.concatenate([x,y], axis=-1)\n        return y\n```\n\n```{.python .input}\n%%tab jax\nclass ConvBlock(nn.Module):\n    num_channels: int\n    training: bool = True\n\n    @nn.compact\n    def __call__(self, X):\n        Y = nn.relu(nn.BatchNorm(not self.training)(X))\n        Y = nn.Conv(self.num_channels, kernel_size=(3, 3), padding=(1, 1))(Y)\n        Y = jnp.concatenate((X, Y), axis=-1)\n        return Y\n```\n\nA *dense block* consists of multiple convolution blocks, each using the same number of output channels. In the forward propagation, however, we concatenate the input and output of each convolution block on the channel dimension. Lazy evaluation allows us to adjust the dimensionality automatically."
    },
    {
      "chunk_id": "4b67670ef8fd_1",
      "chapter": "densenet",
      "heading": "[**Dense Blocks**]",
      "text": "In the forward propagation, however, we concatenate the input and output of each convolution block on the channel dimension. Lazy evaluation allows us to adjust the dimensionality automatically. ```{.python .input}\n%%tab mxnet\nclass DenseBlock(nn.Block):\n    def __init__(self, num_convs, num_channels):\n        super().__init__()\n        self.net = nn.Sequential()\n        for _ in range(num_convs):\n            self.net.add(conv_block(num_channels))\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # Concatenate input and output of each block along the channels\n            X = np.concatenate((X, Y), axis=1)\n        return X\n```\n\n```{.python .input}\n%%tab pytorch\nclass DenseBlock(nn.Module):\n    def __init__(self, num_convs, num_channels):\n        super(DenseBlock, self).__init__()\n        layer = []\n        for i in range(num_convs):\n            layer.append(conv_block(num_channels))\n        self.net = nn.Sequential(*layer)\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # Concatenate input and output of each block along the channels\n            X = torch.cat((X, Y), dim=1)\n        return X\n```\n\n```{.python .input}\n%%tab tensorflow\nclass DenseBlock(tf.keras.layers.Layer):\n    def __init__(self, num_convs, num_channels):\n        super(DenseBlock, self).__init__()\n        self.listLayers = []\n        for _ in range(num_convs):\n            self.listLayers.append(ConvBlock(num_channels))\n\n    def call(self, x):\n        for layer in self.listLayers.layers:\n            x = layer(x)\n        return x\n```\n\n```{.python .input}\n%%tab jax\nclass DenseBlock(nn.Module):\n    num_convs: int\n    num_channels: int\n    training: bool = True\n\n    def setup(self):\n        layer = []\n        for i in range(self.num_convs):\n            layer.append(ConvBlock(self.num_channels, self.training))\n        self.net = nn.Sequential(layer)\n\n    def __call__(self, X):\n        return self.net(X)\n```\n\nIn the following example,\nwe [**define a `DenseBlock` instance**] with two convolution blocks of 10 output channels."
    },
    {
      "chunk_id": "4b67670ef8fd_2",
      "chapter": "densenet",
      "heading": "[**Dense Blocks**]",
      "text": "When using an input with three channels, we will get an output with  $3 + 10 + 10=23$ channels. The number of convolution block channels controls the growth in the number of output channels relative to the number of input channels. This is also referred to as the *growth rate*. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nblk = DenseBlock(2, 10)\nif tab.selected('mxnet'):\n    X = np.random.uniform(size=(4, 3, 8, 8))\n    blk.initialize()\nif tab.selected('pytorch'):\n    X = torch.randn(4, 3, 8, 8)\nif tab.selected('tensorflow'):\n    X = tf.random.uniform((4, 8, 8, 3))\nY = blk(X)\nY.shape\n```\n\n```{.python .input}\n%%tab jax\nblk = DenseBlock(2, 10)\nX = jnp.zeros((4, 8, 8, 3))\nY = blk.init_with_output(d2l.get_key(), X)[0]\nY.shape\n```"
    },
    {
      "chunk_id": "048a813cf7d2_0",
      "chapter": "densenet",
      "heading": "[**Transition Layers**]",
      "text": "Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. A *transition layer* is used to control the complexity of the model. It reduces the number of channels by using a $1\\times 1$ convolution. Moreover, it halves the height and width via average pooling with a stride of 2. ```{.python .input}\n%%tab mxnet\ndef transition_block(num_channels):\n    blk = nn.Sequential()\n    blk.add(nn.BatchNorm(), nn.Activation('relu'),\n            nn.Conv2D(num_channels, kernel_size=1),\n            nn.AvgPool2D(pool_size=2, strides=2))\n    return blk\n```\n\n```{.python .input}\n%%tab pytorch\ndef transition_block(num_channels):\n    return nn.Sequential(\n        nn.LazyBatchNorm2d(), nn.ReLU(),\n        nn.LazyConv2d(num_channels, kernel_size=1),\n        nn.AvgPool2d(kernel_size=2, stride=2))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass TransitionBlock(tf.keras.layers.Layer):\n    def __init__(self, num_channels, **kwargs):\n        super(TransitionBlock, self).__init__(**kwargs)\n        self.batch_norm = tf.keras.layers.BatchNormalization()\n        self.relu = tf.keras.layers.ReLU()\n        self.conv = tf.keras.layers.Conv2D(num_channels, kernel_size=1)\n        self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)\n\n    def call(self, x):\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.conv(x)\n        return self.avg_pool(x)\n```\n\n```{.python .input}\n%%tab jax\nclass TransitionBlock(nn.Module):\n    num_channels: int\n    training: bool = True\n\n    @nn.compact\n    def __call__(self, X):\n        X = nn.BatchNorm(not self.training)(X)\n        X = nn.relu(X)\n        X = nn.Conv(self.num_channels, kernel_size=(1, 1))(X)\n        X = nn.avg_pool(X, window_shape=(2, 2), strides=(2, 2))\n        return X\n```\n\n[**Apply a transition layer**] with 10 channels to the output of the dense block in the previous example. This reduces the number of output channels to 10, and halves the height and width."
    },
    {
      "chunk_id": "048a813cf7d2_1",
      "chapter": "densenet",
      "heading": "[**Transition Layers**]",
      "text": "This reduces the number of output channels to 10, and halves the height and width. ```{.python .input}\n%%tab mxnet\nblk = transition_block(10)\nblk.initialize()\nblk(Y).shape\n```\n\n```{.python .input}\n%%tab pytorch\nblk = transition_block(10)\nblk(Y).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nblk = TransitionBlock(10)\nblk(Y).shape\n```\n\n```{.python .input}\n%%tab jax\nblk = TransitionBlock(10)\nblk.init_with_output(d2l.get_key(), Y)[0].shape\n```"
    },
    {
      "chunk_id": "973953839b08_0",
      "chapter": "densenet",
      "heading": "[**DenseNet Model**]",
      "text": "Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer and max-pooling layer as in ResNet. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass DenseNet(d2l.Classifier):\n    def b1(self):\n        if tab.selected('mxnet'):\n            net = nn.Sequential()\n            net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n                nn.BatchNorm(), nn.Activation('relu'),\n                nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n            return net\n        if tab.selected('pytorch'):\n            return nn.Sequential(\n                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n                nn.LazyBatchNorm2d(), nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n        if tab.selected('tensorflow'):\n            return tf.keras.models.Sequential([\n                tf.keras.layers.Conv2D(\n                    64, kernel_size=7, strides=2, padding='same'),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.ReLU(),\n                tf.keras.layers.MaxPool2D(\n                    pool_size=3, strides=2, padding='same')])\n```\n\n```{.python .input}\n%%tab jax\nclass DenseNet(d2l.Classifier):\n    num_channels: int = 64\n    growth_rate: int = 32\n    arch: tuple = (4, 4, 4, 4)\n    lr: float = 0.1\n    num_classes: int = 10\n    training: bool = True\n\n    def setup(self):\n        self.net = self.create_net()\n\n    def b1(self):\n        return nn.Sequential([\n            nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='same'),\n            nn.BatchNorm(not self.training),\n            nn.relu,\n            lambda x: nn.max_pool(x, window_shape=(3, 3),\n                                  strides=(2, 2), padding='same')\n        ])\n```\n\nThen, similar to the four modules made up of residual blocks that ResNet uses,\nDenseNet uses four dense blocks. As with ResNet, we can set the number of convolutional layers used in each dense block."
    },
    {
      "chunk_id": "973953839b08_1",
      "chapter": "densenet",
      "heading": "[**DenseNet Model**]",
      "text": "As with ResNet, we can set the number of convolutional layers used in each dense block. Here, we set it to 4, consistent with the ResNet-18 model in :numref:`sec_resnet`. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional layers in the dense block to 32, so 128 channels will be added to each dense block. In ResNet, the height and width are reduced between each module by a residual block with a stride of 2. Here, we use the transition layer to halve the height and width and halve the number of channels. Similar to ResNet, a global pooling layer and a fully connected layer are connected at the end to produce the output."
    },
    {
      "chunk_id": "973953839b08_2",
      "chapter": "densenet",
      "heading": "[**DenseNet Model**]",
      "text": "Here, we use the transition layer to halve the height and width and halve the number of channels. Similar to ResNet, a global pooling layer and a fully connected layer are connected at the end to produce the output. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(DenseNet)\ndef __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n             lr=0.1, num_classes=10):\n    super(DenseNet, self).__init__()\n    self.save_hyperparameters()\n    if tab.selected('mxnet'):\n        self.net = nn.Sequential()\n        self.net.add(self.b1())\n        for i, num_convs in enumerate(arch):\n            self.net.add(DenseBlock(num_convs, growth_rate))\n            # The number of output channels in the previous dense block\n            num_channels += num_convs * growth_rate\n            # A transition layer that halves the number of channels is added\n            # between the dense blocks\n            if i != len(arch) - 1:\n                num_channels //= 2\n                self.net.add(transition_block(num_channels))\n        self.net.add(nn.BatchNorm(), nn.Activation('relu'),\n                     nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n        self.net.initialize(init.Xavier())\n    if tab.selected('pytorch'):\n        self.net = nn.Sequential(self.b1())\n        for i, num_convs in enumerate(arch):\n            self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\n                                                              growth_rate))\n            # The number of output channels in the previous dense block\n            num_channels += num_convs * growth_rate\n            # A transition layer that halves the number of channels is added\n            # between the dense blocks\n            if i != len(arch) - 1:\n                num_channels //= 2\n                self.net.add_module(f'tran_blk{i+1}', transition_block(\n                    num_channels))\n        self.net.add_module('last', nn.Sequential(\n            nn.LazyBatchNorm2d(), nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n            nn.LazyLinear(num_classes)))\n        self.net.apply(d2l.init_cnn)\n    if tab.selected('tensorflow'):\n        self.net = tf.keras.models.Sequential(self.b1())\n        for i, num_convs in enumerate(arch):\n            self.net.add(DenseBlock(num_convs, growth_rate))\n            # The number of output channels in the previous dense block\n            num_channels += num_convs * growth_rate\n            # A transition layer that halves the number of channels is added\n            # between the dense blocks\n            if i != len(arch) - 1:\n                num_channels //= 2\n                self.net.add(TransitionBlock(num_channels))\n        self.net.add(tf.keras.models.Sequential([\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.GlobalAvgPool2D(),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(num_classes)]))\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(DenseNet)\ndef create_net(self):\n    net = self.b1()\n    for i, num_convs in enumerate(self.arch):\n        net.layers.extend([DenseBlock(num_convs, self.growth_rate,\n                                      training=self.training)])\n        # The number of output channels in the previous dense block\n        num_channels = self.num_channels + (num_convs * self.growth_rate)\n        # A transition layer that halves the number of channels is added\n        # between the dense blocks\n        if i != len(self.arch) - 1:\n            num_channels //= 2\n            net.layers.extend([TransitionBlock(num_channels,\n                                               training=self.training)])\n    net.layers.extend([\n        nn.BatchNorm(not self.training),\n        nn.relu,\n        lambda x: nn.avg_pool(x, window_shape=x.shape[1:3],\n                              strides=x.shape[1:3], padding='valid'),\n        lambda x: x.reshape((x.shape[0], -1)),\n        nn.Dense(self.num_classes)\n    ])\n    return net\n```"
    },
    {
      "chunk_id": "30c2d766f2b5_0",
      "chapter": "densenet",
      "heading": "[**Training**]",
      "text": "Since we are using a deeper network here, in this section, we will reduce the input height and width from 224 to 96 to simplify the computation.\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nmodel = DenseNet(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nwith d2l.try_gpu():\n    model = DenseNet(lr=0.01)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "f1efb9ab23cb_0",
      "chapter": "densenet",
      "heading": "Summary and Discussion",
      "text": "The main components that comprise DenseNet are dense blocks and transition layers. For the latter, we need to keep the dimensionality under control when composing the network by adding transition layers that shrink the number of channels again.\nIn terms of cross-layer connections, in contrast to ResNet, where inputs and outputs are added together, DenseNet concatenates inputs and outputs on the channel dimension.\nAlthough these concatenation operations\nreuse features to achieve computational efficiency,\nunfortunately they lead to heavy GPU memory consumption.\nAs a result,\napplying DenseNet may require more memory-efficient implementations that may increase training time :cite:`pleiss2017memory`."
    },
    {
      "chunk_id": "bfa679eadf11_0",
      "chapter": "densenet",
      "heading": "Exercises",
      "text": "1. Why do we use average pooling rather than max-pooling in the transition layer?\n1. One of the advantages mentioned in the DenseNet paper is that its model parameters are smaller than those of ResNet. Why is this the case?\n1. One problem for which DenseNet has been criticized is its high memory consumption.\n    1. Is this really the case? Try to change the input shape to $224\\times 224$ to compare the actual GPU memory consumption empirically.\n    1. Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?\n1. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`.\n1. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price prediction task in :numref:`sec_kaggle_house`.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/87)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/88)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/331)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18008)\n:end_tab:"
    },
    {
      "chunk_id": "88ce9dd4e9ec_0",
      "chapter": "googlenet",
      "heading": "googlenet",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Multi-Branch Networks  (GoogLeNet)\n:label:`sec_googlenet`\n\nIn 2014, *GoogLeNet*\nwon the ImageNet Challenge :cite:`Szegedy.Liu.Jia.ea.2015`, using a structure\nthat combined the strengths of NiN :cite:`Lin.Chen.Yan.2013`, repeated blocks :cite:`Simonyan.Zisserman.2014`,\nand a cocktail of convolution kernels. It was arguably also the first network that exhibited a clear distinction among the stem (data ingest), body (data processing), and head (prediction) in a CNN. This design pattern has persisted ever since in the design of deep networks: the *stem* is given by the first two or three convolutions that operate on the image. They extract low-level features from the underlying images. This is followed by a *body* of convolutional blocks. Finally, the *head* maps the features obtained so far to the required classification, segmentation, detection, or tracking problem at hand. The key contribution in GoogLeNet was the design of the network body. It solved the problem of selecting\nconvolution kernels in an ingenious way. While other works tried to identify which convolution, ranging from $1 \\times 1$ to $11 \\times 11$ would be best, it simply *concatenated* multi-branch convolutions. In what follows we introduce a slightly simplified version of GoogLeNet: the original design included a number of tricks for stabilizing training through intermediate loss functions, applied to multiple layers of the network. They are no longer necessary due to the availability of improved training algorithms."
    },
    {
      "chunk_id": "88ce9dd4e9ec_1",
      "chapter": "googlenet",
      "heading": "googlenet",
      "text": "They are no longer necessary due to the availability of improved training algorithms. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx, init\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\nfrom d2l import tensorflow as d2l\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\n```"
    },
    {
      "chunk_id": "bdbb06ba606e_0",
      "chapter": "googlenet",
      "heading": "(**Inception Blocks**)",
      "text": "The basic convolutional block in GoogLeNet is called an *Inception block*,\nstemming from the meme \"we need to go deeper\" from the movie *Inception*. ![Structure of the Inception block.](../img/inception.svg)\n:label:`fig_inception`\n\nAs depicted in :numref:`fig_inception`,\nthe inception block consists of four parallel branches. The first three branches use convolutional layers\nwith window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$\nto extract information from different spatial sizes. The middle two branches also add a $1\\times 1$ convolution of the input\nto reduce the number of channels, reducing the model's complexity. The fourth branch uses a $3\\times 3$ max-pooling layer,\nfollowed by a $1\\times 1$ convolutional layer\nto change the number of channels. The four branches all use appropriate padding to give the input and output the same height and width. Finally, the outputs along each branch are concatenated\nalong the channel dimension and comprise the block's output. The commonly-tuned hyperparameters of the Inception block\nare the number of output channels per layer, i.e., how to allocate capacity among convolutions of different size."
    },
    {
      "chunk_id": "bdbb06ba606e_1",
      "chapter": "googlenet",
      "heading": "(**Inception Blocks**)",
      "text": "The commonly-tuned hyperparameters of the Inception block\nare the number of output channels per layer, i.e., how to allocate capacity among convolutions of different size. ```{.python .input}\n%%tab mxnet\nclass Inception(nn.Block):\n    # c1--c4 are the number of output channels for each branch\n    def __init__(self, c1, c2, c3, c4, **kwargs):\n        super(Inception, self).__init__(**kwargs)\n        # Branch 1\n        self.b1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n        # Branch 2\n        self.b2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n        self.b2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\n                              activation='relu')\n        # Branch 3\n        self.b3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n        self.b3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\n                              activation='relu')\n        # Branch 4\n        self.b4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n        self.b4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n\n    def forward(self, x):\n        b1 = self.b1_1(x)\n        b2 = self.b2_2(self.b2_1(x))\n        b3 = self.b3_2(self.b3_1(x))\n        b4 = self.b4_2(self.b4_1(x))\n        return np.concatenate((b1, b2, b3, b4), axis=1)\n```\n\n```{.python .input}\n%%tab pytorch\nclass Inception(nn.Module):\n    # c1--c4 are the number of output channels for each branch\n    def __init__(self, c1, c2, c3, c4, **kwargs):\n        super(Inception, self).__init__(**kwargs)\n        # Branch 1\n        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n        # Branch 2\n        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n        # Branch 3\n        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n        # Branch 4\n        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n\n    def forward(self, x):\n        b1 = F.relu(self.b1_1(x))\n        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n        b4 = F.relu(self.b4_2(self.b4_1(x)))\n        return torch.cat((b1, b2, b3, b4), dim=1)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Inception(tf.keras.Model):\n    # c1--c4 are the number of output channels for each branch\n    def __init__(self, c1, c2, c3, c4):\n        super().__init__()\n        self.b1_1 = tf.keras.layers.Conv2D(c1, 1, activation='relu')\n        self.b2_1 = tf.keras.layers.Conv2D(c2[0], 1, activation='relu')\n        self.b2_2 = tf.keras.layers.Conv2D(c2[1], 3, padding='same',\n                                           activation='relu')\n        self.b3_1 = tf.keras.layers.Conv2D(c3[0], 1, activation='relu')\n        self.b3_2 = tf.keras.layers.Conv2D(c3[1], 5, padding='same',\n                                           activation='relu')\n        self.b4_1 = tf.keras.layers.MaxPool2D(3, 1, padding='same')\n        self.b4_2 = tf.keras.layers.Conv2D(c4, 1, activation='relu')\n\n    def call(self, x):\n        b1 = self.b1_1(x)\n        b2 = self.b2_2(self.b2_1(x))\n        b3 = self.b3_2(self.b3_1(x))\n        b4 = self.b4_2(self.b4_1(x))\n        return tf.keras.layers.Concatenate()([b1, b2, b3, b4])\n```\n\n```{.python .input}\n%%tab jax\nclass Inception(nn.Module):\n    # `c1`--`c4` are the number of output channels for each branch\n    c1: int\n    c2: tuple\n    c3: tuple\n    c4: int\n\n    def setup(self):\n        # Branch 1\n        self.b1_1 = nn.Conv(self.c1, kernel_size=(1, 1))\n        # Branch 2\n        self.b2_1 = nn.Conv(self.c2[0], kernel_size=(1, 1))\n        self.b2_2 = nn.Conv(self.c2[1], kernel_size=(3, 3), padding='same')\n        # Branch 3\n        self.b3_1 = nn.Conv(self.c3[0], kernel_size=(1, 1))\n        self.b3_2 = nn.Conv(self.c3[1], kernel_size=(5, 5), padding='same')\n        # Branch 4\n        self.b4_1 = lambda x: nn.max_pool(x, window_shape=(3, 3),\n                                          strides=(1, 1), padding='same')\n        self.b4_2 = nn.Conv(self.c4, kernel_size=(1, 1))\n\n    def __call__(self, x):\n        b1 = nn.relu(self.b1_1(x))\n        b2 = nn.relu(self.b2_2(nn.relu(self.b2_1(x))))\n        b3 = nn.relu(self.b3_2(nn.relu(self.b3_1(x))))\n        b4 = nn.relu(self.b4_2(self.b4_1(x)))\n        return jnp.concatenate((b1, b2, b3, b4), axis=-1)\n```\n\nTo gain some intuition for why this network works so well,\nconsider the combination of the filters."
    },
    {
      "chunk_id": "bdbb06ba606e_2",
      "chapter": "googlenet",
      "heading": "(**Inception Blocks**)",
      "text": "They explore the image in a variety of filter sizes. This means that details at different extents\ncan be recognized efficiently by filters of different sizes. At the same time, we can allocate different amounts of parameters\nfor different filters."
    },
    {
      "chunk_id": "57f39a11558d_0",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "As shown in :numref:`fig_inception_full`, GoogLeNet uses a stack of a total of 9 inception blocks, arranged into three groups with max-pooling in between,\nand global average pooling in its head to generate its estimates. Max-pooling between inception blocks reduces the dimensionality. At its stem, the first module is similar to AlexNet and LeNet. ![The GoogLeNet architecture.](../img/inception-full-90.svg)\n:label:`fig_inception_full`\n\nWe can now implement GoogLeNet piece by piece. Let's begin with the stem. The first module uses a 64-channel $7\\times 7$ convolutional layer."
    },
    {
      "chunk_id": "57f39a11558d_1",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "![The GoogLeNet architecture.](../img/inception-full-90.svg)\n:label:`fig_inception_full`\n\nWe can now implement GoogLeNet piece by piece. Let's begin with the stem. The first module uses a 64-channel $7\\times 7$ convolutional layer. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass GoogleNet(d2l.Classifier):\n    def b1(self):\n        if tab.selected('mxnet'):\n            net = nn.Sequential()\n            net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3,\n                              activation='relu'),\n                    nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n            return net\n        if tab.selected('pytorch'):\n            return nn.Sequential(\n                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n                nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n        if tab.selected('tensorflow'):\n            return tf.keras.models.Sequential([\n                tf.keras.layers.Conv2D(64, 7, strides=2, padding='same',\n                                       activation='relu'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2,\n                                          padding='same')])\n```\n\n```{.python .input}\n%%tab jax\nclass GoogleNet(d2l.Classifier):\n    lr: float = 0.1\n    num_classes: int = 10\n\n    def setup(self):\n        self.net = nn.Sequential([self.b1(), self.b2(), self.b3(), self.b4(),\n                                  self.b5(), nn.Dense(self.num_classes)])\n\n    def b1(self):\n        return nn.Sequential([\n                nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='same'),\n                nn.relu,\n                lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2),\n                                      padding='same')])\n```\n\nThe second module uses two convolutional layers:\nfirst, a 64-channel $1\\times 1$ convolutional layer,\nfollowed by a $3\\times 3$ convolutional layer that triples the number of channels. This corresponds to the second branch in the Inception block and concludes the design of the body."
    },
    {
      "chunk_id": "57f39a11558d_2",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "This corresponds to the second branch in the Inception block and concludes the design of the body. At this point we have 192 channels. ```{.python .input}\n%%tab all\n@d2l.add_to_class(GoogleNet)\ndef b2(self):\n    if tab.selected('mxnet'):\n        net = nn.Sequential()\n        net.add(nn.Conv2D(64, kernel_size=1, activation='relu'),\n               nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),\n               nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n        return net\n    if tab.selected('pytorch'):\n        return nn.Sequential(\n            nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n            nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n    if tab.selected('tensorflow'):\n        return tf.keras.Sequential([\n            tf.keras.layers.Conv2D(64, 1, activation='relu'),\n            tf.keras.layers.Conv2D(192, 3, padding='same', activation='relu'),\n            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])\n    if tab.selected('jax'):\n        return nn.Sequential([nn.Conv(64, kernel_size=(1, 1)),\n                              nn.relu,\n                              nn.Conv(192, kernel_size=(3, 3), padding='same'),\n                              nn.relu,\n                              lambda x: nn.max_pool(x, window_shape=(3, 3),\n                                                    strides=(2, 2),\n                                                    padding='same')])\n```\n\nThe third module connects two complete Inception blocks in series. The number of output channels of the first Inception block is\n$64+128+32+32=256$. This amounts to \na ratio of the number of output channels\namong the four branches of $2:4:1:1$. To achieve this, we first reduce the input\ndimensions by $\\frac{1}{2}$ and by $\\frac{1}{12}$ in the second and third branch respectively\nto arrive at $96 = 192/2$ and $16 = 192/12$ channels respectively."
    },
    {
      "chunk_id": "57f39a11558d_3",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "To achieve this, we first reduce the input\ndimensions by $\\frac{1}{2}$ and by $\\frac{1}{12}$ in the second and third branch respectively\nto arrive at $96 = 192/2$ and $16 = 192/12$ channels respectively. The number of output channels of the second Inception block\nis increased to $128+192+96+64=480$, yielding a ratio of $128:192:96:64 = 4:6:3:2$. As before,\nwe need to reduce the number of intermediate dimensions in the second and third channel. A\nscale of $\\frac{1}{2}$ and $\\frac{1}{8}$ respectively suffices, yielding $128$ and $32$ channels\nrespectively. This is captured by the arguments of the following `Inception` block constructors. ```{.python .input}\n%%tab all\n@d2l.add_to_class(GoogleNet)\ndef b3(self):\n    if tab.selected('mxnet'):\n        net = nn.Sequential()\n        net.add(Inception(64, (96, 128), (16, 32), 32),\n               Inception(128, (128, 192), (32, 96), 64),\n               nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n        return net\n    if tab.selected('pytorch'):\n        return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),\n                             Inception(128, (128, 192), (32, 96), 64),\n                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n    if tab.selected('tensorflow'):\n        return tf.keras.models.Sequential([\n            Inception(64, (96, 128), (16, 32), 32),\n            Inception(128, (128, 192), (32, 96), 64),\n            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])\n    if tab.selected('jax'):\n        return nn.Sequential([Inception(64, (96, 128), (16, 32), 32),\n                              Inception(128, (128, 192), (32, 96), 64),\n                              lambda x: nn.max_pool(x, window_shape=(3, 3),\n                                                    strides=(2, 2),\n                                                    padding='same')])\n```\n\nThe fourth module is more complicated."
    },
    {
      "chunk_id": "57f39a11558d_4",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "It connects five Inception blocks in series,\nand they have $192+208+48+64=512$, $160+224+64+64=512$,\n$128+256+64+64=512$, $112+288+64+64=528$,\nand $256+320+128+128=832$ output channels, respectively. The number of channels assigned to these branches is similar\nto that in the third module:\nthe second branch with the $3\\times 3$ convolutional layer\noutputs the largest number of channels,\nfollowed by the first branch with only the $1\\times 1$ convolutional layer,\nthe third branch with the $5\\times 5$ convolutional layer,\nand the fourth branch with the $3\\times 3$ max-pooling layer. The second and third branches will first reduce\nthe number of channels according to the ratio. These ratios are slightly different in different Inception blocks."
    },
    {
      "chunk_id": "57f39a11558d_5",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "The second and third branches will first reduce\nthe number of channels according to the ratio. These ratios are slightly different in different Inception blocks. ```{.python .input}\n%%tab all\n@d2l.add_to_class(GoogleNet)\ndef b4(self):\n    if tab.selected('mxnet'):\n        net = nn.Sequential()\n        net.add(Inception(192, (96, 208), (16, 48), 64),\n                Inception(160, (112, 224), (24, 64), 64),\n                Inception(128, (128, 256), (24, 64), 64),\n                Inception(112, (144, 288), (32, 64), 64),\n                Inception(256, (160, 320), (32, 128), 128),\n                nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n        return net\n    if tab.selected('pytorch'):\n        return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),\n                             Inception(160, (112, 224), (24, 64), 64),\n                             Inception(128, (128, 256), (24, 64), 64),\n                             Inception(112, (144, 288), (32, 64), 64),\n                             Inception(256, (160, 320), (32, 128), 128),\n                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n    if tab.selected('tensorflow'):\n        return tf.keras.Sequential([\n            Inception(192, (96, 208), (16, 48), 64),\n            Inception(160, (112, 224), (24, 64), 64),\n            Inception(128, (128, 256), (24, 64), 64),\n            Inception(112, (144, 288), (32, 64), 64),\n            Inception(256, (160, 320), (32, 128), 128),\n            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')])\n    if tab.selected('jax'):\n        return nn.Sequential([Inception(192, (96, 208), (16, 48), 64),\n                              Inception(160, (112, 224), (24, 64), 64),\n                              Inception(128, (128, 256), (24, 64), 64),\n                              Inception(112, (144, 288), (32, 64), 64),\n                              Inception(256, (160, 320), (32, 128), 128),\n                              lambda x: nn.max_pool(x, window_shape=(3, 3),\n                                                    strides=(2, 2),\n                                                    padding='same')])\n```\n\nThe fifth module has two Inception blocks with $256+320+128+128=832$\nand $384+384+128+128=1024$ output channels."
    },
    {
      "chunk_id": "57f39a11558d_6",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "The number of channels assigned to each branch\nis the same as that in the third and fourth modules,\nbut differs in specific values. It should be noted that the fifth block is followed by the output layer. This block uses the global average pooling layer\nto change the height and width of each channel to 1, just as in NiN. Finally, we turn the output into a two-dimensional array\nfollowed by a fully connected layer\nwhose number of outputs is the number of label classes. ```{.python .input}\n%%tab all\n@d2l.add_to_class(GoogleNet)\ndef b5(self):\n    if tab.selected('mxnet'):\n        net = nn.Sequential()\n        net.add(Inception(256, (160, 320), (32, 128), 128),\n                Inception(384, (192, 384), (48, 128), 128),\n                nn.GlobalAvgPool2D())\n        return net\n    if tab.selected('pytorch'):\n        return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\n                             Inception(384, (192, 384), (48, 128), 128),\n                             nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\n    if tab.selected('tensorflow'):\n        return tf.keras.Sequential([\n            Inception(256, (160, 320), (32, 128), 128),\n            Inception(384, (192, 384), (48, 128), 128),\n            tf.keras.layers.GlobalAvgPool2D(),\n            tf.keras.layers.Flatten()])\n    if tab.selected('jax'):\n        return nn.Sequential([Inception(256, (160, 320), (32, 128), 128),\n                              Inception(384, (192, 384), (48, 128), 128),\n                              # Flax does not provide a GlobalAvgPool2D layer\n                              lambda x: nn.avg_pool(x,\n                                                    window_shape=x.shape[1:3],\n                                                    strides=x.shape[1:3],\n                                                    padding='valid'),\n                              lambda x: x.reshape((x.shape[0], -1))])\n```\n\nNow that we defined all blocks `b1` through `b5`, it is just a matter of assembling them all into a full network."
    },
    {
      "chunk_id": "57f39a11558d_7",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(GoogleNet)\ndef __init__(self, lr=0.1, num_classes=10):\n    super(GoogleNet, self).__init__()\n    self.save_hyperparameters()\n    if tab.selected('mxnet'):\n        self.net = nn.Sequential()\n        self.net.add(self.b1(), self.b2(), self.b3(), self.b4(), self.b5(),\n                     nn.Dense(num_classes))\n        self.net.initialize(init.Xavier())\n    if tab.selected('pytorch'):\n        self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\n                                 self.b5(), nn.LazyLinear(num_classes))\n        self.net.apply(d2l.init_cnn)\n    if tab.selected('tensorflow'):\n        self.net = tf.keras.Sequential([\n            self.b1(), self.b2(), self.b3(), self.b4(), self.b5(),\n            tf.keras.layers.Dense(num_classes)])\n```\n\nThe GoogLeNet model is computationally complex. Note the large number of\nrelatively arbitrary hyperparameters in terms of the number of channels chosen, the number of blocks prior to dimensionality reduction, the relative partitioning of capacity across channels, etc. Much of it is due to the \nfact that at the time when GoogLeNet was introduced, automatic tools for network definition or design exploration \nwere not yet available. For instance, by now we take it for granted that a competent deep learning framework is capable of inferring dimensionalities of input tensors automatically. At the time, many such configurations had to be specified explicitly by the experimenter, thus often slowing down active experimentation. Moreover, the tools needed for automatic exploration were still in flux and initial experiments largely amounted to costly brute-force exploration, genetic algorithms, and similar strategies. For now the only modification we will carry out is to\n[**reduce the input height and width from 224 to 96\nto have a reasonable training time on Fashion-MNIST.**]\nThis simplifies the computation. Let's have a look at the\nchanges in the shape of the output between the various modules."
    },
    {
      "chunk_id": "57f39a11558d_8",
      "chapter": "googlenet",
      "heading": "[**GoogLeNet Model**]",
      "text": "Let's have a look at the\nchanges in the shape of the output between the various modules. ```{.python .input}\n%%tab mxnet, pytorch\nmodel = GoogleNet().layer_summary((1, 1, 96, 96))\n```\n\n```{.python .input}\n%%tab tensorflow, jax\nmodel = GoogleNet().layer_summary((1, 96, 96, 1))\n```"
    },
    {
      "chunk_id": "8b30ed83a076_0",
      "chapter": "googlenet",
      "heading": "[**Training**]",
      "text": "As before, we train our model using the Fashion-MNIST dataset.\n We transform it to $96 \\times 96$ pixel resolution\n before invoking the training procedure.\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nmodel = GoogleNet(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nif tab.selected('pytorch'):\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nwith d2l.try_gpu():\n    model = GoogleNet(lr=0.01)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "5b888ee2a958_0",
      "chapter": "googlenet",
      "heading": "Discussion",
      "text": "A key feature of GoogLeNet is that it is actually *cheaper* to compute than its predecessors\nwhile simultaneously providing improved accuracy. This marks the beginning of a much more deliberate\nnetwork design that trades off the cost of evaluating a network with a reduction in errors. It also marks the beginning of experimentation at a block level with network design hyperparameters, even though it was entirely manual at the time. We will revisit this topic in :numref:`sec_cnn-design` when discussing strategies for network structure exploration. \n\nOver the following sections we will encounter a number of design choices (e.g., batch normalization, residual connections, and channel grouping) that allow us to improve networks significantly. For now, you can be proud to have implemented what is arguably the first truly modern CNN."
    },
    {
      "chunk_id": "20f9282d4a76_0",
      "chapter": "googlenet",
      "heading": "Exercises",
      "text": "1. GoogLeNet was so successful that it went through a number of iterations, progressively improving speed and accuracy. Try to implement and run some of them. They include the following:\n    1. Add a batch normalization layer :cite:`Ioffe.Szegedy.2015`, as described later in :numref:`sec_batch_norm`.\n    1. Make adjustments to the Inception block (width, choice and order of convolutions), as described in :citet:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n    1. Use label smoothing for model regularization, as described in :citet:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n    1. Make further adjustments to the Inception block by adding residual connection :cite:`Szegedy.Ioffe.Vanhoucke.ea.2017`, as described later in :numref:`sec_resnet`.\n1. What is the minimum image size needed for GoogLeNet to work?\n1. Can you design a variant of GoogLeNet that works on Fashion-MNIST's native resolution of $28 \\times 28$ pixels? How would you need to change the stem, the body, and the head of the network, if anything at all?\n1. Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do the latter two network\n   architectures significantly reduce the model parameter size?\n1. Compare the amount of computation needed in GoogLeNet and AlexNet. How does this affect the design of an accelerator chip, e.g., in terms of memory size, memory bandwidth, cache size, the amount of computation, and the benefit of specialized operations?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/81)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/82)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/316)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18004)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Modern Convolutional Neural Networks\n:label:`chap_modern_cnn`\n\nNow that we understand the basics of wiring together CNNs, let's take\na tour of modern CNN architectures. This tour is, by\nnecessity, incomplete, thanks to the plethora of exciting new designs\nbeing added. Their importance derives from the fact that not only can\nthey be used directly for vision tasks, but they also serve as basic\nfeature generators for more advanced tasks such as tracking\n:cite:`Zhang.Sun.Jiang.ea.2021`, segmentation :cite:`Long.Shelhamer.Darrell.2015`, object\ndetection :cite:`Redmon.Farhadi.2018`, or style transformation\n:cite:`Gatys.Ecker.Bethge.2016`. In this chapter, most sections\ncorrespond to a significant CNN architecture that was at some point\n(or currently) the base model upon which many research projects and\ndeployed systems were built. Each of these networks was briefly a\ndominant architecture and many were winners or runners-up in the\n[ImageNet competition](https://www.image-net.org/challenges/LSVRC/)\nwhich has served as a barometer of progress on supervised learning in\ncomputer vision since 2010. It is only recently that Transformers have begun\nto displace CNNs, starting with :citet:`Dosovitskiy.Beyer.Kolesnikov.ea.2021` and \nfollowed by the Swin Transformer :cite:`liu2021swin`. We will cover this development later \nin :numref:`chap_attention-and-transformers`. While the idea of *deep* neural networks is quite simple (stack\ntogether a bunch of layers), performance can vary wildly across\narchitectures and hyperparameter choices. The neural networks\ndescribed in this chapter are the product of intuition, a few\nmathematical insights, and a lot of trial and error. We present these\nmodels in chronological order, partly to convey a sense of the history\nso that you can form your own intuitions about where the field is\nheading and perhaps develop your own architectures."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "We present these\nmodels in chronological order, partly to convey a sense of the history\nso that you can form your own intuitions about where the field is\nheading and perhaps develop your own architectures. For instance,\nbatch normalization and residual connections described in this chapter\nhave offered two popular ideas for training and designing deep models,\nboth of which have since also been applied to architectures beyond computer\nvision. We begin our tour of modern CNNs with AlexNet :cite:`Krizhevsky.Sutskever.Hinton.2012`,\nthe first large-scale network deployed to beat conventional computer\nvision methods on a large-scale vision challenge; the VGG network\n:cite:`Simonyan.Zisserman.2014`, which makes use of a number of\nrepeating blocks of elements; the network in network (NiN) that\nconvolves whole neural networks patch-wise over inputs\n:cite:`Lin.Chen.Yan.2013`; GoogLeNet that uses networks with\nmulti-branch convolutions :cite:`Szegedy.Liu.Jia.ea.2015`; the residual\nnetwork (ResNet) :cite:`He.Zhang.Ren.ea.2016`, which remains one of\nthe most popular off-the-shelf architectures in computer vision;\nResNeXt blocks :cite:`Xie.Girshick.Dollar.ea.2017`\nfor sparser connections;\nand DenseNet\n:cite:`Huang.Liu.Van-Der-Maaten.ea.2017` for a generalization of the\nresidual architecture. Over time many special optimizations for efficient \nnetworks have been developed, such as coordinate shifts (ShiftNet) :cite:`wu2018shift`. This \nculminated in the automatic search for efficient architectures such as \nMobileNet v3 :cite:`Howard.Sandler.Chu.ea.2019`. It also includes the \nsemi-automatic design exploration of :citet:`Radosavovic.Kosaraju.Girshick.ea.2020`\nthat led to the RegNetX/Y which we will discuss later in this chapter. The work is instructive insofar as it offers a path for marrying brute force computation with \nthe ingenuity of an experimenter in the search for efficient design spaces."
    },
    {
      "chunk_id": "01f4e33118cb_2",
      "chapter": "index",
      "heading": "index",
      "text": "The work is instructive insofar as it offers a path for marrying brute force computation with \nthe ingenuity of an experimenter in the search for efficient design spaces. Of note is\nalso the work of :citet:`liu2022convnet` as it shows that training techniques (e.g., optimizers, data augmentation, and regularization)\nplay a pivotal role in improving accuracy. It also shows that long-held assumptions, such as \nthe size of a convolution window, may need to be revisited, given the increase in \ncomputation and data. We will cover this and many more questions in due course throughout this chapter. ```toc\n:maxdepth: 2\n\nalexnet\nvgg\nnin\ngooglenet\nbatch-norm\nresnet\ndensenet\ncnn-design\n```"
    },
    {
      "chunk_id": "b1fdd9edb00d_0",
      "chapter": "nin",
      "heading": "nin",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Network in Network (NiN)\n:label:`sec_nin`\n\nLeNet, AlexNet, and VGG all share a common design pattern:\nextract features exploiting *spatial* structure\nvia a sequence of convolutions and pooling layers\nand post-process the representations via fully connected layers. The improvements upon LeNet by AlexNet and VGG mainly lie\nin how these later networks widen and deepen these two modules. This design poses two major challenges. First, the fully connected layers at the end\nof the architecture consume tremendous numbers of parameters. For instance, even a simple\nmodel such as VGG-11 requires a monstrous matrix, occupying almost\n400MB of RAM in single precision (FP32). This is a significant impediment to computation, in particular on\nmobile and embedded devices. After all, even high-end mobile phones sport no more than 8GB of RAM. At the time VGG was invented, this was an order of magnitude less (the iPhone 4S had 512MB). As such, it would have been difficult to justify spending the majority of memory on an image classifier. Second, it is equally impossible to add fully connected layers\nearlier in the network to increase the degree of nonlinearity: doing so would destroy the\nspatial structure and require potentially even more memory. The *network in network* (*NiN*) blocks :cite:`Lin.Chen.Yan.2013` offer an alternative,\ncapable of solving both problems in one simple strategy. They were proposed based on a very simple insight: (i) use $1 \\times 1$ convolutions to add\nlocal nonlinearities across the channel activations and (ii) use global average pooling to integrate\nacross all locations in the last representation layer. Note that global average pooling would not\nbe effective, were it not for the added nonlinearities. Let's dive into this in detail."
    },
    {
      "chunk_id": "b1fdd9edb00d_1",
      "chapter": "nin",
      "heading": "nin",
      "text": "Note that global average pooling would not\nbe effective, were it not for the added nonlinearities. Let's dive into this in detail. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx, init\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\nfrom d2l import tensorflow as d2l\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "d68f336ed1d1_0",
      "chapter": "nin",
      "heading": "(**NiN Blocks**)",
      "text": "Recall :numref:`subsec_1x1`. In it we said that the inputs and outputs of convolutional layers\nconsist of four-dimensional tensors with axes\ncorresponding to the example, channel, height, and width. Also recall that the inputs and outputs of fully connected layers\nare typically two-dimensional tensors corresponding to the example and feature. The idea behind NiN is to apply a fully connected layer\nat each pixel location (for each height and width). The resulting $1 \\times 1$ convolution can be thought of as\na fully connected layer acting independently on each pixel location. :numref:`fig_nin` illustrates the main structural\ndifferences between VGG and NiN, and their blocks. Note both the difference in the NiN blocks (the initial convolution is followed by $1 \\times 1$ convolutions, whereas VGG retains $3 \\times 3$ convolutions) and at the end where we no longer require a giant fully connected layer."
    },
    {
      "chunk_id": "d68f336ed1d1_1",
      "chapter": "nin",
      "heading": "(**NiN Blocks**)",
      "text": "Note both the difference in the NiN blocks (the initial convolution is followed by $1 \\times 1$ convolutions, whereas VGG retains $3 \\times 3$ convolutions) and at the end where we no longer require a giant fully connected layer. ![Comparing the architectures of VGG and NiN, and of their blocks.](../img/nin.svg)\n:width:`600px`\n:label:`fig_nin`\n\n```{.python .input}\n%%tab mxnet\ndef nin_block(num_channels, kernel_size, strides, padding):\n    blk = nn.Sequential()\n    blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding,\n                      activation='relu'),\n            nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))\n    return blk\n```\n\n```{.python .input}\n%%tab pytorch\ndef nin_block(out_channels, kernel_size, strides, padding):\n    return nn.Sequential(\n        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\n        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())\n```\n\n```{.python .input}\n%%tab tensorflow\ndef nin_block(out_channels, kernel_size, strides, padding):\n    return tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(out_channels, kernel_size, strides=strides,\n                           padding=padding),\n    tf.keras.layers.Activation('relu'),\n    tf.keras.layers.Conv2D(out_channels, 1),\n    tf.keras.layers.Activation('relu'),\n    tf.keras.layers.Conv2D(out_channels, 1),\n    tf.keras.layers.Activation('relu')])\n```\n\n```{.python .input}\n%%tab jax\ndef nin_block(out_channels, kernel_size, strides, padding):\n    return nn.Sequential([\n        nn.Conv(out_channels, kernel_size, strides, padding),\n        nn.relu,\n        nn.Conv(out_channels, kernel_size=(1, 1)), nn.relu,\n        nn.Conv(out_channels, kernel_size=(1, 1)), nn.relu])\n```"
    },
    {
      "chunk_id": "7acf2def7a40_0",
      "chapter": "nin",
      "heading": "[**NiN Model**]",
      "text": "NiN uses the same initial convolution sizes as AlexNet (it was proposed shortly thereafter). The kernel sizes are $11\\times 11$, $5\\times 5$, and $3\\times 3$, respectively,\nand the numbers of output channels match those of AlexNet. Each NiN block is followed by a max-pooling layer\nwith a stride of 2 and a window shape of $3\\times 3$. The second significant difference between NiN and both AlexNet and VGG\nis that NiN avoids fully connected layers altogether. Instead, NiN uses a NiN block with a number of output channels equal to the number of label classes, followed by a *global* average pooling layer,\nyielding a vector of logits. This design significantly reduces the number of required model parameters, albeit at the expense of a potential increase in training time."
    },
    {
      "chunk_id": "7acf2def7a40_1",
      "chapter": "nin",
      "heading": "[**NiN Model**]",
      "text": "This design significantly reduces the number of required model parameters, albeit at the expense of a potential increase in training time. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass NiN(d2l.Classifier):\n    def __init__(self, lr=0.1, num_classes=10):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Sequential()\n            self.net.add(\n                nin_block(96, kernel_size=11, strides=4, padding=0),\n                nn.MaxPool2D(pool_size=3, strides=2),\n                nin_block(256, kernel_size=5, strides=1, padding=2),\n                nn.MaxPool2D(pool_size=3, strides=2),\n                nin_block(384, kernel_size=3, strides=1, padding=1),\n                nn.MaxPool2D(pool_size=3, strides=2),\n                nn.Dropout(0.5),\n                nin_block(num_classes, kernel_size=3, strides=1, padding=1),\n                nn.GlobalAvgPool2D(),\n                nn.Flatten())\n            self.net.initialize(init.Xavier())\n        if tab.selected('pytorch'):\n            self.net = nn.Sequential(\n                nin_block(96, kernel_size=11, strides=4, padding=0),\n                nn.MaxPool2d(3, stride=2),\n                nin_block(256, kernel_size=5, strides=1, padding=2),\n                nn.MaxPool2d(3, stride=2),\n                nin_block(384, kernel_size=3, strides=1, padding=1),\n                nn.MaxPool2d(3, stride=2),\n                nn.Dropout(0.5),\n                nin_block(num_classes, kernel_size=3, strides=1, padding=1),\n                nn.AdaptiveAvgPool2d((1, 1)),\n                nn.Flatten())\n            self.net.apply(d2l.init_cnn)\n        if tab.selected('tensorflow'):\n            self.net = tf.keras.models.Sequential([\n                nin_block(96, kernel_size=11, strides=4, padding='valid'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n                nin_block(256, kernel_size=5, strides=1, padding='same'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n                nin_block(384, kernel_size=3, strides=1, padding='same'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n                tf.keras.layers.Dropout(0.5),\n                nin_block(num_classes, kernel_size=3, strides=1, padding='same'),\n                tf.keras.layers.GlobalAvgPool2D(),\n                tf.keras.layers.Flatten()])\n```\n\n```{.python .input}\n%%tab jax\nclass NiN(d2l.Classifier):\n    lr: float = 0.1\n    num_classes = 10\n    training: bool = True\n\n    def setup(self):\n        self.net = nn.Sequential([\n            nin_block(96, kernel_size=(11, 11), strides=(4, 4), padding=(0, 0)),\n            lambda x: nn.max_pool(x, (3, 3), strides=(2, 2)),\n            nin_block(256, kernel_size=(5, 5), strides=(1, 1), padding=(2, 2)),\n            lambda x: nn.max_pool(x, (3, 3), strides=(2, 2)),\n            nin_block(384, kernel_size=(3, 3), strides=(1, 1), padding=(1, 1)),\n            lambda x: nn.max_pool(x, (3, 3), strides=(2, 2)),\n            nn.Dropout(0.5, deterministic=not self.training),\n            nin_block(self.num_classes, kernel_size=(3, 3), strides=1, padding=(1, 1)),\n            lambda x: nn.avg_pool(x, (5, 5)),  # global avg pooling\n            lambda x: x.reshape((x.shape[0], -1))  # flatten\n        ])\n```\n\nWe create a data example to see [**the output shape of each block**]."
    },
    {
      "chunk_id": "7acf2def7a40_2",
      "chapter": "nin",
      "heading": "[**NiN Model**]",
      "text": "```{.python .input}\n%%tab mxnet, pytorch\nNiN().layer_summary((1, 1, 224, 224))\n```\n\n```{.python .input}\n%%tab tensorflow\nNiN().layer_summary((1, 224, 224, 1))\n```\n\n```{.python .input}\n%%tab jax\nNiN(training=False).layer_summary((1, 224, 224, 1))\n```"
    },
    {
      "chunk_id": "306554988892_0",
      "chapter": "nin",
      "heading": "[**Training**]",
      "text": "As before we use Fashion-MNIST to train the model using the same \noptimizer that we used for AlexNet and VGG.\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nmodel = NiN(lr=0.05)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\nif tab.selected('pytorch'):\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\nwith d2l.try_gpu():\n    model = NiN(lr=0.05)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "3ee0032f568f_0",
      "chapter": "nin",
      "heading": "Summary",
      "text": "NiN has dramatically fewer parameters than AlexNet and VGG. This stems primarily from the fact that it needs no giant fully connected layers. Instead, it uses global average pooling to aggregate across all image locations after the last stage of the network body. This obviates the need for expensive (learned) reduction operations and replaces them by a simple average. What surprised researchers at the time was the fact that this averaging operation did not harm accuracy. Note that averaging across a low-resolution representation (with many channels) also adds to the amount of translation invariance that the network can handle. \n\nChoosing fewer convolutions with wide kernels and replacing them by $1 \\times 1$ convolutions aids the quest for fewer parameters further. It can cater for a significant amount of nonlinearity across channels within any given location. Both $1 \\times 1$ convolutions and global average pooling significantly influenced subsequent CNN designs."
    },
    {
      "chunk_id": "4be39b608a85_0",
      "chapter": "nin",
      "heading": "Exercises",
      "text": "1. Why are there two $1\\times 1$ convolutional layers per NiN block? Increase their number to three. Reduce their number to one. What changes?\n1. What changes if you replace the $1 \\times 1$ convolutions by $3 \\times 3$ convolutions? \n1. What happens if you replace the global average pooling by a fully connected layer (speed, accuracy, number of parameters)?\n1. Calculate the resource usage for NiN.\n    1. What is the number of parameters?\n    1. What is the amount of computation?\n    1. What is the amount of memory needed during training?\n    1. What is the amount of memory needed during prediction?\n1. What are possible problems with reducing the $384 \\times 5 \\times 5$ representation to a $10 \\times 5 \\times 5$ representation in one step?\n1. Use the structural design decisions in VGG that led to VGG-11, VGG-16, and VGG-19 to design a family of NiN-like networks.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/79)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/80)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18003)\n:end_tab:"
    },
    {
      "chunk_id": "e34852be0e88_0",
      "chapter": "resnet",
      "heading": "resnet",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Residual Networks (ResNet) and ResNeXt\n:label:`sec_resnet`\n\nAs we design ever deeper networks it becomes imperative to understand how adding layers can increase the complexity and expressiveness of the network.\nEven more important is the ability to design networks where adding layers makes networks strictly more expressive rather than just different.\nTo make some progress we need a bit of mathematics.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx, init\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\nfrom d2l import tensorflow as d2l\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\nimport jax\n```"
    },
    {
      "chunk_id": "aed904e436da_0",
      "chapter": "resnet",
      "heading": "Function Classes",
      "text": "Consider $\\mathcal{F}$, the class of functions that a specific network architecture (together with learning rates and other hyperparameter settings) can reach. That is, for all $f \\in \\mathcal{F}$ there exists some set of parameters (e.g., weights and biases) that can be obtained through training on a suitable dataset. Let's assume that $f^*$ is the \"truth\" function that we really would like to find. If it is in $\\mathcal{F}$, we are in good shape but typically we will not be quite so lucky. Instead, we will try to find some $f^*_\\mathcal{F}$ which is our best bet within $\\mathcal{F}$. For instance,\ngiven a dataset with features $\\mathbf{X}$\nand labels $\\mathbf{y}$,\nwe might try finding it by solving the following optimization problem:\n\n$$f^*_\\mathcal{F} \\stackrel{\\textrm{def}}{=} \\mathop{\\mathrm{argmin}}_f L(\\mathbf{X}, \\mathbf{y}, f) \\textrm{ subject to } f \\in \\mathcal{F}.$$\n\nWe know that regularization :cite:`tikhonov1977solutions,morozov2012methods` may control complexity of $\\mathcal{F}$\nand achieve consistency, so a larger size of training data\ngenerally leads to better $f^*_\\mathcal{F}$. It is only reasonable to assume that if we design a different and more powerful architecture $\\mathcal{F}'$ we should arrive at a better outcome. In other words, we would expect that $f^*_{\\mathcal{F}'}$ is \"better\" than $f^*_{\\mathcal{F}}$. However, if $\\mathcal{F} \\not\\subseteq \\mathcal{F}'$ there is no guarantee that this should even happen. In fact, $f^*_{\\mathcal{F}'}$ might well be worse. As illustrated by :numref:`fig_functionclasses`,\nfor non-nested function classes, a larger function class does not always move closer to the \"truth\" function $f^*$. For instance,\non the left of :numref:`fig_functionclasses`,\nthough $\\mathcal{F}_3$ is closer to $f^*$ than $\\mathcal{F}_1$, $\\mathcal{F}_6$ moves away and there is no guarantee that further increasing the complexity can reduce the distance from $f^*$."
    },
    {
      "chunk_id": "aed904e436da_1",
      "chapter": "resnet",
      "heading": "Function Classes",
      "text": "For instance,\non the left of :numref:`fig_functionclasses`,\nthough $\\mathcal{F}_3$ is closer to $f^*$ than $\\mathcal{F}_1$, $\\mathcal{F}_6$ moves away and there is no guarantee that further increasing the complexity can reduce the distance from $f^*$. With nested function classes\nwhere $\\mathcal{F}_1 \\subseteq \\cdots \\subseteq \\mathcal{F}_6$\non the right of :numref:`fig_functionclasses`,\nwe can avoid the aforementioned issue from the non-nested function classes. ![For non-nested function classes, a larger (indicated by area) function class does not guarantee we will get closer to the \"truth\" function ($\\mathit{f}^*$). This does not happen in nested function classes.](../img/functionclasses.svg)\n:label:`fig_functionclasses`\n\nThus,\nonly if larger function classes contain the smaller ones are we guaranteed that increasing them strictly increases the expressive power of the network. For deep neural networks,\nif we can\ntrain the newly-added layer into an identity function $f(\\mathbf{x}) = \\mathbf{x}$, the new model will be as effective as the original model. As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors. This is the question that :citet:`He.Zhang.Ren.ea.2016` considered when working on very deep computer vision models. At the heart of their proposed *residual network* (*ResNet*) is the idea that every additional layer should\nmore easily\ncontain the identity function as one of its elements. These considerations are rather profound but they led to a surprisingly simple\nsolution, a *residual block*. With it, ResNet won the ImageNet Large Scale Visual Recognition Challenge in 2015. The design had a profound influence on how to\nbuild deep neural networks. For instance, residual blocks have been added to recurrent networks :cite:`prakash2016neural,kim2017residual`. Likewise, Transformers :cite:`Vaswani.Shazeer.Parmar.ea.2017` use them to stack many layers of networks efficiently."
    },
    {
      "chunk_id": "aed904e436da_2",
      "chapter": "resnet",
      "heading": "Function Classes",
      "text": "For instance, residual blocks have been added to recurrent networks :cite:`prakash2016neural,kim2017residual`. Likewise, Transformers :cite:`Vaswani.Shazeer.Parmar.ea.2017` use them to stack many layers of networks efficiently. It is also used in graph neural networks :cite:`Kipf.Welling.2016` and, as a basic concept, it has been used extensively in computer vision :cite:`Redmon.Farhadi.2018,Ren.He.Girshick.ea.2015`. Note that residual networks are predated by highway networks :cite:`srivastava2015highway` that share some of the motivation, albeit without the elegant parametrization around the identity function."
    },
    {
      "chunk_id": "b51afe9881e2_0",
      "chapter": "resnet",
      "heading": "(**Residual Blocks**)",
      "text": ":label:`subsec_residual-blks`\n\nLet's focus on a local part of a neural network, as depicted in :numref:`fig_residual_block`. Denote the input by $\\mathbf{x}$. We assume that $f(\\mathbf{x})$, the desired underlying mapping we want to obtain by learning, is to be used as input to the activation function on the top. On the left,\nthe portion within the dotted-line box\nmust directly learn $f(\\mathbf{x})$. On the right,\nthe portion within the dotted-line box\nneeds to\nlearn the *residual mapping* $g(\\mathbf{x}) = f(\\mathbf{x}) - \\mathbf{x}$,\nwhich is how the residual block derives its name. If the identity mapping $f(\\mathbf{x}) = \\mathbf{x}$ is the desired underlying mapping,\nthe residual mapping amounts to $g(\\mathbf{x}) = 0$ and it is thus easier to learn:\nwe only need to push the weights and biases\nof the\nupper weight layer (e.g., fully connected layer and convolutional layer)\nwithin the dotted-line box\nto zero. The right figure illustrates the *residual block* of ResNet,\nwhere the solid line carrying the layer input\n$\\mathbf{x}$ to the addition operator\nis called a *residual connection* (or *shortcut connection*). With residual blocks, inputs can\nforward propagate faster through the residual connections across layers. In fact,\nthe residual block\ncan be thought of as\na special case of the multi-branch Inception block:\nit has two branches\none of which is the identity mapping. ![In a regular block (left), the portion within the dotted-line box must directly learn the mapping $\\mathit{f}(\\mathbf{x})$. In a residual block (right), the portion within the dotted-line box needs to learn the residual mapping $\\mathit{g}(\\mathbf{x}) = \\mathit{f}(\\mathbf{x}) - \\mathbf{x}$, making the identity mapping $\\mathit{f}(\\mathbf{x}) = \\mathbf{x}$ easier to learn.](../img/residual-block.svg)\n:label:`fig_residual_block`\n\n\nResNet has VGG's full $3\\times 3$ convolutional layer design. The residual block has two $3\\times 3$ convolutional layers with the same number of output channels."
    },
    {
      "chunk_id": "b51afe9881e2_1",
      "chapter": "resnet",
      "heading": "(**Residual Blocks**)",
      "text": "The residual block has two $3\\times 3$ convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. If we want to change the number of channels, we need to introduce an additional $1\\times 1$ convolutional layer to transform the input into the desired shape for the addition operation. Let's have a look at the code below."
    },
    {
      "chunk_id": "b51afe9881e2_2",
      "chapter": "resnet",
      "heading": "(**Residual Blocks**)",
      "text": "If we want to change the number of channels, we need to introduce an additional $1\\times 1$ convolutional layer to transform the input into the desired shape for the addition operation. Let's have a look at the code below. ```{.python .input}\n%%tab mxnet\nclass Residual(nn.Block):  #@save\n    \"\"\"The Residual block of ResNet models.\"\"\"\n    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n        super().__init__(**kwargs)\n        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n                               strides=strides)\n        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\n                                   strides=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm()\n        self.bn2 = nn.BatchNorm()\n\n    def forward(self, X):\n        Y = npx.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        return npx.relu(Y + X)\n```\n\n```{.python .input}\n%%tab pytorch\nclass Residual(nn.Module):  #@save\n    \"\"\"The Residual block of ResNet models.\"\"\"\n    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n                                   stride=strides)\n        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n                                       stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.LazyBatchNorm2d()\n        self.bn2 = nn.LazyBatchNorm2d()\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return F.relu(Y)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Residual(tf.keras.Model):  #@save\n    \"\"\"The Residual block of ResNet models.\"\"\"\n    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(num_channels, padding='same',\n                                            kernel_size=3, strides=strides)\n        self.conv2 = tf.keras.layers.Conv2D(num_channels, kernel_size=3,\n                                            padding='same')\n        self.conv3 = None\n        if use_1x1conv:\n            self.conv3 = tf.keras.layers.Conv2D(num_channels, kernel_size=1,\n                                                strides=strides)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.bn2 = tf.keras.layers.BatchNormalization()\n\n    def call(self, X):\n        Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3 is not None:\n            X = self.conv3(X)\n        Y += X\n        return tf.keras.activations.relu(Y)\n```\n\n```{.python .input}\n%%tab jax\nclass Residual(nn.Module):  #@save\n    \"\"\"The Residual block of ResNet models.\"\"\"\n    num_channels: int\n    use_1x1conv: bool = False\n    strides: tuple = (1, 1)\n    training: bool = True\n\n    def setup(self):\n        self.conv1 = nn.Conv(self.num_channels, kernel_size=(3, 3),\n                             padding='same', strides=self.strides)\n        self.conv2 = nn.Conv(self.num_channels, kernel_size=(3, 3),\n                             padding='same')\n        if self.use_1x1conv:\n            self.conv3 = nn.Conv(self.num_channels, kernel_size=(1, 1),\n                                 strides=self.strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm(not self.training)\n        self.bn2 = nn.BatchNorm(not self.training)\n\n    def __call__(self, X):\n        Y = nn.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return nn.relu(Y)\n```\n\nThis code generates two types of networks: one where we add the input to the output before applying the ReLU nonlinearity whenever `use_1x1conv=False`; and one where we adjust channels and resolution by means of a $1 \\times 1$ convolution before adding."
    },
    {
      "chunk_id": "b51afe9881e2_3",
      "chapter": "resnet",
      "heading": "(**Residual Blocks**)",
      "text": ":numref:`fig_resnet_block` illustrates this. ![ResNet block with and without $1 \\times 1$ convolution, which transforms the input into the desired shape for the addition operation.](../img/resnet-block.svg)\n:label:`fig_resnet_block`\n\nNow let's look at [**a situation where the input and output are of the same shape**], where $1 \\times 1$ convolution is not needed. ```{.python .input}\n%%tab mxnet, pytorch\nif tab.selected('mxnet'):\n    blk = Residual(3)\n    blk.initialize()\nif tab.selected('pytorch'):\n    blk = Residual(3)\nX = d2l.randn(4, 3, 6, 6)\nblk(X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nblk = Residual(3)\nX = d2l.normal((4, 6, 6, 3))\nY = blk(X)\nY.shape\n```\n\n```{.python .input}\n%%tab jax\nblk = Residual(3)\nX = jax.random.normal(d2l.get_key(), (4, 6, 6, 3))\nblk.init_with_output(d2l.get_key(), X)[0].shape\n```\n\nWe also have the option to [**halve the output height and width while increasing the number of output channels**]. In this case we use $1 \\times 1$ convolutions via `use_1x1conv=True`. This comes in handy at the beginning of each ResNet block to reduce the spatial dimensionality via `strides=2`. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nblk = Residual(6, use_1x1conv=True, strides=2)\nif tab.selected('mxnet'):\n    blk.initialize()\nblk(X).shape\n```\n\n```{.python .input}\n%%tab jax\nblk = Residual(6, use_1x1conv=True, strides=(2, 2))\nblk.init_with_output(d2l.get_key(), X)[0].shape\n```"
    },
    {
      "chunk_id": "72318638fc44_0",
      "chapter": "resnet",
      "heading": "[**ResNet Model**]",
      "text": "The first two layers of ResNet are the same as those of the GoogLeNet we described before: the $7\\times 7$ convolutional layer with 64 output channels and a stride of 2 is followed by the $3\\times 3$ max-pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass ResNet(d2l.Classifier):\n    def b1(self):\n        if tab.selected('mxnet'):\n            net = nn.Sequential()\n            net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n                    nn.BatchNorm(), nn.Activation('relu'),\n                    nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n            return net\n        if tab.selected('pytorch'):\n            return nn.Sequential(\n                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n                nn.LazyBatchNorm2d(), nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n        if tab.selected('tensorflow'):\n            return tf.keras.models.Sequential([\n                tf.keras.layers.Conv2D(64, kernel_size=7, strides=2,\n                                       padding='same'),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Activation('relu'),\n                tf.keras.layers.MaxPool2D(pool_size=3, strides=2,\n                                          padding='same')])\n```\n\n```{.python .input}\n%%tab jax\nclass ResNet(d2l.Classifier):\n    arch: tuple\n    lr: float = 0.1\n    num_classes: int = 10\n    training: bool = True\n\n    def setup(self):\n        self.net = self.create_net()\n\n    def b1(self):\n        return nn.Sequential([\n            nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='same'),\n            nn.BatchNorm(not self.training), nn.relu,\n            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2),\n                                  padding='same')])\n```\n\nGoogLeNet uses four modules made up of Inception blocks."
    },
    {
      "chunk_id": "72318638fc44_1",
      "chapter": "resnet",
      "heading": "[**ResNet Model**]",
      "text": "However, ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a max-pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved."
    },
    {
      "chunk_id": "72318638fc44_2",
      "chapter": "resnet",
      "heading": "[**ResNet Model**]",
      "text": "In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved. ```{.python .input}\n%%tab mxnet\n@d2l.add_to_class(ResNet)\ndef block(self, num_residuals, num_channels, first_block=False):\n    blk = nn.Sequential()\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n        else:\n            blk.add(Residual(num_channels))\n    return blk\n```\n\n```{.python .input}\n%%tab pytorch\n@d2l.add_to_class(ResNet)\ndef block(self, num_residuals, num_channels, first_block=False):\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n        else:\n            blk.append(Residual(num_channels))\n    return nn.Sequential(*blk)\n```\n\n```{.python .input}\n%%tab tensorflow\n@d2l.add_to_class(ResNet)\ndef block(self, num_residuals, num_channels, first_block=False):\n    blk = tf.keras.models.Sequential()\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n        else:\n            blk.add(Residual(num_channels))\n    return blk\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(ResNet)\ndef block(self, num_residuals, num_channels, first_block=False):\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(Residual(num_channels, use_1x1conv=True,\n                                strides=(2, 2), training=self.training))\n        else:\n            blk.append(Residual(num_channels, training=self.training))\n    return nn.Sequential(blk)\n```\n\nThen, we add all the modules to ResNet. Here, two residual blocks are used for each module. Lastly, just like GoogLeNet, we add a global average pooling layer, followed by the fully connected layer output."
    },
    {
      "chunk_id": "72318638fc44_3",
      "chapter": "resnet",
      "heading": "[**ResNet Model**]",
      "text": "Here, two residual blocks are used for each module. Lastly, just like GoogLeNet, we add a global average pooling layer, followed by the fully connected layer output. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(ResNet)\ndef __init__(self, arch, lr=0.1, num_classes=10):\n    super(ResNet, self).__init__()\n    self.save_hyperparameters()\n    if tab.selected('mxnet'):\n        self.net = nn.Sequential()\n        self.net.add(self.b1())\n        for i, b in enumerate(arch):\n            self.net.add(self.block(*b, first_block=(i==0)))\n        self.net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n        self.net.initialize(init.Xavier())\n    if tab.selected('pytorch'):\n        self.net = nn.Sequential(self.b1())\n        for i, b in enumerate(arch):\n            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n        self.net.add_module('last', nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n            nn.LazyLinear(num_classes)))\n        self.net.apply(d2l.init_cnn)\n    if tab.selected('tensorflow'):\n        self.net = tf.keras.models.Sequential(self.b1())\n        for i, b in enumerate(arch):\n            self.net.add(self.block(*b, first_block=(i==0)))\n        self.net.add(tf.keras.models.Sequential([\n            tf.keras.layers.GlobalAvgPool2D(),\n            tf.keras.layers.Dense(units=num_classes)]))\n```\n\n```{.python .input}\n# %%tab jax\n@d2l.add_to_class(ResNet)\ndef create_net(self):\n    net = nn.Sequential([self.b1()])\n    for i, b in enumerate(self.arch):\n        net.layers.extend([self.block(*b, first_block=(i==0))])\n    net.layers.extend([nn.Sequential([\n        # Flax does not provide a GlobalAvg2D layer\n        lambda x: nn.avg_pool(x, window_shape=x.shape[1:3],\n                              strides=x.shape[1:3], padding='valid'),\n        lambda x: x.reshape((x.shape[0], -1)),\n        nn.Dense(self.num_classes)])])\n    return net\n```\n\nThere are four convolutional layers in each module (excluding the $1\\times 1$ convolutional layer)."
    },
    {
      "chunk_id": "72318638fc44_4",
      "chapter": "resnet",
      "heading": "[**ResNet Model**]",
      "text": "Together with the first $7\\times 7$ convolutional layer and the final fully connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18. By configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture of ResNet is similar to that of GoogLeNet, ResNet's structure is simpler and easier to modify. All these factors have resulted in the rapid and widespread use of ResNet. :numref:`fig_resnet18` depicts the full ResNet-18. ![The ResNet-18 architecture.](../img/resnet18-90.svg)\n:label:`fig_resnet18`\n\nBefore training ResNet, let's [**observe how the input shape changes across different modules in ResNet**]. As in all the previous architectures, the resolution decreases while the number of channels increases up until the point where a global average pooling layer aggregates all features. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass ResNet18(ResNet):\n    def __init__(self, lr=0.1, num_classes=10):\n        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n                       lr, num_classes)\n```\n\n```{.python .input}\n%%tab jax\nclass ResNet18(ResNet):\n    arch: tuple = ((2, 64), (2, 128), (2, 256), (2, 512))\n    lr: float = 0.1\n    num_classes: int = 10\n```\n\n```{.python .input}\n%%tab pytorch, mxnet\nResNet18().layer_summary((1, 1, 96, 96))\n```\n\n```{.python .input}\n%%tab tensorflow\nResNet18().layer_summary((1, 96, 96, 1))\n```\n\n```{.python .input}\n%%tab jax\nResNet18(training=False).layer_summary((1, 96, 96, 1))\n```"
    },
    {
      "chunk_id": "bc564fc70366_0",
      "chapter": "resnet",
      "heading": "[**Training**]",
      "text": "We train ResNet on the Fashion-MNIST dataset, just like before. ResNet is quite a powerful and flexible architecture. The plot capturing training and validation loss illustrates a significant gap between both graphs, with the training loss being considerably lower. For a network of this flexibility, more training data would offer distinct benefit in closing the gap and improving accuracy.\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nmodel = ResNet18(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nif tab.selected('pytorch'):\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nwith d2l.try_gpu():\n    model = ResNet18(lr=0.01)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "aac7ff7fb965_0",
      "chapter": "resnet",
      "heading": "ResNeXt",
      "text": ":label:`subsec_resnext`\n\nOne of the challenges one encounters in the design of ResNet is the trade-off between nonlinearity and dimensionality within a given block. That is, we could add more nonlinearity by increasing the number of layers, or by increasing the width of the convolutions. An alternative strategy is to increase the number of channels that can carry information between blocks. Unfortunately, the latter comes with a quadratic penalty since the computational cost of ingesting $c_\\textrm{i}$ channels and emitting $c_\\textrm{o}$ channels is proportional to $\\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o})$ (see our discussion in :numref:`sec_channels`). We can take some inspiration from the Inception block of :numref:`fig_inception` which has information flowing through the block in separate groups. Applying the idea of multiple independent groups to the ResNet block of :numref:`fig_resnet_block` led to the design of ResNeXt :cite:`Xie.Girshick.Dollar.ea.2017`. Different from the smorgasbord of transformations in Inception, \nResNeXt adopts the *same* transformation in all branches,\nthus minimizing the need for manual tuning of each branch. ![The ResNeXt block. The use of grouped convolution with $\\mathit{g}$ groups is $\\mathit{g}$ times faster than a dense convolution. It is a bottleneck residual block when the number of intermediate channels $\\mathit{b}$ is less than $\\mathit{c}$.](../img/resnext-block.svg)\n:label:`fig_resnext_block`\n\nBreaking up a convolution from $c_\\textrm{i}$ to $c_\\textrm{o}$ channels into one of $g$ groups of size $c_\\textrm{i}/g$ generating $g$ outputs of size $c_\\textrm{o}/g$ is called, quite fittingly, a *grouped convolution*. The computational cost (proportionally) is reduced from $\\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o})$ to $\\mathcal{O}(g \\cdot (c_\\textrm{i}/g) \\cdot (c_\\textrm{o}/g)) = \\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o} / g)$, i.e., it is $g$ times faster."
    },
    {
      "chunk_id": "aac7ff7fb965_1",
      "chapter": "resnet",
      "heading": "ResNeXt",
      "text": "The computational cost (proportionally) is reduced from $\\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o})$ to $\\mathcal{O}(g \\cdot (c_\\textrm{i}/g) \\cdot (c_\\textrm{o}/g)) = \\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o} / g)$, i.e., it is $g$ times faster. Even better, the number of parameters needed to generate the output is also reduced from a $c_\\textrm{i} \\times c_\\textrm{o}$ matrix to $g$ smaller matrices of size $(c_\\textrm{i}/g) \\times (c_\\textrm{o}/g)$, again a $g$ times reduction. In what follows we assume that both $c_\\textrm{i}$ and $c_\\textrm{o}$ are divisible by $g$. The only challenge in this design is that no information is exchanged between the $g$ groups. The ResNeXt block of \n:numref:`fig_resnext_block` amends this in two ways: the grouped convolution with a $3 \\times 3$ kernel is sandwiched in between two $1 \\times 1$ convolutions. The second one serves double duty in changing the number of channels back. The benefit is that we only pay the $\\mathcal{O}(c \\cdot b)$ cost for $1 \\times 1$ kernels and can make do with an $\\mathcal{O}(b^2 / g)$ cost for $3 \\times 3$ kernels. Similar to the residual block implementation in\n:numref:`subsec_residual-blks`, the residual connection is replaced (thus generalized) by a $1 \\times 1$ convolution. The right-hand figure in :numref:`fig_resnext_block` provides a much more concise summary of the resulting network block. It will also play a major role in the design of generic modern CNNs in :numref:`sec_cnn-design`. Note that the idea of grouped convolutions dates back to the implementation of AlexNet :cite:`Krizhevsky.Sutskever.Hinton.2012`. When distributing the network across two GPUs with limited memory, the implementation treated each GPU as its own channel with no ill effects. The following implementation of the `ResNeXtBlock` class takes as argument `groups` ($g$), with \n`bot_channels` ($b$) intermediate (bottleneck) channels."
    },
    {
      "chunk_id": "aac7ff7fb965_2",
      "chapter": "resnet",
      "heading": "ResNeXt",
      "text": "The following implementation of the `ResNeXtBlock` class takes as argument `groups` ($g$), with \n`bot_channels` ($b$) intermediate (bottleneck) channels. Lastly, when we need to reduce the height and width of the representation, we add a stride of $2$ by setting `use_1x1conv=True, strides=2`."
    },
    {
      "chunk_id": "aac7ff7fb965_3",
      "chapter": "resnet",
      "heading": "ResNeXt",
      "text": "Lastly, when we need to reduce the height and width of the representation, we add a stride of $2$ by setting `use_1x1conv=True, strides=2`. ```{.python .input}\n%%tab mxnet\nclass ResNeXtBlock(nn.Block):  #@save\n    \"\"\"The ResNeXt block.\"\"\"\n    def __init__(self, num_channels, groups, bot_mul,\n                 use_1x1conv=False, strides=1, **kwargs):\n        super().__init__(**kwargs)\n        bot_channels = int(round(num_channels * bot_mul))\n        self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0,\n                               strides=1)\n        self.conv2 = nn.Conv2D(bot_channels, kernel_size=3, padding=1, \n                               strides=strides, groups=bot_channels//groups)\n        self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0,\n                               strides=1)\n        self.bn1 = nn.BatchNorm()\n        self.bn2 = nn.BatchNorm()\n        self.bn3 = nn.BatchNorm()\n        if use_1x1conv:\n            self.conv4 = nn.Conv2D(num_channels, kernel_size=1,\n                                   strides=strides)\n            self.bn4 = nn.BatchNorm()\n        else:\n            self.conv4 = None\n\n    def forward(self, X):\n        Y = npx.relu(self.bn1(self.conv1(X)))\n        Y = npx.relu(self.bn2(self.conv2(Y)))\n        Y = self.bn3(self.conv3(Y))\n        if self.conv4:\n            X = self.bn4(self.conv4(X))\n        return npx.relu(Y + X)\n```\n\n```{.python .input}\n%%tab pytorch\nclass ResNeXtBlock(nn.Module):  #@save\n    \"\"\"The ResNeXt block.\"\"\"\n    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n                 strides=1):\n        super().__init__()\n        bot_channels = int(round(num_channels * bot_mul))\n        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n                                   stride=strides, padding=1,\n                                   groups=bot_channels//groups)\n        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n        self.bn1 = nn.LazyBatchNorm2d()\n        self.bn2 = nn.LazyBatchNorm2d()\n        self.bn3 = nn.LazyBatchNorm2d()\n        if use_1x1conv:\n            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1, \n                                       stride=strides)\n            self.bn4 = nn.LazyBatchNorm2d()\n        else:\n            self.conv4 = None\n\n    def forward(self, X):\n        Y = F.relu(self.bn1(self.conv1(X)))\n        Y = F.relu(self.bn2(self.conv2(Y)))\n        Y = self.bn3(self.conv3(Y))\n        if self.conv4:\n            X = self.bn4(self.conv4(X))\n        return F.relu(Y + X)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass ResNeXtBlock(tf.keras.Model):  #@save\n    \"\"\"The ResNeXt block.\"\"\"\n    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n                 strides=1):\n        super().__init__()\n        bot_channels = int(round(num_channels * bot_mul))\n        self.conv1 = tf.keras.layers.Conv2D(bot_channels, 1, strides=1)\n        self.conv2 = tf.keras.layers.Conv2D(bot_channels, 3, strides=strides,\n                                            padding=\"same\",\n                                            groups=bot_channels//groups)\n        self.conv3 = tf.keras.layers.Conv2D(num_channels, 1, strides=1)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        if use_1x1conv:\n            self.conv4 = tf.keras.layers.Conv2D(num_channels, 1,\n                                                strides=strides)\n            self.bn4 = tf.keras.layers.BatchNormalization()\n        else:\n            self.conv4 = None\n\n    def call(self, X):\n        Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))\n        Y = tf.keras.activations.relu(self.bn2(self.conv2(Y)))\n        Y = self.bn3(self.conv3(Y))\n        if self.conv4:\n            X = self.bn4(self.conv4(X))\n        return tf.keras.activations.relu(Y + X)\n```\n\n```{.python .input}\n%%tab jax\nclass ResNeXtBlock(nn.Module):  #@save\n    \"\"\"The ResNeXt block.\"\"\"\n    num_channels: int\n    groups: int\n    bot_mul: int\n    use_1x1conv: bool = False\n    strides: tuple = (1, 1)\n    training: bool = True\n\n    def setup(self):\n        bot_channels = int(round(self.num_channels * self.bot_mul))\n        self.conv1 = nn.Conv(bot_channels, kernel_size=(1, 1),\n                               strides=(1, 1))\n        self.conv2 = nn.Conv(bot_channels, kernel_size=(3, 3),\n                               strides=self.strides, padding='same',\n                               feature_group_count=bot_channels//self.groups)\n        self.conv3 = nn.Conv(self.num_channels, kernel_size=(1, 1),\n                               strides=(1, 1))\n        self.bn1 = nn.BatchNorm(not self.training)\n        self.bn2 = nn.BatchNorm(not self.training)\n        self.bn3 = nn.BatchNorm(not self.training)\n        if self.use_1x1conv:\n            self.conv4 = nn.Conv(self.num_channels, kernel_size=(1, 1),\n                                       strides=self.strides)\n            self.bn4 = nn.BatchNorm(not self.training)\n        else:\n            self.conv4 = None\n\n    def __call__(self, X):\n        Y = nn.relu(self.bn1(self.conv1(X)))\n        Y = nn.relu(self.bn2(self.conv2(Y)))\n        Y = self.bn3(self.conv3(Y))\n        if self.conv4:\n            X = self.bn4(self.conv4(X))\n        return nn.relu(Y + X)\n```\n\nIts use is entirely analogous to that of the `ResNetBlock` discussed previously."
    },
    {
      "chunk_id": "aac7ff7fb965_4",
      "chapter": "resnet",
      "heading": "ResNeXt",
      "text": "For instance, when using (`use_1x1conv=False, strides=1`), the input and output are of the same shape. Alternatively, setting `use_1x1conv=True, strides=2` halves the output height and width. ```{.python .input}\n%%tab mxnet, pytorch\nblk = ResNeXtBlock(32, 16, 1)\nif tab.selected('mxnet'):\n    blk.initialize()\nX = d2l.randn(4, 32, 96, 96)\nblk(X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nblk = ResNeXtBlock(32, 16, 1)\nX = d2l.normal((4, 96, 96, 32))\nY = blk(X)\nY.shape\n```\n\n```{.python .input}\n%%tab jax\nblk = ResNeXtBlock(32, 16, 1)\nX = jnp.zeros((4, 96, 96, 32))\nblk.init_with_output(d2l.get_key(), X)[0].shape\n```"
    },
    {
      "chunk_id": "b329a8543001_0",
      "chapter": "resnet",
      "heading": "Summary and Discussion",
      "text": "Nested function classes are desirable since they allow us to obtain strictly *more powerful* rather than also subtly *different* function classes when adding capacity. One way of accomplishing this is by letting additional layers to simply pass through the input to the output. Residual connections allow for this. As a consequence, this changes the inductive bias from simple functions being of the form $f(\\mathbf{x}) = 0$ to simple functions looking like $f(\\mathbf{x}) = \\mathbf{x}$. The residual mapping can learn the identity function more easily, such as pushing parameters in the weight layer to zero. We can train an effective *deep* neural network by having residual blocks. Inputs can forward propagate faster through the residual connections across layers. As a consequence, we can thus train much deeper networks. For instance, the original ResNet paper :cite:`He.Zhang.Ren.ea.2016` allowed for up to 152 layers. Another benefit of residual networks is that it allows us to add layers, initialized as the identity function, *during* the training process. After all, the default behavior of a layer is to let the data pass through unchanged. This can accelerate the training of very large networks in some cases. Prior to residual connections,\nbypassing paths with gating units were introduced\nto effectively train highway networks with over 100 layers\n:cite:`srivastava2015highway`. Using identity functions as bypassing paths,\nResNet performed remarkably well\non multiple computer vision tasks. Residual connections had a major influence on the design of subsequent deep neural networks, of either convolutional or sequential nature. As we will introduce later,\nthe Transformer architecture :cite:`Vaswani.Shazeer.Parmar.ea.2017`\nadopts residual connections (together with other design choices) and is pervasive\nin areas as diverse as\nlanguage, vision, speech, and reinforcement learning."
    },
    {
      "chunk_id": "b329a8543001_1",
      "chapter": "resnet",
      "heading": "Summary and Discussion",
      "text": "As we will introduce later,\nthe Transformer architecture :cite:`Vaswani.Shazeer.Parmar.ea.2017`\nadopts residual connections (together with other design choices) and is pervasive\nin areas as diverse as\nlanguage, vision, speech, and reinforcement learning. ResNeXt is an example for how the design of convolutional neural networks has evolved over time: by being more frugal with computation and trading it off against the size of the activations (number of channels), it allows for faster and more accurate networks at lower cost. An alternative way of viewing grouped convolutions is to think of a block-diagonal matrix for the convolutional weights. Note that there are quite a few such \"tricks\" that lead to more efficient networks. For instance, ShiftNet :cite:`wu2018shift` mimicks the effects of a $3 \\times 3$ convolution, simply by adding shifted activations to the channels, offering increased function complexity, this time without any computational cost. A common feature of the designs we have discussed so far is that the network design is fairly manual, primarily relying on the ingenuity of the designer to find the \"right\" network hyperparameters. While clearly feasible, it is also very costly in terms of human time and there is no guarantee that the outcome is optimal in any sense. In :numref:`sec_cnn-design` we will discuss a number of strategies for obtaining high quality networks in a more automated fashion. In particular, we will review the notion of *network design spaces* that led to the RegNetX/Y models\n:cite:`Radosavovic.Kosaraju.Girshick.ea.2020`."
    },
    {
      "chunk_id": "13aa60143070_0",
      "chapter": "resnet",
      "heading": "Exercises",
      "text": "1. What are the major differences between the Inception block in :numref:`fig_inception` and the residual block? How do they compare in terms of computation, accuracy, and the classes of functions they can describe?\n1. Refer to Table 1 in the ResNet paper :cite:`He.Zhang.Ren.ea.2016` to implement different variants of the network. \n1. For deeper networks, ResNet introduces a \"bottleneck\" architecture to reduce model complexity. Try to implement it.\n1. In subsequent versions of ResNet, the authors changed the \"convolution, batch normalization, and activation\" structure to the \"batch normalization, activation, and convolution\" structure. Make this improvement yourself. See Figure 1 in :citet:`He.Zhang.Ren.ea.2016*1` for details.\n1. Why can't we just increase the complexity of functions without bound, even if the function classes are nested?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/85)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/86)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/8737)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18006)\n:end_tab:"
    },
    {
      "chunk_id": "fa3b49bb1d00_0",
      "chapter": "vgg",
      "heading": "vgg",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Networks Using Blocks (VGG)\n:label:`sec_vgg`\n\nWhile AlexNet offered empirical evidence that deep CNNs\ncan achieve good results, it did not provide a general template\nto guide subsequent researchers in designing new networks.\nIn the following sections, we will introduce several heuristic concepts\ncommonly used to design deep networks.\n\nProgress in this field mirrors that of VLSI (very large scale integration) \nin chip design\nwhere engineers moved from placing transistors\nto logical elements to logic blocks :cite:`Mead.1980`.\nSimilarly, the design of neural network architectures\nhas grown progressively more abstract,\nwith researchers moving from thinking in terms of\nindividual neurons to whole layers,\nand now to blocks, repeating patterns of layers. A decade later, this has now\nprogressed to researchers using entire trained models to repurpose them for different, \nalbeit related, tasks. Such large pretrained models are typically called \n*foundation models* :cite:`bommasani2021opportunities`. \n\nBack to network design. The idea of using blocks first emerged from the\nVisual Geometry Group (VGG) at Oxford University,\nin their eponymously-named *VGG* network :cite:`Simonyan.Zisserman.2014`.\nIt is easy to implement these repeated structures in code\nwith any modern deep learning framework by using loops and subroutines.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx, init\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\nfrom d2l import tensorflow as d2l\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\n```"
    },
    {
      "chunk_id": "bc081b3def57_0",
      "chapter": "vgg",
      "heading": "(**VGG Blocks**)",
      "text": ":label:`subsec_vgg-blocks`\n\nThe basic building block of CNNs\nis a sequence of the following:\n(i) a convolutional layer\nwith padding to maintain the resolution,\n(ii) a nonlinearity such as a ReLU,\n(iii) a pooling layer such\nas max-pooling to reduce the resolution. One of the problems with \nthis approach is that the spatial resolution decreases quite rapidly. In particular, \nthis imposes a hard limit of $\\log_2 d$ convolutional layers on the network before all \ndimensions ($d$) are used up. For instance, in the case of ImageNet, it would be impossible to have \nmore than 8 convolutional layers in this way. The key idea of :citet:`Simonyan.Zisserman.2014` was to use *multiple* convolutions in between downsampling\nvia max-pooling in the form of a block. They were primarily interested in whether deep or \nwide networks perform better. For instance, the successive application of two $3 \\times 3$ convolutions\ntouches the same pixels as a single $5 \\times 5$ convolution does. At the same time, the latter uses approximately \nas many parameters ($25 \\cdot c^2$) as three $3 \\times 3$ convolutions do ($3 \\cdot 9 \\cdot c^2$). In a rather detailed analysis they showed that deep and narrow networks significantly outperform their shallow counterparts. This set deep learning on a quest for ever deeper networks with over 100 layers for typical applications. Stacking $3 \\times 3$ convolutions\nhas become a gold standard in later deep networks (a design decision only to be revisited recently by \n:citet:`liu2022convnet`). Consequently, fast implementations for small convolutions have become a staple on GPUs :cite:`lavin2016fast`. Back to VGG: a VGG block consists of a *sequence* of convolutions with $3\\times3$ kernels with padding of 1 \n(keeping height and width) followed by a $2 \\times 2$ max-pooling layer with stride of 2\n(halving height and width after each block). In the code below, we define a function called `vgg_block`\nto implement one VGG block."
    },
    {
      "chunk_id": "bc081b3def57_1",
      "chapter": "vgg",
      "heading": "(**VGG Blocks**)",
      "text": "In the code below, we define a function called `vgg_block`\nto implement one VGG block. The function below takes two arguments,\ncorresponding to the number of convolutional layers `num_convs`\nand the number of output channels `num_channels`. ```{.python .input  n=2}\n%%tab mxnet\ndef vgg_block(num_convs, num_channels):\n    blk = nn.Sequential()\n    for _ in range(num_convs):\n        blk.add(nn.Conv2D(num_channels, kernel_size=3,\n                          padding=1, activation='relu'))\n    blk.add(nn.MaxPool2D(pool_size=2, strides=2))\n    return blk\n```\n\n```{.python .input  n=3}\n%%tab pytorch\ndef vgg_block(num_convs, out_channels):\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n        layers.append(nn.ReLU())\n    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n    return nn.Sequential(*layers)\n```\n\n```{.python .input  n=4}\n%%tab tensorflow\ndef vgg_block(num_convs, num_channels):\n    blk = tf.keras.models.Sequential()\n    for _ in range(num_convs):\n        blk.add(\n            tf.keras.layers.Conv2D(num_channels, kernel_size=3,\n                                   padding='same', activation='relu'))\n    blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n    return blk\n```\n\n```{.python .input}\n%%tab jax\ndef vgg_block(num_convs, out_channels):\n    layers = []\n    for _ in range(num_convs):\n        layers.append(nn.Conv(out_channels, kernel_size=(3, 3), padding=(1, 1)))\n        layers.append(nn.relu)\n    layers.append(lambda x: nn.max_pool(x, window_shape=(2, 2), strides=(2, 2)))\n    return nn.Sequential(layers)\n```"
    },
    {
      "chunk_id": "d7a9b32c943e_0",
      "chapter": "vgg",
      "heading": "[**VGG Network**]",
      "text": ":label:`subsec_vgg-network`\n\nLike AlexNet and LeNet, \nthe VGG Network can be partitioned into two parts:\nthe first consisting mostly of convolutional and pooling layers\nand the second consisting of fully connected layers that are identical to those in AlexNet. The key difference is \nthat the convolutional layers are grouped in nonlinear transformations that \nleave the dimensonality unchanged, followed by a resolution-reduction step, as \ndepicted in :numref:`fig_vgg`. ![From AlexNet to VGG. The key difference is that VGG consists of blocks of layers, whereas AlexNet's layers are all designed individually.](../img/vgg.svg)\n:width:`400px`\n:label:`fig_vgg`\n\nThe convolutional part of the network connects several VGG blocks from :numref:`fig_vgg` (also defined in the `vgg_block` function)\nin succession. This grouping of convolutions is a pattern that has \nremained almost unchanged over the past decade, although the specific choice of \noperations has undergone considerable modifications. The variable `arch` consists of a list of tuples (one per block),\nwhere each contains two values: the number of convolutional layers\nand the number of output channels,\nwhich are precisely the arguments required to call\nthe `vgg_block` function. As such, VGG defines a *family* of networks rather than just \na specific manifestation. To build a specific network we simply iterate over `arch` to compose the blocks."
    },
    {
      "chunk_id": "d7a9b32c943e_1",
      "chapter": "vgg",
      "heading": "[**VGG Network**]",
      "text": "As such, VGG defines a *family* of networks rather than just \na specific manifestation. To build a specific network we simply iterate over `arch` to compose the blocks. ```{.python .input  n=5}\n%%tab pytorch, mxnet, tensorflow\nclass VGG(d2l.Classifier):\n    def __init__(self, arch, lr=0.1, num_classes=10):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Sequential()\n            for (num_convs, num_channels) in arch:\n                self.net.add(vgg_block(num_convs, num_channels))\n            self.net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n                         nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n                         nn.Dense(num_classes))\n            self.net.initialize(init.Xavier())\n        if tab.selected('pytorch'):\n            conv_blks = []\n            for (num_convs, out_channels) in arch:\n                conv_blks.append(vgg_block(num_convs, out_channels))\n            self.net = nn.Sequential(\n                *conv_blks, nn.Flatten(),\n                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n                nn.LazyLinear(num_classes))\n            self.net.apply(d2l.init_cnn)\n        if tab.selected('tensorflow'):\n            self.net = tf.keras.models.Sequential()\n            for (num_convs, num_channels) in arch:\n                self.net.add(vgg_block(num_convs, num_channels))\n            self.net.add(\n                tf.keras.models.Sequential([\n                tf.keras.layers.Flatten(),\n                tf.keras.layers.Dense(4096, activation='relu'),\n                tf.keras.layers.Dropout(0.5),\n                tf.keras.layers.Dense(4096, activation='relu'),\n                tf.keras.layers.Dropout(0.5),\n                tf.keras.layers.Dense(num_classes)]))\n```\n\n```{.python .input  n=5}\n%%tab jax\nclass VGG(d2l.Classifier):\n    arch: list\n    lr: float = 0.1\n    num_classes: int = 10\n    training: bool = True\n\n    def setup(self):\n        conv_blks = []\n        for (num_convs, out_channels) in self.arch:\n            conv_blks.append(vgg_block(num_convs, out_channels))\n\n        self.net = nn.Sequential([\n            *conv_blks,\n            lambda x: x.reshape((x.shape[0], -1)),  # flatten\n            nn.Dense(4096), nn.relu,\n            nn.Dropout(0.5, deterministic=not self.training),\n            nn.Dense(4096), nn.relu,\n            nn.Dropout(0.5, deterministic=not self.training),\n            nn.Dense(self.num_classes)])\n```\n\nThe original VGG network had five convolutional blocks,\namong which the first two have one convolutional layer each\nand the latter three contain two convolutional layers each."
    },
    {
      "chunk_id": "d7a9b32c943e_2",
      "chapter": "vgg",
      "heading": "[**VGG Network**]",
      "text": "The first block has 64 output channels\nand each subsequent block doubles the number of output channels,\nuntil that number reaches 512. Since this network uses eight convolutional layers\nand three fully connected layers, it is often called VGG-11. ```{.python .input  n=6}\n%%tab pytorch, mxnet\nVGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n    (1, 1, 224, 224))\n```\n\n```{.python .input  n=7}\n%%tab tensorflow\nVGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n    (1, 224, 224, 1))\n```\n\n```{.python .input}\n%%tab jax\nVGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)),\n    training=False).layer_summary((1, 224, 224, 1))\n```\n\nAs you can see, we halve height and width at each block,\nfinally reaching a height and width of 7\nbefore flattening the representations\nfor processing by the fully connected part of the network. :citet:`Simonyan.Zisserman.2014` described several other variants of VGG. In fact, it has become the norm to propose *families* of networks with \ndifferent speed--accuracy trade-off when introducing a new architecture."
    },
    {
      "chunk_id": "894f90c3c7b0_0",
      "chapter": "vgg",
      "heading": "Training",
      "text": "[**Since VGG-11 is computationally more demanding than AlexNet\nwe construct a network with a smaller number of channels.**]\nThis is more than sufficient for training on Fashion-MNIST.\nThe [**model training**] process is similar to that of AlexNet in :numref:`sec_alexnet`. \nAgain observe the close match between validation and training loss, \nsuggesting only a small amount of overfitting.\n\n```{.python .input  n=8}\n%%tab mxnet, pytorch, jax\nmodel = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\nif tab.selected('pytorch'):\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n```\n\n```{.python .input  n=9}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\nwith d2l.try_gpu():\n    model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "599abd245867_0",
      "chapter": "vgg",
      "heading": "Summary",
      "text": "One might argue that VGG is the first truly modern convolutional neural network. While AlexNet introduced many of the components of what make deep learning effective at scale, it is VGG that arguably introduced key properties such as blocks of multiple convolutions and a preference for deep and narrow networks. It is also the first network that is actually an entire family of similarly parametrized models, giving the practitioner ample trade-off between complexity and speed. This is also the place where modern deep learning frameworks shine. It is no longer necessary to generate XML configuration files to specify a network but rather, to assemble said networks through simple Python code. \n\nMore recently ParNet :cite:`Goyal.Bochkovskiy.Deng.ea.2021` demonstrated that it is possible to achieve competitive performance using a much more shallow architecture through a large number of parallel computations. This is an exciting development and there is hope that it will influence architecture designs in the future. For the remainder of the chapter, though, we will follow the path of scientific progress over the past decade."
    },
    {
      "chunk_id": "82e50e21b5c7_0",
      "chapter": "vgg",
      "heading": "Exercises",
      "text": "1. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory. \n    1. Compare the number of parameters needed for AlexNet and VGG.\n    1. Compare the number of floating point operations used in the convolutional layers and in the fully connected layers. \n    1. How could you reduce the computational cost created by the fully connected layers?\n1. When displaying the dimensions associated with the various layers of the network, we only see the information associated with eight blocks (plus some auxiliary transforms), even though the network has 11 layers. Where did the remaining three layers go?\n1. Use Table 1 in the VGG paper :cite:`Simonyan.Zisserman.2014` to construct other common models, such as VGG-16 or VGG-19.\n1. Upsampling the resolution in Fashion-MNIST eight-fold from $28 \\times 28$ to $224 \\times 224$ dimensions is very wasteful. Try modifying the network architecture and resolution conversion, e.g., to 56 or to 84 dimensions for its input instead. Can you do so without reducing the accuracy of the network? Consult the VGG paper :cite:`Simonyan.Zisserman.2014` for ideas on adding more nonlinearities prior to downsampling.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/77)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/78)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/277)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18002)\n:end_tab:"
    },
    {
      "chunk_id": "73d0021e6f49_0",
      "chapter": "channels",
      "heading": "channels",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Multiple Input and Multiple Output Channels\n:label:`sec_channels`\n\nWhile we described the multiple channels\nthat comprise each image (e.g., color images have the standard RGB channels\nto indicate the amount of red, green and blue) and convolutional layers for multiple channels in :numref:`subsec_why-conv-channels`,\nuntil now, we simplified all of our numerical examples\nby working with just a single input and a single output channel.\nThis allowed us to think of our inputs, convolution kernels,\nand outputs each as two-dimensional tensors.\n\nWhen we add channels into the mix,\nour inputs and hidden representations\nboth become three-dimensional tensors.\nFor example, each RGB input image has shape $3\\times h\\times w$.\nWe refer to this axis, with a size of 3, as the *channel* dimension. The notion of\nchannels is as old as CNNs themselves: for instance LeNet-5 :cite:`LeCun.Jackel.Bottou.ea.1995` uses them. \nIn this section, we will take a deeper look\nat convolution kernels with multiple input and multiple output channels.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```"
    },
    {
      "chunk_id": "4edf20383f3e_0",
      "chapter": "channels",
      "heading": "Multiple Input Channels",
      "text": "When the input data contains multiple channels,\nwe need to construct a convolution kernel\nwith the same number of input channels as the input data,\nso that it can perform cross-correlation with the input data. Assuming that the number of channels for the input data is $c_\\textrm{i}$,\nthe number of input channels of the convolution kernel also needs to be $c_\\textrm{i}$. If our convolution kernel's window shape is $k_\\textrm{h}\\times k_\\textrm{w}$,\nthen, when $c_\\textrm{i}=1$, we can think of our convolution kernel\nas just a two-dimensional tensor of shape $k_\\textrm{h}\\times k_\\textrm{w}$. However, when $c_\\textrm{i}>1$, we need a kernel\nthat contains a tensor of shape $k_\\textrm{h}\\times k_\\textrm{w}$ for *every* input channel. Concatenating these $c_\\textrm{i}$ tensors together\nyields a convolution kernel of shape $c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$. Since the input and convolution kernel each have $c_\\textrm{i}$ channels,\nwe can perform a cross-correlation operation\non the two-dimensional tensor of the input\nand the two-dimensional tensor of the convolution kernel\nfor each channel, adding the $c_\\textrm{i}$ results together\n(summing over the channels)\nto yield a two-dimensional tensor. This is the result of a two-dimensional cross-correlation\nbetween a multi-channel input and\na multi-input-channel convolution kernel. :numref:`fig_conv_multi_in` provides an example \nof a two-dimensional cross-correlation with two input channels. The shaded portions are the first output element\nas well as the input and kernel tensor elements used for the output computation:\n$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$. ![Cross-correlation computation with two input channels.](../img/conv-multi-in.svg)\n:label:`fig_conv_multi_in`\n\n\nTo make sure we really understand what is going on here,\nwe can (**implement cross-correlation operations with multiple input channels**) ourselves."
    },
    {
      "chunk_id": "4edf20383f3e_1",
      "chapter": "channels",
      "heading": "Multiple Input Channels",
      "text": "Notice that all we are doing is performing a cross-correlation operation\nper channel and then adding up the results. ```{.python .input}\n%%tab mxnet, pytorch, jax\ndef corr2d_multi_in(X, K):\n    # Iterate through the 0th dimension (channel) of K first, then add them up\n    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\n```\n\n```{.python .input}\n%%tab tensorflow\ndef corr2d_multi_in(X, K):\n    # Iterate through the 0th dimension (channel) of K first, then add them up\n    return tf.reduce_sum([d2l.corr2d(x, k) for x, k in zip(X, K)], axis=0)\n```\n\nWe can construct the input tensor `X` and the kernel tensor `K`\ncorresponding to the values in :numref:`fig_conv_multi_in`\nto (**validate the output**) of the cross-correlation operation. ```{.python .input}\n%%tab all\nX = d2l.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\nK = d2l.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n\ncorr2d_multi_in(X, K)\n```"
    },
    {
      "chunk_id": "79dba9062d98_0",
      "chapter": "channels",
      "heading": "Multiple Output Channels",
      "text": ":label:`subsec_multi-output-channels`\n\nRegardless of the number of input channels,\nso far we always ended up with one output channel. However, as we discussed in :numref:`subsec_why-conv-channels`,\nit turns out to be essential to have multiple channels at each layer. In the most popular neural network architectures,\nwe actually increase the channel dimension\nas we go deeper in the neural network,\ntypically downsampling to trade off spatial resolution\nfor greater *channel depth*. Intuitively, you could think of each channel\nas responding to a different set of features. The reality is a bit more complicated than this. A naive interpretation would suggest \nthat representations are learned independently per pixel or per channel. Instead, channels are optimized to be jointly useful. This means that rather than mapping a single channel to an edge detector, it may simply mean \nthat some direction in channel space corresponds to detecting edges. Denote by $c_\\textrm{i}$ and $c_\\textrm{o}$ the number\nof input and output channels, respectively,\nand by $k_\\textrm{h}$ and $k_\\textrm{w}$ the height and width of the kernel. To get an output with multiple channels,\nwe can create a kernel tensor\nof shape $c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$\nfor *every* output channel. We concatenate them on the output channel dimension,\nso that the shape of the convolution kernel\nis $c_\\textrm{o}\\times c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$. In cross-correlation operations,\nthe result on each output channel is calculated\nfrom the convolution kernel corresponding to that output channel\nand takes input from all channels in the input tensor. We implement a cross-correlation function\nto [**calculate the output of multiple channels**] as shown below. ```{.python .input}\n%%tab all\ndef corr2d_multi_in_out(X, K):\n    # Iterate through the 0th dimension of K, and each time, perform\n    # cross-correlation operations with input X."
    },
    {
      "chunk_id": "79dba9062d98_1",
      "chapter": "channels",
      "heading": "Multiple Output Channels",
      "text": "```{.python .input}\n%%tab all\ndef corr2d_multi_in_out(X, K):\n    # Iterate through the 0th dimension of K, and each time, perform\n    # cross-correlation operations with input X. All of the results are\n    # stacked together\n    return d2l.stack([corr2d_multi_in(X, k) for k in K], 0)\n```\n\nWe construct a trivial convolution kernel with three output channels\nby concatenating the kernel tensor for `K` with `K+1` and `K+2`. ```{.python .input}\n%%tab all\nK = d2l.stack((K, K + 1, K + 2), 0)\nK.shape\n```\n\nBelow, we perform cross-correlation operations\non the input tensor `X` with the kernel tensor `K`. Now the output contains three channels. The result of the first channel is consistent\nwith the result of the previous input tensor `X`\nand the multi-input channel,\nsingle-output channel kernel. ```{.python .input}\n%%tab all\ncorr2d_multi_in_out(X, K)\n```"
    },
    {
      "chunk_id": "2cf13df9dc69_0",
      "chapter": "channels",
      "heading": "$1\\times 1$ Convolutional Layer",
      "text": ":label:`subsec_1x1`\n\nAt first, a [**$1 \\times 1$ convolution**], i.e., $k_\\textrm{h} = k_\\textrm{w} = 1$,\ndoes not seem to make much sense. After all, a convolution correlates adjacent pixels. A $1 \\times 1$ convolution obviously does not. Nonetheless, they are popular operations that are sometimes included\nin the designs of complex deep networks :cite:`Lin.Chen.Yan.2013,Szegedy.Ioffe.Vanhoucke.ea.2017`. Let's see in some detail what it actually does. Because the minimum window is used,\nthe $1\\times 1$ convolution loses the ability\nof larger convolutional layers\nto recognize patterns consisting of interactions\namong adjacent elements in the height and width dimensions. The only computation of the $1\\times 1$ convolution occurs\non the channel dimension. :numref:`fig_conv_1x1` shows the cross-correlation computation\nusing the $1\\times 1$ convolution kernel\nwith 3 input channels and 2 output channels. Note that the inputs and outputs have the same height and width. Each element in the output is derived\nfrom a linear combination of elements *at the same position*\nin the input image. You could think of the $1\\times 1$ convolutional layer\nas constituting a fully connected layer applied at every single pixel location\nto transform the $c_\\textrm{i}$ corresponding input values into $c_\\textrm{o}$ output values. Because this is still a convolutional layer,\nthe weights are tied across pixel location. Thus the $1\\times 1$ convolutional layer requires $c_\\textrm{o}\\times c_\\textrm{i}$ weights\n(plus the bias). Also note that convolutional layers are typically followed \nby nonlinearities. This ensures that $1 \\times 1$ convolutions cannot simply be \nfolded into other convolutions. ![The cross-correlation computation uses the $1\\times 1$ convolution kernel with three input channels and two output channels. The input and output have the same height and width.](../img/conv-1x1.svg)\n:label:`fig_conv_1x1`\n\nLet's check whether this works in practice:\nwe implement a $1 \\times 1$ convolution\nusing a fully connected layer."
    },
    {
      "chunk_id": "2cf13df9dc69_1",
      "chapter": "channels",
      "heading": "$1\\times 1$ Convolutional Layer",
      "text": "The input and output have the same height and width.](../img/conv-1x1.svg)\n:label:`fig_conv_1x1`\n\nLet's check whether this works in practice:\nwe implement a $1 \\times 1$ convolution\nusing a fully connected layer. The only thing is that we need to make some adjustments\nto the data shape before and after the matrix multiplication. ```{.python .input}\n%%tab all\ndef corr2d_multi_in_out_1x1(X, K):\n    c_i, h, w = X.shape\n    c_o = K.shape[0]\n    X = d2l.reshape(X, (c_i, h * w))\n    K = d2l.reshape(K, (c_o, c_i))\n    # Matrix multiplication in the fully connected layer\n    Y = d2l.matmul(K, X)\n    return d2l.reshape(Y, (c_o, h, w))\n```\n\nWhen performing $1\\times 1$ convolutions,\nthe above function is equivalent to the previously implemented cross-correlation function `corr2d_multi_in_out`. Let's check this with some sample data. ```{.python .input}\n%%tab mxnet, pytorch\nX = d2l.normal(0, 1, (3, 3, 3))\nK = d2l.normal(0, 1, (2, 3, 1, 1))\nY1 = corr2d_multi_in_out_1x1(X, K)\nY2 = corr2d_multi_in_out(X, K)\nassert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6\n```\n\n```{.python .input}\n%%tab tensorflow\nX = d2l.normal((3, 3, 3), 0, 1)\nK = d2l.normal((2, 3, 1, 1), 0, 1)\nY1 = corr2d_multi_in_out_1x1(X, K)\nY2 = corr2d_multi_in_out(X, K)\nassert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6\n```\n\n```{.python .input}\n%%tab jax\nX = jax.random.normal(jax.random.PRNGKey(d2l.get_seed()), (3, 3, 3)) + 0 * 1\nK = jax.random.normal(jax.random.PRNGKey(d2l.get_seed()), (2, 3, 1, 1)) + 0 * 1\nY1 = corr2d_multi_in_out_1x1(X, K)\nY2 = corr2d_multi_in_out(X, K)\nassert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6\n```"
    },
    {
      "chunk_id": "137e7dbd19a4_0",
      "chapter": "channels",
      "heading": "Discussion",
      "text": "Channels allow us to combine the best of both worlds: MLPs that allow for significant nonlinearities and convolutions that allow for *localized* analysis of features. In particular, channels allow the CNN to reason with multiple features, such as edge and shape detectors at the same time. They also offer a practical trade-off between the drastic parameter reduction arising from translation invariance and locality, and the need for expressive and diverse models in computer vision. \n\nNote, though, that this flexibility comes at a price. Given an image of size $(h \\times w)$, the cost for computing a $k \\times k$ convolution is $\\mathcal{O}(h \\cdot w \\cdot k^2)$. For $c_\\textrm{i}$ and $c_\\textrm{o}$ input and output channels respectively this increases to $\\mathcal{O}(h \\cdot w \\cdot k^2 \\cdot c_\\textrm{i} \\cdot c_\\textrm{o})$. For a $256 \\times 256$ pixel image with a $5 \\times 5$ kernel and $128$ input and output channels respectively this amounts to over 53 billion operations (we count multiplications and additions separately). Later on we will encounter effective strategies to cut down on the cost, e.g., by requiring the channel-wise operations to be block-diagonal, leading to architectures such as ResNeXt :cite:`Xie.Girshick.Dollar.ea.2017`."
    },
    {
      "chunk_id": "6da3dc2b5d3f_0",
      "chapter": "channels",
      "heading": "Exercises",
      "text": "1. Assume that we have two convolution kernels of size $k_1$ and $k_2$, respectively \n   (with no nonlinearity in between). 1. Prove that the result of the operation can be expressed by a single convolution. 1. What is the dimensionality of the equivalent single convolution? 1. Is the converse true, i.e., can you always decompose a convolution into two smaller ones? 1. Assume an input of shape $c_\\textrm{i}\\times h\\times w$ and a convolution kernel of shape \n   $c_\\textrm{o}\\times c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$, padding of $(p_\\textrm{h}, p_\\textrm{w})$, and stride of $(s_\\textrm{h}, s_\\textrm{w})$. 1. What is the computational cost (multiplications and additions) for the forward propagation? 1. What is the memory footprint? 1. What is the memory footprint for the backward computation? 1. What is the computational cost for the backpropagation? 1. By what factor does the number of calculations increase if we double both the number of input channels \n   $c_\\textrm{i}$ and the number of output channels $c_\\textrm{o}$? What happens if we double the padding? 1. Are the variables `Y1` and `Y2` in the final example of this section exactly the same? Why? 1. Express convolutions as a matrix multiplication, even when the convolution window is not $1 \\times 1$. 1. Your task is to implement fast convolutions with a $k \\times k$ kernel. One of the algorithm candidates \n   is to scan horizontally across the source, reading a $k$-wide strip and computing the $1$-wide output strip \n   one value at a time. The alternative is to read a $k + \\Delta$ wide strip and compute a $\\Delta$-wide \n   output strip. Why is the latter preferable? Is there a limit to how large you should choose $\\Delta$? 1. Assume that we have a $c \\times c$ matrix. 1. How much faster is it to multiply with a block-diagonal matrix if the matrix is broken up into $b$ blocks? 1. What is the downside of having $b$ blocks? How could you fix it, at least partly?"
    },
    {
      "chunk_id": "6da3dc2b5d3f_1",
      "chapter": "channels",
      "heading": "Exercises",
      "text": "1. Assume that we have a $c \\times c$ matrix. 1. How much faster is it to multiply with a block-diagonal matrix if the matrix is broken up into $b$ blocks? 1. What is the downside of having $b$ blocks? How could you fix it, at least partly? :begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/69)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/70)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/273)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17998)\n:end_tab:"
    },
    {
      "chunk_id": "29a646958c53_0",
      "chapter": "conv-layer",
      "heading": "conv-layer",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Convolutions for Images\n:label:`sec_conv_layer`\n\nNow that we understand how convolutional layers work in theory,\nwe are ready to see how they work in practice.\nBuilding on our motivation of convolutional neural networks\nas efficient architectures for exploring structure in image data,\nwe stick with images as our running example.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```"
    },
    {
      "chunk_id": "65960a8dc655_0",
      "chapter": "conv-layer",
      "heading": "The Cross-Correlation Operation",
      "text": "Recall that strictly speaking, convolutional layers\nare a  misnomer, since the operations they express\nare more accurately described as cross-correlations. Based on our descriptions of convolutional layers in :numref:`sec_why-conv`,\nin such a layer, an input tensor\nand a kernel tensor are combined\nto produce an output tensor through a (**cross-correlation operation.**)\n\nLet's ignore channels for now and see how this works\nwith two-dimensional data and hidden representations. In :numref:`fig_correlation`,\nthe input is a two-dimensional tensor\nwith a height of 3 and width of 3. We mark the shape of the tensor as $3 \\times 3$ or ($3$, $3$). The height and width of the kernel are both 2. The shape of the *kernel window* (or *convolution window*)\nis given by the height and width of the kernel\n(here it is $2 \\times 2$). ![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+1\\times1+3\\times2+4\\times3=19$.](../img/correlation.svg)\n:label:`fig_correlation`\n\nIn the two-dimensional cross-correlation operation,\nwe begin with the convolution window positioned\nat the upper-left corner of the input tensor\nand slide it across the input tensor,\nboth from left to right and top to bottom. When the convolution window slides to a certain position,\nthe input subtensor contained in that window\nand the kernel tensor are multiplied elementwise\nand the resulting tensor is summed up\nyielding a single scalar value. This result gives the value of the output tensor\nat the corresponding location. Here, the output tensor has a height of 2 and width of 2\nand the four elements are derived from\nthe two-dimensional cross-correlation operation:\n\n$$\n0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n4\\times0+5\\times1+7\\times2+8\\times3=43. $$\n\nNote that along each axis, the output size\nis slightly smaller than the input size."
    },
    {
      "chunk_id": "65960a8dc655_1",
      "chapter": "conv-layer",
      "heading": "The Cross-Correlation Operation",
      "text": "$$\n\nNote that along each axis, the output size\nis slightly smaller than the input size. Because the kernel has width and height greater than $1$,\nwe can only properly compute the cross-correlation\nfor locations where the kernel fits wholly within the image,\nthe output size is given by the input size $n_\\textrm{h} \\times n_\\textrm{w}$\nminus the size of the convolution kernel $k_\\textrm{h} \\times k_\\textrm{w}$\nvia\n\n$$(n_\\textrm{h}-k_\\textrm{h}+1) \\times (n_\\textrm{w}-k_\\textrm{w}+1).$$\n\nThis is the case since we need enough space\nto \"shift\" the convolution kernel across the image. Later we will see how to keep the size unchanged\nby padding the image with zeros around its boundary\nso that there is enough space to shift the kernel. Next, we implement this process in the `corr2d` function,\nwhich accepts an input tensor `X` and a kernel tensor `K`\nand returns an output tensor `Y`."
    },
    {
      "chunk_id": "65960a8dc655_2",
      "chapter": "conv-layer",
      "heading": "The Cross-Correlation Operation",
      "text": "Next, we implement this process in the `corr2d` function,\nwhich accepts an input tensor `X` and a kernel tensor `K`\nand returns an output tensor `Y`. ```{.python .input}\n%%tab mxnet\ndef corr2d(X, K):  #@save\n    \"\"\"Compute 2D cross-correlation.\"\"\"\n    h, w = K.shape\n    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))\n    return Y\n```\n\n```{.python .input}\n%%tab pytorch\ndef corr2d(X, K):  #@save\n    \"\"\"Compute 2D cross-correlation.\"\"\"\n    h, w = K.shape\n    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))\n    return Y\n```\n\n```{.python .input}\n%%tab jax\ndef corr2d(X, K):  #@save\n    \"\"\"Compute 2D cross-correlation.\"\"\"\n    h, w = K.shape\n    Y = jnp.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y = Y.at[i, j].set((X[i:i + h, j:j + w] * K).sum())\n    return Y\n```\n\n```{.python .input}\n%%tab tensorflow\ndef corr2d(X, K):  #@save\n    \"\"\"Compute 2D cross-correlation.\"\"\"\n    h, w = K.shape\n    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j].assign(tf.reduce_sum(\n                X[i: i + h, j: j + w] * K))\n    return Y\n```\n\nWe can construct the input tensor `X` and the kernel tensor `K`\nfrom :numref:`fig_correlation`\nto [**validate the output of the above implementation**]\nof the two-dimensional cross-correlation operation. ```{.python .input}\n%%tab all\nX = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\nK = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\ncorr2d(X, K)\n```"
    },
    {
      "chunk_id": "6973d4fa5572_0",
      "chapter": "conv-layer",
      "heading": "Convolutional Layers",
      "text": "A convolutional layer cross-correlates the input and kernel\nand adds a scalar bias to produce an output. The two parameters of a convolutional layer\nare the kernel and the scalar bias. When training models based on convolutional layers,\nwe typically initialize the kernels randomly,\njust as we would with a fully connected layer. We are now ready to [**implement a two-dimensional convolutional layer**]\nbased on the `corr2d` function defined above. In the `__init__` constructor method,\nwe declare `weight` and `bias` as the two model parameters. The forward propagation method\ncalls the `corr2d` function and adds the bias."
    },
    {
      "chunk_id": "6973d4fa5572_1",
      "chapter": "conv-layer",
      "heading": "Convolutional Layers",
      "text": "In the `__init__` constructor method,\nwe declare `weight` and `bias` as the two model parameters. The forward propagation method\ncalls the `corr2d` function and adds the bias. ```{.python .input}\n%%tab mxnet\nclass Conv2D(nn.Block):\n    def __init__(self, kernel_size, **kwargs):\n        super().__init__(**kwargs)\n        self.weight = self.params.get('weight', shape=kernel_size)\n        self.bias = self.params.get('bias', shape=(1,))\n\n    def forward(self, x):\n        return corr2d(x, self.weight.data()) + self.bias.data()\n```\n\n```{.python .input}\n%%tab pytorch\nclass Conv2D(nn.Module):\n    def __init__(self, kernel_size):\n        super().__init__()\n        self.weight = nn.Parameter(torch.rand(kernel_size))\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Conv2D(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def build(self, kernel_size):\n        initializer = tf.random_normal_initializer()\n        self.weight = self.add_weight(name='w', shape=kernel_size,\n                                      initializer=initializer)\n        self.bias = self.add_weight(name='b', shape=(1, ),\n                                    initializer=initializer)\n\n    def call(self, inputs):\n        return corr2d(inputs, self.weight) + self.bias\n```\n\n```{.python .input}\n%%tab jax\nclass Conv2D(nn.Module):\n    kernel_size: int\n\n    def setup(self):\n        self.weight = nn.param('w', nn.initializers.uniform, self.kernel_size)\n        self.bias = nn.param('b', nn.initializers.zeros, 1)\n\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n```\n\nIn\n$h \\times w$ convolution\nor an $h \\times w$ convolution kernel,\nthe height and width of the convolution kernel are $h$ and $w$, respectively. We also refer to\na convolutional layer with an $h \\times w$\nconvolution kernel simply as an $h \\times w$ convolutional layer."
    },
    {
      "chunk_id": "f24e76807bfd_0",
      "chapter": "conv-layer",
      "heading": "Object Edge Detection in Images",
      "text": "Let's take a moment to parse [**a simple application of a convolutional layer:\ndetecting the edge of an object in an image**]\nby finding the location of the pixel change.\nFirst, we construct an \"image\" of $6\\times 8$ pixels.\nThe middle four columns are black ($0$) and the rest are white ($1$).\n\n```{.python .input}\n%%tab mxnet, pytorch\nX = d2l.ones((6, 8))\nX[:, 2:6] = 0\nX\n```\n\n```{.python .input}\n%%tab tensorflow\nX = tf.Variable(tf.ones((6, 8)))\nX[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\nX\n```\n\n```{.python .input}\n%%tab jax\nX = jnp.ones((6, 8))\nX = X.at[:, 2:6].set(0)\nX\n```\n\nNext, we construct a kernel `K` with a height of 1 and a width of 2.\nWhen we perform the cross-correlation operation with the input,\nif the horizontally adjacent elements are the same,\nthe output is 0. Otherwise, the output is nonzero.\nNote that this kernel is a special case of a finite difference operator. At location $(i,j)$ it computes $x_{i,j} - x_{(i+1),j}$, i.e., it computes the difference between the values of horizontally adjacent pixels. This is a discrete approximation of the first derivative in the horizontal direction. After all, for a function $f(i,j)$ its derivative $-\\partial_i f(i,j) = \\lim_{\\epsilon \\to 0} \\frac{f(i,j) - f(i+\\epsilon,j)}{\\epsilon}$. Let's see how this works in practice.\n\n```{.python .input}\n%%tab all\nK = d2l.tensor([[1.0, -1.0]])\n```\n\nWe are ready to perform the cross-correlation operation\nwith arguments `X` (our input) and `K` (our kernel).\nAs you can see, [**we detect $1$ for the edge from white to black\nand $-1$ for the edge from black to white.**]\nAll other outputs take value $0$.\n\n```{.python .input}\n%%tab all\nY = corr2d(X, K)\nY\n```\n\nWe can now apply the kernel to the transposed image.\nAs expected, it vanishes. [**The kernel `K` only detects vertical edges.**]\n\n```{.python .input}\n%%tab all\ncorr2d(d2l.transpose(X), K)\n```"
    },
    {
      "chunk_id": "6729323eb147_0",
      "chapter": "conv-layer",
      "heading": "Learning a Kernel",
      "text": "Designing an edge detector by finite differences `[1, -1]` is neat\nif we know this is precisely what we are looking for. However, as we look at larger kernels,\nand consider successive layers of convolutions,\nit might be impossible to specify\nprecisely what each filter should be doing manually. Now let's see whether we can [**learn the kernel that generated `Y` from `X`**]\nby looking at the input--output pairs only. We first construct a convolutional layer\nand initialize its kernel as a random tensor. Next, in each iteration, we will use the squared error\nto compare `Y` with the output of the convolutional layer. We can then calculate the gradient to update the kernel. For the sake of simplicity,\nin the following\nwe use the built-in class\nfor two-dimensional convolutional layers\nand ignore the bias. ```{.python .input}\n%%tab mxnet\n# Construct a two-dimensional convolutional layer with 1 output channel and a\n# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\nconv2d = nn.Conv2D(1, kernel_size=(1, 2), use_bias=False)\nconv2d.initialize()\n\n# The two-dimensional convolutional layer uses four-dimensional input and\n# output in the format of (example, channel, height, width), where the batch\n# size (number of examples in the batch) and the number of channels are both 1\nX = X.reshape(1, 1, 6, 8)\nY = Y.reshape(1, 1, 6, 7)\nlr = 3e-2  # Learning rate\n\nfor i in range(10):\n    with autograd.record():\n        Y_hat = conv2d(X)\n        l = (Y_hat - Y) ** 2\n    l.backward()\n    # Update the kernel\n    conv2d.weight.data()[:] -= lr * conv2d.weight.grad()\n    if (i + 1) % 2 == 0:\n        print(f'epoch {i + 1}, loss {float(l.sum()):.3f}')\n```\n\n```{.python .input}\n%%tab pytorch\n# Construct a two-dimensional convolutional layer with 1 output channel and a\n# kernel of shape (1, 2)."
    },
    {
      "chunk_id": "6729323eb147_1",
      "chapter": "conv-layer",
      "heading": "Learning a Kernel",
      "text": "For the sake of simplicity, we ignore the bias here\nconv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n\n# The two-dimensional convolutional layer uses four-dimensional input and\n# output in the format of (example, channel, height, width), where the batch\n# size (number of examples in the batch) and the number of channels are both 1\nX = X.reshape((1, 1, 6, 8))\nY = Y.reshape((1, 1, 6, 7))\nlr = 3e-2  # Learning rate\n\nfor i in range(10):\n    Y_hat = conv2d(X)\n    l = (Y_hat - Y) ** 2\n    conv2d.zero_grad()\n    l.sum().backward()\n    # Update the kernel\n    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n    if (i + 1) % 2 == 0:\n        print(f'epoch {i + 1}, loss {l.sum():.3f}')\n```\n\n```{.python .input}\n%%tab tensorflow\n# Construct a two-dimensional convolutional layer with 1 output channel and a\n# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\nconv2d = tf.keras.layers.Conv2D(1, (1, 2), use_bias=False)\n\n# The two-dimensional convolutional layer uses four-dimensional input and\n# output in the format of (example, height, width, channel), where the batch\n# size (number of examples in the batch) and the number of channels are both 1\nX = tf.reshape(X, (1, 6, 8, 1))\nY = tf.reshape(Y, (1, 6, 7, 1))\nlr = 3e-2  # Learning rate\n\nY_hat = conv2d(X)\nfor i in range(10):\n    with tf.GradientTape(watch_accessed_variables=False) as g:\n        g.watch(conv2d.weights[0])\n        Y_hat = conv2d(X)\n        l = (abs(Y_hat - Y)) ** 2\n        # Update the kernel\n        update = tf.multiply(lr, g.gradient(l, conv2d.weights[0]))\n        weights = conv2d.get_weights()\n        weights[0] = conv2d.weights[0] - update\n        conv2d.set_weights(weights)\n        if (i + 1) % 2 == 0:\n            print(f'epoch {i + 1}, loss {tf.reduce_sum(l):.3f}')\n```\n\n```{.python .input}\n%%tab jax\n# Construct a two-dimensional convolutional layer with 1 output channel and a\n# kernel of shape (1, 2)."
    },
    {
      "chunk_id": "6729323eb147_2",
      "chapter": "conv-layer",
      "heading": "Learning a Kernel",
      "text": "For the sake of simplicity, we ignore the bias here\nconv2d = nn.Conv(1, kernel_size=(1, 2), use_bias=False, padding='VALID')\n\n# The two-dimensional convolutional layer uses four-dimensional input and\n# output in the format of (example, height, width, channel), where the batch\n# size (number of examples in the batch) and the number of channels are both 1\nX = X.reshape((1, 6, 8, 1))\nY = Y.reshape((1, 6, 7, 1))\nlr = 3e-2  # Learning rate\n\nparams = conv2d.init(jax.random.PRNGKey(d2l.get_seed()), X)\n\ndef loss(params, X, Y):\n    Y_hat = conv2d.apply(params, X)\n    return ((Y_hat - Y) ** 2).sum()\n\nfor i in range(10):\n    l, grads = jax.value_and_grad(loss)(params, X, Y)\n    # Update the kernel\n    params = jax.tree_map(lambda p, g: p - lr * g, params, grads)\n    if (i + 1) % 2 == 0:\n        print(f'epoch {i + 1}, loss {l:.3f}')\n```\n\nNote that the error has dropped to a small value after 10 iterations. Now we will [**take a look at the kernel tensor we learned.**]\n\n```{.python .input}\n%%tab mxnet\nd2l.reshape(conv2d.weight.data(), (1, 2))\n```\n\n```{.python .input}\n%%tab pytorch\nd2l.reshape(conv2d.weight.data, (1, 2))\n```\n\n```{.python .input}\n%%tab tensorflow\nd2l.reshape(conv2d.get_weights()[0], (1, 2))\n```\n\n```{.python .input}\n%%tab jax\nparams['params']['kernel'].reshape((1, 2))\n```\n\nIndeed, the learned kernel tensor is remarkably close\nto the kernel tensor `K` we defined earlier."
    },
    {
      "chunk_id": "04482ed30f4e_0",
      "chapter": "conv-layer",
      "heading": "Cross-Correlation and Convolution",
      "text": "Recall our observation from :numref:`sec_why-conv` of the correspondence\nbetween the cross-correlation and convolution operations.\nHere let's continue to consider two-dimensional convolutional layers.\nWhat if such layers\nperform strict convolution operations\nas defined in :eqref:`eq_2d-conv-discrete`\ninstead of cross-correlations?\nIn order to obtain the output of the strict *convolution* operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the *cross-correlation* operation with the input tensor.\n\nIt is noteworthy that since kernels are learned from data in deep learning,\nthe outputs of convolutional layers remain unaffected\nno matter such layers\nperform\neither the strict convolution operations\nor the cross-correlation operations.\n\nTo illustrate this, suppose that a convolutional layer performs *cross-correlation* and learns the kernel in :numref:`fig_correlation`, which is here denoted as the matrix $\\mathbf{K}$.\nAssuming that other conditions remain unchanged,\nwhen this layer instead performs strict *convolution*,\nthe learned kernel $\\mathbf{K}'$ will be the same as $\\mathbf{K}$\nafter $\\mathbf{K}'$ is\nflipped both horizontally and vertically.\nThat is to say,\nwhen the convolutional layer\nperforms strict *convolution*\nfor the input in :numref:`fig_correlation`\nand $\\mathbf{K}'$,\nthe same output in :numref:`fig_correlation`\n(cross-correlation of the input and $\\mathbf{K}$)\nwill be obtained.\n\nIn keeping with standard terminology in deep learning literature,\nwe will continue to refer to the cross-correlation operation\nas a convolution even though, strictly-speaking, it is slightly different.\nFurthermore,\nwe use the term *element* to refer to\nan entry (or component) of any tensor representing a layer representation or a convolution kernel."
    },
    {
      "chunk_id": "4355c0a3a333_0",
      "chapter": "conv-layer",
      "heading": "Feature Map and Receptive Field",
      "text": "As described in :numref:`subsec_why-conv-channels`,\nthe convolutional layer output in\n:numref:`fig_correlation`\nis sometimes called a *feature map*,\nas it can be regarded as\nthe learned representations (features)\nin the spatial dimensions (e.g., width and height)\nto the subsequent layer. In CNNs,\nfor any element $x$ of some layer,\nits *receptive field* refers to\nall the elements (from all the previous layers)\nthat may affect the calculation of $x$\nduring the forward propagation. Note that the receptive field\nmay be larger than the actual size of the input. Let's continue to use :numref:`fig_correlation` to explain the receptive field. Given the $2 \\times 2$ convolution kernel,\nthe receptive field of the shaded output element (of value $19$)\nis\nthe four elements in the shaded portion of the input. Now let's denote the $2 \\times 2$\noutput as $\\mathbf{Y}$\nand consider a deeper CNN\nwith an additional $2 \\times 2$ convolutional layer that takes $\\mathbf{Y}$\nas its input, outputting\na single element $z$. In this case,\nthe receptive field of $z$\non $\\mathbf{Y}$ includes all the four elements of $\\mathbf{Y}$,\nwhile\nthe receptive field\non the input includes all the nine input elements. Thus,\nwhen any element in a feature map\nneeds a larger receptive field\nto detect input features over a broader area,\nwe can build a deeper network. Receptive fields derive their name from neurophysiology. A series of experiments on a range of animals using different stimuli\n:cite:`Hubel.Wiesel.1959,Hubel.Wiesel.1962,Hubel.Wiesel.1968` explored the response of what is called the visual\ncortex on said stimuli. By and large they found that lower levels respond to edges and related\nshapes. Later on, :citet:`Field.1987` illustrated this effect on natural\nimages with, what can only be called, convolutional kernels. We reprint a key figure in :numref:`field_visual` to illustrate the striking similarities. ![Figure and caption taken from :citet:`Field.1987`: An example of coding with six different channels."
    },
    {
      "chunk_id": "4355c0a3a333_1",
      "chapter": "conv-layer",
      "heading": "Feature Map and Receptive Field",
      "text": "We reprint a key figure in :numref:`field_visual` to illustrate the striking similarities. ![Figure and caption taken from :citet:`Field.1987`: An example of coding with six different channels. (Left) Examples of the six types of sensor associated with each channel. (Right) Convolution of the image in (Middle) with the six sensors shown in (Left). The response of the individual sensors is determined by sampling these filtered images at a distance proportional to the size of the sensor (shown with dots). This diagram shows the response of only the even symmetric sensors.](../img/field-visual.png)\n:label:`field_visual`\n\nAs it turns out, this relation even holds for the features computed by deeper layers of networks trained on image classification tasks, as demonstrated in, for example, :citet:`Kuzovkin.Vicente.Petton.ea.2018`. Suffice it to say, convolutions have proven to be an incredibly powerful tool for computer vision, both in biology and in code. As such, it is not surprising (in hindsight) that they heralded the recent success in deep learning."
    },
    {
      "chunk_id": "c9e7eebeabba_0",
      "chapter": "conv-layer",
      "heading": "Summary",
      "text": "The core computation required for a convolutional layer is a cross-correlation operation. We saw that a simple nested for-loop is all that is required to compute its value. If we have multiple input and multiple output channels, we are  performing a matrix--matrix operation between channels. As can be seen, the computation is straightforward and, most importantly, highly *local*. This affords significant hardware optimization and many recent results in computer vision are only possible because of that. After all, it means that chip designers can invest in fast computation rather than memory when it comes to optimizing for convolutions. While this may not lead to optimal designs for other applications, it does open the door to ubiquitous and affordable computer vision.\n\nIn terms of convolutions themselves, they can be used for many purposes, for example detecting edges and lines, blurring images, or sharpening them. Most importantly, it is not necessary that the statistician (or engineer) invents suitable filters. Instead, we can simply *learn* them from data. This replaces feature engineering heuristics by evidence-based statistics. Lastly, and quite delightfully, these filters are not just advantageous for building deep networks but they also correspond to receptive fields and feature maps in the brain. This gives us confidence that we are on the right track."
    },
    {
      "chunk_id": "f8aaaf3ac226_0",
      "chapter": "conv-layer",
      "heading": "Exercises",
      "text": "1. Construct an image `X` with diagonal edges.\n    1. What happens if you apply the kernel `K` in this section to it?\n    1. What happens if you transpose `X`?\n    1. What happens if you transpose `K`?\n1. Design some kernels manually.\n    1. Given a directional vector $\\mathbf{v} = (v_1, v_2)$, derive an edge-detection kernel that detects\n       edges orthogonal to $\\mathbf{v}$, i.e., edges in the direction $(v_2, -v_1)$.\n    1. Derive a finite difference operator for the second derivative. What is the minimum\n       size of the convolutional kernel associated with it? Which structures in images respond most strongly to it?\n    1. How would you design a blur kernel? Why might you want to use such a kernel?\n    1. What is the minimum size of a kernel to obtain a derivative of order $d$?\n1. When you try to automatically find the gradient for the `Conv2D` class we created, what kind of error message do you see?\n1. How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel tensors?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/65)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/66)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/271)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17996)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Convolutional Neural Networks\n:label:`chap_cnn`\n\nImage data is represented as a two-dimensional grid of pixels, be the image\nmonochromatic or in color. Accordingly each pixel corresponds to one\nor multiple numerical values respectively. So far we have ignored this rich\nstructure and treated images as vectors of numbers by *flattening* them, irrespective of the spatial relation between pixels. This\ndeeply unsatisfying approach was necessary in order to feed the\nresulting one-dimensional vectors through a fully connected MLP. Because these networks are invariant to the order of the features, we\ncould get similar results regardless of whether we preserve an order\ncorresponding to the spatial structure of the pixels or if we permute\nthe columns of our design matrix before fitting the MLP's parameters. Ideally, we would leverage our prior knowledge that nearby pixels\nare typically related to each other, to build efficient models for\nlearning from image data. This chapter introduces *convolutional neural networks* (CNNs)\n:cite:`LeCun.Jackel.Bottou.ea.1995`, a powerful family of neural networks that\nare designed for precisely this purpose. CNN-based architectures are\nnow ubiquitous in the field of computer vision. For instance, on the Imagnet collection\n:cite:`Deng.Dong.Socher.ea.2009` it was only the use of convolutional neural\nnetworks, in short Convnets, that provided significant performance\nimprovements :cite:`Krizhevsky.Sutskever.Hinton.2012`. Modern CNNs, as they are called colloquially, owe their design to\ninspirations from biology, group theory, and a healthy dose of\nexperimental tinkering. In addition to their sample efficiency in\nachieving accurate models, CNNs tend to be computationally efficient,\nboth because they require fewer parameters than fully connected\narchitectures and because convolutions are easy to parallelize across\nGPU cores :cite:`Chetlur.Woolley.Vandermersch.ea.2014`."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "Consequently, practitioners often\napply CNNs whenever possible, and increasingly they have emerged as\ncredible competitors even on tasks with a one-dimensional sequence\nstructure, such as audio :cite:`Abdel-Hamid.Mohamed.Jiang.ea.2014`, text\n:cite:`Kalchbrenner.Grefenstette.Blunsom.2014`, and time series analysis\n:cite:`LeCun.Bengio.ea.1995`, where recurrent neural networks are\nconventionally used. Some clever adaptations of CNNs have also\nbrought them to bear on graph-structured data :cite:`Kipf.Welling.2016` and\nin recommender systems. First, we will dive more deeply into the motivation for convolutional\nneural networks. This is followed by a walk through the basic operations\nthat comprise the backbone of all convolutional networks. These include the convolutional layers themselves,\nnitty-gritty details including padding and stride,\nthe pooling layers used to aggregate information\nacross adjacent spatial regions,\nthe use of multiple channels  at each layer,\nand a careful discussion of the structure of modern architectures. We will conclude the chapter with a full working example of LeNet,\nthe first convolutional network successfully deployed,\nlong before the rise of modern deep learning. In the next chapter, we will dive into full implementations\nof some popular and comparatively recent CNN architectures\nwhose designs represent most of the techniques\ncommonly used by modern practitioners. ```toc\n:maxdepth: 2\n\nwhy-conv\nconv-layer\npadding-and-strides\nchannels\npooling\nlenet\n```"
    },
    {
      "chunk_id": "77c21b6a13ce_0",
      "chapter": "lenet",
      "heading": "lenet",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Convolutional Neural Networks (LeNet)\n:label:`sec_lenet`\n\nWe now have all the ingredients required to assemble\na fully-functional CNN. In our earlier encounter with image data, we applied\na linear model with softmax regression (:numref:`sec_softmax_scratch`)\nand an MLP (:numref:`sec_mlp-implementation`)\nto pictures of clothing in the Fashion-MNIST dataset. To make such data amenable we first flattened each image from a $28\\times28$ matrix\ninto a fixed-length $784$-dimensional vector,\nand thereafter processed them in fully connected layers. Now that we have a handle on convolutional layers,\nwe can retain the spatial structure in our images. As an additional benefit of replacing fully connected layers with convolutional layers,\nwe will enjoy more parsimonious models that require far fewer parameters. In this section, we will introduce *LeNet*,\namong the first published CNNs\nto capture wide attention for its performance on computer vision tasks. The model was introduced by (and named for) Yann LeCun,\nthen a researcher at AT&T Bell Labs,\nfor the purpose of recognizing handwritten digits in images :cite:`LeCun.Bottou.Bengio.ea.1998`. This work represented the culmination\nof a decade of research developing the technology;\nLeCun's team published the first study to successfully\ntrain CNNs via backpropagation :cite:`LeCun.Boser.Denker.ea.1989`. At the time LeNet achieved outstanding results\nmatching the performance of support vector machines,\nthen a dominant approach in supervised learning, achieving an error rate of less than 1% per digit. LeNet was eventually adapted to recognize digits\nfor processing deposits in ATM machines. To this day, some ATMs still run the code\nthat Yann LeCun and his colleague Leon Bottou wrote in the 1990s!"
    },
    {
      "chunk_id": "77c21b6a13ce_1",
      "chapter": "lenet",
      "heading": "lenet",
      "text": "LeNet was eventually adapted to recognize digits\nfor processing deposits in ATM machines. To this day, some ATMs still run the code\nthat Yann LeCun and his colleague Leon Bottou wrote in the 1990s! ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\nfrom d2l import tensorflow as d2l\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\nfrom types import FunctionType\n```"
    },
    {
      "chunk_id": "8fe5921da867_0",
      "chapter": "lenet",
      "heading": "LeNet",
      "text": "At a high level, (**LeNet (LeNet-5) consists of two parts:\n(i) a convolutional encoder consisting of two convolutional layers; and\n(ii) a dense block consisting of three fully connected layers**). The architecture is summarized in :numref:`img_lenet`. ![Data flow in LeNet. The input is a handwritten digit, the output is a probability over 10 possible outcomes.](../img/lenet.svg)\n:label:`img_lenet`\n\nThe basic units in each convolutional block\nare a convolutional layer, a sigmoid activation function,\nand a subsequent average pooling operation. Note that while ReLUs and max-pooling work better,\nthey had not yet been discovered. Each convolutional layer uses a $5\\times 5$ kernel\nand a sigmoid activation function. These layers map spatially arranged inputs\nto a number of two-dimensional feature maps, typically\nincreasing the number of channels. The first convolutional layer has 6 output channels,\nwhile the second has 16. Each $2\\times2$ pooling operation (stride 2)\nreduces dimensionality by a factor of $4$ via spatial downsampling. The convolutional block emits an output with shape given by\n(batch size, number of channel, height, width). In order to pass output from the convolutional block\nto the dense block,\nwe must flatten each example in the minibatch. In other words, we take this four-dimensional input and transform it\ninto the two-dimensional input expected by fully connected layers:\nas a reminder, the two-dimensional representation that we desire uses the first dimension to index examples in the minibatch\nand the second to give the flat vector representation of each example. LeNet's dense block has three fully connected layers,\nwith 120, 84, and 10 outputs, respectively. Because we are still performing classification,\nthe 10-dimensional output layer corresponds\nto the number of possible output classes."
    },
    {
      "chunk_id": "8fe5921da867_1",
      "chapter": "lenet",
      "heading": "LeNet",
      "text": "LeNet's dense block has three fully connected layers,\nwith 120, 84, and 10 outputs, respectively. Because we are still performing classification,\nthe 10-dimensional output layer corresponds\nto the number of possible output classes. While getting to the point where you truly understand\nwhat is going on inside LeNet may have taken a bit of work,\nwe hope that the following code snippet will convince you\nthat implementing such models with modern deep learning frameworks\nis remarkably simple. We need only to instantiate a `Sequential` block\nand chain together the appropriate layers,\nusing Xavier initialization as\nintroduced in :numref:`subsec_xavier`."
    },
    {
      "chunk_id": "8fe5921da867_2",
      "chapter": "lenet",
      "heading": "LeNet",
      "text": "We need only to instantiate a `Sequential` block\nand chain together the appropriate layers,\nusing Xavier initialization as\nintroduced in :numref:`subsec_xavier`. ```{.python .input}\n%%tab pytorch\ndef init_cnn(module):  #@save\n    \"\"\"Initialize weights for CNNs.\"\"\"\n    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n        nn.init.xavier_uniform_(module.weight)\n```\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass LeNet(d2l.Classifier):  #@save\n    \"\"\"The LeNet-5 model.\"\"\"\n    def __init__(self, lr=0.1, num_classes=10):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Sequential()\n            self.net.add(\n                nn.Conv2D(channels=6, kernel_size=5, padding=2,\n                          activation='sigmoid'),\n                nn.AvgPool2D(pool_size=2, strides=2),\n                nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n                nn.AvgPool2D(pool_size=2, strides=2),\n                nn.Dense(120, activation='sigmoid'),\n                nn.Dense(84, activation='sigmoid'),\n                nn.Dense(num_classes))\n            self.net.initialize(init.Xavier())\n        if tab.selected('pytorch'):\n            self.net = nn.Sequential(\n                nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n                nn.AvgPool2d(kernel_size=2, stride=2),\n                nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n                nn.AvgPool2d(kernel_size=2, stride=2),\n                nn.Flatten(),\n                nn.LazyLinear(120), nn.Sigmoid(),\n                nn.LazyLinear(84), nn.Sigmoid(),\n                nn.LazyLinear(num_classes))\n        if tab.selected('tensorflow'):\n            self.net = tf.keras.models.Sequential([\n                tf.keras.layers.Conv2D(filters=6, kernel_size=5,\n                                       activation='sigmoid', padding='same'),\n                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n                tf.keras.layers.Conv2D(filters=16, kernel_size=5,\n                                       activation='sigmoid'),\n                tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n                tf.keras.layers.Flatten(),\n                tf.keras.layers.Dense(120, activation='sigmoid'),\n                tf.keras.layers.Dense(84, activation='sigmoid'),\n                tf.keras.layers.Dense(num_classes)])\n```\n\n```{.python .input}\n%%tab jax\nclass LeNet(d2l.Classifier):  #@save\n    \"\"\"The LeNet-5 model.\"\"\"\n    lr: float = 0.1\n    num_classes: int = 10\n    kernel_init: FunctionType = nn.initializers.xavier_uniform\n\n    def setup(self):\n        self.net = nn.Sequential([\n            nn.Conv(features=6, kernel_size=(5, 5), padding='SAME',\n                    kernel_init=self.kernel_init()),\n            nn.sigmoid,\n            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n            nn.Conv(features=16, kernel_size=(5, 5), padding='VALID',\n                    kernel_init=self.kernel_init()),\n            nn.sigmoid,\n            lambda x: nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2)),\n            lambda x: x.reshape((x.shape[0], -1)),  # flatten\n            nn.Dense(features=120, kernel_init=self.kernel_init()),\n            nn.sigmoid,\n            nn.Dense(features=84, kernel_init=self.kernel_init()),\n            nn.sigmoid,\n            nn.Dense(features=self.num_classes, kernel_init=self.kernel_init())\n        ])\n```\n\nWe have taken some liberty in the reproduction of LeNet insofar as we have replaced the Gaussian activation layer by\na softmax layer."
    },
    {
      "chunk_id": "8fe5921da867_3",
      "chapter": "lenet",
      "heading": "LeNet",
      "text": "This greatly simplifies the implementation, not least due to the\nfact that the Gaussian decoder is rarely used nowadays. Other than that, this network matches\nthe original LeNet-5 architecture. :begin_tab:`pytorch, mxnet, tensorflow`\nLet's see what happens inside the network. By passing a\nsingle-channel (black and white)\n$28 \\times 28$ image through the network\nand printing the output shape at each layer,\nwe can [**inspect the model**] to ensure\nthat its operations line up with\nwhat we expect from :numref:`img_lenet_vert`. :end_tab:\n\n:begin_tab:`jax`\nLet's see what happens inside the network. By passing a\nsingle-channel (black and white)\n$28 \\times 28$ image through the network\nand printing the output shape at each layer,\nwe can [**inspect the model**] to ensure\nthat its operations line up with\nwhat we expect from :numref:`img_lenet_vert`. Flax provides `nn.tabulate`, a nifty method to summarise the layers and\nparameters in our network. Here we use the `bind` method to create a bounded model. The variables are now bound to the `d2l.Module` class, i.e., this bounded model\nbecomes a stateful object which can then be used to access the `Sequential`\nobject attribute `net` and the `layers` within. Note that the `bind` method should\nonly be used for interactive experimentation, and is not a direct\nreplacement for the `apply` method."
    },
    {
      "chunk_id": "8fe5921da867_4",
      "chapter": "lenet",
      "heading": "LeNet",
      "text": "Note that the `bind` method should\nonly be used for interactive experimentation, and is not a direct\nreplacement for the `apply` method. :end_tab:\n\n![Compressed notation for LeNet-5.](../img/lenet-vert.svg)\n:label:`img_lenet_vert`\n\n```{.python .input}\n%%tab mxnet, pytorch\n@d2l.add_to_class(d2l.Classifier)  #@save\ndef layer_summary(self, X_shape):\n    X = d2l.randn(*X_shape)\n    for layer in self.net:\n        X = layer(X)\n        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n        \nmodel = LeNet()\nmodel.layer_summary((1, 1, 28, 28))\n```\n\n```{.python .input}\n%%tab tensorflow\n@d2l.add_to_class(d2l.Classifier)  #@save\ndef layer_summary(self, X_shape):\n    X = d2l.normal(X_shape)\n    for layer in self.net.layers:\n        X = layer(X)\n        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n\nmodel = LeNet()\nmodel.layer_summary((1, 28, 28, 1))\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.Classifier)  #@save\ndef layer_summary(self, X_shape, key=d2l.get_key()):\n    X = jnp.zeros(X_shape)\n    params = self.init(key, X)\n    bound_model = self.clone().bind(params, mutable=['batch_stats'])\n    _ = bound_model(X)\n    for layer in bound_model.net.layers:\n        X = layer(X)\n        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n\nmodel = LeNet()\nmodel.layer_summary((1, 28, 28, 1))\n```\n\nNote that the height and width of the representation\nat each layer throughout the convolutional block\nis reduced (compared with the previous layer). The first convolutional layer uses two pixels of padding\nto compensate for the reduction in height and width\nthat would otherwise result from using a $5 \\times 5$ kernel. As an aside, the image size of $28 \\times 28$ pixels in the original\nMNIST OCR dataset is a result of *trimming* two pixel rows (and columns) from the\noriginal scans that measured $32 \\times 32$ pixels. This was done primarily to\nsave space (a 30% reduction) at a time when megabytes mattered."
    },
    {
      "chunk_id": "8fe5921da867_5",
      "chapter": "lenet",
      "heading": "LeNet",
      "text": "This was done primarily to\nsave space (a 30% reduction) at a time when megabytes mattered. In contrast, the second convolutional layer forgoes padding,\nand thus the height and width are both reduced by four pixels. As we go up the stack of layers,\nthe number of channels increases layer-over-layer\nfrom 1 in the input to 6 after the first convolutional layer\nand 16 after the second convolutional layer. However, each pooling layer halves the height and width. Finally, each fully connected layer reduces dimensionality,\nfinally emitting an output whose dimension\nmatches the number of classes."
    },
    {
      "chunk_id": "f0222c4bfaca_0",
      "chapter": "lenet",
      "heading": "Training",
      "text": "Now that we have implemented the model,\nlet's [**run an experiment to see how the LeNet-5 model fares on Fashion-MNIST**].\n\nWhile CNNs have fewer parameters,\nthey can still be more expensive to compute\nthan similarly deep MLPs\nbecause each parameter participates in many more\nmultiplications.\nIf you have access to a GPU, this might be a good time\nto put it into action to speed up training.\nNote that\nthe `d2l.Trainer` class takes care of all details.\nBy default, it initializes the model parameters on the\navailable devices.\nJust as with MLPs, our loss function is cross-entropy,\nand we minimize it via minibatch stochastic gradient descent.\n\n```{.python .input}\n%%tab pytorch, mxnet, jax\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128)\nmodel = LeNet(lr=0.1)\nif tab.selected('pytorch'):\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab tensorflow\ntrainer = d2l.Trainer(max_epochs=10)\ndata = d2l.FashionMNIST(batch_size=128)\nwith d2l.try_gpu():\n    model = LeNet(lr=0.1)\n    trainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "9d455ce88874_0",
      "chapter": "lenet",
      "heading": "Summary",
      "text": "We have made significant progress in this chapter. We moved from the MLPs of the 1980s to the CNNs of the 1990s and early 2000s. The architectures proposed, e.g., in the form of LeNet-5 remain meaningful, even to this day. It is worth comparing the error rates on Fashion-MNIST achievable with LeNet-5 both to the very best possible with MLPs (:numref:`sec_mlp-implementation`) and those with significantly more advanced architectures such as ResNet (:numref:`sec_resnet`). LeNet is much more similar to the latter than to the former. One of the primary differences, as we shall see, is that greater amounts of computation enabled significantly more complex architectures.\n\nA second difference is the relative ease with which we were able to implement LeNet. What used to be an engineering challenge worth months of C++ and assembly code, engineering to improve SN, an early Lisp-based deep learning tool :cite:`Bottou.Le-Cun.1988`, and finally experimentation with models can now be accomplished in minutes. It is this incredible productivity boost that has democratized deep learning model development tremendously. In the next chapter, we will journey down this rabbit hole to see where it takes us."
    },
    {
      "chunk_id": "2e7db7159870_0",
      "chapter": "lenet",
      "heading": "Exercises",
      "text": "1. Let's modernize LeNet. Implement and test the following changes:\n    1. Replace average pooling with max-pooling.\n    1. Replace the softmax layer with ReLU.\n1. Try to change the size of the LeNet style network to improve its accuracy in addition to max-pooling and ReLU.\n    1. Adjust the convolution window size.\n    1. Adjust the number of output channels.\n    1. Adjust the number of convolution layers.\n    1. Adjust the number of fully connected layers.\n    1. Adjust the learning rates and other training details (e.g., initialization and number of epochs).\n1. Try out the improved network on the original MNIST dataset.\n1. Display the activations of the first and second layer of LeNet for different inputs (e.g., sweaters and coats).\n1. What happens to the activations when you feed significantly different images into the network (e.g., cats, cars, or even random noise)?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/73)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/74)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/275)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18000)\n:end_tab:"
    },
    {
      "chunk_id": "62d860da80fd_0",
      "chapter": "padding-and-strides",
      "heading": "padding-and-strides",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Padding and Stride\n:label:`sec_padding`\n\nRecall the example of a convolution in :numref:`fig_correlation`. \nThe input had both a height and width of 3\nand the convolution kernel had both a height and width of 2,\nyielding an output representation with dimension $2\\times2$.\nAssuming that the input shape is $n_\\textrm{h}\\times n_\\textrm{w}$\nand the convolution kernel shape is $k_\\textrm{h}\\times k_\\textrm{w}$,\nthe output shape will be $(n_\\textrm{h}-k_\\textrm{h}+1) \\times (n_\\textrm{w}-k_\\textrm{w}+1)$: \nwe can only shift the convolution kernel so far until it runs out\nof pixels to apply the convolution to. \n\nIn the following we will explore a number of techniques, \nincluding padding and strided convolutions,\nthat offer more control over the size of the output. \nAs motivation, note that since kernels generally\nhave width and height greater than $1$,\nafter applying many successive convolutions,\nwe tend to wind up with outputs that are\nconsiderably smaller than our input.\nIf we start with a $240 \\times 240$ pixel image,\nten layers of $5 \\times 5$ convolutions\nreduce the image to $200 \\times 200$ pixels,\nslicing off $30 \\%$ of the image and with it\nobliterating any interesting information\non the boundaries of the original image.\n*Padding* is the most popular tool for handling this issue.\nIn other cases, we may want to reduce the dimensionality drastically,\ne.g., if we find the original input resolution to be unwieldy.\n*Strided convolutions* are a popular technique that can help in these instances.\n\n```{.python .input}\n%%tab mxnet\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "75cb4b632f89_0",
      "chapter": "padding-and-strides",
      "heading": "Padding",
      "text": "As described above, one tricky issue when applying convolutional layers\nis that we tend to lose pixels on the perimeter of our image. Consider :numref:`img_conv_reuse` that depicts the pixel utilization as a function of the convolution kernel size and the position within the image. The pixels in the corners are hardly used at all. ![Pixel utilization for convolutions of size $1 \\times 1$, $2 \\times 2$, and $3 \\times 3$ respectively.](../img/conv-reuse.svg)\n:label:`img_conv_reuse`\n\nSince we typically use small kernels,\nfor any given convolution\nwe might only lose a few pixels\nbut this can add up as we apply\nmany successive convolutional layers. One straightforward solution to this problem\nis to add extra pixels of filler around the boundary of our input image,\nthus increasing the effective size of the image. Typically, we set the values of the extra pixels to zero. In :numref:`img_conv_pad`, we pad a $3 \\times 3$ input,\nincreasing its size to $5 \\times 5$. The corresponding output then increases to a $4 \\times 4$ matrix. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+0\\times2+0\\times3=0$. ![Two-dimensional cross-correlation with padding.](../img/conv-pad.svg)\n:label:`img_conv_pad`\n\nIn general, if we add a total of $p_\\textrm{h}$ rows of padding\n(roughly half on top and half on bottom)\nand a total of $p_\\textrm{w}$ columns of padding\n(roughly half on the left and half on the right),\nthe output shape will be\n\n$$(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+1)\\times(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+1).$$\n\nThis means that the height and width of the output\nwill increase by $p_\\textrm{h}$ and $p_\\textrm{w}$, respectively. In many cases, we will want to set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$\nto give the input and output the same height and width. This will make it easier to predict the output shape of each layer\nwhen constructing the network."
    },
    {
      "chunk_id": "75cb4b632f89_1",
      "chapter": "padding-and-strides",
      "heading": "Padding",
      "text": "In many cases, we will want to set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$\nto give the input and output the same height and width. This will make it easier to predict the output shape of each layer\nwhen constructing the network. Assuming that $k_\\textrm{h}$ is odd here,\nwe will pad $p_\\textrm{h}/2$ rows on both sides of the height. If $k_\\textrm{h}$ is even, one possibility is to\npad $\\lceil p_\\textrm{h}/2\\rceil$ rows on the top of the input\nand $\\lfloor p_\\textrm{h}/2\\rfloor$ rows on the bottom. We will pad both sides of the width in the same way. CNNs commonly use convolution kernels\nwith odd height and width values, such as 1, 3, 5, or 7. Choosing odd kernel sizes has the benefit\nthat we can preserve the dimensionality\nwhile padding with the same number of rows on top and bottom,\nand the same number of columns on left and right. Moreover, this practice of using odd kernels\nand padding to precisely preserve dimensionality\noffers a clerical benefit. For any two-dimensional tensor `X`,\nwhen the kernel's size is odd\nand the number of padding rows and columns\non all sides are the same,\nthereby producing an output with the same height and width as the input,\nwe know that the output `Y[i, j]` is calculated\nby cross-correlation of the input and convolution kernel\nwith the window centered on `X[i, j]`. In the following example, we create a two-dimensional convolutional layer\nwith a height and width of 3\nand (**apply 1 pixel of padding on all sides.**)\nGiven an input with a height and width of 8,\nwe find that the height and width of the output is also 8. ```{.python .input}\n%%tab mxnet\n# We define a helper function to calculate convolutions."
    },
    {
      "chunk_id": "75cb4b632f89_2",
      "chapter": "padding-and-strides",
      "heading": "Padding",
      "text": "```{.python .input}\n%%tab mxnet\n# We define a helper function to calculate convolutions. It initializes \n# the convolutional layer weights and performs corresponding dimensionality \n# elevations and reductions on the input and output\ndef comp_conv2d(conv2d, X):\n    conv2d.initialize()\n    # (1, 1) indicates that batch size and the number of channels are both 1\n    X = X.reshape((1, 1) + X.shape)\n    Y = conv2d(X)\n    # Strip the first two dimensions: examples and channels\n    return Y.reshape(Y.shape[2:])\n\n# 1 row and column is padded on either side, so a total of 2 rows or columns are added\nconv2d = nn.Conv2D(1, kernel_size=3, padding=1)\nX = np.random.uniform(size=(8, 8))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab pytorch\n# We define a helper function to calculate convolutions. It initializes the\n# convolutional layer weights and performs corresponding dimensionality\n# elevations and reductions on the input and output\ndef comp_conv2d(conv2d, X):\n    # (1, 1) indicates that batch size and the number of channels are both 1\n    X = X.reshape((1, 1) + X.shape)\n    Y = conv2d(X)\n    # Strip the first two dimensions: examples and channels\n    return Y.reshape(Y.shape[2:])\n\n# 1 row and column is padded on either side, so a total of 2 rows or columns\n# are added\nconv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\nX = torch.rand(size=(8, 8))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\n# We define a helper function to calculate convolutions."
    },
    {
      "chunk_id": "75cb4b632f89_3",
      "chapter": "padding-and-strides",
      "heading": "Padding",
      "text": "It initializes\n# the convolutional layer weights and performs corresponding dimensionality\n# elevations and reductions on the input and output\ndef comp_conv2d(conv2d, X):\n    # (1, 1) indicates that batch size and the number of channels are both 1\n    X = tf.reshape(X, (1, ) + X.shape + (1, ))\n    Y = conv2d(X)\n    # Strip the first two dimensions: examples and channels\n    return tf.reshape(Y, Y.shape[1:3])\n# 1 row and column is padded on either side, so a total of 2 rows or columns\n# are added\nconv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding='same')\nX = tf.random.uniform(shape=(8, 8))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab jax\n# We define a helper function to calculate convolutions. It initializes\n# the convolutional layer weights and performs corresponding dimensionality\n# elevations and reductions on the input and output\ndef comp_conv2d(conv2d, X):\n    # (1, X.shape, 1) indicates that batch size and the number of channels are both 1\n    key = jax.random.PRNGKey(d2l.get_seed())\n    X = X.reshape((1,) + X.shape + (1,))\n    Y, _ = conv2d.init_with_output(key, X)\n    # Strip the dimensions: examples and channels\n    return Y.reshape(Y.shape[1:3])\n# 1 row and column is padded on either side, so a total of 2 rows or columns are added\nconv2d = nn.Conv(1, kernel_size=(3, 3), padding='SAME')\nX = jax.random.uniform(jax.random.PRNGKey(d2l.get_seed()), shape=(8, 8))\ncomp_conv2d(conv2d, X).shape\n```\n\nWhen the height and width of the convolution kernel are different,\nwe can make the output and input have the same height and width\nby [**setting different padding numbers for height and width.**]\n\n```{.python .input}\n%%tab mxnet\n# We use a convolution kernel with height 5 and width 3. The padding on\n# either side of the height and width are 2 and 1, respectively\nconv2d = nn.Conv2D(1, kernel_size=(5, 3), padding=(2, 1))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab pytorch\n# We use a convolution kernel with height 5 and width 3."
    },
    {
      "chunk_id": "75cb4b632f89_4",
      "chapter": "padding-and-strides",
      "heading": "Padding",
      "text": "The padding on either\n# side of the height and width are 2 and 1, respectively\nconv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\n# We use a convolution kernel with height 5 and width 3. The padding on\n# either side of the height and width are 2 and 1, respectively\nconv2d = tf.keras.layers.Conv2D(1, kernel_size=(5, 3), padding='same')\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab jax\n# We use a convolution kernel with height 5 and width 3. The padding on\n# either side of the height and width are 2 and 1, respectively\nconv2d = nn.Conv(1, kernel_size=(5, 3), padding=(2, 1))\ncomp_conv2d(conv2d, X).shape\n```"
    },
    {
      "chunk_id": "f27040c444b3_0",
      "chapter": "padding-and-strides",
      "heading": "Stride",
      "text": "When computing the cross-correlation,\nwe start with the convolution window\nat the upper-left corner of the input tensor,\nand then slide it over all locations both down and to the right. In the previous examples, we defaulted to sliding one element at a time. However, sometimes, either for computational efficiency\nor because we wish to downsample,\nwe move our window more than one element at a time,\nskipping the intermediate locations. This is particularly useful if the convolution \nkernel is large since it captures a large area of the underlying image. We refer to the number of rows and columns traversed per slide as *stride*. So far, we have used strides of 1, both for height and width. Sometimes, we may want to use a larger stride. :numref:`img_conv_stride` shows a two-dimensional cross-correlation operation\nwith a stride of 3 vertically and 2 horizontally. The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+1\\times2+2\\times3=8$, $0\\times0+6\\times1+0\\times2+0\\times3=6$. We can see that when the second element of the first column is generated,\nthe convolution window slides down three rows. The convolution window slides two columns to the right\nwhen the second element of the first row is generated. When the convolution window continues to slide two columns to the right on the input,\nthere is no output because the input element cannot fill the window\n(unless we add another column of padding)."
    },
    {
      "chunk_id": "f27040c444b3_1",
      "chapter": "padding-and-strides",
      "heading": "Stride",
      "text": "When the convolution window continues to slide two columns to the right on the input,\nthere is no output because the input element cannot fill the window\n(unless we add another column of padding). ![Cross-correlation with strides of 3 and 2 for height and width, respectively.](../img/conv-stride.svg)\n:label:`img_conv_stride`\n\nIn general, when the stride for the height is $s_\\textrm{h}$\nand the stride for the width is $s_\\textrm{w}$, the output shape is\n\n$$\\lfloor(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+s_\\textrm{h})/s_\\textrm{h}\\rfloor \\times \\lfloor(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+s_\\textrm{w})/s_\\textrm{w}\\rfloor.$$\n\nIf we set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$,\nthen the output shape can be simplified to\n$\\lfloor(n_\\textrm{h}+s_\\textrm{h}-1)/s_\\textrm{h}\\rfloor \\times \\lfloor(n_\\textrm{w}+s_\\textrm{w}-1)/s_\\textrm{w}\\rfloor$. Going a step further, if the input height and width\nare divisible by the strides on the height and width,\nthen the output shape will be $(n_\\textrm{h}/s_\\textrm{h}) \\times (n_\\textrm{w}/s_\\textrm{w})$. Below, we [**set the strides on both the height and width to 2**],\nthus halving the input height and width. ```{.python .input}\n%%tab mxnet\nconv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab pytorch\nconv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nconv2d = tf.keras.layers.Conv2D(1, kernel_size=3, padding='same', strides=2)\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab jax\nconv2d = nn.Conv(1, kernel_size=(3, 3), padding=1, strides=2)\ncomp_conv2d(conv2d, X).shape\n```\n\nLet's look at (**a slightly more complicated example**)."
    },
    {
      "chunk_id": "f27040c444b3_2",
      "chapter": "padding-and-strides",
      "heading": "Stride",
      "text": "```{.python .input}\n%%tab mxnet\nconv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab pytorch\nconv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nconv2d = tf.keras.layers.Conv2D(1, kernel_size=(3,5), padding='valid',\n                                strides=(3, 4))\ncomp_conv2d(conv2d, X).shape\n```\n\n```{.python .input}\n%%tab jax\nconv2d = nn.Conv(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))\ncomp_conv2d(conv2d, X).shape\n```"
    },
    {
      "chunk_id": "6e143ff0c5fa_0",
      "chapter": "padding-and-strides",
      "heading": "Summary and Discussion",
      "text": "Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input to avoid undesirable shrinkage of the output. Moreover, it ensures that all pixels are used equally frequently. Typically we pick symmetric padding on both sides of the input height and width. In this case we refer to $(p_\\textrm{h}, p_\\textrm{w})$ padding. Most commonly we set $p_\\textrm{h} = p_\\textrm{w}$, in which case we simply state that we choose padding $p$. \n\nA similar convention applies to strides. When horizontal stride $s_\\textrm{h}$ and vertical stride $s_\\textrm{w}$ match, we simply talk about stride $s$. The stride can reduce the resolution of the output, for example reducing the height and width of the output to only $1/n$ of the height and width of the input for $n > 1$. By default, the padding is 0 and the stride is 1. \n\nSo far all padding that we discussed simply extended images with zeros. This has significant computational benefit since it is trivial to accomplish. Moreover, operators can be engineered to take advantage of this padding implicitly without the need to allocate additional memory. At the same time, it allows CNNs to encode implicit position information within an image, simply by learning where the \"whitespace\" is. There are many alternatives to zero-padding. :citet:`Alsallakh.Kokhlikyan.Miglani.ea.2020` provided an extensive overview of those (albeit without a clear case for when to use nonzero paddings unless artifacts occur)."
    },
    {
      "chunk_id": "938a16158faf_0",
      "chapter": "padding-and-strides",
      "heading": "Exercises",
      "text": "1. Given the final code example in this section with kernel size $(3, 5)$, padding $(0, 1)$, and stride $(3, 4)$, \n   calculate the output shape to check if it is consistent with the experimental result.\n1. For audio signals, what does a stride of 2 correspond to?\n1. Implement mirror padding, i.e., padding where the border values are simply mirrored to extend tensors. \n1. What are the computational benefits of a stride larger than 1?\n1. What might be statistical benefits of a stride larger than 1?\n1. How would you implement a stride of $\\frac{1}{2}$? What does it correspond to? When would this be useful?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/67)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/68)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/272)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17997)\n:end_tab:"
    },
    {
      "chunk_id": "806baa1037d1_0",
      "chapter": "pooling",
      "heading": "pooling",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Pooling\n:label:`sec_pooling`\n\nIn many cases our ultimate task asks some global question about the image,\ne.g., *does it contain a cat?* Consequently, the units of our final layer \nshould be sensitive to the entire input. By gradually aggregating information, yielding coarser and coarser maps,\nwe accomplish this goal of ultimately learning a global representation,\nwhile keeping all of the advantages of convolutional layers at the intermediate layers of processing. The deeper we go in the network,\nthe larger the receptive field (relative to the input)\nto which each hidden node is sensitive. Reducing spatial resolution \naccelerates this process, \nsince the convolution kernels cover a larger effective area. Moreover, when detecting lower-level features, such as edges\n(as discussed in :numref:`sec_conv_layer`),\nwe often want our representations to be somewhat invariant to translation. For instance, if we take the image `X`\nwith a sharp delineation between black and white\nand shift the whole image by one pixel to the right,\ni.e., `Z[i, j] = X[i, j + 1]`,\nthen the output for the new image `Z` might be vastly different. The edge will have shifted by one pixel. In reality, objects hardly ever occur exactly at the same place. In fact, even with a tripod and a stationary object,\nvibration of the camera due to the movement of the shutter\nmight shift everything by a pixel or so\n(high-end cameras are loaded with special features to address this problem). This section introduces *pooling layers*,\nwhich serve the dual purposes of\nmitigating the sensitivity of convolutional layers to location\nand of spatially downsampling representations."
    },
    {
      "chunk_id": "806baa1037d1_1",
      "chapter": "pooling",
      "heading": "pooling",
      "text": "This section introduces *pooling layers*,\nwhich serve the dual purposes of\nmitigating the sensitivity of convolutional layers to location\nand of spatially downsampling representations. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "6c04484493ed_0",
      "chapter": "pooling",
      "heading": "Maximum Pooling and Average Pooling",
      "text": "Like convolutional layers, *pooling* operators\nconsist of a fixed-shape window that is slid over\nall regions in the input according to its stride,\ncomputing a single output for each location traversed\nby the fixed-shape window (sometimes known as the *pooling window*). However, unlike the cross-correlation computation\nof the inputs and kernels in the convolutional layer,\nthe pooling layer contains no parameters (there is no *kernel*). Instead, pooling operators are deterministic,\ntypically calculating either the maximum or the average value\nof the elements in the pooling window. These operations are called *maximum pooling* (*max-pooling* for short)\nand *average pooling*, respectively. *Average pooling* is essentially as old as CNNs. The idea is akin to \ndownsampling an image. Rather than just taking the value of every second (or third) \npixel for the lower resolution image, we can average over adjacent pixels to obtain \nan image with better signal-to-noise ratio since we are combining the information \nfrom multiple adjacent pixels. *Max-pooling* was introduced in \n:citet:`Riesenhuber.Poggio.1999` in the context of cognitive neuroscience to describe \nhow information aggregation might be aggregated hierarchically for the purpose \nof object recognition; there already was an earlier version in speech recognition :cite:`Yamaguchi.Sakamoto.Akabane.ea.1990`. In almost all cases, max-pooling, as it is also referred to, \nis preferable to average pooling. In both cases, as with the cross-correlation operator,\nwe can think of the pooling window\nas starting from the upper-left of the input tensor\nand sliding across it from left to right and top to bottom. At each location that the pooling window hits,\nit computes the maximum or average\nvalue of the input subtensor in the window,\ndepending on whether max or average pooling is employed. ![Max-pooling with a pooling window shape of $2\\times 2$."
    },
    {
      "chunk_id": "6c04484493ed_1",
      "chapter": "pooling",
      "heading": "Maximum Pooling and Average Pooling",
      "text": "At each location that the pooling window hits,\nit computes the maximum or average\nvalue of the input subtensor in the window,\ndepending on whether max or average pooling is employed. ![Max-pooling with a pooling window shape of $2\\times 2$. The shaded portions are the first output element as well as the input tensor elements used for the output computation: $\\max(0, 1, 3, 4)=4$.](../img/pooling.svg)\n:label:`fig_pooling`\n\nThe output tensor in :numref:`fig_pooling`  has a height of 2 and a width of 2. The four elements are derived from the maximum value in each pooling window:\n\n$$\n\\max(0, 1, 3, 4)=4,\\\\\n\\max(1, 2, 4, 5)=5,\\\\\n\\max(3, 4, 6, 7)=7,\\\\\n\\max(4, 5, 7, 8)=8.\\\\\n$$\n\nMore generally, we can define a $p \\times q$ pooling layer by aggregating over \na region of said size. Returning to the problem of edge detection, \nwe use the output of the convolutional layer\nas input for $2\\times 2$ max-pooling. Denote by `X` the input of the convolutional layer input and `Y` the pooling layer output. Regardless of whether or not the values of `X[i, j]`, `X[i, j + 1]`, \n`X[i+1, j]` and `X[i+1, j + 1]` are different,\nthe pooling layer always outputs `Y[i, j] = 1`. That is to say, using the $2\\times 2$ max-pooling layer,\nwe can still detect if the pattern recognized by the convolutional layer\nmoves no more than one element in height or width. In the code below, we (**implement the forward propagation\nof the pooling layer**) in the `pool2d` function. This function is similar to the `corr2d` function\nin :numref:`sec_conv_layer`. However, no kernel is needed, computing the output\nas either the maximum or the average of each region in the input."
    },
    {
      "chunk_id": "6c04484493ed_2",
      "chapter": "pooling",
      "heading": "Maximum Pooling and Average Pooling",
      "text": "This function is similar to the `corr2d` function\nin :numref:`sec_conv_layer`. However, no kernel is needed, computing the output\nas either the maximum or the average of each region in the input. ```{.python .input}\n%%tab mxnet, pytorch\ndef pool2d(X, pool_size, mode='max'):\n    p_h, p_w = pool_size\n    Y = d2l.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            if mode == 'max':\n                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n            elif mode == 'avg':\n                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n    return Y\n```\n\n```{.python .input}\n%%tab jax\ndef pool2d(X, pool_size, mode='max'):\n    p_h, p_w = pool_size\n    Y = jnp.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            if mode == 'max':\n                Y = Y.at[i, j].set(X[i: i + p_h, j: j + p_w].max())\n            elif mode == 'avg':\n                Y = Y.at[i, j].set(X[i: i + p_h, j: j + p_w].mean())\n    return Y\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n\ndef pool2d(X, pool_size, mode='max'):\n    p_h, p_w = pool_size\n    Y = tf.Variable(tf.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w +1)))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            if mode == 'max':\n                Y[i, j].assign(tf.reduce_max(X[i: i + p_h, j: j + p_w]))\n            elif mode =='avg':\n                Y[i, j].assign(tf.reduce_mean(X[i: i + p_h, j: j + p_w]))\n    return Y\n```\n\nWe can construct the input tensor `X` in :numref:`fig_pooling` to [**validate the output of the two-dimensional max-pooling layer**]. ```{.python .input}\n%%tab all\nX = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\npool2d(X, (2, 2))\n```\n\nAlso, we can experiment with (**the average pooling layer**). ```{.python .input}\n%%tab all\npool2d(X, (2, 2), 'avg')\n```"
    },
    {
      "chunk_id": "342b734f5c4e_0",
      "chapter": "pooling",
      "heading": "[**Padding and Stride**]",
      "text": "As with convolutional layers, pooling layers\nchange the output shape. And as before, we can adjust the operation to achieve a desired output shape\nby padding the input and adjusting the stride. We can demonstrate the use of padding and strides\nin pooling layers via the built-in two-dimensional max-pooling layer from the deep learning framework. We first construct an input tensor `X` whose shape has four dimensions,\nwhere the number of examples (batch size) and number of channels are both 1. :begin_tab:`tensorflow`\nNote that unlike other frameworks, TensorFlow\nprefers and is optimized for *channels-last* input. :end_tab:\n\n```{.python .input}\n%%tab mxnet, pytorch\nX = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 1, 4, 4))\nX\n```\n\n```{.python .input}\n%%tab tensorflow, jax\nX = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 4, 4, 1))\nX\n```\n\nSince pooling aggregates information from an area, (**deep learning frameworks default to matching pooling window sizes and stride.**) For instance, if we use a pooling window of shape `(3, 3)`\nwe get a stride shape of `(3, 3)` by default. ```{.python .input}\n%%tab mxnet\npool2d = nn.MaxPool2D(3)\n# Pooling has no model parameters, hence it needs no initialization\npool2d(X)\n```\n\n```{.python .input}\n%%tab pytorch\npool2d = nn.MaxPool2d(3)\n# Pooling has no model parameters, hence it needs no initialization\npool2d(X)\n```\n\n```{.python .input}\n%%tab tensorflow\npool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3])\n# Pooling has no model parameters, hence it needs no initialization\npool2d(X)\n```\n\n```{.python .input}\n%%tab jax\n# Pooling has no model parameters, hence it needs no initialization\nnn.max_pool(X, window_shape=(3, 3), strides=(3, 3))\n```\n\nNeedless to say, [**the stride and padding can be manually specified**] to override framework defaults if required."
    },
    {
      "chunk_id": "342b734f5c4e_1",
      "chapter": "pooling",
      "heading": "[**Padding and Stride**]",
      "text": "```{.python .input}\n%%tab mxnet\npool2d = nn.MaxPool2D(3, padding=1, strides=2)\npool2d(X)\n```\n\n```{.python .input}\n%%tab pytorch\npool2d = nn.MaxPool2d(3, padding=1, stride=2)\npool2d(X)\n```\n\n```{.python .input}\n%%tab tensorflow\npaddings = tf.constant([[0, 0], [1,0], [1,0], [0,0]])\nX_padded = tf.pad(X, paddings, \"CONSTANT\")\npool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3], padding='valid',\n                                   strides=2)\npool2d(X_padded)\n```\n\n```{.python .input}\n%%tab jax\nX_padded = jnp.pad(X, ((0, 0), (1, 0), (1, 0), (0, 0)), mode='constant')\nnn.max_pool(X_padded, window_shape=(3, 3), padding='VALID', strides=(2, 2))\n```\n\nOf course, we can specify an arbitrary rectangular pooling window with arbitrary height and width respectively, as the example below shows. ```{.python .input}\n%%tab mxnet\npool2d = nn.MaxPool2D((2, 3), padding=(0, 1), strides=(2, 3))\npool2d(X)\n```\n\n```{.python .input}\n%%tab pytorch\npool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\npool2d(X)\n```\n\n```{.python .input}\n%%tab tensorflow\npaddings = tf.constant([[0, 0], [0, 0], [1, 1], [0, 0]])\nX_padded = tf.pad(X, paddings, \"CONSTANT\")\n\npool2d = tf.keras.layers.MaxPool2D(pool_size=[2, 3], padding='valid',\n                                   strides=(2, 3))\npool2d(X_padded)\n```\n\n```{.python .input}\n%%tab jax\n\nX_padded = jnp.pad(X, ((0, 0), (0, 0), (1, 1), (0, 0)), mode='constant')\nnn.max_pool(X_padded, window_shape=(2, 3), strides=(2, 3), padding='VALID')\n```"
    },
    {
      "chunk_id": "30c692b2ee50_0",
      "chapter": "pooling",
      "heading": "Multiple Channels",
      "text": "When processing multi-channel input data,\n[**the pooling layer pools each input channel separately**],\nrather than summing the inputs up over channels\nas in a convolutional layer.\nThis means that the number of output channels for the pooling layer\nis the same as the number of input channels.\nBelow, we will concatenate tensors `X` and `X + 1`\non the channel dimension to construct an input with two channels.\n\n:begin_tab:`tensorflow`\nNote that this will require a\nconcatenation along the last dimension for TensorFlow due to the channels-last syntax.\n:end_tab:\n\n```{.python .input}\n%%tab mxnet, pytorch\nX = d2l.concat((X, X + 1), 1)\nX\n```\n\n```{.python .input}\n%%tab tensorflow, jax\n# Concatenate along `dim=3` due to channels-last syntax\nX = d2l.concat([X, X + 1], 3)\nX\n```\n\nAs we can see, the number of output channels is still two after pooling.\n\n```{.python .input}\n%%tab mxnet\npool2d = nn.MaxPool2D(3, padding=1, strides=2)\npool2d(X)\n```\n\n```{.python .input}\n%%tab pytorch\npool2d = nn.MaxPool2d(3, padding=1, stride=2)\npool2d(X)\n```\n\n```{.python .input}\n%%tab tensorflow\npaddings = tf.constant([[0, 0], [1,0], [1,0], [0,0]])\nX_padded = tf.pad(X, paddings, \"CONSTANT\")\npool2d = tf.keras.layers.MaxPool2D(pool_size=[3, 3], padding='valid',\n                                   strides=2)\npool2d(X_padded)\n\n```\n\n```{.python .input}\n%%tab jax\nX_padded = jnp.pad(X, ((0, 0), (1, 0), (1, 0), (0, 0)), mode='constant')\nnn.max_pool(X_padded, window_shape=(3, 3), padding='VALID', strides=(2, 2))\n```\n\n:begin_tab:`tensorflow`\nNote that the output for the TensorFlow pooling appears at first glance to be different, however\nnumerically the same results are presented as MXNet and PyTorch.\nThe difference lies in the dimensionality, and reading the\noutput vertically yields the same output as the other implementations.\n:end_tab:"
    },
    {
      "chunk_id": "83ececf8d75f_0",
      "chapter": "pooling",
      "heading": "Summary",
      "text": "Pooling is an exceedingly simple operation. It does exactly what its name indicates, aggregate results over a window of values. All convolution semantics, such as strides and padding apply in the same way as they did previously. Note that pooling is indifferent to channels, i.e., it leaves the number of channels unchanged and it applies to each channel separately. Lastly, of the two popular pooling choices, max-pooling is preferable to average pooling, as it confers some degree of invariance to output. A popular choice is to pick a pooling window size of $2 \\times 2$ to quarter the spatial resolution of output. \n\nNote that there are many more ways of reducing resolution beyond pooling. For instance, in stochastic pooling :cite:`Zeiler.Fergus.2013` and fractional max-pooling :cite:`Graham.2014` aggregation is combined with randomization. This can slightly improve the accuracy in some cases. Lastly, as we will see later with the attention mechanism, there are more refined ways of aggregating over outputs, e.g., by using the alignment between a query and representation vectors."
    },
    {
      "chunk_id": "ecc45ba1a577_0",
      "chapter": "pooling",
      "heading": "Exercises",
      "text": "1. Implement average pooling through a convolution. \n1. Prove that max-pooling cannot be implemented through a convolution alone. \n1. Max-pooling can be accomplished using ReLU operations, i.e., $\\textrm{ReLU}(x) = \\max(0, x)$.\n    1. Express $\\max (a, b)$ by using only ReLU operations.\n    1. Use this to implement max-pooling by means of convolutions and ReLU layers. \n    1. How many channels and layers do you need for a $2 \\times 2$ convolution? How many for a $3 \\times 3$ convolution?\n1. What is the computational cost of the pooling layer? Assume that the input to the pooling layer is of size $c\\times h\\times w$, the pooling window has a shape of $p_\\textrm{h}\\times p_\\textrm{w}$ with a padding of $(p_\\textrm{h}, p_\\textrm{w})$ and a stride of $(s_\\textrm{h}, s_\\textrm{w})$.\n1. Why do you expect max-pooling and average pooling to work differently?\n1. Do we need a separate minimum pooling layer? Can you replace it with another operation?\n1. We could use the softmax operation for pooling. Why might it not be so popular?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/71)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/72)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/274)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17999)\n:end_tab:"
    },
    {
      "chunk_id": "8e1e607f8510_0",
      "chapter": "why-conv",
      "heading": "why-conv",
      "text": "# From Fully Connected Layers to Convolutions\n:label:`sec_why-conv`\n\nTo this day,\nthe models that we have discussed so far\nremain appropriate options\nwhen we are dealing with tabular data. By tabular, we mean that the data consist\nof rows corresponding to examples\nand columns corresponding to features. With tabular data, we might anticipate\nthat the patterns we seek could involve\ninteractions among the features,\nbut we do not assume any structure *a priori*\nconcerning how the features interact. Sometimes, we truly lack the knowledge to be able to guide the construction of fancier architectures. In these cases, an MLP\nmay be the best that we can do. However, for high-dimensional perceptual data,\nsuch structureless networks can grow unwieldy. For instance, let's return to our running example\nof distinguishing cats from dogs. Say that we do a thorough job in data collection,\ncollecting an annotated dataset of one-megapixel photographs. This means that each input to the network has one million dimensions. Even an aggressive reduction to one thousand hidden dimensions\nwould require a fully connected layer\ncharacterized by $10^6 \\times 10^3 = 10^9$ parameters. Unless we have lots of GPUs, a talent\nfor distributed optimization,\nand an extraordinary amount of patience,\nlearning the parameters of this network\nmay turn out to be infeasible. A careful reader might object to this argument\non the basis that one megapixel resolution may not be necessary. However, while we might be able\nto get away with one hundred thousand pixels,\nour hidden layer of size 1000 grossly underestimates\nthe number of hidden units that it takes\nto learn good representations of images,\nso a practical system will still require billions of parameters. Moreover, learning a classifier by fitting so many parameters\nmight require collecting an enormous dataset. And yet today both humans and computers are able\nto distinguish cats from dogs quite well,\nseemingly contradicting these intuitions."
    },
    {
      "chunk_id": "8e1e607f8510_1",
      "chapter": "why-conv",
      "heading": "why-conv",
      "text": "Moreover, learning a classifier by fitting so many parameters\nmight require collecting an enormous dataset. And yet today both humans and computers are able\nto distinguish cats from dogs quite well,\nseemingly contradicting these intuitions. That is because images exhibit rich structure\nthat can be exploited by humans\nand machine learning models alike. Convolutional neural networks (CNNs) are one creative way\nthat machine learning has embraced for exploiting\nsome of the known structure in natural images."
    },
    {
      "chunk_id": "3204be6df216_0",
      "chapter": "why-conv",
      "heading": "Invariance",
      "text": "Imagine that we want to detect an object in an image. It seems reasonable that whatever method\nwe use to recognize objects should not be overly concerned\nwith the precise location of the object in the image. Ideally, our system should exploit this knowledge. Pigs usually do not fly and planes usually do not swim. Nonetheless, we should still recognize\na pig were one to appear at the top of the image. We can draw some inspiration here\nfrom the children's game \"Where's Waldo\"\n(which itself has inspired many real-life imitations, such as that depicted in :numref:`img_waldo`). The game consists of a number of chaotic scenes\nbursting with activities. Waldo shows up somewhere in each,\ntypically lurking in some unlikely location. The reader's goal is to locate him. Despite his characteristic outfit,\nthis can be surprisingly difficult,\ndue to the large number of distractions. However, *what Waldo looks like*\ndoes not depend upon *where Waldo is located*. We could sweep the image with a Waldo detector\nthat could assign a score to each patch,\nindicating the likelihood that the patch contains Waldo. In fact, many object detection and segmentation algorithms \nare based on this approach :cite:`Long.Shelhamer.Darrell.2015`. CNNs systematize this idea of *spatial invariance*,\nexploiting it to learn useful representations\nwith fewer parameters. ![Can you find Waldo (image courtesy of William Murphy (Infomatique))?](../img/waldo-football.jpg)\n:width:`400px`\n:label:`img_waldo`\n\nWe can now make these intuitions more concrete \nby enumerating a few desiderata to guide our design\nof a neural network architecture suitable for computer vision:\n\n1. In the earliest layers, our network\n   should respond similarly to the same patch,\n   regardless of where it appears in the image. This principle is called *translation invariance* (or *translation equivariance*). 1. The earliest layers of the network should focus on local regions,\n   without regard for the contents of the image in distant regions. This is the *locality* principle."
    },
    {
      "chunk_id": "3204be6df216_1",
      "chapter": "why-conv",
      "heading": "Invariance",
      "text": "This principle is called *translation invariance* (or *translation equivariance*). 1. The earliest layers of the network should focus on local regions,\n   without regard for the contents of the image in distant regions. This is the *locality* principle. Eventually, these local representations can be aggregated\n   to make predictions at the whole image level. 1. As we proceed, deeper layers should be able to capture longer-range features of the \n   image, in a way similar to higher level vision in nature. Let's see how this translates into mathematics."
    },
    {
      "chunk_id": "9d826c36fdbd_0",
      "chapter": "why-conv",
      "heading": "Constraining the MLP",
      "text": "To start off, we can consider an MLP\nwith two-dimensional images $\\mathbf{X}$ as inputs\nand their immediate hidden representations\n$\\mathbf{H}$ similarly represented as matrices (they are two-dimensional tensors in code), where both $\\mathbf{X}$ and $\\mathbf{H}$ have the same shape. Let that sink in. We now imagine that not only the inputs but\nalso the hidden representations possess spatial structure. Let $[\\mathbf{X}]_{i, j}$ and $[\\mathbf{H}]_{i, j}$ denote the pixel\nat location $(i,j)$\nin the input image and hidden representation, respectively. Consequently, to have each of the hidden units\nreceive input from each of the input pixels,\nwe would switch from using weight matrices\n(as we did previously in MLPs)\nto representing our parameters\nas fourth-order weight tensors $\\mathsf{W}$. Suppose that $\\mathbf{U}$ contains biases,\nwe could formally express the fully connected layer as\n\n$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}$$\n\nThe switch from $\\mathsf{W}$ to $\\mathsf{V}$ is entirely cosmetic for now\nsince there is a one-to-one correspondence\nbetween coefficients in both fourth-order tensors. We simply re-index the subscripts $(k, l)$\nsuch that $k = i+a$ and $l = j+b$. In other words, we set $[\\mathsf{V}]_{i, j, a, b} = [\\mathsf{W}]_{i, j, i+a, j+b}$. The indices $a$ and $b$ run over both positive and negative offsets,\ncovering the entire image. For any given location ($i$, $j$) in the hidden representation $[\\mathbf{H}]_{i, j}$,\nwe compute its value by summing over pixels in $x$,\ncentered around $(i, j)$ and weighted by $[\\mathsf{V}]_{i, j, a, b}$. Before we carry on, let's consider the total number of parameters required for a *single* layer in this parametrization: a $1000 \\times 1000$ image (1 megapixel) is mapped to a $1000 \\times 1000$ hidden representation."
    },
    {
      "chunk_id": "9d826c36fdbd_1",
      "chapter": "why-conv",
      "heading": "Constraining the MLP",
      "text": "Before we carry on, let's consider the total number of parameters required for a *single* layer in this parametrization: a $1000 \\times 1000$ image (1 megapixel) is mapped to a $1000 \\times 1000$ hidden representation. This requires $10^{12}$ parameters, far beyond what computers currently can handle."
    },
    {
      "chunk_id": "500f760bab32_0",
      "chapter": "why-conv",
      "heading": "Translation Invariance",
      "text": "Now let's invoke the first principle\nestablished above: translation invariance :cite:`Zhang.ea.1988`.\nThis implies that a shift in the input $\\mathbf{X}$\nshould simply lead to a shift in the hidden representation $\\mathbf{H}$.\nThis is only possible if $\\mathsf{V}$ and $\\mathbf{U}$ do not actually depend on $(i, j)$. As such,\nwe have $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$ and $\\mathbf{U}$ is a constant, say $u$.\nAs a result, we can simplify the definition for $\\mathbf{H}$:\n\n$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n\n\nThis is a *convolution*!\nWe are effectively weighting pixels at $(i+a, j+b)$\nin the vicinity of location $(i, j)$ with coefficients $[\\mathbf{V}]_{a, b}$\nto obtain the value $[\\mathbf{H}]_{i, j}$.\nNote that $[\\mathbf{V}]_{a, b}$ needs many fewer coefficients than $[\\mathsf{V}]_{i, j, a, b}$ since it\nno longer depends on the location within the image. Consequently, the number of parameters required is no longer $10^{12}$ but a much more reasonable $4 \\times 10^6$: we still have the dependency on $a, b \\in (-1000, 1000)$. In short, we have made significant progress. Time-delay neural networks (TDNNs) are some of the first examples to exploit this idea :cite:`Waibel.Hanazawa.Hinton.ea.1989`."
    },
    {
      "chunk_id": "af233e13bac4_0",
      "chapter": "why-conv",
      "heading": "Locality",
      "text": "Now let's invoke the second principle: locality. As motivated above, we believe that we should not have\nto look very far away from location $(i, j)$\nin order to glean relevant information\nto assess what is going on at $[\\mathbf{H}]_{i, j}$. This means that outside some range $|a|> \\Delta$ or $|b| > \\Delta$,\nwe should set $[\\mathbf{V}]_{a, b} = 0$. Equivalently, we can rewrite $[\\mathbf{H}]_{i, j}$ as\n\n$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n:eqlabel:`eq_conv-layer`\n\nThis reduces the number of parameters from $4 \\times 10^6$ to $4 \\Delta^2$, where $\\Delta$ is typically smaller than $10$. As such, we reduced the number of parameters by another four orders of magnitude. Note that :eqref:`eq_conv-layer`, is what is called, in a nutshell, a *convolutional layer*. *Convolutional neural networks* (CNNs)\nare a special family of neural networks that contain convolutional layers. In the deep learning research community,\n$\\mathbf{V}$ is referred to as a *convolution kernel*,\na *filter*, or simply the layer's *weights* that are learnable parameters. While previously, we might have required billions of parameters\nto represent just a single layer in an image-processing network,\nwe now typically need just a few hundred, without\naltering the dimensionality of either\nthe inputs or the hidden representations. The price paid for this drastic reduction in parameters\nis that our features are now translation invariant\nand that our layer can only incorporate local information,\nwhen determining the value of each hidden activation. All learning depends on imposing inductive bias. When that bias agrees with reality,\nwe get sample-efficient models\nthat generalize well to unseen data. But of course, if those biases do not agree with reality,\ne.g., if images turned out not to be translation invariant,\nour models might struggle even to fit our training data."
    },
    {
      "chunk_id": "af233e13bac4_1",
      "chapter": "why-conv",
      "heading": "Locality",
      "text": "But of course, if those biases do not agree with reality,\ne.g., if images turned out not to be translation invariant,\nour models might struggle even to fit our training data. This dramatic reduction in parameters brings us to our last desideratum, \nnamely that deeper layers should represent larger and more complex aspects \nof an image. This can be achieved by interleaving nonlinearities and convolutional \nlayers repeatedly."
    },
    {
      "chunk_id": "6430016a6613_0",
      "chapter": "why-conv",
      "heading": "Convolutions",
      "text": "Let's briefly review why :eqref:`eq_conv-layer` is called a convolution. \nIn mathematics, the *convolution* between two functions :cite:`Rudin.1973`,\nsay $f, g: \\mathbb{R}^d \\to \\mathbb{R}$ is defined as\n\n$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n\nThat is, we measure the overlap between $f$ and $g$\nwhen one function is \"flipped\" and shifted by $\\mathbf{x}$.\nWhenever we have discrete objects, the integral turns into a sum.\nFor instance, for vectors from\nthe set of square-summable infinite-dimensional vectors\nwith index running over $\\mathbb{Z}$ we obtain the following definition:\n\n$$(f * g)(i) = \\sum_a f(a) g(i-a).$$\n\nFor two-dimensional tensors, we have a corresponding sum\nwith indices $(a, b)$ for $f$ and $(i-a, j-b)$ for $g$, respectively:\n\n$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$$\n:eqlabel:`eq_2d-conv-discrete`\n\nThis looks similar to :eqref:`eq_conv-layer`, with one major difference.\nRather than using $(i+a, j+b)$, we are using the difference instead.\nNote, though, that this distinction is mostly cosmetic\nsince we can always match the notation between\n:eqref:`eq_conv-layer` and :eqref:`eq_2d-conv-discrete`.\nOur original definition in :eqref:`eq_conv-layer` more properly\ndescribes a *cross-correlation*.\nWe will come back to this in the following section."
    },
    {
      "chunk_id": "f3bf43130599_0",
      "chapter": "why-conv",
      "heading": "Channels",
      "text": ":label:`subsec_why-conv-channels`\n\nReturning to our Waldo detector, let's see what this looks like. The convolutional layer picks windows of a given size\nand weighs intensities according to the filter $\\mathsf{V}$, as demonstrated in :numref:`fig_waldo_mask`. We might aim to learn a model so that\nwherever the \"waldoness\" is highest,\nwe should find a peak in the hidden layer representations. ![Detect Waldo (image courtesy of William Murphy (Infomatique)).](../img/waldo-mask.jpg)\n:width:`400px`\n:label:`fig_waldo_mask`\n\nThere is just one problem with this approach. So far, we blissfully ignored that images consist\nof three channels: red, green, and blue. In sum, images are not two-dimensional objects\nbut rather third-order tensors,\ncharacterized by a height, width, and channel,\ne.g., with shape $1024 \\times 1024 \\times 3$ pixels. While the first two of these axes concern spatial relationships,\nthe third can be regarded as assigning\na multidimensional representation to each pixel location. We thus index $\\mathsf{X}$ as $[\\mathsf{X}]_{i, j, k}$. The convolutional filter has to adapt accordingly. Instead of $[\\mathbf{V}]_{a,b}$, we now have $[\\mathsf{V}]_{a,b,c}$. Moreover, just as our input consists of a third-order tensor,\nit turns out to be a good idea to similarly formulate\nour hidden representations as third-order tensors $\\mathsf{H}$. In other words, rather than just having a single hidden representation\ncorresponding to each spatial location,\nwe want an entire vector of hidden representations\ncorresponding to each spatial location. We could think of the hidden representations as comprising\na number of two-dimensional grids stacked on top of each other. As in the inputs, these are sometimes called *channels*. They are also sometimes called *feature maps*,\nas each provides a spatialized set\nof learned features for the subsequent layer. Intuitively, you might imagine that at lower layers that are closer to inputs,\nsome channels could become specialized to recognize edges while\nothers could recognize textures."
    },
    {
      "chunk_id": "f3bf43130599_1",
      "chapter": "why-conv",
      "heading": "Channels",
      "text": "Intuitively, you might imagine that at lower layers that are closer to inputs,\nsome channels could become specialized to recognize edges while\nothers could recognize textures. To support multiple channels in both inputs ($\\mathsf{X}$) and hidden representations ($\\mathsf{H}$),\nwe can add a fourth coordinate to $\\mathsf{V}$: $[\\mathsf{V}]_{a, b, c, d}$. Putting everything together we have:\n\n$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$$\n:eqlabel:`eq_conv-layer-channels`\n\nwhere $d$ indexes the output channels in the hidden representations $\\mathsf{H}$. The subsequent convolutional layer will go on to take a third-order tensor, $\\mathsf{H}$, as input. We take\n:eqref:`eq_conv-layer-channels`,\nbecause of its generality, as\nthe definition of a convolutional layer for multiple channels, where $\\mathsf{V}$ is a kernel or filter of the layer. There are still many operations that we need to address. For instance, we need to figure out how to combine all the hidden representations\nto a single output, e.g., whether there is a Waldo *anywhere* in the image. We also need to decide how to compute things efficiently,\nhow to combine multiple layers,\nappropriate activation functions,\nand how to make reasonable design choices\nto yield networks that are effective in practice. We turn to these issues in the remainder of the chapter."
    },
    {
      "chunk_id": "bf13fba9cd02_0",
      "chapter": "why-conv",
      "heading": "Summary and Discussion",
      "text": "In this section we derived the structure of convolutional neural networks from first principles. While it is unclear whether this was the route taken to the invention of CNNs, it is satisfying to know that they are the *right* choice when applying reasonable principles to how image processing and computer vision algorithms should operate, at least at lower levels. In particular, translation invariance in images implies that all patches of an image will be treated in the same manner. Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden representations. Some of the earliest references to CNNs are in the form of the Neocognitron :cite:`Fukushima.1982`. \n\nA second principle that we encountered in our reasoning is how to reduce the number of parameters in a function class without limiting its expressive power, at least, whenever certain assumptions on the model hold. We saw a dramatic reduction of complexity as a result of this restriction, turning computationally and statistically infeasible problems into tractable models. \n\nAdding channels allowed us to bring back some of the complexity that was lost due to the restrictions imposed on the convolutional kernel by locality and translation invariance. Note that it is quite natural to add channels other than just red, green, and blue. Many satellite \nimages, in particular for agriculture and meteorology, have tens to hundreds of channels, \ngenerating hyperspectral images instead. They report data on many different wavelengths. In the following we will see how to use convolutions effectively to manipulate the dimensionality of the images they operate on, how to move from location-based to channel-based representations, and how to deal with large numbers of categories efficiently."
    },
    {
      "chunk_id": "f0520d4d3c3d_0",
      "chapter": "why-conv",
      "heading": "Exercises",
      "text": "1. Assume that the size of the convolution kernel is $\\Delta = 0$.\n   Show that in this case the convolution kernel\n   implements an MLP independently for each set of channels. This leads to the Network in Network \n   architectures :cite:`Lin.Chen.Yan.2013`. \n1. Audio data is often represented as a one-dimensional sequence. \n    1. When might you want to impose locality and translation invariance for audio? \n    1. Derive the convolution operations for audio.\n    1. Can you treat audio using the same tools as computer vision? Hint: use the spectrogram.\n1. Why might translation invariance not be a good idea after all? Give an example. \n1. Do you think that convolutional layers might also be applicable for text data?\n   Which problems might you encounter with language?\n1. What happens with convolutions when an object is at the boundary of an image?\n1. Prove that the convolution is symmetric, i.e., $f * g = g * f$.\n\n[Discussions](https://discuss.d2l.ai/t/64)"
    },
    {
      "chunk_id": "08bc0ad0e67a_0",
      "chapter": "gp-inference",
      "heading": "gp-inference",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n#required_libs(\"gpytorch\")\n```\n\n# Gaussian Process Inference\n\nIn this section, we will show how to perform posterior inference and make predictions using the GP priors we introduced in the last section. We will start with regression, where we can perform inference in _closed form_. This is a \"GPs in a nutshell\" section to quickly get up and running with Gaussian processes in practice. We'll start coding all the basic operations from scratch, and then introduce [GPyTorch](https://gpytorch.ai/), which will make working with state-of-the-art Gaussian processes and integration with deep neural networks much more convenient. We will consider these more advanced topics in depth in the next section. In that section, we will also consider settings where approximate inference is required --- classification, point processes, or any non-Gaussian likelihoods."
    },
    {
      "chunk_id": "de008919b856_0",
      "chapter": "gp-inference",
      "heading": "Posterior Inference for Regression",
      "text": "An _observation_ model relates the function we want to learn, $f(x)$, to our observations $y(x)$, both indexed by some input $x$. In classification, $x$ could be the pixels of an image, and $y$ could be the associated class label. In regression, $y$ typically represents a continuous output, such as a land surface temperature, a sea-level, a $CO_2$ concentration, etc. In regression, we often assume the outputs are given by a latent noise-free function $f(x)$ plus i.i.d. Gaussian noise $\\epsilon(x)$: \n\n$$y(x) = f(x) + \\epsilon(x),$$\n:eqlabel:`eq_gp-regression`\n\nwith $\\epsilon(x) \\sim \\mathcal{N}(0,\\sigma^2)$. Let $\\mathbf{y} = y(X) = (y(x_1),\\dots,y(x_n))^{\\top}$ be a vector of our training observations, and $\\textbf{f} = (f(x_1),\\dots,f(x_n))^{\\top}$ be a vector of the latent noise-free function values, queried at the training inputs $X = {x_1, \\dots, x_n}$. We will assume $f(x) \\sim \\mathcal{GP}(m,k)$, which means that any collection of function values $\\textbf{f}$ has a joint multivariate Gaussian distribution, with mean vector $\\mu_i = m(x_i)$ and covariance matrix $K_{ij} = k(x_i,x_j)$. The RBF kernel $k(x_i,x_j) = a^2 \\exp\\left(-\\frac{1}{2\\ell^2}||x_i-x_j||^2\\right)$ would be a standard choice of covariance function. For notational simplicity, we will assume the mean function $m(x)=0$; our derivations can easily be generalized later on. Suppose we want to make predictions at a set of inputs $$X_* = x_{*1},x_{*2},\\dots,x_{*m}.$$ Then we want to find $x^2$ and $p(\\mathbf{f}_* | \\mathbf{y}, X)$. In the regression setting, we can conveniently find this distribution by using Gaussian identities, after finding the joint distribution over $\\mathbf{f}_* = f(X_*)$ and $\\mathbf{y}$. If we evaluate equation :eqref:`eq_gp-regression` at the training inputs $X$, we have $\\mathbf{y} = \\mathbf{f} + \\mathbf{\\epsilon}$."
    },
    {
      "chunk_id": "de008919b856_1",
      "chapter": "gp-inference",
      "heading": "Posterior Inference for Regression",
      "text": "If we evaluate equation :eqref:`eq_gp-regression` at the training inputs $X$, we have $\\mathbf{y} = \\mathbf{f} + \\mathbf{\\epsilon}$. By the definition of a Gaussian process (see last section), $\\mathbf{f} \\sim \\mathcal{N}(0,K(X,X))$ where $K(X,X)$ is an $n \\times n$ matrix formed by evaluating our covariance function (aka _kernel_) at all possible pairs of inputs $x_i, x_j \\in X$. $\\mathbf{\\epsilon}$ is simply a vector comprised of iid samples from $\\mathcal{N}(0,\\sigma^2)$ and thus has distribution $\\mathcal{N}(0,\\sigma^2I)$. $\\mathbf{y}$ is therefore a sum of two independent multivariate Gaussian variables, and thus has distribution $\\mathcal{N}(0, K(X,X) + \\sigma^2I)$. One can also show that $\\textrm{cov}(\\mathbf{f}_*, \\mathbf{y}) = \\textrm{cov}(\\mathbf{y},\\mathbf{f}_*)^{\\top} = K(X_*,X)$ where $K(X_*,X)$ is an $m \\times n$ matrix formed by evaluating the kernel at all pairs of test and training inputs. $$\n\\begin{bmatrix}\n\\mathbf{y} \\\\\n\\mathbf{f}_*\n\\end{bmatrix}\n\\sim\n\\mathcal{N}\\left(0, \n\\mathbf{A} = \\begin{bmatrix}\nK(X,X)+\\sigma^2I & K(X,X_*) \\\\\nK(X_*,X) & K(X_*,X_*)\n\\end{bmatrix}\n\\right)\n$$\n\nWe can then use standard Gaussian identities to find the conditional distribution from the joint distribution (see, e.g., Bishop Chapter 2), \n$\\mathbf{f}_* | \\mathbf{y}, X, X_* \\sim \\mathcal{N}(m_*,S_*)$, where $m_* = K(X_*,X)[K(X,X)+\\sigma^2I]^{-1}\\textbf{y}$, and $S = K(X_*,X_*) - K(X_*,X)[K(X,X)+\\sigma^2I]^{-1}K(X,X_*)$. Typically, we do not need to make use of the full predictive covariance matrix $S$, and instead use the diagonal of $S$ for uncertainty about each prediction. Often for this reason we write the predictive distribution for a single test point $x_*$, rather than a collection of test points. The kernel matrix has parameters $\\theta$ that we also wish to estimate, such the amplitude $a$ and lengthscale $\\ell$ of the RBF kernel above."
    },
    {
      "chunk_id": "de008919b856_2",
      "chapter": "gp-inference",
      "heading": "Posterior Inference for Regression",
      "text": "The kernel matrix has parameters $\\theta$ that we also wish to estimate, such the amplitude $a$ and lengthscale $\\ell$ of the RBF kernel above. For these purposes we use the _marginal likelihood_, $p(\\textbf{y} | \\theta, X)$, which we already derived in working out the marginal distributions to find the joint distribution over $\\textbf{y},\\textbf{f}_*$. As we will see, the marginal likelihood compartmentalizes into model fit and model complexity terms, and automatically encodes a notion of Occam's razor for learning hyperparameters. For a full discussion, see MacKay Ch. 28 :cite:`mackay2003information`, and Rasmussen and Williams Ch. 5 :cite:`rasmussen2006gaussian`. ```{.python .input}\nfrom d2l import torch as d2l\nimport numpy as np\nfrom scipy.spatial import distance_matrix\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\nimport math\nimport torch\nimport gpytorch\nimport os\n\nd2l.set_figsize()\n```"
    },
    {
      "chunk_id": "0fbb3be949e6_0",
      "chapter": "gp-inference",
      "heading": "Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression",
      "text": "We list here the equations you will use for learning hyperparameters and making predictions in Gaussian process regression. Again, we assume a vector of regression targets $\\textbf{y}$, indexed by inputs $X = \\{x_1,\\dots,x_n\\}$, and we wish to make a prediction at a test input $x_*$. We assume i.i.d. additive zero-mean Gaussian noise with variance $\\sigma^2$. We use a Gaussian process prior $f(x) \\sim \\mathcal{GP}(m,k)$ for the latent noise-free function, with mean function $m$ and kernel function $k$. The kernel itself has parameters $\\theta$ that we want to learn. For example, if we use an RBF kernel, $k(x_i,x_j) = a^2\\exp\\left(-\\frac{1}{2\\ell^2}||x-x'||^2\\right)$, we want to learn $\\theta = \\{a^2, \\ell^2\\}$. Let $K(X,X)$ represent an $n \\times n$ matrix corresponding to evaluating the kernel for all possible pairs of $n$ training inputs. Let $K(x_*,X)$ represent a $1 \\times n$ vector formed by evaluating $k(x_*, x_i)$, $i=1,\\dots,n$. Let $\\mu$ be a mean vector formed by evaluating the mean function $m(x)$ at every training points $x$.\n\nTypically in working with Gaussian processes, we follow a two-step procedure. \n1. Learn kernel hyperparameters $\\hat{\\theta}$ by maximizing the marginal likelihood with respect to these hyperparameters.\n2. Use the predictive mean as a point predictor, and 2 times the predictive standard deviation to form a 95\\% credible set, conditioning on these learned hyperparameters $\\hat{\\theta}$.\n\nThe log marginal likelihood is simply a log Gaussian density, which has the form:\n$$\\log p(\\textbf{y} | \\theta, X) = -\\frac{1}{2}\\textbf{y}^{\\top}[K_{\\theta}(X,X) + \\sigma^2I]^{-1}\\textbf{y} - \\frac{1}{2}\\log|K_{\\theta}(X,X)| + c$$\n\nThe predictive distribution has the form:\n$$p(y_* | x_*, \\textbf{y}, \\theta) = \\mathcal{N}(a_*,v_*)$$\n$$a_* = k_{\\theta}(x_*,X)[K_{\\theta}(X,X)+\\sigma^2I]^{-1}(\\textbf{y}-\\mu) + \\mu$$\n$$v_* = k_{\\theta}(x_*,x_*) - K_{\\theta}(x_*,X)[K_{\\theta}(X,X)+\\sigma^2I]^{-1}k_{\\theta}(X,x_*)$$"
    },
    {
      "chunk_id": "62a8308a7e52_0",
      "chapter": "gp-inference",
      "heading": "Interpreting Equations for Learning and Predictions",
      "text": "There are some key points to note about the predictive distributions for Gaussian processes:\n\n* Despite the flexibility of the model class, it is possible to do _exact_ Bayesian inference for GP regression in _closed form_. Aside from learning the kernel hyperparameters, there is no _training_. We can write down exactly what equations we want to use to make predictions. Gaussian processes are relatively exceptional in this respect, and it has greatly contributed to their convenience, versatility, and continued popularity. * The predictive mean $a_*$ is a linear combination of the training targets $\\textbf{y}$, weighted by the kernel $k_{\\theta}(x_*,X)[K_{\\theta}(X,X)+\\sigma^2I]^{-1}$. As we will see, the kernel (and its hyperparameters) thus plays a crucial role in the generalization properties of the model. * The predictive mean explicitly depends on the target values $\\textbf{y}$ but the predictive variance does not. The predictive uncertainty instead grows as the test input $x_*$ moves away from the target locations $X$, as governed by the kernel function. However, uncertainty will implicitly depend on the values of the targets $\\textbf{y}$ through the kernel hyperparameters $\\theta$, which are learned from the data. * The marginal likelihood compartmentalizes into model fit and model complexity (log determinant) terms. The marginal likelihood tends to select for hyperparameters that provide the simplest fits that are still consistent with the data. * The key computational bottlenecks come from solving a linear system and computing a log determinant over an $n \\times n$ symmetric positive definite matrix $K(X,X)$ for $n$ training points. Naively, these operations each incur $\\mathcal{O}(n^3)$ computations, as well as $\\mathcal{O}(n^2)$ storage for each entry of the kernel (covariance) matrix, often starting with a Cholesky decomposition."
    },
    {
      "chunk_id": "62a8308a7e52_1",
      "chapter": "gp-inference",
      "heading": "Interpreting Equations for Learning and Predictions",
      "text": "Naively, these operations each incur $\\mathcal{O}(n^3)$ computations, as well as $\\mathcal{O}(n^2)$ storage for each entry of the kernel (covariance) matrix, often starting with a Cholesky decomposition. Historically, these bottlenecks have limited GPs to problems with fewer than about 10,000 training points, and have given GPs a reputation for \"being slow\" that has been inaccurate now for almost a decade. In advanced topics, we will discuss how GPs can be scaled to problems with millions of points. * For popular choices of kernel functions, $K(X,X)$ is often close to singular, which can cause numerical issues when performing Cholesky decompositions or other operations intended to solve linear systems. Fortunately, in regression we are often working with $K_{\\theta}(X,X)+\\sigma^2I$, such that the noise variance $\\sigma^2$ gets added to the diagonal of $K(X,X)$, significantly improving its conditioning. If the noise variance is small, or we are doing noise free regression, it is common practice to add a small amount of \"jitter\" to the diagonal, on the order of $10^{-6}$, to improve conditioning."
    },
    {
      "chunk_id": "e26d9a5711aa_0",
      "chapter": "gp-inference",
      "heading": "Worked Example from Scratch",
      "text": "Let's create some regression data, and then fit the data with a GP, implementing every step from scratch. We'll sample data from \n$$y(x) = \\sin(x) + \\frac{1}{2}\\sin(4x) + \\epsilon,$$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$. The noise free function we wish to find is $f(x) = \\sin(x) + \\frac{1}{2}\\sin(4x)$. We'll start by using a noise standard deviation $\\sigma = 0.25$. ```{.python .input}\ndef data_maker1(x, sig):\n    return np.sin(x) + 0.5 * np.sin(4 * x) + np.random.randn(x.shape[0]) * sig\n\nsig = 0.25\ntrain_x, test_x = np.linspace(0, 5, 50), np.linspace(0, 5, 500)\ntrain_y, test_y = data_maker1(train_x, sig=sig), data_maker1(test_x, sig=0.)\n\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y)\nd2l.plt.xlabel(\"x\", fontsize=20)\nd2l.plt.ylabel(\"Observations y\", fontsize=20)\nd2l.plt.show()\n```\n\nHere we see the noisy observations as circles, and the noise-free function in blue that we wish to find. Now, let's specify a GP prior over the latent noise-free function, $f(x)\\sim \\mathcal{GP}(m,k)$. We'll use a mean function $m(x) = 0$, and an RBF covariance function (kernel)\n$$k(x_i,x_j) = a^2\\exp\\left(-\\frac{1}{2\\ell^2}||x-x'||^2\\right).$$\n\n```{.python .input}\nmean = np.zeros(test_x.shape[0])\ncov = d2l.rbfkernel(test_x, test_x, ls=0.2)\n```\n\nWe have started with a length-scale of 0.2. Before we fit the data, it is important to consider whether we have specified a reasonable prior. Let's visualize some sample functions from this prior, as well as the 95\\% credible set (we believe there's a 95\\% chance that the true function is within this region). ```{.python .input}\nprior_samples = np.random.multivariate_normal(mean=mean, cov=cov, size=5)\nd2l.plt.plot(test_x, prior_samples.T, color='black', alpha=0.5)\nd2l.plt.plot(test_x, mean, linewidth=2.)\nd2l.plt.fill_between(test_x, mean - 2 * np.diag(cov), mean + 2 * np.diag(cov), \n                 alpha=0.25)\nd2l.plt.show()\n```\n\nDo these samples look reasonable? Are the high-level properties of the functions aligned with the type of data we are trying to model?"
    },
    {
      "chunk_id": "e26d9a5711aa_1",
      "chapter": "gp-inference",
      "heading": "Worked Example from Scratch",
      "text": "Are the high-level properties of the functions aligned with the type of data we are trying to model? Now let's form the mean and variance of the posterior predictive distribution at any arbitrary test point $x_*$. $$\n\\bar{f}_{*} = K(x, x_*)^T (K(x, x) + \\sigma^2 I)^{-1}y\n$$\n\n$$\nV(f_{*}) = K(x_*, x_*) - K(x, x_*)^T (K(x, x) + \\sigma^2 I)^{-1}K(x, x_*)\n$$\n\nBefore we make predictions, we should learn our kernel hyperparameters $\\theta$ and noise variance $\\sigma^2$. Let's initialize our length-scale at 0.75, as our prior functions looked too quickly varying compared to the data we are fitting. We'll also guess a noise standard deviation $\\sigma$ of 0.75. In order to learn these parameters, we will maximize the marginal likelihood with respect to these parameters. $$\n\\log p(y | X) = \\log \\int p(y | f, X)p(f | X)df\n$$\n$$\n\\log p(y | X) = -\\frac{1}{2}y^T(K(x, x) + \\sigma^2 I)^{-1}y - \\frac{1}{2}\\log |K(x, x) + \\sigma^2 I| - \\frac{n}{2}\\log 2\\pi\n$$\n\n\nPerhaps our prior functions were too quickly varying. Let's guess a length-scale of 0.4. We'll also guess a noise standard deviation of 0.75. These are simply hyperparameter initializations --- we will learn these parameters from the marginal likelihood. ```{.python .input}\nell_est = 0.4\npost_sig_est = 0.5\n\ndef neg_MLL(pars):\n    K = d2l.rbfkernel(train_x, train_x, ls=pars[0])\n    kernel_term = -0.5 * train_y @ \\\n        np.linalg.inv(K + pars[1] ** 2 * np.eye(train_x.shape[0])) @ train_y\n    logdet = -0.5 * np.log(np.linalg.det(K + pars[1] ** 2 * \\\n                                         np.eye(train_x.shape[0])))\n    const = -train_x.shape[0] / 2. * np.log(2 * np.pi)\n    \n    return -(kernel_term + logdet + const)\n\n\nlearned_hypers = optimize.minimize(neg_MLL, x0=np.array([ell_est,post_sig_est]), \n                                   bounds=((0.01, 10.), (0.01, 10.)))\nell = learned_hypers.x[0]\npost_sig_est = learned_hypers.x[1]\n```\n\nIn this instance, we learn a length-scale of 0.299, and a noise standard deviation of 0.24."
    },
    {
      "chunk_id": "e26d9a5711aa_2",
      "chapter": "gp-inference",
      "heading": "Worked Example from Scratch",
      "text": "Note that the learned noise is extremely close to the true noise, which helps indicate that our GP is a very well-specified to this problem. In general, it is crucial to put careful thought into selecting the kernel and initializing the hyperparameters. While marginal likelihood optimization can be relatively robust to initialization, it is not immune to poor initializations. Try running the above script with a variety of initializations and see what results you find. Now, let's make predictions with these learned hypers. ```{.python .input}\nK_x_xstar = d2l.rbfkernel(train_x, test_x, ls=ell)\nK_x_x = d2l.rbfkernel(train_x, train_x, ls=ell)\nK_xstar_xstar = d2l.rbfkernel(test_x, test_x, ls=ell)\n\npost_mean = K_x_xstar.T @ np.linalg.inv((K_x_x + \\\n                post_sig_est ** 2 * np.eye(train_x.shape[0]))) @ train_y\npost_cov = K_xstar_xstar - K_x_xstar.T @ np.linalg.inv((K_x_x + \\\n                post_sig_est ** 2 * np.eye(train_x.shape[0]))) @ K_x_xstar\n\nlw_bd = post_mean - 2 * np.sqrt(np.diag(post_cov))\nup_bd = post_mean + 2 * np.sqrt(np.diag(post_cov))\n\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth=2.)\nd2l.plt.plot(test_x, post_mean, linewidth=2.)\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)\nd2l.plt.legend(['Observed Data', 'True Function', 'Predictive Mean', '95% Set on True Func'])\nd2l.plt.show()\n```\n\nWe see the posterior mean in orange almost perfectly matches the true noise free function! Note that the 95\\% credible set we are showing is for the latent _noise free_ (true) function, and not the data points. We see that this credible set entirely contains the true function, and does not seem overly wide or narrow. We would not want nor expect it to contain the data points."
    },
    {
      "chunk_id": "e26d9a5711aa_3",
      "chapter": "gp-inference",
      "heading": "Worked Example from Scratch",
      "text": "We see that this credible set entirely contains the true function, and does not seem overly wide or narrow. We would not want nor expect it to contain the data points. If we wish to have a credible set for the observations, we should compute\n\n```{.python .input}\nlw_bd_observed = post_mean - 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)\nup_bd_observed = post_mean + 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)\n```\n\nThere are two sources of uncertainty, _epistemic_ uncertainty, representing _reducible_ uncertainty, and _aleatoric_ or _irreducible_ uncertainty. The _epistemic_ uncertainty here represents uncertainty about the true values of the noise free function. This uncertainty should grow as we move away from the data points, as away from the data there are a greater variety of function values consistent with our data. As we observe more and more data, our beliefs about the true function become more confident, and the epistemic uncertainty disappears. The _aleatoric_ uncertainty in this instance is the observation noise, since the data are given to us with this noise, and it cannot be reduced. The _epistemic_ uncertainty in the data is captured by variance of the latent noise free function np.diag(post\\_cov). The _aleatoric_ uncertainty is captured by the noise variance post_sig_est**2. Unfortunately, people are often careless about how they represent uncertainty, with many papers showing error bars that are completely undefined, no clear sense of whether we are visualizing epistemic or aleatoric uncertainty or both, and confusing noise variances with noise standard deviations, standard deviations with standard errors, confidence intervals with credible sets, and so on. Without being precise about what the uncertainty represents, it is essentially meaningless. In the spirit of playing close attention to what our uncertainty represents, it is crucial to note that we are taking _two times_ the _square root_ of our variance estimate for the noise free function."
    },
    {
      "chunk_id": "e26d9a5711aa_4",
      "chapter": "gp-inference",
      "heading": "Worked Example from Scratch",
      "text": "In the spirit of playing close attention to what our uncertainty represents, it is crucial to note that we are taking _two times_ the _square root_ of our variance estimate for the noise free function. Since our predictive distribution is Gaussian, this quantity enables us to form a 95\\% credible set, representing our beliefs about the interval which is 95\\% likely to contain the ground truth function. The noise _variance_ is living on a completely different scale, and is much less interpretable. Finally, let's take a look at 20 posterior samples. These samples tell us what types of functions we believe might fit our data, a posteriori. ```{.python .input}\npost_samples = np.random.multivariate_normal(post_mean, post_cov, size=20)\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth=2.)\nd2l.plt.plot(test_x, post_mean, linewidth=2.)\nd2l.plt.plot(test_x, post_samples.T, color='gray', alpha=0.25)\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)\nplt.legend(['Observed Data', 'True Function', 'Predictive Mean', 'Posterior Samples'])\nd2l.plt.show()\n```\n\nIn basic regression applications, it is most common to use the posterior predictive mean and standard deviation as a point predictor and metric for uncertainty, respectively. In more advanced applications, such as Bayesian optimization with Monte Carlo acquisition functions, or Gaussian processes for model-based RL, it often necessary to take posterior samples. However, even if not strictly required in the basic applications, these samples give us more intuition about the fit we have for the data, and are often useful to include in visualizations."
    },
    {
      "chunk_id": "e55321bbff2e_0",
      "chapter": "gp-inference",
      "heading": "Making Life Easy with GPyTorch",
      "text": "As we have seen, it is actually pretty easy to implement basic Gaussian process regression entirely from scratch. However, as soon as we want to explore a variety of kernel choices, consider approximate inference (which is needed even for classification), combine GPs with neural networks, or even have a dataset larger than about 10,000 points, then an implementation from scratch becomes unwieldy and cumbersome. Some of the most effective methods for scalable GP inference, such as SKI (also known as KISS-GP), can require hundreds of lines of code implementing advanced numerical linear algebra routines. In these cases, the _GPyTorch_ library will make our lives a lot easier. We'll be discussing GPyTorch more in future notebooks on Gaussian process numerics, and advanced methods. The GPyTorch library contains [many examples](https://github.com/cornellius-gp/gpytorch/tree/master/examples). To get a feel for the package, we will walk through the [simple regression example](https://github.com/cornellius-gp/gpytorch/blob/master/examples/01_Exact_GPs/Simple_GP_Regression.ipynb), showing how it can be adapted to reproduce our above results using GPyTorch. This may seem like a lot of code to simply reproduce the basic regression above, and in a sense, it is. But we can immediately use a variety of kernels, scalable inference techniques, and approximate inference, by only changing a few lines of code from below, instead of writing potentially thousands of lines of new code."
    },
    {
      "chunk_id": "e55321bbff2e_1",
      "chapter": "gp-inference",
      "heading": "Making Life Easy with GPyTorch",
      "text": "But we can immediately use a variety of kernels, scalable inference techniques, and approximate inference, by only changing a few lines of code from below, instead of writing potentially thousands of lines of new code. ```{.python .input}\n# First let's convert our data into tensors for use with PyTorch\ntrain_x = torch.tensor(train_x)\ntrain_y = torch.tensor(train_y)\ntest_y = torch.tensor(test_y)\n\n# We are using exact GP inference with a zero mean and RBF kernel\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel())\n    \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n```\n\nThis code block puts the data in the right format for GPyTorch, and specifies that we are using exact inference, as well\nthe mean function (zero) and kernel function (RBF) that we want to use. We can use any other kernel very easily, by \ncalling, for instance, gpytorch.kernels.matern_kernel(), or gpyotrch.kernels.spectral_mixture_kernel(). So far, we have\nonly discussed exact inference, where it is possible to infer a predictive distribution without making any approximations. For Gaussian processes, we can only perform exact inference when we have a Gaussian likelihood; more specifically, when we\nassume that our observations are generated as a noise-free function represented by a Gaussian process, plus Gaussian noise. In future notebooks, we will consider other settings, such as classification, where we cannot make these assumptions."
    },
    {
      "chunk_id": "e55321bbff2e_2",
      "chapter": "gp-inference",
      "heading": "Making Life Easy with GPyTorch",
      "text": "In future notebooks, we will consider other settings, such as classification, where we cannot make these assumptions. ```{.python .input}\n# Initialize Gaussian likelihood\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ExactGPModel(train_x, train_y, likelihood)\ntraining_iter = 50\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n# Use the adam optimizer, includes GaussianLikelihood parameters\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)  \n# Set our loss as the negative log GP marginal likelihood\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n```\n\nHere, we explicitly specify the likelihood we want to use (Gaussian), the objective we will use for training kernel hyperparameters (here, the marginal likelihood), and the procedure we we want to use for optimizing that objective (in this case, Adam). We note that while we are using Adam, which is a \"stochastic\" optimizer, in this case, it is full-batch Adam. Because the marginal likelihood does not factorize over data instances, we cannot use an optimizer over \"mini-batches\" of data and be guaranteed convergence. Other optimizers, such as L-BFGS, are also supported by GPyTorch. Unlike in standard deep learning, doing a good job of optimizing the marginal likelihood corresponds strongly with good generalization, which often inclines us towards powerful optimizers like L-BFGS, assuming they are not prohibitively expensive."
    },
    {
      "chunk_id": "e55321bbff2e_3",
      "chapter": "gp-inference",
      "heading": "Making Life Easy with GPyTorch",
      "text": "Unlike in standard deep learning, doing a good job of optimizing the marginal likelihood corresponds strongly with good generalization, which often inclines us towards powerful optimizers like L-BFGS, assuming they are not prohibitively expensive. ```{.python .input}\nfor i in range(training_iter):\n    # Zero gradients from previous iteration\n    optimizer.zero_grad()\n    # Output from model\n    output = model(train_x)\n    # Calc loss and backprop gradients\n    loss = -mll(output, train_y)\n    loss.backward()\n    if i % 10 == 0:\n        print(f'Iter {i+1:d}/{training_iter:d} - Loss: {loss.item():.3f} '\n              f'squared lengthscale: '\n              f'{model.covar_module.base_kernel.lengthscale.item():.3f} '\n              f'noise variance: {model.likelihood.noise.item():.3f}')\n    optimizer.step()\n```\n\nHere we actually run the optimization procedure, outputting the values of the loss every 10 iterations. ```{.python .input}\n# Get into evaluation (predictive posterior) mode\ntest_x = torch.tensor(test_x)\nmodel.eval()\nlikelihood.eval()\nobserved_pred = likelihood(model(test_x)) \n```\n\nThe above codeblock enables us to make predictions on our test inputs. ```{.python .input}\nwith torch.no_grad():\n    # Initialize plot\n    f, ax = d2l.plt.subplots(1, 1, figsize=(4, 3))\n    # Get upper and lower bounds for 95\\% credible set (in this case, in\n    # observation space)\n    lower, upper = observed_pred.confidence_region()\n    ax.scatter(train_x.numpy(), train_y.numpy())\n    ax.plot(test_x.numpy(), test_y.numpy(), linewidth=2.)\n    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), linewidth=2.)\n    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.25)\n    ax.set_ylim([-1.5, 1.5])\n    ax.legend(['True Function', 'Predictive Mean', 'Observed Data',\n               '95% Credible Set'])\n```\n\nFinally, we plot the fit. We see the fits are virtually identical. A few things to note: GPyTorch is working with _squared_ length-scales and observation noise."
    },
    {
      "chunk_id": "e55321bbff2e_4",
      "chapter": "gp-inference",
      "heading": "Making Life Easy with GPyTorch",
      "text": "We see the fits are virtually identical. A few things to note: GPyTorch is working with _squared_ length-scales and observation noise. For example, our learned noise standard deviation in the for scratch code is about 0.283. The noise variance found by GPyTorch is $0.81 \\approx 0.283^2$. In the GPyTorch plot, we also show the credible set in the _observation space_ rather than the latent function space, to demonstrate that they indeed cover the observed datapoints."
    },
    {
      "chunk_id": "00df4b8773ca_0",
      "chapter": "gp-inference",
      "heading": "Summary",
      "text": "We can combine a Gaussian process prior with data to form a posterior, which we use to make predictions. We can also form a marginal likelihood, which is useful for automatic learning of kernel hyperparameters, which control properties such as the rate of variation of the Gaussian process. The mechanics of forming the posterior and learning kernel hyperparameters for regression are simple, involving about a dozen lines of code. This notebook is a good reference for any reader wanting to quickly get \"up and running\" with Gaussian processes. We also introduced the GPyTorch library. Although the GPyTorch code for basic regression is relatively long, it can be trivially modified for other kernel functions, or more advanced functionality we will discuss in future notebooks, such as scalable inference, or non-Gaussian likelihoods for classification."
    },
    {
      "chunk_id": "94b2387925ff_0",
      "chapter": "gp-inference",
      "heading": "Exercises",
      "text": "1. We have emphasized the importance of _learning_ kernel hyperparameters, and the effect of hyperparameters and kernels on the generalization properties of Gaussian processes. Try skipping the step where we learn hypers, and instead guess a variety of length-scales and noise variances, and check their effect on predictions. What happens when you use a large length-scale? A small length-scale? A large noise variance? A small noise variance? 2. We have said that the marginal likelihood is not a convex objective, but that hyperparameters like length-scale and noise variance can be reliably estimated in GP regression. This is generally true --- in fact, the marginal likelihood is _much_ better at learning length-scale hyperparameters than conventional approaches in spatial statistics, which involve fitting empirical autocorrelation functions (\"covariograms\"). Arguably, the biggest contribution from machine learning to Gaussian process research, at least before recent work on scalable inference, was the introduction of the marginal lkelihood for hyperparameter learning. *However*, different pairings of even these parameters provide interpretably different plausible explanations for many datasets, leading to local optima in our objective. If we use a large length-scale, then we assume the true underlying function is slowly varying. If the observed data _are_ varying significantly, then the only we can plausibly have a large length-scale is with a large noise-variance. If we use a small length-scale, on the  other hand, our fit will be very sensitive to the variations in the data, leaving little room to explain variations with noise (aleatoric uncertainty). Try seeing if you can find these local optima: initialize with very large length-scale with large noise, and small length-scales with small noise. Do you converge to different solutions? 3. We have said that a fundamental advantage of Bayesian methods is in naturally representing _epistemic_ uncertainty."
    },
    {
      "chunk_id": "94b2387925ff_1",
      "chapter": "gp-inference",
      "heading": "Exercises",
      "text": "Do you converge to different solutions? 3. We have said that a fundamental advantage of Bayesian methods is in naturally representing _epistemic_ uncertainty. In the above example, we cannot fully see the effects of epistemic uncertainty. Try instead to predict with `test_x = np.linspace(0, 10, 1000)`. What happens to the 95\\% credible set as your predictions move beyond the data? Does it cover the true function in that interval? What happens if you only visualize aleatoric uncertainty in that region? 4. Try running the above example, but instead with 10,000, 20,000 and 40,000 training points, and measure the runtimes. How does the training time scale? Alternatively, how do the runtimes scale with the number of test points? Is it different for the predictive mean and the predictive variance? Answer this question both by theoretically working out the training and testing time complexities, and by running the code above with a different number of points. 5. Try running the GPyTorch example with different covariance functions, such as the Matern kernel. How do the results change? How about the spectral mixture kernel, found in the GPyTorch library? Are some easier to train the marginal likelihood than others? Are some more valuable for long-range versus short-range predictions? 6. In our GPyTorch example, we plotted the predictive distribution including observation noise, while in our \"from scratch\" example, we only included epistemic uncertainty. Re-do the GPyTorch example, but this time only plotting epistemic uncertainty, and compare to the from-scratch results. Do the predictive distributions now look the same? (They should.)\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12117)\n:end_tab:"
    },
    {
      "chunk_id": "f690c26e0024_0",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "# Introduction to Gaussian Processes\n\nIn many cases, machine learning amounts to estimating parameters from data. These parameters are often numerous and relatively uninterpretable --- such as the weights of a neural network. Gaussian processes, by contrast, provide a mechanism for directly reasoning about the high-level properties of functions that could fit our data. For example, we may have a sense of whether these functions are quickly varying, periodic, involve conditional independencies, or translation invariance. Gaussian processes enable us to easily incorporate these properties into our model, by directly specifying a Gaussian distribution over the function values that could fit our data. Let's get a feel for how Gaussian processes operate, by starting with some examples. Suppose we observe the following dataset, of regression targets (outputs), $y$, indexed by inputs, $x$. As an example, the targets could be changes in carbon dioxide concentrations, and the inputs could be the times at which these targets have been recorded. What are some features of the data? How quickly does it seem to varying? Do we have data points collected at regular intervals, or are there missing inputs? How would you imagine filling in the missing regions, or forecasting up until $x=25$? ![Observed data.](../img/gp-observed-data.svg)\n\nIn order to fit the data with a Gaussian process, we start by specifying a prior distribution over what types of functions we might believe to be reasonable. Here we show several sample functions from a Gaussian process. Does this prior look reasonable? Note here we are not looking for functions that fit our dataset, but instead for specifying reasonable high-level properties of the solutions, such as how quickly they vary with inputs. Note that we will see code for reproducing all of the plots in this notebook, in the next notebooks on priors and inference."
    },
    {
      "chunk_id": "f690c26e0024_1",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "Note that we will see code for reproducing all of the plots in this notebook, in the next notebooks on priors and inference. ![Sample prior functions that we may want to represent with our model.](../img/gp-sample-prior-functions.svg)\n\nOnce we condition on data, we can use this prior to infer a posterior distribution over functions that could fit the data. Here we show sample posterior functions. ![Sample posterior functions, once we have observed the data.](../img/gp-sample-posterior-functions.svg)\n\nWe see that each of these functions are entirely consistent with our data, perfectly running through each observation. In order to use these posterior samples to make predictions, we can average the values of every possible sample function from the posterior, to create the curve below, in thick blue. Note that we do not actually have to take an infinite number of samples to compute this expectation; as we will see later, we can compute the expectation in closed form. ![Posterior samples, alongside posterior mean, which can be used for point predictions, in blue.](../img/gp-posterior-samples.svg)\n\nWe may also want a representation of uncertainty, so we know how confident we should be in our predictions. Intuitively, we should have more uncertainty where there is more variability in the sample posterior functions, as this tells us there are many more possible values the true function could take. This type of uncertainty is called _epistemic uncertainty_, which is the _reducible uncertainty_ associated with lack of information. As we acquire more data, this type of uncertainty disappears, as there will be increasingly fewer solutions consistent with what we observe. Like with the posterior mean, we can compute the posterior variance (the variability of these functions in the posterior) in closed form. With shade, we show two times the posterior standard deviation on either side of the mean, creating a _credible interval_ that has a 95% probability of containing the true value of the function for any input $x$."
    },
    {
      "chunk_id": "f690c26e0024_2",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "With shade, we show two times the posterior standard deviation on either side of the mean, creating a _credible interval_ that has a 95% probability of containing the true value of the function for any input $x$. ![Posterior samples, including 95% credible set.](../img/gp-posterior-samples-95.svg)\n\nThe plot looks somewhat cleaner if we remove the posterior samples, simply visualizing the data, posterior mean, and 95% credible set. Notice how the uncertainty grows away from the data, a property of epistemic uncertainty. ![Point predictions, and credible set.](../img/gp-point-predictions.svg)\n\nThe properties of the Gaussian process that we used to fit the data are strongly controlled by what's called a _covariance function_, also known as a _kernel_. The covariance function we used is called the _RBF (Radial Basis Function) kernel_, which has the form\n$$ k_{\\textrm{RBF}}(x,x') = \\textrm{Cov}(f(x),f(x')) = a^2 \\exp\\left(-\\frac{1}{2\\ell^2}||x-x'||^2\\right) $$\n\nThe _hyperparameters_ of this kernel are interpretable. The _amplitude_ parameter $a$ controls the vertical scale over which the function is varying, and the _length-scale_ parameter\n$\\ell$\ncontrols the rate of variation (the wiggliness) of the function. Larger $a$ means larger function values, and larger \n$\\ell$ \nmeans more slowly varying functions. Let's see what happens to our sample prior and posterior functions as we vary $a$ and \n$\\ell$. The _length-scale_ has a particularly pronounced effect on the predictions and uncertainty of a GP. At \n$||x-x'|| = \\ell$\n, the covariance between a pair of function values is $a^2\\exp(-0.5)$. At larger distances than \n$\\ell$\n, the values of the function values becomes nearly uncorrelated. This means that if we want to make a prediction at a point $x_*$, then function values with inputs $x$ such that \n$||x-x'||>\\ell$\nwill not have a strong effect on our predictions. Let's see how changing the lengthscale affects sample prior and posterior functions, and credible sets. The above fits use a length-scale of $2$."
    },
    {
      "chunk_id": "f690c26e0024_3",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "Let's see how changing the lengthscale affects sample prior and posterior functions, and credible sets. The above fits use a length-scale of $2$. Let's now consider \n$\\ell = 0.1, 0.5, 2, 5, 10$\n. A length-scale of $0.1$ is very small relative to the range of the input domain we are considering, $25$. For example, the values of the function at $x=5$ and $x=10$ will have essentially no correlation at such a length-scale. On the other hand, for a length-scale of $10$, the function values at these inputs will be highly correlated. Note that the vertical scale changes in the following figures. ![priorpoint1](../img/gp-priorpoint1.svg)\n![postpoint1](../img/gp-postpoint1.svg)\n\n![priorpoint5](../img/gp-priorpoint5.svg)\n![postpoint5](../img/gp-postpoint5.svg)\n\n![prior2](../img/gp-prior2.svg)\n![post2](../img/gp-post2.svg)\n\n![prior5](../img/gp-prior5.svg)\n![post5](../img/gp-post5.svg)\n\nNotice as the length-scale increases the 'wiggliness' of the functions decrease, and our uncertainty decreases. If the length-scale is small, the uncertainty will quickly increase as we move away from the data, as the datapoints become less informative about the function values. Now, let's vary the amplitude parameter, holding the length-scale fixed at $2$. Note the vertical scale is held fixed for the prior samples, and varies for the posterior samples, so you can clearly see both the increasing scale of the function, and the fits to the data. ![priorap1](../img/gp-priorap1.svg)\n![postapoint1](../img/gp-postapoint1.svg)\n\n![priora2](../img/gp-priora2.svg)\n![posta2](../img/gp-posta2.svg)\n\n![priora8](../img/gp-priora8.svg)\n![posta8](../img/gp-posta8.svg)\n\nWe see the amplitude parameter affects the scale of the function, but not the rate of variation. At this point, we also have the sense that the generalization performance of our procedure will depend on having reasonable values for these hyperparameters. Values of $\\ell=2$ and $a=1$ appeared to provide reasonable fits, while some of the other values did not."
    },
    {
      "chunk_id": "f690c26e0024_4",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "Values of $\\ell=2$ and $a=1$ appeared to provide reasonable fits, while some of the other values did not. Fortunately, there is a robust and automatic way to specify these hyperparameters, using what is called the _marginal likelihood_, which we will return to in the notebook on inference. So what is a GP, really? As we started, a GP simply says that any collection of function values \n$f(x_1),\\dots,f(x_n)$, \nindexed by any collection of inputs \n$x_1,\\dots,x_n$ \nhas a joint multivariate Gaussian distribution. The mean vector $\\mu$ of this distribution is given by a _mean function_, which is typically taken to be a constant or zero. The covariance matrix of this distribution is given by the _kernel_ evaluated at all pairs of the inputs $x$. $$\\begin{bmatrix}f(x) \\\\f(x_1) \\\\ \\vdots \\\\ f(x_n) \\end{bmatrix}\\sim \\mathcal{N}\\left(\\mu, \\begin{bmatrix}k(x,x) & k(x, x_1) & \\dots & k(x,x_n) \\\\ k(x_1,x) & k(x_1,x_1) & \\dots & k(x_1,x_n) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ k(x_n, x) & k(x_n, x_1) & \\dots & k(x_n,x_n) \\end{bmatrix}\\right)$$\n:eqlabel:`eq_gp_prior`\n\nEquation :eqref:`eq_gp_prior` specifies a GP prior. We can compute the conditional distribution of $f(x)$ for any $x$ given $f(x_1), \\dots, f(x_n)$, the function values we have observed. This conditional distribution is called the _posterior_, and it is what we use to make predictions. In particular, \n\n$$f(x) | f(x_1), \\dots, f(x_n) \\sim \\mathcal{N}(m,s^2)$$ \n\nwhere\n\n$$m = k(x,x_{1:n}) k(x_{1:n},x_{1:n})^{-1} f(x_{1:n})$$ \n\n$$s^2 = k(x,x) - k(x,x_{1:n})k(x_{1:n},x_{1:n})^{-1}k(x,x_{1:n})$$ \n\nwhere $k(x,x_{1:n})$ is a $1 \\times n$ vector formed by evaluating $k(x,x_{i})$ for $i=1,\\dots,n$ and $k(x_{1:n},x_{1:n})$ is an $n \\times n$ matrix formed by evaluating $k(x_i,x_j)$ for $i,j = 1,\\dots,n$. $m$ is what we can use as a point predictor for any $x$, and $s^2$ is what we use for uncertainty: if we want to create an interval with a 95% probability that $f(x)$ is in the interval, we would use $m \\pm 2s$."
    },
    {
      "chunk_id": "f690c26e0024_5",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "$m$ is what we can use as a point predictor for any $x$, and $s^2$ is what we use for uncertainty: if we want to create an interval with a 95% probability that $f(x)$ is in the interval, we would use $m \\pm 2s$. The predictive means and uncertainties for all the above figures were created using these equations. The observed data points were given by \n$f(x_1), \\dots, f(x_n)$\nand chose a fine grained set of $x$ points to make predictions. Let's suppose we observe a single datapoint, $f(x_1)$, and we want to determine the value of $f(x)$ at some $x$. Because $f(x)$ is described by a Gaussian process, we know the joint distribution over \n$(f(x), f(x_1))$ \nis Gaussian: \n\n$$\n\\begin{bmatrix}\nf(x) \\\\ \nf(x_1) \\\\\n\\end{bmatrix}\n\\sim\n\\mathcal{N}\\left(\\mu, \n\\begin{bmatrix}\nk(x,x) & k(x, x_1) \\\\\nk(x_1,x) & k(x_1,x_1)\n\\end{bmatrix}\n\\right)\n$$\n\nThe off-diagonal expression $k(x,x_1) = k(x_1,x)$ \ntells us how correlated the function values will be --- how strongly determined $f(x)$\nwill be from $f(x_1)$. We have seen already that if we use a large length-scale, relative to the distance between $x$ and $x_1$, \n$||x-x_1||$, then the function values will be highly correlated. We can visualize the process of determining $f(x)$ from $f(x_1)$ both in the space of functions, and in the joint distribution over $f(x_1), f(x)$. Let's initially consider an $x$ such that $k(x,x_1) = 0.9$, and $k(x,x)=1$, meaning that the value of $f(x)$ is moderately correlated with the value of $f(x_1)$. In the joint distribution, the contours of constant probability will be relatively narrow ellipses. Suppose we observe $f(x_1) = 1.2$. To condition on this value of $f(x_1)$, \nwe can draw a horizontal line at $1.2$ on our plot of the density, and see that the value of $f(x)$ \nis mostly constrained to $[0.64,1.52]$. We have also drawn this plot in function space, showing the observed\npoint $f(x_1)$ in orange, and 1 standard deviation of the Gaussian process predictive distribution for $f(x)$ \nin blue, about the mean value of $1.08$."
    },
    {
      "chunk_id": "f690c26e0024_6",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "We have also drawn this plot in function space, showing the observed\npoint $f(x_1)$ in orange, and 1 standard deviation of the Gaussian process predictive distribution for $f(x)$ \nin blue, about the mean value of $1.08$. ![Contours of constant probability of a bivariate Gaussian density over $f(x_1)$ and $f(x)$ with $k(x,x_1) = 0.9$.](https://user-images.githubusercontent.com/6753639/206867364-b4707db5-0c2e-4ae4-a412-8292bca4d08d.svg)\n![Gaussian process predictive distribution in function space at $f(x)$, with $k(x,x_1) = 0.9$.](https://user-images.githubusercontent.com/6753639/206867367-3815720c-93c8-4b4b-80e7-296db1d3553b.svg)\n\nNow suppose we have a stronger correlation, $k(x,x_1) = 0.95$. Now the ellipses have narrowed further, and the value of $f(x)$ \nis even more strongly determined by $f(x_1)$. Drawing a horizontal line at $1.2$, we see the contours for $f(x)$\nsupport values mostly within $[0.83, 1.45]$. Again, we also show the plot in function space, with one standard \ndeviation about the mean predictive value of $1.14$. ![Contours of constant probability of a bivariate Gaussian density over $f(x_1)$ and $f(x)$ with $k(x,x_1) = 0.95$.](https://user-images.githubusercontent.com/6753639/206867797-20e42783-31de-4c50-8103-e9441ba6d0a9.svg)\n![Gaussian process predictive distribution in function space at $f(x)$, with $k(x,x_1)$ = 0.95.](https://user-images.githubusercontent.com/6753639/206867800-d9fc7add-649d-492c-8848-cab07c8fb83e.svg)\n\nWe see that the posterior mean predictor of our Gaussian process is closer to $1.2$, because there is now a stronger correlation. We also see that our uncertainty (the error bars) have somewhat decreased. Despite the strong correlation between these function values, our uncertainty is still righly quite large, because we have only observed a single data point! This procedure can give us a posterior on $f(x)$ for any $x$, for any number of points we have observed. Suppose we observe $f(x_1), f(x_2)$."
    },
    {
      "chunk_id": "f690c26e0024_7",
      "chapter": "gp-intro",
      "heading": "gp-intro",
      "text": "This procedure can give us a posterior on $f(x)$ for any $x$, for any number of points we have observed. Suppose we observe $f(x_1), f(x_2)$. We now visualize the posterior for $f(x)$ at a particular $x=x'$ in function space. The exact distribution for $f(x)$ is given by the above equations. $f(x)$ is Gaussian distributed, with mean \n\n$$m = k(x,x_{1:3}) k(x_{1:3},x_{1:3})^{-1} f(x_{1:3})$$\n\nand variance \n\n$$s^2 = k(x,x) - k(x,x_{1:3})k(x_{1:3},x_{1:3})^{-1}k(x,x_{1:3})$$\n\nIn this introductory notebook, we have been considering _noise free_ observations. As we will see, it is easy to include observation noise. If we assume that the data are generated from a latent noise free function $f(x)$ plus iid Gaussian noise \n$\\epsilon(x) \\sim \\mathcal{N}(0,\\sigma^2)$\nwith variance $\\sigma^2$, then our covariance function simply becomes \n$k(x_i,x_j) \\to k(x_i,x_j) + \\delta_{ij}\\sigma^2$,\nwhere $\\delta_{ij} = 1$ if $i=j$ and $0$ otherwise. We have already started getting some intuition about how we can use a Gaussian process to specify a prior and posterior over solutions, and how the kernel function affects the properties of these solutions. In the following notebooks, we will precisely show how to specify a Gaussian process prior, introduce and derive various kernel functions, and then go through the mechanics of how to automatically learn kernel hyperparameters, and form a Gaussian process posterior to make predictions. While it takes time and practice to get used to concepts such as a \"distributions over functions\", the actual mechanics of finding the GP predictive equations is actually quite simple --- making it easy to get practice to form an intuitive understanding of these concepts."
    },
    {
      "chunk_id": "4867a54eed07_0",
      "chapter": "gp-intro",
      "heading": "Summary",
      "text": "In typical machine learning, we specify a function with some free parameters (such as a neural network and its weights), and we focus on estimating those parameters, which may not be interpretable. With a Gaussian process, we instead reason about distributions over functions directly, which enables us to reason about the high-level properties of the solutions. These properties are controlled by a covariance function (kernel), which often has a few highly interpretable hyperparameters. These hyperparameters include the _length-scale_, which controls how rapidly (how wiggily) the functions are. Another hyperparameter is the amplitude, which controls the vertical scale over which our functions are varying. \nRepresenting many different functions that can fit the data, and combining them all together into a predictive distribution, is a distinctive feature of Bayesian methods. Because there is a greater amount of variability between possible solutions far away from the data, our uncertainty intuitively grows as we move from the data. \n\n\nA Gaussian process represents a distribution over functions by specifying a multivariate normal (Gaussian) distribution over all possible function values. It is possible to easily manipulate Gaussian distributions to find the distribution of one function value based on the values of any set of other values. In other words, if we observe a set of points, then we can condition on these points and infer a distribution over what the value of the function might look like at any other input. How we model the correlations between these points is determined by the covariance function and is what defines the generalization properties of the Gaussian process. While it takes time to get used to Gaussian processes, they are easy to work with, have many applications, and help us understand and develop other model classes, like neural networks."
    },
    {
      "chunk_id": "7ea9fe5507ee_0",
      "chapter": "gp-intro",
      "heading": "Exercises",
      "text": "1. What is the difference between epistemic uncertainty versus observation uncertainty?\n2. Besides rate of variation and amplitude, what other properties of functions might we want to consider, and what would be real-world examples of functions that have those properties?\n3. The RBF covariance function we considered says that covariances (and correlations) between observations decrease with their distance in the input space (times, spatial locations, etc.). Is this a reasonable assumption? Why or why not?\n4. Is a sum of two Gaussian variables Gaussian? Is a product of two Gaussian variables Gaussian? If (a,b) have a joint Gaussian distribution, is a|b (a given b) Gaussian? Is a Gaussian?\n5. Repeat the exercise where we observe a data point at $f(x_1) = 1.2$, but now suppose we additionally observe $f(x_2) = 1.4$. Let $k(x,x_1) = 0.9$, and $k(x,x_2) = 0.8$. Will we be more or less certain about the value of $f(x)$, than when we had only observed $f(x_1)$? What is the mean and 95\\% credible set for our value of $f(x)$ now? \n6. Do you think increasing our estimate of observation noise would increase or decrease our estimate of the length-scale of the ground truth function?\n7. As we move away from the data, suppose the uncertainty in our predictive distribution increases to a point, then stops increasing. Why might that happen?\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12115)\n:end_tab:"
    },
    {
      "chunk_id": "b44e1d18cc39_0",
      "chapter": "gp-priors",
      "heading": "gp-priors",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['pytorch'])\n```\n\n# Gaussian Process Priors\n\nUnderstanding Gaussian processes (GPs) is important for reasoning about model construction and generalization, and for achieving state-of-the-art performance in a variety of applications, including active learning, and hyperparameter tuning in deep learning. GPs are everywhere, and it is in our interests to know what they are and how we can use them.\n\nIn this section, we introduce Gaussian process _priors_ over functions. In the next notebook, we show how to use these priors to do _posterior inference_ and make predictions. The next section can be viewed as \"GPs in a nutshell\", quickly giving what you need to apply Gaussian processes in practice.\n\n```{.python .input}\nfrom d2l import torch as d2l\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\nd2l.set_figsize()\n```"
    },
    {
      "chunk_id": "ef6bb2c69b60_0",
      "chapter": "gp-priors",
      "heading": "Definition",
      "text": "A Gaussian process is defined as _a collection of random variables, any finite number of which have a joint Gaussian distribution_. If a function $f(x)$ is a Gaussian process, with _mean function_ $m(x)$ and _covariance function_ or _kernel_ $k(x,x')$, $f(x) \\sim \\mathcal{GP}(m, k)$, then any collection of function values queried at any collection of input points $x$ (times, spatial locations, image pixels, etc.), has a joint multivariate Gaussian distribution with mean vector $\\mu$ and covariance matrix $K$: $f(x_1),\\dots,f(x_n) \\sim \\mathcal{N}(\\mu, K)$, where $\\mu_i = E[f(x_i)] = m(x_i)$ and $K_{ij} = \\textrm{Cov}(f(x_i),f(x_j)) = k(x_i,x_j)$.\n\nThis definition may seem abstract and inaccessible, but Gaussian processes are in fact very simple objects. Any function\n\n$$f(x) = w^{\\top} \\phi(x) = \\langle w, \\phi(x) \\rangle,$$ :eqlabel:`eq_gp-function`\n\nwith $w$ drawn from a Gaussian (normal) distribution, and $\\phi$ being any vector of basis functions, for example $\\phi(x) = (1, x, x^2, ..., x^d)^{\\top}$,\nis a Gaussian process. Moreover, any Gaussian process f(x) can be expressed in the form of equation :eqref:`eq_gp-function`. Let's consider a few concrete examples, to begin getting acquainted with Gaussian processes, after which we can appreciate how simple and useful they really are."
    },
    {
      "chunk_id": "226ec74060c0_0",
      "chapter": "gp-priors",
      "heading": "A Simple Gaussian Process",
      "text": "Suppose $f(x) = w_0 + w_1 x$, and $w_0, w_1 \\sim \\mathcal{N}(0,1)$, with $w_0, w_1, x$ all in one dimension. We can equivalently write this function as the inner product $f(x) = (w_0, w_1)(1, x)^{\\top}$. In :eqref:`eq_gp-function` above, $w = (w_0, w_1)^{\\top}$ and $\\phi(x) = (1,x)^{\\top}$.\n\nFor any $x$, $f(x)$ is a sum of two Gaussian random variables. Since Gaussians are closed under addition, $f(x)$ is also a Gaussian random variable for any $x$. In fact, we can compute for any particular $x$ that $f(x)$ is $\\mathcal{N}(0,1+x^2)$. Similarly, the joint distribution for any collection of function values, $(f(x_1),\\dots,f(x_n))$, for any collection of inputs $x_1,\\dots,x_n$, is a multivariate Gaussian distribution. Therefore $f(x)$ is a Gaussian process.\n\nIn short, $f(x)$ is a _random function_, or a _distribution over functions_. We can gain some insights into this distribution by repeatedly sampling values for $w_0, w_1$, and visualizing the corresponding functions $f(x)$, which are straight lines with slopes and different intercepts, as follows:\n\n```{.python .input}\ndef lin_func(x, n_sample):\n    preds = np.zeros((n_sample, x.shape[0]))\n    for ii in range(n_sample):\n        w = np.random.normal(0, 1, 2)\n        y = w[0] + w[1] * x\n        preds[ii, :] = y\n    return preds\n\nx_points = np.linspace(-5, 5, 50)\nouts = lin_func(x_points, 10)\nlw_bd = -2 * np.sqrt((1 + x_points ** 2))\nup_bd = 2 * np.sqrt((1 + x_points ** 2))\n\nd2l.plt.fill_between(x_points, lw_bd, up_bd, alpha=0.25)\nd2l.plt.plot(x_points, np.zeros(len(x_points)), linewidth=4, color='black')\nd2l.plt.plot(x_points, outs.T)\nd2l.plt.xlabel(\"x\", fontsize=20)\nd2l.plt.ylabel(\"f(x)\", fontsize=20)\nd2l.plt.show()\n```\n\nIf $w_0$ and $w_1$ are instead drawn from $\\mathcal{N}(0,\\alpha^2)$, how do you imagine varying $\\alpha$ affects the distribution over functions?"
    },
    {
      "chunk_id": "d949f79bf844_0",
      "chapter": "gp-priors",
      "heading": "From Weight Space to Function Space",
      "text": "In the plot above, we saw how a distribution over parameters in a model induces a distribution over functions. While we often have ideas about the functions we want to model --- whether they're smooth, periodic, quickly varying, etc. --- it is relatively tedious to reason about the parameters, which are largely uninterpretable. Fortunately, Gaussian processes provide an easy mechanism to reason _directly_ about functions. Since a Gaussian distribution is entirely defined by its first two moments, its mean and covariance matrix, a Gaussian process by extension is defined by its mean function and covariance function.\n\nIn the above example, the mean function\n\n$$m(x) = E[f(x)] = E[w_0 + w_1x] = E[w_0] + E[w_1]x = 0+0 = 0.$$\n\nSimilarly, the covariance function is\n\n$$k(x,x') = \\textrm{Cov}(f(x),f(x')) = E[f(x)f(x')]-E[f(x)]E[f(x')] = E[w_0^2 + w_0w_1x' + w_1w_0x + w_1^2xx'] = 1 + xx'.$$\n\nOur distribution over functions can now be directly specified and sampled from, without needing to sample from the distribution over parameters. For example, to draw from $f(x)$, we can simply form our multivariate Gaussian distribution associated with any collection of $x$ we want to query, and sample from it directly. We will begin to see just how advantageous this formulation will be.\n\nFirst, we note that essentially the same derivation for the simple straight line model above can be applied to find the mean and covariance function for _any_ model of the form $f(x) = w^{\\top} \\phi(x)$, with $w \\sim \\mathcal{N}(u,S)$. In this case, the mean function $m(x) = u^{\\top}\\phi(x)$, and the covariance function $k(x,x') = \\phi(x)^{\\top}S\\phi(x')$. Since $\\phi(x)$ can represent a vector of any non-linear basis functions, we are considering a very general model class, including models with an even an _infinite_ number of parameters."
    },
    {
      "chunk_id": "b05ecf85e58c_0",
      "chapter": "gp-priors",
      "heading": "The Radial Basis Function (RBF) Kernel",
      "text": "The _radial basis function_ (RBF) kernel is the most popular covariance function for Gaussian processes, and kernel machines in general. This kernel has the form $k_{\\textrm{RBF}}(x,x') = a^2\\exp\\left(-\\frac{1}{2\\ell^2}||x-x'||^2\\right)$, where $a$ is an amplitude parameter, and $\\ell$ is a _lengthscale_ hyperparameter. Let's derive this kernel starting from weight space. Consider the function\n\n$$f(x) = \\sum_{i=1}^J w_i \\phi_i(x), w_i  \\sim \\mathcal{N}\\left(0,\\frac{\\sigma^2}{J}\\right), \\phi_i(x) = \\exp\\left(-\\frac{(x-c_i)^2}{2\\ell^2 }\\right).$$\n\n$f(x)$ is a sum of radial basis functions, with width $\\ell$, centred at the points $c_i$, as shown in the following figure. We can recognize $f(x)$ as having the form $w^{\\top} \\phi(x)$, where $w = (w_1,\\dots,w_J)^{\\top}$ and $\\phi(x)$ is a vector containing each of the radial basis functions. The covariance function of this Gaussian process is then\n\n$$k(x,x') = \\frac{\\sigma^2}{J} \\sum_{i=1}^{J} \\phi_i(x)\\phi_i(x').$$\n\nNow let's consider what happens as we take the number of parameters (and basis functions) to infinity. Let $c_J = \\log J$, $c_1 = -\\log J$, and $c_{i+1}-c_{i} = \\Delta c = 2\\frac{\\log J}{J}$, and $J \\to \\infty$. The covariance function becomes the Riemann sum:\n\n$$k(x,x') = \\lim_{J \\to \\infty} \\frac{\\sigma^2}{J} \\sum_{i=1}^{J} \\phi_i(x)\\phi_i(x') = \\int_{c_0}^{c_\\infty} \\phi_c(x)\\phi_c(x') dc.$$\n\nBy setting $c_0 = -\\infty$ and $c_\\infty = \\infty$, we spread the infinitely many basis functions across the whole real line, each\na distance $\\Delta c \\to 0$ apart:\n\n$$k(x,x') = \\int_{-\\infty}^{\\infty} \\exp(-\\frac{(x-c)^2}{2\\ell^2}) \\exp(-\\frac{(x'-c)^2}{2\\ell^2 }) dc = \\sqrt{\\pi}\\ell \\sigma^2 \\exp(-\\frac{(x-x')^2}{2(\\sqrt{2} \\ell)^2}) \\propto k_{\\textrm{RBF}}(x,x').$$\n\nIt is worth taking a moment to absorb what we have done here. By moving into the function space representation, we have derived how to represent a model with an _infinite_ number of parameters, using a finite amount of computation."
    },
    {
      "chunk_id": "b05ecf85e58c_1",
      "chapter": "gp-priors",
      "heading": "The Radial Basis Function (RBF) Kernel",
      "text": "By moving into the function space representation, we have derived how to represent a model with an _infinite_ number of parameters, using a finite amount of computation. A Gaussian process with an RBF kernel is a _universal approximator_, capable of representing any continuous function to arbitrary precision. We can intuitively see why from the above derivation. We can collapse each radial basis function to a point mass taking $\\ell \\to 0$, and give each point mass any height we wish. So a Gaussian process with an RBF kernel is a model with an infinite number of parameters and much more flexibility than any finite neural network. Perhaps all the fuss about _overparametrized_ neural networks is misplaced. As we will see, GPs with RBF kernels do not overfit, and in fact provide especially compelling generalization performance on small datasets. Moreover, the examples in :cite:`zhang2021understanding`, such as the ability to fit images with random labels perfectly, but still generalize well on structured problems, (can be perfectly reproduced using Gaussian processes) :cite:`wilson2020bayesian`. Neural networks are not as distinct as we make them out to be. We can build further intuition about Gaussian processes with RBF kernels, and hyperparameters such as _length-scale_, by sampling directly from the distribution over functions. As before, this involves a simple procedure:\n\n1. Choose the input $x$ points we want to query the GP: $x_1,\\dots,x_n$. 2. Evaluate $m(x_i)$, $i = 1,\\dots,n$, and $k(x_i,x_j)$ for $i,j = 1,\\dots,n$ to respectively form the mean vector and covariance matrix $\\mu$ and $K$, where $(f(x_1),\\dots,f(x_n)) \\sim \\mathcal{N}(\\mu, K)$. 3. Sample from this multivariate Gaussian distribution to obtain the sample function values. 4. Sample more times to visualize more sample functions queried at those points. We illustrate this process in the figure below."
    },
    {
      "chunk_id": "b05ecf85e58c_2",
      "chapter": "gp-priors",
      "heading": "The Radial Basis Function (RBF) Kernel",
      "text": "3. Sample from this multivariate Gaussian distribution to obtain the sample function values. 4. Sample more times to visualize more sample functions queried at those points. We illustrate this process in the figure below. ```{.python .input}\ndef rbfkernel(x1, x2, ls=4.):  #@save\n    dist = distance_matrix(np.expand_dims(x1, 1), np.expand_dims(x2, 1))\n    return np.exp(-(1. / ls / 2) * (dist ** 2))\n\nx_points = np.linspace(0, 5, 50)\nmeanvec = np.zeros(len(x_points))\ncovmat = rbfkernel(x_points,x_points, 1)\n\nprior_samples= np.random.multivariate_normal(meanvec, covmat, size=5);\nd2l.plt.plot(x_points, prior_samples.T, alpha=0.5)\nd2l.plt.show()\n```"
    },
    {
      "chunk_id": "18450b5f33af_0",
      "chapter": "gp-priors",
      "heading": "The Neural Network Kernel",
      "text": "Research on Gaussian processes in machine learning was triggered by research on neural networks. Radford Neal was pursuing ever larger Bayesian neural networks, ultimately showing in 1994 (later published in 1996, as it was one of the most infamous NeurIPS rejections) that such networks with an infinite number of hidden units become Gaussian processes with particular kernel functions :cite:`neal1996bayesian`. Interest in this derivation has re-surfaced, with ideas like the neural tangent kernel being used to investigate the generalization properties of neural networks :cite:`matthews2018gaussian` :cite:`novak2018bayesian`. We can derive the neural network kernel as follows. Consider a neural network function $f(x)$ with one hidden layer:\n\n$$f(x) = b + \\sum_{i=1}^{J} v_i h(x; u_i).$$\n\n$b$ is a bias, $v_i$ are the hidden to output weights, $h$ is any bounded hidden unit transfer function, $u_i$ are the input to hidden weights, and $J$ is the number of hidden units. Let $b$ and $v_i$ be independent with zero mean and variances $\\sigma_b^2$ and $\\sigma_v^2/J$, respectively, and let the $u_i$ have independent identical distributions. We can then use the central limit theorem to show that any collection of function values $f(x_1),\\dots,f(x_n)$ has a joint multivariate Gaussian distribution. The mean and covariance function of the corresponding Gaussian process are:\n\n$$m(x) = E[f(x)] = 0$$\n\n$$k(x,x') = \\textrm{cov}[f(x),f(x')] = E[f(x)f(x')] = \\sigma_b^2 + \\frac{1}{J} \\sum_{i=1}^{J} \\sigma_v^2 E[h_i(x; u_i)h_i(x'; u_i)]$$\n\nIn some cases, we can essentially evaluate this covariance function in closed form. Let $h(x; u) = \\textrm{erf}(u_0 + \\sum_{j=1}^{P} u_j x_j)$, where $\\textrm{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{z} e^{-t^2} dt$, and $u \\sim \\mathcal{N}(0,\\Sigma)$. Then $k(x,x') = \\frac{2}{\\pi} \\textrm{sin}(\\frac{2 \\tilde{x}^{\\top} \\Sigma \\tilde{x}'}{\\sqrt{(1 + 2 \\tilde{x}^{\\top} \\Sigma \\tilde{x})(1 + 2 \\tilde{x}'^{\\top} \\Sigma \\tilde{x}')}})$."
    },
    {
      "chunk_id": "18450b5f33af_1",
      "chapter": "gp-priors",
      "heading": "The Neural Network Kernel",
      "text": "Then $k(x,x') = \\frac{2}{\\pi} \\textrm{sin}(\\frac{2 \\tilde{x}^{\\top} \\Sigma \\tilde{x}'}{\\sqrt{(1 + 2 \\tilde{x}^{\\top} \\Sigma \\tilde{x})(1 + 2 \\tilde{x}'^{\\top} \\Sigma \\tilde{x}')}})$. The RBF kernel is _stationary_, meaning that it is _translation invariant_, and therefore can be written as a function of $\\tau = x-x'$. Intuitively, stationarity means that the high-level properties of the function, such as rate of variation, do not change as we move in input space. The neural network kernel, however, is _non-stationary_. Below, we show sample functions from a Gaussian process with this kernel. We can see that the function looks qualitatively different near the origin."
    },
    {
      "chunk_id": "4c77013cbabd_0",
      "chapter": "gp-priors",
      "heading": "Summary",
      "text": "The first step in performing Bayesian inference involves specifying a prior. Gaussian processes can be used to specify a whole prior over functions. Starting from a traditional \"weight space\" view of modelling, we can induce a prior over functions by starting with the functional form of a model, and introducing a distribution over its parameters. We can alternatively specify a prior distribution directly in function space, with properties controlled by a kernel. The function-space approach has many advantages. We can build models that actually correspond to an infinite number of parameters, but use a finite amount of computation! Moreover, while these models have a great amount of flexibility, they also make strong assumptions about what types of functions are a priori likely, leading to relatively good generalization on small datasets.\n\nThe assumptions of models in function space are intuitively controlled by kernels, which often encode higher level properties of functions, such as smoothness and periodicity. Many kernels are stationary, meaning that they are translation invariant. Functions drawn from a Gaussian process with a stationary kernel have roughly the same high-level properties (such as rate of variation) regardless of where we look in the input space.\n\nGaussian processes are a relatively general model class, containing many examples of models we are already familiar with, including polynomials, Fourier series, and so on, as long as we have a Gaussian prior over the parameters. They also include neural networks with an infinite number of parameters, even without Gaussian distributions over the parameters. This connection, discovered by Radford Neal, triggered machine learning researchers to move away from neural networks, and towards Gaussian processes."
    },
    {
      "chunk_id": "d2b67e40200e_0",
      "chapter": "gp-priors",
      "heading": "Exercises",
      "text": "1. Draw sample prior functions from a GP with an Ornstein-Uhlenbeck (OU) kernel, $k_{\\textrm{OU}}(x,x') = \\exp\\left(-\\frac{1}{2\\ell}||x - x'|\\right)$. If you fix the lengthscale $\\ell$ to be the same, how do these functions look different than sample functions from a GP with an RBF kernel?\n\n2. How does changing the _amplitude_ $a^2$ of the RBF kernel affect the distribution over functions?\n\n3. Suppose we form $u(x) = f(x) + 2g(x)$, where $f(x) \\sim \\mathcal{GP}(m_1,k_1)$ and $g(x) \\sim \\mathcal{GP}(m_2,k_2)$. Is $u(x)$ a Gaussian process, and if so, what is its mean and covariance function?\n\n4. Suppose we form $g(x) = a(x)f(x)$, where $f(x) \\sim \\mathcal{GP}(0,k)$ and $a(x) = x^2$. Is $g(x)$ a Gaussian process, and if so, what is its mean and covariance function? What is the effect of $a(x)$? What do sample functions drawn from $g(x)$ look like?\n\n5. Suppose we form $u(x) = f(x)g(x)$, where $f(x) \\sim \\mathcal{GP}(m_1,k_1)$ and $g(x) \\sim \\mathcal{GP}(m_2,k_2)$. Is $u(x)$ a Gaussian process, and if so, what is its mean and covariance function?\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12116)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Gaussian Processes\n:label:`chap_gp`\n\n**Andrew Gordon Wilson** (*New York University and Amazon*)\n\n\nGaussian processes (GPs) are ubitiquous. You have already encountered many examples of GPs without realizing it. Any model that is linear in its parameters with a Gaussian distribution over the parameters is a Gaussian process. This class spans discrete models, including random walks, and autoregressive processes, as well as continuous models, including Bayesian linear regression models, polynomials, Fourier series, radial basis functions, and even neural networks with an infinite number of hidden units. There is a running joke that \"everything is a special case of a Gaussian process\". Learning about Gaussian processes is important for three reasons: (1) they provide a _function space_ perspective of modelling, which makes understanding a variety of model classes, including deep neural networks, much more approachable; (2) they have an extraordinary range of applications where they are state-of-the-art, including active learning, hyperparameter learning, auto-ML, and spatiotemporal regression; (3) over the last few years, algorithmic advances have made Gaussian processes increasingly scalable and relevant, harmonizing with deep learning through frameworks such as [GPyTorch](https://gpytorch.ai) :cite:`Gardner.Pleiss.Weinberger.Bindel.Wilson.2018`. Indeed, GPs and and deep neural networks are not competing approaches, but highly complementary, and can be combined to great effect. These algorithmic advances are not just relevant to Gaussian processes, but provide a foundation in numerical methods that is broadly useful in deep learning. In this chapter, we introduce Gaussian processes. In the introductory notebook, we start by reasoning intuitively about what Gaussian processes are and how they directly model functions. In the priors notebook, we focus on how to specify Gaussian process priors."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "In the introductory notebook, we start by reasoning intuitively about what Gaussian processes are and how they directly model functions. In the priors notebook, we focus on how to specify Gaussian process priors. We directly connect the tradiational weight-space approach to modelling to function space, which will help us reason about constructing and understanding machine learning models, including deep neural networks. We then introduce popular covariance functions, also known as _kernels_, which control the generalization properties of a Gaussian process. A GP with a given kernel defines a prior over functions. In the inference notebook, we will show how to use data to infer a _posterior_, in order to make predictions. This notebook contains from-scratch code for making predictions with a Gaussian process, as well as an introduction to GPyTorch. In upcoming notebooks, we will introduce the numerics behind Gaussian processes, which is useful for scaling Gaussian processes but also a powerful general foundation for deep learning, and advanced use-cases such as hyperparameter tuning in deep learning. Our examples will make use of GPyTorch, which makes Gaussian processes scale, and is closely integrated with deep learning functionality and PyTorch. ```toc\n:maxdepth: 2\n\ngp-intro\ngp-priors\ngp-inference\n```"
    },
    {
      "chunk_id": "da9c786e0291_0",
      "chapter": "dcgan",
      "heading": "dcgan",
      "text": "# Deep Convolutional Generative Adversarial Networks\n:label:`sec_dcgan`\n\nIn :numref:`sec_basic_gan`, we introduced the basic ideas behind how GANs work. We showed that they can draw samples from some simple, easy-to-sample distribution, like a uniform or normal distribution, and transform them into samples that appear to match the distribution of some dataset. And while our example of matching a 2D Gaussian distribution got the point across, it is not especially exciting.\n\nIn this section, we will demonstrate how you can use GANs to generate photorealistic images. We will be basing our models on the deep convolutional GANs (DCGAN) introduced in :citet:`Radford.Metz.Chintala.2015`. We will borrow the convolutional architecture that have proven so successful for discriminative computer vision problems and show how via GANs, they can be leveraged to generate photorealistic images.\n\n```{.python .input}\n#@tab mxnet\nfrom mxnet import gluon, init, np, npx\nfrom mxnet.gluon import nn\nfrom d2l import mxnet as d2l\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nfrom torch import nn\nimport warnings\n```\n\n```{.python .input}\n#@tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```"
    },
    {
      "chunk_id": "8a87521e536e_0",
      "chapter": "dcgan",
      "heading": "The Pokemon Dataset",
      "text": "The dataset we will use is a collection of Pokemon sprites obtained from [pokemondb](https://pokemondb.net/sprites). First download, extract and load this dataset. ```{.python .input}\n#@tab mxnet\n#@save\nd2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',\n                           'c065c0e2593b8b161a2d7873e42418bf6a21106c')\n\ndata_dir = d2l.download_extract('pokemon')\npokemon = gluon.data.vision.datasets.ImageFolderDataset(data_dir)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nd2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',\n                           'c065c0e2593b8b161a2d7873e42418bf6a21106c')\n\ndata_dir = d2l.download_extract('pokemon')\npokemon = torchvision.datasets.ImageFolder(data_dir)\n```\n\n```{.python .input}\n#@tab tensorflow\n#@save\nd2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',\n                           'c065c0e2593b8b161a2d7873e42418bf6a21106c')\n\ndata_dir = d2l.download_extract('pokemon')\nbatch_size = 256\npokemon = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir, batch_size=batch_size, image_size=(64, 64))\n```\n\nWe resize each image into $64\\times 64$. The `ToTensor` transformation will project the pixel value into $[0, 1]$, while our generator will use the tanh function to obtain outputs in $[-1, 1]$. Therefore we normalize the data with $0.5$ mean and $0.5$ standard deviation to match the value range."
    },
    {
      "chunk_id": "8a87521e536e_1",
      "chapter": "dcgan",
      "heading": "The Pokemon Dataset",
      "text": "The `ToTensor` transformation will project the pixel value into $[0, 1]$, while our generator will use the tanh function to obtain outputs in $[-1, 1]$. Therefore we normalize the data with $0.5$ mean and $0.5$ standard deviation to match the value range. ```{.python .input}\n#@tab mxnet\nbatch_size = 256\ntransformer = gluon.data.vision.transforms.Compose([\n    gluon.data.vision.transforms.Resize(64),\n    gluon.data.vision.transforms.ToTensor(),\n    gluon.data.vision.transforms.Normalize(0.5, 0.5)\n])\ndata_iter = gluon.data.DataLoader(\n    pokemon.transform_first(transformer), batch_size=batch_size,\n    shuffle=True, num_workers=d2l.get_dataloader_workers())\n```\n\n```{.python .input}\n#@tab pytorch\nbatch_size = 256\ntransformer = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((64, 64)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(0.5, 0.5)\n])\npokemon.transform = transformer\ndata_iter = torch.utils.data.DataLoader(\n    pokemon, batch_size=batch_size,\n    shuffle=True, num_workers=d2l.get_dataloader_workers())\n```\n\n```{.python .input}\n#@tab tensorflow\ndef transform_func(X):\n    X = X / 255. X = (X - 0.5) / (0.5)\n    return X\n\n# For TF>=2.4 use `num_parallel_calls = tf.data.AUTOTUNE`\ndata_iter = pokemon.map(lambda x, y: (transform_func(x), y),\n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndata_iter = data_iter.cache().shuffle(buffer_size=1000).prefetch(\n    buffer_size=tf.data.experimental.AUTOTUNE)\n```\n\nLet's visualize the first 20 images."
    },
    {
      "chunk_id": "8a87521e536e_2",
      "chapter": "dcgan",
      "heading": "The Pokemon Dataset",
      "text": "```{.python .input}\n#@tab mxnet\nd2l.set_figsize((4, 4))\nfor X, y in data_iter:\n    imgs = X[:20,:,:,:].transpose(0, 2, 3, 1)/2+0.5\n    d2l.show_images(imgs, num_rows=4, num_cols=5)\n    break\n```\n\n```{.python .input}\n#@tab pytorch\nwarnings.filterwarnings('ignore')\nd2l.set_figsize((4, 4))\nfor X, y in data_iter:\n    imgs = X[:20,:,:,:].permute(0, 2, 3, 1)/2+0.5\n    d2l.show_images(imgs, num_rows=4, num_cols=5)\n    break\n```\n\n```{.python .input}\n#@tab tensorflow\nd2l.set_figsize(figsize=(4, 4))\nfor X, y in data_iter.take(1):\n    imgs = X[:20, :, :, :] / 2 + 0.5\n    d2l.show_images(imgs, num_rows=4, num_cols=5)\n```"
    },
    {
      "chunk_id": "89cae05992bd_0",
      "chapter": "dcgan",
      "heading": "The Generator",
      "text": "The generator needs to map the noise variable $\\mathbf z\\in\\mathbb R^d$, a length-$d$ vector, to a RGB image with width and height to be $64\\times 64$ . In :numref:`sec_fcn` we introduced the fully convolutional network that uses transposed convolution layer (refer to :numref:`sec_transposed_conv`) to enlarge input size. The basic block of the generator contains a transposed convolution layer followed by the batch normalization and ReLU activation."
    },
    {
      "chunk_id": "89cae05992bd_1",
      "chapter": "dcgan",
      "heading": "The Generator",
      "text": "The basic block of the generator contains a transposed convolution layer followed by the batch normalization and ReLU activation. ```{.python .input}\n#@tab mxnet\nclass G_block(nn.Block):\n    def __init__(self, channels, kernel_size=4,\n                 strides=2, padding=1, **kwargs):\n        super(G_block, self).__init__(**kwargs)\n        self.conv2d_trans = nn.Conv2DTranspose(\n            channels, kernel_size, strides, padding, use_bias=False)\n        self.batch_norm = nn.BatchNorm()\n        self.activation = nn.Activation('relu')\n\n    def forward(self, X):\n        return self.activation(self.batch_norm(self.conv2d_trans(X)))\n```\n\n```{.python .input}\n#@tab pytorch\nclass G_block(nn.Module):\n    def __init__(self, out_channels, in_channels=3, kernel_size=4, strides=2,\n                 padding=1, **kwargs):\n        super(G_block, self).__init__(**kwargs)\n        self.conv2d_trans = nn.ConvTranspose2d(in_channels, out_channels,\n                                kernel_size, strides, padding, bias=False)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.activation = nn.ReLU()\n\n    def forward(self, X):\n        return self.activation(self.batch_norm(self.conv2d_trans(X)))\n```\n\n```{.python .input}\n#@tab tensorflow\nclass G_block(tf.keras.layers.Layer):\n    def __init__(self, out_channels, kernel_size=4, strides=2, padding=\"same\",\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.conv2d_trans = tf.keras.layers.Conv2DTranspose(\n            out_channels, kernel_size, strides, padding, use_bias=False)\n        self.batch_norm = tf.keras.layers.BatchNormalization()\n        self.activation = tf.keras.layers.ReLU()\n\n    def call(self, X):\n        return self.activation(self.batch_norm(self.conv2d_trans(X)))\n```\n\nIn default, the transposed convolution layer uses a $k_h = k_w = 4$ kernel, a $s_h = s_w = 2$ strides, and a $p_h = p_w = 1$ padding. With a input shape of $n_h^{'} \\times n_w^{'} = 16 \\times 16$, the generator block will double input's width and height."
    },
    {
      "chunk_id": "89cae05992bd_2",
      "chapter": "dcgan",
      "heading": "The Generator",
      "text": "With a input shape of $n_h^{'} \\times n_w^{'} = 16 \\times 16$, the generator block will double input's width and height. $$\n\\begin{aligned}\nn_h^{'} \\times n_w^{'} &= [(n_h k_h - (n_h-1)(k_h-s_h)- 2p_h] \\times [(n_w k_w - (n_w-1)(k_w-s_w)- 2p_w]\\\\\n  &= [(k_h + s_h (n_h-1)- 2p_h] \\times [(k_w + s_w (n_w-1)- 2p_w]\\\\\n  &= [(4 + 2 \\times (16-1)- 2 \\times 1] \\times [(4 + 2 \\times (16-1)- 2 \\times 1]\\\\\n  &= 32 \\times 32 .\\\\\n\\end{aligned}\n$$\n\n```{.python .input}\n#@tab mxnet\nx = np.zeros((2, 3, 16, 16))\ng_blk = G_block(20)\ng_blk.initialize()\ng_blk(x).shape\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.zeros((2, 3, 16, 16))\ng_blk = G_block(20)\ng_blk(x).shape\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.zeros((2, 16, 16, 3))  # Channel last convention\ng_blk = G_block(20)\ng_blk(x).shape\n```\n\nIf changing the transposed convolution layer to a $4\\times 4$ kernel, $1\\times 1$ strides and zero padding. With a input size of $1 \\times 1$, the output will have its width and height increased by 3 respectively. ```{.python .input}\n#@tab mxnet\nx = np.zeros((2, 3, 1, 1))\ng_blk = G_block(20, strides=1, padding=0)\ng_blk.initialize()\ng_blk(x).shape\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.zeros((2, 3, 1, 1))\ng_blk = G_block(20, strides=1, padding=0)\ng_blk(x).shape\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.zeros((2, 1, 1, 3))\n# `padding=\"valid\"` corresponds to no padding\ng_blk = G_block(20, strides=1, padding=\"valid\")\ng_blk(x).shape\n```\n\nThe generator consists of four basic blocks that increase input's both width and height from 1 to 32. At the same time, it first projects the latent variable into $64\\times 8$ channels, and then halve the channels each time. At last, a transposed convolution layer is used to generate the output. It further doubles the width and height to match the desired $64\\times 64$ shape, and reduces the channel size to $3$. The tanh activation function is applied to project output values into the $(-1, 1)$ range."
    },
    {
      "chunk_id": "89cae05992bd_3",
      "chapter": "dcgan",
      "heading": "The Generator",
      "text": "It further doubles the width and height to match the desired $64\\times 64$ shape, and reduces the channel size to $3$. The tanh activation function is applied to project output values into the $(-1, 1)$ range. ```{.python .input}\n#@tab mxnet\nn_G = 64\nnet_G = nn.Sequential()\nnet_G.add(G_block(n_G*8, strides=1, padding=0),  # Output: (64 * 8, 4, 4)\n          G_block(n_G*4),  # Output: (64 * 4, 8, 8)\n          G_block(n_G*2),  # Output: (64 * 2, 16, 16)\n          G_block(n_G),    # Output: (64, 32, 32)\n          nn.Conv2DTranspose(\n              3, kernel_size=4, strides=2, padding=1, use_bias=False,\n              activation='tanh'))  # Output: (3, 64, 64)\n```\n\n```{.python .input}\n#@tab pytorch\nn_G = 64\nnet_G = nn.Sequential(\n    G_block(in_channels=100, out_channels=n_G*8,\n            strides=1, padding=0),                  # Output: (64 * 8, 4, 4)\n    G_block(in_channels=n_G*8, out_channels=n_G*4), # Output: (64 * 4, 8, 8)\n    G_block(in_channels=n_G*4, out_channels=n_G*2), # Output: (64 * 2, 16, 16)\n    G_block(in_channels=n_G*2, out_channels=n_G),   # Output: (64, 32, 32)\n    nn.ConvTranspose2d(in_channels=n_G, out_channels=3,\n                       kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh())  # Output: (3, 64, 64)\n```\n\n```{.python .input}\n#@tab tensorflow\nn_G = 64\nnet_G = tf.keras.Sequential([\n    # Output: (4, 4, 64 * 8)\n    G_block(out_channels=n_G*8, strides=1, padding=\"valid\"),\n    G_block(out_channels=n_G*4), # Output: (8, 8, 64 * 4)\n    G_block(out_channels=n_G*2), # Output: (16, 16, 64 * 2)\n    G_block(out_channels=n_G), # Output: (32, 32, 64)\n    # Output: (64, 64, 3)\n    tf.keras.layers.Conv2DTranspose(\n        3, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n        activation=\"tanh\")\n])\n```\n\nGenerate a 100 dimensional latent variable to verify the generator's output shape."
    },
    {
      "chunk_id": "89cae05992bd_4",
      "chapter": "dcgan",
      "heading": "The Generator",
      "text": "```{.python .input}\n#@tab mxnet\nx = np.zeros((1, 100, 1, 1))\nnet_G.initialize()\nnet_G(x).shape\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.zeros((1, 100, 1, 1))\nnet_G(x).shape\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.zeros((1, 1, 1, 100))\nnet_G(x).shape\n```"
    },
    {
      "chunk_id": "3054aa8d0336_0",
      "chapter": "dcgan",
      "heading": "Discriminator",
      "text": "The discriminator is a normal convolutional network network except that it uses a leaky ReLU as its activation function. Given $\\alpha \\in[0, 1]$, its definition is\n\n$$\\textrm{leaky ReLU}(x) = \\begin{cases}x & \\textrm{if}\\ x > 0\\\\ \\alpha x &\\textrm{otherwise}\\end{cases}.$$\n\nAs it can be seen, it is normal ReLU if $\\alpha=0$, and an identity function if $\\alpha=1$. For $\\alpha \\in (0, 1)$, leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the \"dying ReLU\" problem that a neuron might always output a negative value and therefore cannot make any progress since the gradient of ReLU is 0. ```{.python .input}\n#@tab mxnet,pytorch\nalphas = [0, .2, .4, .6, .8, 1]\nx = d2l.arange(-2, 1, 0.1)\nY = [d2l.numpy(nn.LeakyReLU(alpha)(x)) for alpha in alphas]\nd2l.plot(d2l.numpy(x), Y, 'x', 'y', alphas)\n```\n\n```{.python .input}\n#@tab tensorflow\nalphas = [0, .2, .4, .6, .8, 1]\nx = tf.range(-2, 1, 0.1)\nY = [tf.keras.layers.LeakyReLU(alpha)(x).numpy() for alpha in alphas]\nd2l.plot(x.numpy(), Y, 'x', 'y', alphas)\n```\n\nThe basic block of the discriminator is a convolution layer followed by a batch normalization layer and a leaky ReLU activation. The hyperparameters of the convolution layer are similar to the transpose convolution layer in the generator block."
    },
    {
      "chunk_id": "3054aa8d0336_1",
      "chapter": "dcgan",
      "heading": "Discriminator",
      "text": "The hyperparameters of the convolution layer are similar to the transpose convolution layer in the generator block. ```{.python .input}\n#@tab mxnet\nclass D_block(nn.Block):\n    def __init__(self, channels, kernel_size=4, strides=2,\n                 padding=1, alpha=0.2, **kwargs):\n        super(D_block, self).__init__(**kwargs)\n        self.conv2d = nn.Conv2D(\n            channels, kernel_size, strides, padding, use_bias=False)\n        self.batch_norm = nn.BatchNorm()\n        self.activation = nn.LeakyReLU(alpha)\n\n    def forward(self, X):\n        return self.activation(self.batch_norm(self.conv2d(X)))\n```\n\n```{.python .input}\n#@tab pytorch\nclass D_block(nn.Module):\n    def __init__(self, out_channels, in_channels=3, kernel_size=4, strides=2,\n                padding=1, alpha=0.2, **kwargs):\n        super(D_block, self).__init__(**kwargs)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size,\n                                strides, padding, bias=False)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.activation = nn.LeakyReLU(alpha, inplace=True)\n\n    def forward(self, X):\n        return self.activation(self.batch_norm(self.conv2d(X)))\n```\n\n```{.python .input}\n#@tab tensorflow\nclass D_block(tf.keras.layers.Layer):\n    def __init__(self, out_channels, kernel_size=4, strides=2, padding=\"same\",\n                 alpha=0.2, **kwargs):\n        super().__init__(**kwargs)\n        self.conv2d = tf.keras.layers.Conv2D(out_channels, kernel_size,\n                                             strides, padding, use_bias=False)\n        self.batch_norm = tf.keras.layers.BatchNormalization()\n        self.activation = tf.keras.layers.LeakyReLU(alpha)\n\n    def call(self, X):\n        return self.activation(self.batch_norm(self.conv2d(X)))\n```\n\nA basic block with default settings will halve the width and height of the inputs, as we demonstrated in :numref:`sec_padding`."
    },
    {
      "chunk_id": "3054aa8d0336_2",
      "chapter": "dcgan",
      "heading": "Discriminator",
      "text": "For example, given a input shape $n_h = n_w = 16$, with a kernel shape $k_h = k_w = 4$, a stride shape $s_h = s_w = 2$, and a padding shape $p_h = p_w = 1$, the output shape will be:\n\n$$\n\\begin{aligned}\nn_h^{'} \\times n_w^{'} &= \\lfloor(n_h-k_h+2p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+2p_w+s_w)/s_w\\rfloor\\\\\n  &= \\lfloor(16-4+2\\times 1+2)/2\\rfloor \\times \\lfloor(16-4+2\\times 1+2)/2\\rfloor\\\\\n  &= 8 \\times 8 .\\\\\n\\end{aligned}\n$$\n\n```{.python .input}\n#@tab mxnet\nx = np.zeros((2, 3, 16, 16))\nd_blk = D_block(20)\nd_blk.initialize()\nd_blk(x).shape\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.zeros((2, 3, 16, 16))\nd_blk = D_block(20)\nd_blk(x).shape\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.zeros((2, 16, 16, 3))\nd_blk = D_block(20)\nd_blk(x).shape\n```\n\nThe discriminator is a mirror of the generator."
    },
    {
      "chunk_id": "3054aa8d0336_3",
      "chapter": "dcgan",
      "heading": "Discriminator",
      "text": "```{.python .input}\n#@tab mxnet\nn_D = 64\nnet_D = nn.Sequential()\nnet_D.add(D_block(n_D),   # Output: (64, 32, 32)\n          D_block(n_D*2),  # Output: (64 * 2, 16, 16)\n          D_block(n_D*4),  # Output: (64 * 4, 8, 8)\n          D_block(n_D*8),  # Output: (64 * 8, 4, 4)\n          nn.Conv2D(1, kernel_size=4, use_bias=False))  # Output: (1, 1, 1)\n```\n\n```{.python .input}\n#@tab pytorch\nn_D = 64\nnet_D = nn.Sequential(\n    D_block(n_D),  # Output: (64, 32, 32)\n    D_block(in_channels=n_D, out_channels=n_D*2),  # Output: (64 * 2, 16, 16)\n    D_block(in_channels=n_D*2, out_channels=n_D*4),  # Output: (64 * 4, 8, 8)\n    D_block(in_channels=n_D*4, out_channels=n_D*8),  # Output: (64 * 8, 4, 4)\n    nn.Conv2d(in_channels=n_D*8, out_channels=1,\n              kernel_size=4, bias=False))  # Output: (1, 1, 1)\n```\n\n```{.python .input}\n#@tab tensorflow\nn_D = 64\nnet_D = tf.keras.Sequential([\n    D_block(n_D), # Output: (32, 32, 64)\n    D_block(out_channels=n_D*2), # Output: (16, 16, 64 * 2)\n    D_block(out_channels=n_D*4), # Output: (8, 8, 64 * 4)\n    D_block(out_channels=n_D*8), # Outupt: (4, 4, 64 * 64)\n    # Output: (1, 1, 1)\n    tf.keras.layers.Conv2D(1, kernel_size=4, use_bias=False)\n])\n```\n\nIt uses a convolution layer with output channel $1$ as the last layer to obtain a single prediction value. ```{.python .input}\n#@tab mxnet\nx = np.zeros((1, 3, 64, 64))\nnet_D.initialize()\nnet_D(x).shape\n```\n\n```{.python .input}\n#@tab pytorch\nx = torch.zeros((1, 3, 64, 64))\nnet_D(x).shape\n```\n\n```{.python .input}\n#@tab tensorflow\nx = tf.zeros((1, 64, 64, 3))\nnet_D(x).shape\n```"
    },
    {
      "chunk_id": "b9a80fde2580_0",
      "chapter": "dcgan",
      "heading": "Training",
      "text": "Compared to the basic GAN in :numref:`sec_basic_gan`, we use the same learning rate for both generator and discriminator since they are similar to each other. In addition, we change $\\beta_1$ in Adam (:numref:`sec_adam`) from $0.9$ to $0.5$. It decreases the smoothness of the momentum, the exponentially weighted moving average of past gradients, to take care of the rapid changing gradients because the generator and the discriminator fight with each other. Besides, the random generated noise `Z`, is a 4-D tensor and we are using GPU to accelerate the computation."
    },
    {
      "chunk_id": "b9a80fde2580_1",
      "chapter": "dcgan",
      "heading": "Training",
      "text": "Besides, the random generated noise `Z`, is a 4-D tensor and we are using GPU to accelerate the computation. ```{.python .input}\n#@tab mxnet\ndef train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,\n          device=d2l.try_gpu()):\n    loss = gluon.loss.SigmoidBCELoss()\n    net_D.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)\n    net_G.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)\n    trainer_hp = {'learning_rate': lr, 'beta1': 0.5}\n    trainer_D = gluon.Trainer(net_D.collect_params(), 'adam', trainer_hp)\n    trainer_G = gluon.Trainer(net_G.collect_params(), 'adam', trainer_hp)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n                            legend=['discriminator', 'generator'])\n    animator.fig.subplots_adjust(hspace=0.3)\n    for epoch in range(1, num_epochs + 1):\n        # Train one epoch\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n        for X, _ in data_iter:\n            batch_size = X.shape[0]\n            Z = np.random.normal(0, 1, size=(batch_size, latent_dim, 1, 1))\n            X, Z = X.as_in_ctx(device), Z.as_in_ctx(device),\n            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),\n                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),\n                       batch_size)\n        # Show generated examples\n        Z = np.random.normal(0, 1, size=(21, latent_dim, 1, 1), ctx=device)\n        # Normalize the synthetic data to N(0, 1)\n        fake_x = net_G(Z).transpose(0, 2, 3, 1) / 2 + 0.5\n        imgs = np.concatenate(\n            [np.concatenate([fake_x[i * 7 + j] for j in range(7)], axis=1)\n             for i in range(len(fake_x)//7)], axis=0)\n        animator.axes[1].cla()\n        animator.axes[1].imshow(imgs.asnumpy())\n        # Show the losses\n        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n        animator.add(epoch, (loss_D, loss_G))\n    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,\n          device=d2l.try_gpu()):\n    loss = nn.BCEWithLogitsLoss(reduction='sum')\n    for w in net_D.parameters():\n        nn.init.normal_(w, 0, 0.02)\n    for w in net_G.parameters():\n        nn.init.normal_(w, 0, 0.02)\n    net_D, net_G = net_D.to(device), net_G.to(device)\n    trainer_hp = {'lr': lr, 'betas': [0.5,0.999]}\n    trainer_D = torch.optim.Adam(net_D.parameters(), **trainer_hp)\n    trainer_G = torch.optim.Adam(net_G.parameters(), **trainer_hp)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n                            legend=['discriminator', 'generator'])\n    animator.fig.subplots_adjust(hspace=0.3)\n    for epoch in range(1, num_epochs + 1):\n        # Train one epoch\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n        for X, _ in data_iter:\n            batch_size = X.shape[0]\n            Z = torch.normal(0, 1, size=(batch_size, latent_dim, 1, 1))\n            X, Z = X.to(device), Z.to(device)\n            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),\n                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),\n                       batch_size)\n        # Show generated examples\n        Z = torch.normal(0, 1, size=(21, latent_dim, 1, 1), device=device)\n        # Normalize the synthetic data to N(0, 1)\n        fake_x = net_G(Z).permute(0, 2, 3, 1) / 2 + 0.5\n        imgs = torch.cat(\n            [torch.cat([\n                fake_x[i * 7 + j].cpu().detach() for j in range(7)], dim=1)\n             for i in range(len(fake_x)//7)], dim=0)\n        animator.axes[1].cla()\n        animator.axes[1].imshow(imgs)\n        # Show the losses\n        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n        animator.add(epoch, (loss_D, loss_G))\n    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')\n```\n\n```{.python .input}\n#@tab tensorflow\ndef train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,\n          device=d2l.try_gpu()):\n    loss = tf.keras.losses.BinaryCrossentropy(\n        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n\n    for w in net_D.trainable_variables:\n        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n    for w in net_G.trainable_variables:\n        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n\n    optimizer_hp = {\"lr\": lr, \"beta_1\": 0.5, \"beta_2\": 0.999}\n    optimizer_D = tf.keras.optimizers.Adam(**optimizer_hp)\n    optimizer_G = tf.keras.optimizers.Adam(**optimizer_hp)\n\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n                            legend=['discriminator', 'generator'])\n    animator.fig.subplots_adjust(hspace=0.3)\n\n    for epoch in range(1, num_epochs + 1):\n        # Train one epoch\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(3) # loss_D, loss_G, num_examples\n        for X, _ in data_iter:\n            batch_size = X.shape[0]\n            Z = tf.random.normal(mean=0, stddev=1,\n                                 shape=(batch_size, 1, 1, latent_dim))\n            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, optimizer_D),\n                       d2l.update_G(Z, net_D, net_G, loss, optimizer_G),\n                       batch_size)\n\n        # Show generated examples\n        Z = tf.random.normal(mean=0, stddev=1, shape=(21, 1, 1, latent_dim))\n        # Normalize the synthetic data to N(0, 1)\n        fake_x = net_G(Z) / 2 + 0.5\n        imgs = tf.concat([tf.concat([fake_x[i * 7 + j] for j in range(7)],\n                                    axis=1)\n                          for i in range(len(fake_x) // 7)], axis=0)\n        animator.axes[1].cla()\n        animator.axes[1].imshow(imgs)\n        # Show the losses\n        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n        animator.add(epoch, (loss_D, loss_G))\n    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device._device_name)}')\n```\n\nWe train the model with a small number of epochs just for demonstration."
    },
    {
      "chunk_id": "b9a80fde2580_2",
      "chapter": "dcgan",
      "heading": "Training",
      "text": "For better performance,\nthe variable `num_epochs` can be set to a larger number. ```{.python .input}\n#@tab mxnet, pytorch\nlatent_dim, lr, num_epochs = 100, 0.005, 20\ntrain(net_D, net_G, data_iter, num_epochs, lr, latent_dim)\n```\n\n```{.python .input}\n#@tab tensorflow\nlatent_dim, lr, num_epochs = 100, 0.0005, 40\ntrain(net_D, net_G, data_iter, num_epochs, lr, latent_dim)\n```"
    },
    {
      "chunk_id": "03e5b61f642a_0",
      "chapter": "dcgan",
      "heading": "Summary",
      "text": "* DCGAN architecture has four convolutional layers for the Discriminator and four \"fractionally-strided\" convolutional layers for the Generator.\n* The Discriminator is a 4-layer strided convolutions with batch normalization (except its input layer) and leaky ReLU activations.\n* Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the \u201cdying ReLU\u201d problem and helps the gradients flow easier through the architecture."
    },
    {
      "chunk_id": "dab119f62365_0",
      "chapter": "dcgan",
      "heading": "Exercises",
      "text": "1. What will happen if we use standard ReLU activation rather than leaky ReLU?\n1. Apply DCGAN on Fashion-MNIST and see which category works well and which does not.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/409)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1083)\n:end_tab:"
    },
    {
      "chunk_id": "e7972854919f_0",
      "chapter": "gan",
      "heading": "gan",
      "text": "# Generative Adversarial Networks\n:label:`sec_basic_gan`\n\nThroughout most of this book, we have talked about how to make predictions. In some form or another, we used deep neural networks to learn mappings from data examples to labels. This kind of learning is called discriminative learning, as in, we'd like to be able to discriminate between photos of cats and photos of dogs. Classifiers and regressors are both examples of discriminative learning. And neural networks trained by backpropagation have upended everything we thought we knew about discriminative learning on large complicated datasets. Classification accuracies on high-res images have gone from useless to human-level (with some caveats) in just 5-6 years. We will spare you another spiel about all the other discriminative tasks where deep neural networks do astoundingly well. But there is more to machine learning than just solving discriminative tasks. For example, given a large dataset, without any labels, we might want to learn a model that concisely captures the characteristics of this data. Given such a model, we could sample synthetic data examples that resemble the distribution of the training data. For example, given a large corpus of photographs of faces, we might want to be able to generate a new photorealistic image that looks like it might plausibly have come from the same dataset. This kind of learning is called generative modeling. Until recently, we had no method that could synthesize novel photorealistic images. But the success of deep neural networks for discriminative learning opened up new possibilities. One big trend over the last three years has been the application of discriminative deep nets to overcome challenges in problems that we do not generally think of as supervised learning problems. The recurrent neural network language models are one example of using a discriminative network (trained to predict the next character) that once trained can act as a generative model."
    },
    {
      "chunk_id": "e7972854919f_1",
      "chapter": "gan",
      "heading": "gan",
      "text": "The recurrent neural network language models are one example of using a discriminative network (trained to predict the next character) that once trained can act as a generative model. In 2014, a breakthrough paper introduced Generative adversarial networks (GANs) :cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`, a clever new way to leverage the power of discriminative models to get good generative models. At their heart, GANs rely on the idea that a data generator is good if we cannot tell fake data apart from real data. In statistics, this is called a two-sample test - a test to answer the question whether datasets $X=\\{x_1,\\ldots, x_n\\}$ and $X'=\\{x'_1,\\ldots, x'_n\\}$ were drawn from the same distribution. The main difference between most statistics papers and GANs is that the latter use this idea in a constructive way. In other words, rather than just training a model to say \"hey, these two datasets do not look like they came from the same distribution\", they use the [two-sample test](https://en.wikipedia.org/wiki/Two-sample_hypothesis_testing) to provide training signals to a generative model. This allows us to improve the data generator until it generates something that resembles the real data. At the very least, it needs to fool the classifier even if our classifier is a state of the art deep neural network. ![Generative Adversarial Networks](../img/gan.svg)\n:label:`fig_gan`\n\n\nThe GAN architecture is illustrated in :numref:`fig_gan`. As you can see, there are two pieces in GAN architecture - first off, we need a device (say, a deep network but it really could be anything, such as a game rendering engine) that might potentially be able to generate data that looks just like the real thing. If we are dealing with images, this needs to generate images. If we are dealing with speech, it needs to generate audio sequences, and so on. We call this the generator network. The second component is the discriminator network. It attempts to distinguish fake and real data from each other."
    },
    {
      "chunk_id": "e7972854919f_2",
      "chapter": "gan",
      "heading": "gan",
      "text": "If we are dealing with speech, it needs to generate audio sequences, and so on. We call this the generator network. The second component is the discriminator network. It attempts to distinguish fake and real data from each other. Both networks are in competition with each other. The generator network attempts to fool the discriminator network. At that point, the discriminator network adapts to the new fake data. This information, in turn is used to improve the generator network, and so on. The discriminator is a binary classifier to distinguish if the input $x$ is real (from real data) or fake (from the generator). Typically, the discriminator outputs a scalar prediction $o\\in\\mathbb R$ for input $\\mathbf x$, such as using a fully connected layer with hidden size 1, and then applies sigmoid function to obtain the predicted probability $D(\\mathbf x) = 1/(1+e^{-o})$. Assume the label $y$ for the true data is $1$ and $0$ for the fake data. We train the discriminator to minimize the cross-entropy loss, *i.e.*,\n\n$$ \\min_D \\{ - y \\log D(\\mathbf x) - (1-y)\\log(1-D(\\mathbf x)) \\},$$\n\nFor the generator, it first draws some parameter $\\mathbf z\\in\\mathbb R^d$ from a source of randomness, *e.g.*, a normal distribution $\\mathbf z \\sim \\mathcal{N} (0, 1)$. We often call $\\mathbf z$ as the latent variable. It then applies a function to generate $\\mathbf x'=G(\\mathbf z)$. The goal of the generator is to fool the discriminator to classify $\\mathbf x'=G(\\mathbf z)$ as true data, *i.e.*, we want $D( G(\\mathbf z)) \\approx 1$. In other words, for a given discriminator $D$, we update the parameters of the generator $G$ to maximize the cross-entropy loss when $y=0$, *i.e.*,\n\n$$ \\max_G \\{ - (1-y) \\log(1-D(G(\\mathbf z))) \\} = \\max_G \\{ - \\log(1-D(G(\\mathbf z))) \\}.$$\n\nIf the generator does a perfect job, then $D(\\mathbf x')\\approx 1$, so the above loss is near 0, which results in the gradients that are too small to make good progress for the discriminator."
    },
    {
      "chunk_id": "e7972854919f_3",
      "chapter": "gan",
      "heading": "gan",
      "text": "So commonly, we minimize the following loss:\n\n$$ \\min_G \\{ - y \\log(D(G(\\mathbf z))) \\} = \\min_G \\{ - \\log(D(G(\\mathbf z))) \\}, $$\n\nwhich is just feeding $\\mathbf x'=G(\\mathbf z)$ into the discriminator but giving label $y=1$. To sum up, $D$ and $G$ are playing a \"minimax\" game with the comprehensive objective function:\n\n$$\\min_D \\max_G \\{ -E_{x \\sim \\textrm{Data}} \\log D(\\mathbf x) - E_{z \\sim \\textrm{Noise}} \\log(1 - D(G(\\mathbf z))) \\}.$$\n\n\n\nMany of the GANs applications are in the context of images. As a demonstration purpose, we are going to content ourselves with fitting a much simpler distribution first. We will illustrate what happens if we use GANs to build the world's most inefficient estimator of parameters for a Gaussian. Let's get started. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n#@tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```"
    },
    {
      "chunk_id": "605c05b6f043_0",
      "chapter": "gan",
      "heading": "Generate Some \"Real\" Data",
      "text": "Since this is going to be the world's lamest example, we simply generate data drawn from a Gaussian.\n\n```{.python .input}\n#@tab mxnet, pytorch\nX = d2l.normal(0.0, 1, (1000, 2))\nA = d2l.tensor([[1, 2], [-0.1, 0.5]])\nb = d2l.tensor([1, 2])\ndata = d2l.matmul(X, A) + b\n```\n\n```{.python .input}\n#@tab tensorflow\nX = d2l.normal((1000, 2), 0.0, 1)\nA = d2l.tensor([[1, 2], [-0.1, 0.5]])\nb = d2l.tensor([1, 2], tf.float32)\ndata = d2l.matmul(X, A) + b\n```\n\nLet's see what we got. This should be a Gaussian shifted in some rather arbitrary way with mean $b$ and covariance matrix $A^TA$.\n\n```{.python .input}\n#@tab mxnet, pytorch\nd2l.set_figsize()\nd2l.plt.scatter(d2l.numpy(data[:100, 0]), d2l.numpy(data[:100, 1]));\nprint(f'The covariance matrix is\\n{d2l.matmul(A.T, A)}')\n```\n\n```{.python .input}\n#@tab tensorflow\nd2l.set_figsize()\nd2l.plt.scatter(d2l.numpy(data[:100, 0]), d2l.numpy(data[:100, 1]));\nprint(f'The covariance matrix is\\n{tf.matmul(A, A, transpose_a=True)}')\n```\n\n```{.python .input}\n#@tab all\nbatch_size = 8\ndata_iter = d2l.load_array((data,), batch_size)\n```"
    },
    {
      "chunk_id": "ddfb8acebc2a_0",
      "chapter": "gan",
      "heading": "Generator",
      "text": "Our generator network will be the simplest network possible - a single layer linear model. This is since we will be driving that linear network with a Gaussian data generator. Hence, it literally only needs to learn the parameters to fake things perfectly.\n\n```{.python .input}\n#@tab mxnet\nnet_G = nn.Sequential()\nnet_G.add(nn.Dense(2))\n```\n\n```{.python .input}\n#@tab pytorch\nnet_G = nn.Sequential(nn.Linear(2, 2))\n```\n\n```{.python .input}\n#@tab tensorflow\nnet_G = tf.keras.layers.Dense(2)\n```"
    },
    {
      "chunk_id": "2bdfcde24739_0",
      "chapter": "gan",
      "heading": "Discriminator",
      "text": "For the discriminator we will be a bit more discriminating: we will use an MLP with 3 layers to make things a bit more interesting.\n\n```{.python .input}\n#@tab mxnet\nnet_D = nn.Sequential()\nnet_D.add(nn.Dense(5, activation='tanh'),\n          nn.Dense(3, activation='tanh'),\n          nn.Dense(1))\n```\n\n```{.python .input}\n#@tab pytorch\nnet_D = nn.Sequential(\n    nn.Linear(2, 5), nn.Tanh(),\n    nn.Linear(5, 3), nn.Tanh(),\n    nn.Linear(3, 1))\n```\n\n```{.python .input}\n#@tab tensorflow\nnet_D = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(5, activation=\"tanh\", input_shape=(2,)),\n    tf.keras.layers.Dense(3, activation=\"tanh\"),\n    tf.keras.layers.Dense(1)\n])\n```"
    },
    {
      "chunk_id": "0f3596c17543_0",
      "chapter": "gan",
      "heading": "Training",
      "text": "First we define a function to update the discriminator. ```{.python .input}\n#@tab mxnet\n#@save\ndef update_D(X, Z, net_D, net_G, loss, trainer_D):\n    \"\"\"Update discriminator.\"\"\"\n    batch_size = X.shape[0]\n    ones = np.ones((batch_size,), ctx=X.ctx)\n    zeros = np.zeros((batch_size,), ctx=X.ctx)\n    with autograd.record():\n        real_Y = net_D(X)\n        fake_X = net_G(Z)\n        # Do not need to compute gradient for `net_G`, detach it from\n        # computing gradients. fake_Y = net_D(fake_X.detach())\n        loss_D = (loss(real_Y, ones) + loss(fake_Y, zeros)) / 2\n    loss_D.backward()\n    trainer_D.step(batch_size)\n    return float(loss_D.sum())\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef update_D(X, Z, net_D, net_G, loss, trainer_D):\n    \"\"\"Update discriminator.\"\"\"\n    batch_size = X.shape[0]\n    ones = torch.ones((batch_size,), device=X.device)\n    zeros = torch.zeros((batch_size,), device=X.device)\n    trainer_D.zero_grad()\n    real_Y = net_D(X)\n    fake_X = net_G(Z)\n    # Do not need to compute gradient for `net_G`, detach it from\n    # computing gradients."
    },
    {
      "chunk_id": "0f3596c17543_1",
      "chapter": "gan",
      "heading": "Training",
      "text": "fake_Y = net_D(fake_X.detach())\n    loss_D = (loss(real_Y, ones.reshape(real_Y.shape)) +\n              loss(fake_Y, zeros.reshape(fake_Y.shape))) / 2\n    loss_D.backward()\n    trainer_D.step()\n    return loss_D\n```\n\n```{.python .input}\n#@tab tensorflow\n#@save\ndef update_D(X, Z, net_D, net_G, loss, optimizer_D):\n    \"\"\"Update discriminator.\"\"\"\n    batch_size = X.shape[0]\n    ones = tf.ones((batch_size,)) # Labels corresponding to real data\n    zeros = tf.zeros((batch_size,)) # Labels corresponding to fake data\n    # Do not need to compute gradient for `net_G`, so it is outside GradientTape\n    fake_X = net_G(Z)\n    with tf.GradientTape() as tape:\n        real_Y = net_D(X)\n        fake_Y = net_D(fake_X)\n        # We multiply the loss by batch_size to match PyTorch's BCEWithLogitsLoss\n        loss_D = (loss(ones, tf.squeeze(real_Y)) + loss(\n            zeros, tf.squeeze(fake_Y))) * batch_size / 2\n    grads_D = tape.gradient(loss_D, net_D.trainable_variables)\n    optimizer_D.apply_gradients(zip(grads_D, net_D.trainable_variables))\n    return loss_D\n```\n\nThe generator is updated similarly. Here we reuse the cross-entropy loss but change the label of the fake data from $0$ to $1$."
    },
    {
      "chunk_id": "0f3596c17543_2",
      "chapter": "gan",
      "heading": "Training",
      "text": "Here we reuse the cross-entropy loss but change the label of the fake data from $0$ to $1$. ```{.python .input}\n#@tab mxnet\n#@save\ndef update_G(Z, net_D, net_G, loss, trainer_G):\n    \"\"\"Update generator.\"\"\"\n    batch_size = Z.shape[0]\n    ones = np.ones((batch_size,), ctx=Z.ctx)\n    with autograd.record():\n        # We could reuse `fake_X` from `update_D` to save computation\n        fake_X = net_G(Z)\n        # Recomputing `fake_Y` is needed since `net_D` is changed\n        fake_Y = net_D(fake_X)\n        loss_G = loss(fake_Y, ones)\n    loss_G.backward()\n    trainer_G.step(batch_size)\n    return float(loss_G.sum())\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef update_G(Z, net_D, net_G, loss, trainer_G):\n    \"\"\"Update generator.\"\"\"\n    batch_size = Z.shape[0]\n    ones = torch.ones((batch_size,), device=Z.device)\n    trainer_G.zero_grad()\n    # We could reuse `fake_X` from `update_D` to save computation\n    fake_X = net_G(Z)\n    # Recomputing `fake_Y` is needed since `net_D` is changed\n    fake_Y = net_D(fake_X)\n    loss_G = loss(fake_Y, ones.reshape(fake_Y.shape))\n    loss_G.backward()\n    trainer_G.step()\n    return loss_G\n```\n\n```{.python .input}\n#@tab tensorflow\n#@save\ndef update_G(Z, net_D, net_G, loss, optimizer_G):\n    \"\"\"Update generator.\"\"\"\n    batch_size = Z.shape[0]\n    ones = tf.ones((batch_size,))\n    with tf.GradientTape() as tape:\n        # We could reuse `fake_X` from `update_D` to save computation\n        fake_X = net_G(Z)\n        # Recomputing `fake_Y` is needed since `net_D` is changed\n        fake_Y = net_D(fake_X)\n        # We multiply the loss by batch_size to match PyTorch's BCEWithLogits loss\n        loss_G = loss(ones, tf.squeeze(fake_Y)) * batch_size\n    grads_G = tape.gradient(loss_G, net_G.trainable_variables)\n    optimizer_G.apply_gradients(zip(grads_G, net_G.trainable_variables))\n    return loss_G\n```\n\nBoth the discriminator and the generator performs a binary logistic regression with the cross-entropy loss. We use Adam to smooth the training process."
    },
    {
      "chunk_id": "0f3596c17543_3",
      "chapter": "gan",
      "heading": "Training",
      "text": "We use Adam to smooth the training process. In each iteration, we first update the discriminator and then the generator. We visualize both losses and generated examples."
    },
    {
      "chunk_id": "0f3596c17543_4",
      "chapter": "gan",
      "heading": "Training",
      "text": "We use Adam to smooth the training process. In each iteration, we first update the discriminator and then the generator. We visualize both losses and generated examples. ```{.python .input}\n#@tab mxnet\ndef train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n    loss = gluon.loss.SigmoidBCELoss()\n    net_D.initialize(init=init.Normal(0.02), force_reinit=True)\n    net_G.initialize(init=init.Normal(0.02), force_reinit=True)\n    trainer_D = gluon.Trainer(net_D.collect_params(),\n                              'adam', {'learning_rate': lr_D})\n    trainer_G = gluon.Trainer(net_G.collect_params(),\n                              'adam', {'learning_rate': lr_G})\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n                            legend=['discriminator', 'generator'])\n    animator.fig.subplots_adjust(hspace=0.3)\n    for epoch in range(num_epochs):\n        # Train one epoch\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n        for X in data_iter:\n            batch_size = X.shape[0]\n            Z = np.random.normal(0, 1, size=(batch_size, latent_dim))\n            metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),\n                       update_G(Z, net_D, net_G, loss, trainer_G),\n                       batch_size)\n        # Visualize generated examples\n        Z = np.random.normal(0, 1, size=(100, latent_dim))\n        fake_X = net_G(Z).asnumpy()\n        animator.axes[1].cla()\n        animator.axes[1].scatter(data[:, 0], data[:, 1])\n        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n        animator.axes[1].legend(['real', 'generated'])\n        # Show the losses\n        loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]\n        animator.add(epoch + 1, (loss_D, loss_G))\n    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n          f'{metric[2] / timer.stop():.1f} examples/sec')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n    loss = nn.BCEWithLogitsLoss(reduction='sum')\n    for w in net_D.parameters():\n        nn.init.normal_(w, 0, 0.02)\n    for w in net_G.parameters():\n        nn.init.normal_(w, 0, 0.02)\n    trainer_D = torch.optim.Adam(net_D.parameters(), lr=lr_D)\n    trainer_G = torch.optim.Adam(net_G.parameters(), lr=lr_G)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n                            legend=['discriminator', 'generator'])\n    animator.fig.subplots_adjust(hspace=0.3)\n    for epoch in range(num_epochs):\n        # Train one epoch\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n        for (X,) in data_iter:\n            batch_size = X.shape[0]\n            Z = torch.normal(0, 1, size=(batch_size, latent_dim))\n            metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),\n                       update_G(Z, net_D, net_G, loss, trainer_G),\n                       batch_size)\n        # Visualize generated examples\n        Z = torch.normal(0, 1, size=(100, latent_dim))\n        fake_X = net_G(Z).detach().numpy()\n        animator.axes[1].cla()\n        animator.axes[1].scatter(data[:, 0], data[:, 1])\n        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n        animator.axes[1].legend(['real', 'generated'])\n        # Show the losses\n        loss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]\n        animator.add(epoch + 1, (loss_D, loss_G))\n    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n          f'{metric[2] / timer.stop():.1f} examples/sec')\n```\n\n```{.python .input}\n#@tab tensorflow\ndef train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\n    loss = tf.keras.losses.BinaryCrossentropy(\n        from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n    for w in net_D.trainable_variables:\n        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n    for w in net_G.trainable_variables:\n        w.assign(tf.random.normal(mean=0, stddev=0.02, shape=w.shape))\n    optimizer_D = tf.keras.optimizers.Adam(learning_rate=lr_D)\n    optimizer_G = tf.keras.optimizers.Adam(learning_rate=lr_G)\n    animator = d2l.Animator(\n        xlabel=\"epoch\", ylabel=\"loss\", xlim=[1, num_epochs], nrows=2,\n        figsize=(5, 5), legend=[\"discriminator\", \"generator\"])\n    animator.fig.subplots_adjust(hspace=0.3)\n    for epoch in range(num_epochs):\n        # Train one epoch\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n        for (X,) in data_iter:\n            batch_size = X.shape[0]\n            Z = tf.random.normal(\n                mean=0, stddev=1, shape=(batch_size, latent_dim))\n            metric.add(update_D(X, Z, net_D, net_G, loss, optimizer_D),\n                       update_G(Z, net_D, net_G, loss, optimizer_G),\n                       batch_size)\n        # Visualize generated examples\n        Z = tf.random.normal(mean=0, stddev=1, shape=(100, latent_dim))\n        fake_X = net_G(Z)\n        animator.axes[1].cla()\n        animator.axes[1].scatter(data[:, 0], data[:, 1])\n        animator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\n        animator.axes[1].legend([\"real\", \"generated\"])\n\n        # Show the losses\n        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n        animator.add(epoch + 1, (loss_D, loss_G))\n\n    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n          f'{metric[2] / timer.stop():.1f} examples/sec')\n```\n\nNow we specify the hyperparameters to fit the Gaussian distribution."
    },
    {
      "chunk_id": "0f3596c17543_5",
      "chapter": "gan",
      "heading": "Training",
      "text": "```{.python .input}\n#@tab all\nlr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20\ntrain(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\n      latent_dim, d2l.numpy(data[:100]))\n```"
    },
    {
      "chunk_id": "69ff6bc718ab_0",
      "chapter": "gan",
      "heading": "Summary",
      "text": "* Generative adversarial networks (GANs) composes of two deep networks, the generator and the discriminator.\n* The generator generates the image as much closer to the true image as possible to fool the discriminator, via maximizing the cross-entropy loss, *i.e.*, $\\max \\log(D(\\mathbf{x'}))$.\n* The discriminator tries to distinguish the generated images from the true images, via minimizing the cross-entropy loss, *i.e.*, $\\min - y \\log D(\\mathbf{x}) - (1-y)\\log(1-D(\\mathbf{x}))$."
    },
    {
      "chunk_id": "3a04b91abe58_0",
      "chapter": "gan",
      "heading": "Exercises",
      "text": "* Does an equilibrium exist where the generator wins, *i.e.* the discriminator ends up unable to distinguish the two distributions on finite samples?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/408)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1082)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Generative Adversarial Networks\n:label:`chap_gans`\n\n```toc\n:maxdepth: 2\n\ngan\ndcgan\n```"
    },
    {
      "chunk_id": "20c6f2a56f26_0",
      "chapter": "hyperopt-api",
      "heading": "hyperopt-api",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n```\n\n# Hyperparameter Optimization API\n:label:`sec_api_hpo`\n\nBefore we dive into the methodology, we will first discuss a basic code\nstructure that allows us to efficiently implement various HPO algorithms. In\ngeneral, all HPO algorithms considered here need to implement two decision\nmaking primitives, *searching* and *scheduling*. First, they need to sample new\nhyperparameter configurations, which often involves some kind of search over the\nconfiguration space. Second, for each configuration, an HPO algorithm needs to\nschedule its evaluation and decide how many resources to allocate for it. Once\nwe start to evaluate a configuration, we will refer to it as a *trial*. We map\nthese decisions to two classes, `HPOSearcher` and `HPOScheduler`. On top of that,\nwe also provide a `HPOTuner` class that executes the optimization process.\n\nThis concept of scheduler and searcher is also implemented in popular HPO\nlibraries, such as Syne Tune :cite:`salinas-automl22`, Ray Tune\n:cite:`liaw-arxiv18` or Optuna :cite:`akiba-sigkdd19`.\n\n```{.python .input  n=2}\n%%tab pytorch\nimport time\nfrom d2l import torch as d2l\nfrom scipy import stats\n```"
    },
    {
      "chunk_id": "c6fef8d09c18_0",
      "chapter": "hyperopt-api",
      "heading": "Searcher",
      "text": "Below we define a base class for searchers, which provides a new candidate\nconfiguration through the `sample_configuration` function. A simple way to\nimplement this function would be to sample configurations uniformly at random,\nas we did for random search in :numref:`sec_what_is_hpo`. More sophisticated\nalgorithms, such as Bayesian optimization, will make these\ndecisions based on the performance of previous trials. As a result, these\nalgorithms are able to sample more promising candidates over time. We add the\n`update` function in order to update the history of previous trials, which can\nthen be exploited to improve our sampling distribution.\n\n```{.python .input  n=3}\n%%tab pytorch\nclass HPOSearcher(d2l.HyperParameters):  #@save\n    def sample_configuration() -> dict:\n        raise NotImplementedError\n\n    def update(self, config: dict, error: float, additional_info=None):\n        pass\n```\n\nThe following code shows how to implement our random search optimizer from the\nprevious section in this API. As a slight extension, we allow the user to\nprescribe the first configuration to be evaluated via `initial_config`, while\nsubsequent ones are drawn at random.\n\n```{.python .input  n=4}\n%%tab pytorch\nclass RandomSearcher(HPOSearcher):  #@save\n    def __init__(self, config_space: dict, initial_config=None):\n        self.save_hyperparameters()\n\n    def sample_configuration(self) -> dict:\n        if self.initial_config is not None:\n            result = self.initial_config\n            self.initial_config = None\n        else:\n            result = {\n                name: domain.rvs()\n                for name, domain in self.config_space.items()\n            }\n        return result\n```"
    },
    {
      "chunk_id": "e9d08511aaae_0",
      "chapter": "hyperopt-api",
      "heading": "Scheduler",
      "text": "Beyond sampling configurations for new trials, we also need to decide when and\nfor how long to run a trial. In practice, all these decisions are done by the\n`HPOScheduler`, which delegates the choice of new configurations to a\n`HPOSearcher`. The `suggest` method is called whenever some resource for training\nbecomes available. Apart from invoking `sample_configuration` of a searcher, it\nmay also decide upon parameters like `max_epochs` (i.e., how long to train the\nmodel for). The `update` method is called whenever a trial returns a new\nobservation.\n\n```{.python .input  n=5}\n%%tab pytorch\nclass HPOScheduler(d2l.HyperParameters):  #@save\n    def suggest(self) -> dict:\n        raise NotImplementedError\n    \n    def update(self, config: dict, error: float, info=None):\n        raise NotImplementedError\n```\n\nTo implement random search, but also other HPO algorithms, we only need a basic\nscheduler that schedules a new configuration every time new resources become\navailable.\n\n```{.python .input  n=6}\n%%tab pytorch\nclass BasicScheduler(HPOScheduler):  #@save\n    def __init__(self, searcher: HPOSearcher):\n        self.save_hyperparameters()\n\n    def suggest(self) -> dict:\n        return self.searcher.sample_configuration()\n\n    def update(self, config: dict, error: float, info=None):\n        self.searcher.update(config, error, additional_info=info)\n```"
    },
    {
      "chunk_id": "9811fa52cfcf_0",
      "chapter": "hyperopt-api",
      "heading": "Tuner",
      "text": "Finally, we need a component that runs the scheduler/searcher and does some\nbook-keeping of the results. The following code implements a sequential\nexecution of the HPO trials that evaluates one training job after the next and\nwill serve as a basic example. We will later use *Syne Tune* for more scalable\ndistributed HPO cases.\n\n```{.python .input  n=7}\n%%tab pytorch\nclass HPOTuner(d2l.HyperParameters):  #@save\n    def __init__(self, scheduler: HPOScheduler, objective: callable):\n        self.save_hyperparameters()\n        # Bookeeping results for plotting\n        self.incumbent = None\n        self.incumbent_error = None\n        self.incumbent_trajectory = []\n        self.cumulative_runtime = []\n        self.current_runtime = 0\n        self.records = []\n\n    def run(self, number_of_trials):\n        for i in range(number_of_trials):\n            start_time = time.time()\n            config = self.scheduler.suggest()\n            print(f\"Trial {i}: config = {config}\")\n            error = self.objective(**config)\n            error = float(d2l.numpy(error.cpu()))\n            self.scheduler.update(config, error)\n            runtime = time.time() - start_time\n            self.bookkeeping(config, error, runtime)\n            print(f\"    error = {error}, runtime = {runtime}\")\n```"
    },
    {
      "chunk_id": "dcb3b81006b6_0",
      "chapter": "hyperopt-api",
      "heading": "Bookkeeping the Performance of HPO Algorithms",
      "text": "With any HPO algorithm, we are mostly interested in the best performing\nconfiguration (called *incumbent*) and its validation error after a given \nwall-clock time. This is why we track `runtime` per iteration, which includes\nboth the time to run an evaluation (call of `objective`) and the time to\nmake a decision (call of `scheduler.suggest`). In the sequel, we will plot\n`cumulative_runtime` against `incumbent_trajectory` in  order to visualize the\n*any-time performance* of the HPO algorithm defined in  terms of `scheduler`\n(and `searcher`). This allows us to quantify not only how well the configuration\nfound by an optimizer works, but also how quickly an optimizer is able to find it.\n\n```{.python .input  n=8}\n%%tab pytorch\n@d2l.add_to_class(HPOTuner)  #@save\ndef bookkeeping(self, config: dict, error: float, runtime: float):\n    self.records.append({\"config\": config, \"error\": error, \"runtime\": runtime})\n    # Check if the last hyperparameter configuration performs better \n    # than the incumbent\n    if self.incumbent is None or self.incumbent_error > error:\n        self.incumbent = config\n        self.incumbent_error = error\n    # Add current best observed performance to the optimization trajectory\n    self.incumbent_trajectory.append(self.incumbent_error)\n    # Update runtime\n    self.current_runtime += runtime\n    self.cumulative_runtime.append(self.current_runtime)\n```"
    },
    {
      "chunk_id": "5618c04747ba_0",
      "chapter": "hyperopt-api",
      "heading": "Example: Optimizing the Hyperparameters of a Convolutional Neural Network",
      "text": "We now use our new implementation of random search to optimize the\n*batch size* and *learning rate* of the `LeNet` convolutional neural network\nfrom :numref:`sec_lenet`. We being by defining the objective function, which\nwill once more be validation error.\n\n```{.python .input  n=9}\n%%tab pytorch\ndef hpo_objective_lenet(learning_rate, batch_size, max_epochs=10):  #@save\n    model = d2l.LeNet(lr=learning_rate, num_classes=10)\n    trainer = d2l.HPOTrainer(max_epochs=max_epochs, num_gpus=1)\n    data = d2l.FashionMNIST(batch_size=batch_size)\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n    trainer.fit(model=model, data=data)\n    validation_error = trainer.validation_error()\n    return validation_error\n```\n\nWe also need to define the configuration space. Moreover, the first configuration\nto be evaluated is the default setting used in :numref:`sec_lenet`.\n\n```{.python .input  n=10}\nconfig_space = {\n    \"learning_rate\": stats.loguniform(1e-2, 1),\n    \"batch_size\": stats.randint(32, 256),\n}\ninitial_config = {\n    \"learning_rate\": 0.1,\n    \"batch_size\": 128,\n}\n```\n\nNow we can start our random search:\n\n```{.python .input}\nsearcher = RandomSearcher(config_space, initial_config=initial_config)\nscheduler = BasicScheduler(searcher=searcher)\ntuner = HPOTuner(scheduler=scheduler, objective=hpo_objective_lenet)\ntuner.run(number_of_trials=5)\n```\n\nBelow we plot the optimization trajectory of the incumbent to get the any-time\nperformance of random search:\n\n```{.python .input  n=11}\nboard = d2l.ProgressBoard(xlabel=\"time\", ylabel=\"error\")\nfor time_stamp, error in zip(\n    tuner.cumulative_runtime, tuner.incumbent_trajectory\n):\n    board.draw(time_stamp, error, \"random search\", every_n=1)\n```"
    },
    {
      "chunk_id": "93c0f66f7349_0",
      "chapter": "hyperopt-api",
      "heading": "Comparing HPO Algorithms",
      "text": "Just as with training algorithms or model architectures, it is important to\nunderstand how to best compare different HPO algorithms. Each HPO run depends\non two major sources of randomness: the random effects of the training process,\nsuch as random weight initialization or mini-batch ordering, and the intrinsic\nrandomness of the HPO algorithm itself, such as the random sampling of random\nsearch. Hence, when comparing different algorithms, it is crucial to run each\nexperiment several times and report statistics, such as mean or median, across\na population of multiple repetitions of an algorithm based on different seeds\nof the random number generator.\n\nTo illustrate this, we compare random search (see :numref:`sec_rs`) and Bayesian\noptimization :cite:`snoek-nips12` on tuning the hyperparameters of a feed-forward\nneural network. Each algorithm was evaluated\n$50$ times with a different random seed. The solid line indicates the average\nperformance of the incumbent across these $50$ repetitions and the dashed line\nthe standard deviation. We can see that random search and Bayesian optimization\nperform roughly the same up to ~1000 seconds, but Bayesian optimization can\nmake use of the past observation to identify better configurations and thus\nquickly outperforms random search afterwards.\n\n\n![Example any-time performance plot to compare two algorithms A and B.](../img/example_anytime_performance.svg)\n:label:`example_anytime_performance`"
    },
    {
      "chunk_id": "08eb71fedec3_0",
      "chapter": "hyperopt-api",
      "heading": "Summary",
      "text": "This section laid out a simple, yet flexible interface to implement various HPO\nalgorithms that we will look at in this chapter. Similar interfaces can be found\nin popular open-source HPO frameworks. We also looked at how we can compare HPO\nalgorithms, and potential pitfall one needs to be aware."
    },
    {
      "chunk_id": "ff480b891de5_0",
      "chapter": "hyperopt-api",
      "heading": "Exercises",
      "text": "1. The goal of this exercise is to implement the objective function for a slightly more challenging HPO problem, and to run more realistic experiments. We will use the two hidden layer MLP `DropoutMLP` implemented in :numref:`sec_dropout`. 1. Code up the objective function, which should depend on all hyperparameters of the model and `batch_size`. Use `max_epochs=50`. GPUs do not help here, so `num_gpus=0`. Hint: Modify `hpo_objective_lenet`. 2. Choose a sensible search space, where `num_hiddens_1`, `num_hiddens_2` are integers in $[8, 1024]$, and dropout values lie in $[0, 0.95]$, while `batch_size` lies in $[16, 384]$. Provide code for `config_space`, using sensible distributions from `scipy.stats`. 3. Run random search on this example with `number_of_trials=20` and plot the results. Make sure to first evaluate the default configuration of :numref:`sec_dropout`, which is `initial_config = {'num_hiddens_1': 256, 'num_hiddens_2': 256, 'dropout_1': 0.5, 'dropout_2': 0.5, 'lr': 0.1, 'batch_size': 256}`. 2. In this exercise, you will implement a new searcher (subclass of `HPOSearcher`) which makes decisions based on past data. It depends on parameters `probab_local`, `num_init_random`. Its `sample_configuration` method works as follows. For the first `num_init_random` calls, do the same as `RandomSearcher.sample_configuration`. Otherwise, with probability `1 - probab_local`, do the same as `RandomSearcher.sample_configuration`. Otherwise, pick the configuration which attained the smallest validation error so far, select one of its hyperparameters at random, and sample its value randomly like in `RandomSearcher.sample_configuration`, but leave all other values the same. Return this configuration, which is identical to the best configuration so far, except in this one hyperparameter. 1. Code up this new `LocalSearcher`. Hint: Your searcher requires `config_space` as argument at construction. Feel free to use a member of type `RandomSearcher`. You will also have to implement the `update` method. 2."
    },
    {
      "chunk_id": "ff480b891de5_1",
      "chapter": "hyperopt-api",
      "heading": "Exercises",
      "text": "1. Code up this new `LocalSearcher`. Hint: Your searcher requires `config_space` as argument at construction. Feel free to use a member of type `RandomSearcher`. You will also have to implement the `update` method. 2. Re-run the experiment from the previous exercise, but using your new searcher instead of `RandomSearcher`. Experiment with different values for `probab_local`, `num_init_random`. However, note that a proper comparison between different HPO methods requires repeating experiments several times, and ideally considering a number of benchmark tasks. :begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12092)\n:end_tab:"
    },
    {
      "chunk_id": "d00c330c4c90_0",
      "chapter": "hyperopt-intro",
      "heading": "hyperopt-intro",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n```\n\n# What Is Hyperparameter Optimization? :label:`sec_what_is_hpo`\n\nAs we have seen in the previous chapters, deep neural networks come with a\nlarge number of parameters or weights that are learned during training. On\ntop of these, every neural network has additional *hyperparameters* that need\nto be configured by the user. For example, to ensure that stochastic gradient\ndescent converges to a local optimum of the training loss\n(see :numref:`chap_optimization`), we have to adjust the learning rate and batch\nsize. To avoid overfitting on training datasets,\nwe might have to set regularization parameters, such as weight decay\n(see :numref:`sec_weight_decay`) or dropout (see :numref:`sec_dropout`). We can\ndefine the capacity and inductive bias of the model by setting the number of\nlayers and number of units or filters per layer (i.e., the effective number\nof weights). Unfortunately, we cannot simply adjust these hyperparameters by minimizing the\ntraining loss, because this would lead to overfitting on the training data. For\nexample, setting regularization parameters, such as dropout or weight decay\nto zero leads to a small training loss, but might hurt the generalization\nperformance. ![Typical workflow in machine learning that consists of training the model multiple times with different hyperparameters.](../img/ml_workflow.svg)\n:label:`ml_workflow`\n\nWithout a different form of automation, hyperparameters have to be set manually\nin a trial-and-error fashion, in what amounts to a time-consuming and difficult\npart of machine learning workflows. For example, consider training\na ResNet (see :numref:`sec_resnet`) on CIFAR-10, which requires more than 2 hours\non an Amazon Elastic Cloud Compute (EC2) `g4dn.xlarge` instance. Even just\ntrying ten hyperparameter configurations in sequence, this would already take us\nroughly one day."
    },
    {
      "chunk_id": "d00c330c4c90_1",
      "chapter": "hyperopt-intro",
      "heading": "hyperopt-intro",
      "text": "Even just\ntrying ten hyperparameter configurations in sequence, this would already take us\nroughly one day. To make matters worse, hyperparameters are usually not directly\ntransferable across architectures and datasets\n:cite:`feurer-arxiv22,wistuba-ml18,bardenet-icml13a`, and need to be re-optimized\nfor every new task. Also, for most hyperparameters, there are no rule-of-thumbs,\nand expert knowledge is required to find sensible values. *Hyperparameter optimization (HPO)* algorithms are designed to tackle this\nproblem in a principled and automated fashion :cite:`feurer-automlbook18a`, by\nframing it as a global optimization problem. The default objective is the error\non a hold-out validation dataset, but could in principle be any other business\nmetric. It can be combined with or constrained by secondary objectives, such as\ntraining time, inference time, or model complexity. Recently, hyperparameter optimization has been extended to *neural architecture\nsearch (NAS)* :cite:`elsken-arxiv18a,wistuba-arxiv19`, where the goal is to find\nentirely new neural network architectures. Compared to classical HPO, NAS is even\nmore expensive in terms of computation and requires additional efforts to remain\nfeasible in practice. Both, HPO and NAS can be considered as sub-fields of \nAutoML :cite:`hutter-book19a`, which aims to automate the entire ML pipeline. In this section we will introduce HPO and show how we can automatically find\nthe best hyperparameters of the logistic regression example introduced in\n:numref:`sec_softmax_concise`."
    },
    {
      "chunk_id": "9bcbbd924a60_0",
      "chapter": "hyperopt-intro",
      "heading": "The Optimization Problem",
      "text": ":label:`sec_definition_hpo`\n\nWe will start with a simple toy problem: searching for the learning rate of the\nmulti-class logistic regression model `SoftmaxRegression` from\n:numref:`sec_softmax_concise` to minimize the validation error on the Fashion\nMNIST dataset. While other hyperparameters like batch size or number of epochs\nare also worth tuning, we focus on learning rate alone for simplicity.\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom scipy import stats\n```\n\nBefore we can run HPO, we first need to define two ingredients: the objective\nfunction and the configuration space."
    },
    {
      "chunk_id": "d5641b3b3f50_0",
      "chapter": "hyperopt-intro",
      "heading": "The Objective Function",
      "text": "The performance of a learning algorithm can be seen as a function\n$f: \\mathcal{X} \\rightarrow \\mathbb{R}$ that maps from the hyperparameter space\n$\\mathbf{x} \\in \\mathcal{X}$ to the validation loss. For every evaluation of\n$f(\\mathbf{x})$, we have to train and validate our machine learning model, which\ncan be time and compute intensive in the case of deep neural networks trained on\nlarge datasets. Given our criterion $f(\\mathbf{x})$ our goal is to find\n$\\mathbf{x}_{\\star} \\in \\mathrm{argmin}_{\\mathbf{x} \\in \\mathcal{X}} f(\\mathbf{x})$. There is no simple way to compute gradients of $f$ with respect to $\\mathbf{x}$,\nbecause it would require to propagate the gradient through the entire training\nprocess. While there is recent work :cite:`maclaurin-icml15,franceschi-icml17a`\nto drive HPO by approximate \"hypergradients\", none of the existing approaches\nare competitive with the state-of-the-art yet, and we will not discuss them\nhere. Furthermore, the computational burden of evaluating $f$ requires HPO\nalgorithms to approach the global optimum with as few samples as possible. The training of neural networks is stochastic (e.g., weights are randomly\ninitialized, mini-batches are randomly sampled), so that our observations will\nbe noisy: $y \\sim f(\\mathbf{x}) + \\epsilon$, where we usually assume that the\n$\\epsilon \\sim N(0, \\sigma)$ observation noise is Gaussian distributed. Faced with all these challenges, we usually try to identify a small set of well\nperforming hyperparameter configurations quickly, instead of hitting the global\noptima exactly. However, due to large computational demands of most neural\nnetworks models, even this can take days or weeks of compute. We will explore\nin :numref:`sec_mf_hpo` how we can speed-up the optimization process by either\ndistributing the search or using cheaper-to-evaluate approximations of the\nobjective function. We begin with a method for computing the validation error of a model."
    },
    {
      "chunk_id": "d5641b3b3f50_1",
      "chapter": "hyperopt-intro",
      "heading": "The Objective Function",
      "text": "We begin with a method for computing the validation error of a model. ```{.python .input  n=8}\n%%tab pytorch\nclass HPOTrainer(d2l.Trainer):  #@save\n    def validation_error(self):\n        self.model.eval()\n        accuracy = 0\n        val_batch_idx = 0\n        for batch in self.val_dataloader:\n            with torch.no_grad():\n                x, y = self.prepare_batch(batch)\n                y_hat = self.model(x)\n                accuracy += self.model.accuracy(y_hat, y)\n            val_batch_idx += 1\n        return 1 -  accuracy / val_batch_idx\n```\n\nWe optimize validation error with respect to the hyperparameter configuration\n`config`, consisting of the `learning_rate`. For each evaluation, we train our\nmodel for `max_epochs` epochs, then compute and return its validation error:\n\n```{.python .input  n=5}\n%%tab pytorch\ndef hpo_objective_softmax_classification(config, max_epochs=8):\n    learning_rate = config[\"learning_rate\"]\n    trainer = d2l.HPOTrainer(max_epochs=max_epochs)\n    data = d2l.FashionMNIST(batch_size=16)\n    model = d2l.SoftmaxRegression(num_outputs=10, lr=learning_rate)\n    trainer.fit(model=model, data=data)\n    return d2l.numpy(trainer.validation_error())\n```"
    },
    {
      "chunk_id": "51d8d77c90d9_0",
      "chapter": "hyperopt-intro",
      "heading": "The Configuration Space",
      "text": ":label:`sec_intro_config_spaces`\n\nAlong with the objective function $f(\\mathbf{x})$, we also need to define the\nfeasible set $\\mathbf{x} \\in \\mathcal{X}$ to optimize over, known as\n*configuration space* or *search space*. For our logistic regression example,\nwe will use:\n\n```{.python .input  n=6}\nconfig_space = {\"learning_rate\": stats.loguniform(1e-4, 1)}\n```\n\nHere we use the use the `loguniform` object from SciPy, which represents a\nuniform distribution between -4 and -1 in the logarithmic space. This object\nallows us to sample random variables from this distribution. Each hyperparameter has a data type, such as `float` for `learning_rate`, as\nwell as a closed bounded range (i.e., lower and upper bounds). We usually assign\na prior distribution (e.g, uniform or log-uniform) to each hyperparameter to\nsample from. Some positive parameters, such as `learning_rate`, are best\nrepresented on a logarithmic scale as optimal values can differ by several\norders of magnitude, while others, such as momentum, come with linear scale. Below we show a simple example of a configuration space consisting of typical\nhyperparameters of a multi-layer perceptron including their type and standard\nranges."
    },
    {
      "chunk_id": "51d8d77c90d9_1",
      "chapter": "hyperopt-intro",
      "heading": "The Configuration Space",
      "text": "Below we show a simple example of a configuration space consisting of typical\nhyperparameters of a multi-layer perceptron including their type and standard\nranges. : Example configuration space of multi-layer perceptron\n:label:`tab_example_configspace`\n\n| Name                | Type        |Hyperparameter Ranges           | log-scale |\n| :----:              | :----:      |:------------------------------:|:---------:|\n| learning rate       | float       |      $[10^{-6},10^{-1}]$       |    yes    |\n| batch size          | integer     |           $[8,256]$            |    yes    |\n| momentum            | float       |           $[0,0.99]$           |    no     |\n| activation function | categorical | $\\{\\textrm{tanh}, \\textrm{relu}\\}$ |     -     |\n| number of units     | integer     |          $[32, 1024]$          |    yes    |\n| number of layers    | integer     |            $[1, 6]$            |    no     |\n\n\n\nIn general, the structure of the configuration space $\\mathcal{X}$ can be complex\nand it can be quite different from $\\mathbb{R}^d$. In practice, some\nhyperparameters may depend on the value of others. For example, assume we try\nto tune the number of layers for a multi-layer perceptron, and for each layer\nthe number of units. The number of units of the $l\\textrm{-th}$ layer is\nrelevant only if the network has at least $l+1$ layers. These advanced HPO\nproblems are beyond the scope of this chapter. We refer the interested reader\nto :cite:`hutter-lion11a,jenatton-icml17a,baptista-icml18a`. The configuration space plays an important role for hyperparameter optimization,\nsince no algorithms can find something that is not included in the configuration\nspace. On the other hand, if the ranges are too large, the computation budget\nto find well performing configurations might become infeasible."
    },
    {
      "chunk_id": "fea77362a0c2_0",
      "chapter": "hyperopt-intro",
      "heading": "Random Search",
      "text": ":label:`sec_rs`\n\n*Random search* is the first hyperparameter optimization algorithm we will\nconsider. The main idea of random search is to independently sample from the\nconfiguration space until a predefined budget (e.g maximum\nnumber of iterations) is exhausted, and to return the best observed\nconfiguration. All evaluations can be executed independently in parallel (see\n:numref:`sec_rs_async`), but here we use a sequential loop for simplicity. ```{.python .input  n=7}\nerrors, values = [], []\nnum_iterations = 5\n\nfor i in range(num_iterations):\n    learning_rate = config_space[\"learning_rate\"].rvs()\n    print(f\"Trial {i}: learning_rate = {learning_rate}\")\n    y = hpo_objective_softmax_classification({\"learning_rate\": learning_rate})\n    print(f\"    validation_error = {y}\")\n    values.append(learning_rate)\n    errors.append(y)\n```\n\nThe best learning rate is then simply the one with the lowest validation error. ```{.python .input  n=7}\nbest_idx = np.argmin(errors)\nprint(f\"optimal learning rate = {values[best_idx]}\")\n```\n\nDue to its simplicity and generality, random search is one of the most frequently\nused HPO algorithms. It does not require any sophisticated implementation and\ncan be applied to any configuration space as long as we can define some\nprobability distribution for each hyperparameter. Unfortunately random search also comes with a few shortcomings. First, it does\nnot adapt the sampling distribution based on the previous observations it\ncollected so far. Hence, it is equally likely to sample a poorly performing\nconfiguration than a better performing configuration. Second, the same amount\nof resources are spent for all configurations, even though some may show poor\ninitial performance and are less likely to outperform previously seen\nconfigurations. In the next sections we will look at more sample efficient hyperparameter\noptimization algorithms that overcome the shortcomings of random search by\nusing a model to guide the search."
    },
    {
      "chunk_id": "fea77362a0c2_1",
      "chapter": "hyperopt-intro",
      "heading": "Random Search",
      "text": "In the next sections we will look at more sample efficient hyperparameter\noptimization algorithms that overcome the shortcomings of random search by\nusing a model to guide the search. We will also look at algorithms that\nautomatically stop the evaluation process of poorly performing configurations\nto speed up the optimization process."
    },
    {
      "chunk_id": "f54dbc10a23c_0",
      "chapter": "hyperopt-intro",
      "heading": "Summary",
      "text": "In this section we introduced hyperparameter optimization (HPO) and how we can\nphrase it as a global optimization by defining a configuration space and an\nobjective function. We also implemented our first HPO algorithm, random search,\nand applied it on a simple softmax classification problem.\n\nWhile random search is very simple, it is the better alternative to grid\nsearch, which simply evaluates a fixed set of hyperparameters. Random search\nsomewhat mitigates the curse of dimensionality :cite:`bellman-science66`, and\ncan be far more efficient than grid search if the criterion most strongly\ndepends on a small subset of the hyperparameters."
    },
    {
      "chunk_id": "c63e76c88dcc_0",
      "chapter": "hyperopt-intro",
      "heading": "Exercises",
      "text": "1. In this chapter, we optimize the validation error of a model after training on a disjoint training set. For simplicity, our code uses `Trainer.val_dataloader`, which maps to a loader around `FashionMNIST.val`. 1. Convince yourself (by looking at the code) that this means we use the original FashionMNIST training set (60000 examples) for training, and the original *test set* (10000 examples) for validation. 2. Why could this practice be problematic? Hint: Re-read :numref:`sec_generalization_basics`, especially about *model selection*. 3. What should we have done instead? 2. We stated above that hyperparameter optimization by gradient descent is very hard to do. Consider a small problem, such as training a two-layer perceptron on the FashionMNIST dataset (:numref:`sec_mlp-implementation`) with a batch size of 256. We would like to tune the learning rate of SGD in order to minimize a validation metric after one epoch of training. 1. Why cannot we use validation *error* for this purpose? What metric on the validation set would you use? 2. Sketch (roughly) the computational graph of the validation metric after training for one epoch. You may assume that initial weights and hyperparameters (such as learning rate) are input nodes to this graph. Hint: Re-read about computational graphs in :numref:`sec_backprop`. 3. Give a rough estimate of the number of floating point values you need to store during a forward pass on this graph. Hint: FashionMNIST has 60000 cases. Assume the required memory is dominated by the activations after each layer, and look up the layer widths in :numref:`sec_mlp-implementation`. 5. Apart from the sheer amount of compute and storage required, what other issues would gradient-based hyperparameter optimization run into? Hint: Re-read about vanishing and exploding gradients in :numref:`sec_numerical_stability`. 6. *Advanced*: Read :cite:`maclaurin-icml15` for an elegant (yet still somewhat unpractical) approach to gradient-based HPO. 3."
    },
    {
      "chunk_id": "c63e76c88dcc_1",
      "chapter": "hyperopt-intro",
      "heading": "Exercises",
      "text": "Hint: Re-read about vanishing and exploding gradients in :numref:`sec_numerical_stability`. 6. *Advanced*: Read :cite:`maclaurin-icml15` for an elegant (yet still somewhat unpractical) approach to gradient-based HPO. 3. Grid search is another HPO baseline, where we define an equi-spaced grid for each hyperparameter, then iterate over the (combinatorial) Cartesian product in order to suggest configurations. 1. We stated above that random search can be much more efficient than grid search for HPO on a sizable number of hyperparameters, if the criterion most strongly depends on a small subset of the hyperparameters. Why is this? Hint: Read :cite:`bergstra2011algorithms`. :begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12090)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Hyperparameter Optimization\n:label:`chap_hyperopt`\n\n**Aaron Klein** (*Amazon*), **Matthias Seeger** (*Amazon*), and **Cedric Archambeau** (*Amazon*)\n\nThe performance of every machine learning model depends on its hyperparameters.\nThey control the learning algorithm or the structure of the underlying\nstatistical model. However, there is no general way to choose hyperparameters\nin practice. Instead, hyperparameters are often set in a trial-and-error manner\nor sometimes left to their default values by practitioners, leading to\nsuboptimal generalization.\n\nHyperparameter optimization provides a systematic approach to this problem, by\ncasting it as an optimization problem: a good set of hyperparameters should (at\nleast) minimize a validation error. Compared to most other optimization problems\narising in machine learning, hyperparameter optimization is a nested one, where\neach iteration requires training and validating a machine learning model.\n\nIn this chapter, we will first introduce the basics of hyperparameter\noptimization. We will also present some recent advancements that improve the\noverall efficiency of hyperparameter optimization by exploiting cheap-to-evaluate\nproxies of the original objective function. At the end of this chapter, you\nshould be able to apply state-of-the-art hyperparameter optimization techniques\nto optimize the hyperparameter of your own machine learning algorithm.\n\n```toc\n:maxdepth: 2\n\nhyperopt-intro\nhyperopt-api\nrs-async.md\nsh-intro\nsh-async\n```"
    },
    {
      "chunk_id": "017455c74d9e_0",
      "chapter": "rs-async",
      "heading": "rs-async",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n#required_libs(\"syne-tune[gpsearchers]==0.3.2\")\n```\n\n# Asynchronous Random Search\n:label:`sec_rs_async`\n\nAs we have seen in the previous :numref:`sec_api_hpo`, we might have to wait\nhours or even days before random search returns a good hyperparameter\nconfiguration, because of the expensive evaluation of hyperparameter\nconfigurations. In practice, we have often access to a pool of resources such as\nmultiple GPUs on the same machine or multiple machines with a single GPU. This\nbegs the question: *How do we efficiently distribute random search?*\n\nIn general, we distinguish between synchronous and asynchronous parallel\nhyperparameter optimization (see :numref:`distributed_scheduling`). In the\nsynchronous setting, we wait for all concurrently running trials to finish,\nbefore we start the next batch. Consider configuration spaces that contain\nhyperparameters such as the number of filters or number of layers of a deep\nneural network. Hyperparameter configurations that contain a larger number of \nlayers of filters will naturally take more time to finish, and all other trials\nin the same batch will have to wait at synchronisation points (grey area in\n:numref:`distributed_scheduling`) before we can continue the optimization\nprocess. In the asynchronous setting we immediately schedule a new trial as soon as resources\nbecome available. This will optimally exploit our resources, since we can avoid any\nsynchronisation overhead. For random search, each new hyperparameter configuration\nis chosen independently of all others, and in particular without exploiting\nobservations from any prior evaluation. This means we can trivially parallelize random\nsearch asynchronously. This is not straight-forward with more sophisticated methods\nthat make decision based on previous observations (see :numref:`sec_sh_async`)."
    },
    {
      "chunk_id": "017455c74d9e_1",
      "chapter": "rs-async",
      "heading": "rs-async",
      "text": "This means we can trivially parallelize random\nsearch asynchronously. This is not straight-forward with more sophisticated methods\nthat make decision based on previous observations (see :numref:`sec_sh_async`). While we need access to more resources than in the sequential setting, asynchronous\nrandom search exhibits a linear speed-up, in that a certain performance is reached\n$K$ times faster if $K$ trials can be run in parallel. ![Distributing the hyperparameter optimization process either synchronously or asynchronously. Compared to the sequential setting, we can reduce the overall wall-clock time while keep the total compute constant. Synchronous scheduling might lead to idling workers in the case of stragglers.](../img/distributed_scheduling.svg)\n:label:`distributed_scheduling`\n\nIn this notebook, we will look at asynchronous random search that, where trials are\nexecuted in multiple python processes on the same machine. Distributed job scheduling\nand execution is difficult to implement from scratch. We will use *Syne Tune*\n:cite:`salinas-automl22`, which provides us with a simple interface for asynchronous\nHPO. Syne Tune is designed to be run with different execution back-ends, and the\ninterested reader is invited to study its simple APIs in order to learn more about\ndistributed HPO. ```{.python .input}\nfrom d2l import torch as d2l\nimport logging\nlogging.basicConfig(level=logging.INFO)\nfrom syne_tune.config_space import loguniform, randint\nfrom syne_tune.backend.python_backend import PythonBackend\nfrom syne_tune.optimizer.baselines import RandomSearch\nfrom syne_tune import Tuner, StoppingCriterion\nfrom syne_tune.experiments import load_experiment\n```"
    },
    {
      "chunk_id": "54c7f1800423_0",
      "chapter": "rs-async",
      "heading": "Objective Function",
      "text": "First, we have to define a new objective function such that it now returns the\nperformance back to Syne Tune via the `report` callback.\n\n```{.python .input  n=34}\ndef hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):\n    from d2l import torch as d2l    \n    from syne_tune import Reporter\n\n    model = d2l.LeNet(lr=learning_rate, num_classes=10)\n    trainer = d2l.HPOTrainer(max_epochs=1, num_gpus=1)\n    data = d2l.FashionMNIST(batch_size=batch_size)\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n    report = Reporter() \n    for epoch in range(1, max_epochs + 1):\n        if epoch == 1:\n            # Initialize the state of Trainer\n            trainer.fit(model=model, data=data) \n        else:\n            trainer.fit_epoch()\n        validation_error = d2l.numpy(trainer.validation_error().cpu())\n        report(epoch=epoch, validation_error=float(validation_error))\n```\n\nNote that the `PythonBackend` of Syne Tune requires dependencies to be imported\ninside the function definition."
    },
    {
      "chunk_id": "162e3d789fcf_0",
      "chapter": "rs-async",
      "heading": "Asynchronous Scheduler",
      "text": "First, we define the number of workers that evaluate trials concurrently. We\nalso need to specify how long we want to run random search, by defining an\nupper limit on the total wall-clock time. ```{.python .input  n=37}\nn_workers = 2  # Needs to be <= the number of available GPUs\n\nmax_wallclock_time = 12 * 60  # 12 minutes\n```\n\nNext, we state which metric we want to optimize and whether we want to minimize or\nmaximize this metric. Namely, `metric` needs to correspond to the argument name\npassed to the `report` callback. ```{.python .input  n=38}\nmode = \"min\"\nmetric = \"validation_error\"\n```\n\nWe use the configuration space from our previous example. In Syne Tune, this\ndictionary can also be used to pass constant attributes to the training script. We make use of this feature in order to pass `max_epochs`. Moreover, we specify\nthe first configuration to be evaluated in `initial_config`. ```{.python .input  n=39}\nconfig_space = {\n    \"learning_rate\": loguniform(1e-2, 1),\n    \"batch_size\": randint(32, 256),\n    \"max_epochs\": 10,\n}\ninitial_config = {\n    \"learning_rate\": 0.1,\n    \"batch_size\": 128,\n}\n```\n\nNext, we need to specify the back-end for job executions. Here we just consider\nthe distribution on a local machine where parallel jobs are executed as\nsub-processes. However, for large scale HPO, we could run this also on a cluster\nor cloud environment, where each trial consumes a full instance. ```{.python .input  n=40}\ntrial_backend = PythonBackend(\n    tune_function=hpo_objective_lenet_synetune,\n    config_space=config_space,\n)\n```\n\nWe can now create the scheduler for asynchronous random search, which is similar\nin behaviour to our `BasicScheduler` from :numref:`sec_api_hpo`. ```{.python .input  n=41}\nscheduler = RandomSearch(\n    config_space,\n    metric=metric,\n    mode=mode,\n    points_to_evaluate=[initial_config],\n)\n```\n\nSyne Tune also features a `Tuner`, where the main experiment loop and\nbookkeeping is centralized, and interactions between scheduler and back-end are\nmediated."
    },
    {
      "chunk_id": "162e3d789fcf_1",
      "chapter": "rs-async",
      "heading": "Asynchronous Scheduler",
      "text": "```{.python .input  n=42}\nstop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)\n\ntuner = Tuner(\n    trial_backend=trial_backend,\n    scheduler=scheduler, \n    stop_criterion=stop_criterion,\n    n_workers=n_workers,\n    print_update_interval=int(max_wallclock_time * 0.6),\n)\n```\n\nLet us run our distributed HPO experiment. According to our stopping criterion,\nit will run for about 12 minutes. ```{.python .input  n=43}\ntuner.run()\n```\n\nThe logs of all evaluated hyperparameter configurations are stored for further\nanalysis. At any time during the tuning job, we can easily get the results\nobtained so far and plot the incumbent trajectory. ```{.python .input  n=46}\nd2l.set_figsize()\ntuning_experiment = load_experiment(tuner.name)\ntuning_experiment.plot()\n```"
    },
    {
      "chunk_id": "ca9261b3c502_0",
      "chapter": "rs-async",
      "heading": "Visualize the Asynchronous Optimization Process",
      "text": "Below we visualize how the learning curves of every trial (each color in the plot represents a trial) evolve during the\nasynchronous optimization process. At any point in time, there are as many trials\nrunning concurrently as we have workers. Once a trial finishes, we immediately\nstart the next trial, without waiting for the other trials to finish. Idle time\nof workers is reduced to a minimum with asynchronous scheduling.\n\n```{.python .input  n=45}\nd2l.set_figsize([6, 2.5])\nresults = tuning_experiment.results\n\nfor trial_id in results.trial_id.unique():\n    df = results[results[\"trial_id\"] == trial_id]\n    d2l.plt.plot(\n        df[\"st_tuner_time\"],\n        df[\"validation_error\"],\n        marker=\"o\"\n    )\n    \nd2l.plt.xlabel(\"wall-clock time\")\nd2l.plt.ylabel(\"objective function\")\n```"
    },
    {
      "chunk_id": "aabd8eaaee82_0",
      "chapter": "rs-async",
      "heading": "Summary",
      "text": "We can reduce the waiting time for random search substantially by distribution\ntrials across parallel resources. In general, we distinguish between synchronous\nscheduling and asynchronous scheduling. Synchronous scheduling means that we\nsample a new batch of hyperparameter configurations once the previous batch\nfinished. If we have a stragglers - trials that takes more time to finish than\nother trials - our workers need to wait at synchronization points. Asynchronous\nscheduling evaluates a new hyperparameter configurations as soon as resources\nbecome available, and, hence, ensures that all workers are busy at any point in\ntime. While random search is easy to distribute asynchronously and does not\nrequire any change of the actual algorithm, other methods require some additional\nmodifications."
    },
    {
      "chunk_id": "839fa465d234_0",
      "chapter": "rs-async",
      "heading": "Exercises",
      "text": "1. Consider the `DropoutMLP` model implemented in :numref:`sec_dropout`, and used in Exercise 1 of :numref:`sec_api_hpo`.\n    1. Implement an objective function `hpo_objective_dropoutmlp_synetune` to be used with Syne Tune. Make sure that your function reports the validation error after every epoch.\n    2. Using the setup of Exercise 1 in :numref:`sec_api_hpo`, compare random search to Bayesian optimization. If you use SageMaker, feel free to use Syne Tune's benchmarking facilities in order to run experiments in parallel. Hint: Bayesian optimization is provided as `syne_tune.optimizer.baselines.BayesianOptimization`.\n    3. For this exercise, you need to run on an instance with at least 4 CPU cores. For one of the methods used above (random search, Bayesian optimization), run experiments with `n_workers=1`, `n_workers=2`, `n_workers=4`, and compare results (incumbent trajectories). At least for random search, you should observe linear scaling with respect to the number of workers. Hint: For robust results, you may have to average over several repetitions each.\n2. *Advanced*. The goal of this exercise is to implement a new scheduler in Syne Tune.\n    1. Create a virtual environment containing both the [d2lbook](https://github.com/d2l-ai/d2l-en/blob/master/INFO.md#installation-for-developers) and [syne-tune](https://syne-tune.readthedocs.io/en/latest/getting_started.html) sources.\n    2. Implement the `LocalSearcher` from Exercise 2 in :numref:`sec_api_hpo` as a new searcher in Syne Tune. Hint: Read [this tutorial](https://syne-tune.readthedocs.io/en/latest/tutorials/developer/README.html). Alternatively, you may follow this [example](https://syne-tune.readthedocs.io/en/latest/examples.html#launch-hpo-experiment-with-home-made-scheduler).\n    3. Compare your new `LocalSearcher` with `RandomSearch` on the `DropoutMLP` benchmark.\n\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12093)\n:end_tab:"
    },
    {
      "chunk_id": "82eabc3f9df9_0",
      "chapter": "sh-async",
      "heading": "sh-async",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n#required_libs(\"syne-tune[gpsearchers]==0.3.2\")\n```\n\n# Asynchronous Successive Halving\n\n:label:`sec_sh_async`\n\nAs we have seen in :numref:`sec_rs_async`, we can accelerate HPO by\ndistributing the evaluation of hyperparameter configurations across either\nmultiple instances or multiples CPUs / GPUs on a single instance. However,\ncompared to random search, it is not straightforward to run\nsuccessive halving (SH) asynchronously in a distributed setting. Before we can\ndecide which configuration to run next, we first have to collect all\nobservations at the current rung level. This requires to\nsynchronize workers at each rung level. For example, for the lowest rung level\n$r_{\\mathrm{min}}$, we first have to evaluate all $N = \\eta^K$ configurations, before we\ncan promote the $\\frac{1}{\\eta}$ of them to the next rung level. In any distributed system, synchronization typically implies idle time for workers. First, we often observe high variations in training time across hyperparameter\nconfigurations. For example, assuming the number of filters per layer is a\nhyperparameter, then networks with less filters finish training faster than\nnetworks with more filters, which implies idle worker time due to stragglers. Moreover, the number of slots in a rung level is not always a multiple of the number\nof workers, in which case some workers may even sit idle for a full batch. Figure :numref:`synchronous_sh` shows the scheduling of synchronous SH with $\\eta=2$\nfor four different trials with two workers. We start with evaluating Trial-0 and\nTrial-1 for one epoch and immediately continue with the next two trials once they\nare finished. We first have to wait until Trial-2 finishes, which takes\nsubstantially more time than the other trials, before we can promote the best two\ntrials, i.e., Trial-0 and Trial-3 to the next rung level. This causes idle time for\nWorker-1. Then, we continue with Rung 1."
    },
    {
      "chunk_id": "82eabc3f9df9_1",
      "chapter": "sh-async",
      "heading": "sh-async",
      "text": "This causes idle time for\nWorker-1. Then, we continue with Rung 1. Also, here Trial-3 takes longer than Trial-0,\nwhich leads to an additional ideling time of Worker-0. Once, we reach Rung-2, only\nthe best trial, Trial-0, remains which occupies only one worker. To avoid that\nWorker-1 idles during that time, most implementaitons of SH continue already with\nthe next round, and start evaluating new trials (e.g Trial-4) on the first rung. ![Synchronous successive halving with two workers.](../img/sync_sh.svg)\n:label:`synchronous_sh`\n\nAsynchronous successive halving (ASHA) :cite:`li-arxiv18` adapts SH to the asynchronous\nparallel scenario. The main idea of ASHA is to promote configurations to the next rung\nlevel as soon as we collected at least $\\eta$ observations on the current rung level. This decision rule may lead to suboptimal promotions: configurations can be promoted to the\nnext rung level, which in hindsight do not compare favourably against most others\nat the same rung level. On the other hand, we get rid of all synchronization points\nthis way. In practice, such suboptimal initial promotions have only a modest impact on\nperformance, not only because the ranking of hyperparameter configurations is often\nfairly consistent across rung levels, but also because rungs grow over time and\nreflect the distribution of metric values at this level better and better. If a\nworker is free, but no configuration can be promoted, we start a new configuration\nwith $r = r_{\\mathrm{min}}$, i.e the first rung level. :numref:`asha` shows the scheduling of the same configurations for ASHA. Once Trial-1\nfinishes, we collect the results of two trials (i.e Trial-0 and Trial-1) and\nimmediately promote the better of them (Trial-0) to the next rung level. After Trial-0\nfinishes on rung 1, there are too few trials there in order to support a further\npromotion. Hence, we continue with rung 0 and evaluate Trial-3. Once Trial-3 finishes,\nTrial-2 is still pending."
    },
    {
      "chunk_id": "82eabc3f9df9_2",
      "chapter": "sh-async",
      "heading": "sh-async",
      "text": "After Trial-0\nfinishes on rung 1, there are too few trials there in order to support a further\npromotion. Hence, we continue with rung 0 and evaluate Trial-3. Once Trial-3 finishes,\nTrial-2 is still pending. At this point we have 3 trials evaluated on rung 0 and one\ntrial evaluated already on rung 1. Since Trial-3 performs worse than Trial-0 at rung 0,\nand $\\eta=2$, we cannot promote any new trial yet, and Worker-1 starts Trial-4 from\nscratch instead. However, once Trial-2 finishes and\nscores worse than Trial-3, the latter is promoted towards rung 1. Afterwards, we\ncollected 2 evaluations on rung 1, which means we can now promote Trial-0 towards\nrung 2. At the same time, Worker-1 continues with evaluating new trials (i.e.,\nTrial-5) on rung 0. ![Asynchronous successive halving (ASHA) with two workers.](../img/asha.svg)\n:label:`asha`\n\n```{.python .input}\nfrom d2l import torch as d2l\nimport logging\nlogging.basicConfig(level=logging.INFO)\nimport matplotlib.pyplot as plt\nfrom syne_tune.config_space import loguniform, randint\nfrom syne_tune.backend.python_backend import PythonBackend\nfrom syne_tune.optimizer.baselines import ASHA\nfrom syne_tune import Tuner, StoppingCriterion\nfrom syne_tune.experiments import load_experiment\n```"
    },
    {
      "chunk_id": "b004dab083c3_0",
      "chapter": "sh-async",
      "heading": "Objective Function",
      "text": "We will use *Syne Tune* with the same objective function as in\n:numref:`sec_rs_async`.\n\n```{.python .input  n=54}\ndef hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):\n    from d2l import torch as d2l\n    from syne_tune import Reporter\n\n    model = d2l.LeNet(lr=learning_rate, num_classes=10)\n    trainer = d2l.HPOTrainer(max_epochs=1, num_gpus=1)\n    data = d2l.FashionMNIST(batch_size=batch_size)\n    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n    report = Reporter()\n    for epoch in range(1, max_epochs + 1):\n        if epoch == 1:\n            # Initialize the state of Trainer\n            trainer.fit(model=model, data=data)\n        else:\n            trainer.fit_epoch()\n        validation_error = d2l.numpy(trainer.validation_error().cpu())\n        report(epoch=epoch, validation_error=float(validation_error))\n```\n\nWe will also use the same configuration space as before:\n\n```{.python .input  n=55}\nmin_number_of_epochs = 2\nmax_number_of_epochs = 10\neta = 2\n\nconfig_space = {\n    \"learning_rate\": loguniform(1e-2, 1),\n    \"batch_size\": randint(32, 256),\n    \"max_epochs\": max_number_of_epochs,\n}\ninitial_config = {\n    \"learning_rate\": 0.1,\n    \"batch_size\": 128,\n}\n```"
    },
    {
      "chunk_id": "bcde08c1212b_0",
      "chapter": "sh-async",
      "heading": "Asynchronous Scheduler",
      "text": "First, we define the number of workers that evaluate trials concurrently. We\nalso need to specify how long we want to run random search, by defining an\nupper limit on the total wall-clock time. ```{.python .input  n=56}\nn_workers = 2  # Needs to be <= the number of available GPUs\nmax_wallclock_time = 12 * 60  # 12 minutes\n```\n\nThe code for running ASHA is a simple variation of what we did for asynchronous\nrandom search. ```{.python .input  n=56}\nmode = \"min\"\nmetric = \"validation_error\"\nresource_attr = \"epoch\"\n\nscheduler = ASHA(\n    config_space,\n    metric=metric,\n    mode=mode,\n    points_to_evaluate=[initial_config],\n    max_resource_attr=\"max_epochs\",\n    resource_attr=resource_attr,\n    grace_period=min_number_of_epochs,\n    reduction_factor=eta,\n)\n```\n\nHere, `metric` and `resource_attr` specify the key names used with the `report`\ncallback, and `max_resource_attr` denotes which input to the objective function\ncorresponds to $r_{\\mathrm{max}}$. Moreover, `grace_period` provides $r_{\\mathrm{min}}$, and\n`reduction_factor` is $\\eta$. We can run Syne Tune as before (this will\ntake about 12 minutes):\n\n```{.python .input  n=57}\ntrial_backend = PythonBackend(\n    tune_function=hpo_objective_lenet_synetune,\n    config_space=config_space,\n)\n\nstop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)\ntuner = Tuner(\n    trial_backend=trial_backend,\n    scheduler=scheduler,\n    stop_criterion=stop_criterion,\n    n_workers=n_workers,\n    print_update_interval=int(max_wallclock_time * 0.6),\n)\ntuner.run()\n```\n\nNote that we are running a variant of ASHA where underperforming trials are\nstopped early. This is different to our implementation in\n:numref:`sec_mf_hpo_sh`, where each training job is started with a fixed\n`max_epochs`. In the latter case, a well-performing trial which reaches the\nfull 10 epochs, first needs to train 1, then 2, then 4, then 8 epochs, each\ntime starting from scratch."
    },
    {
      "chunk_id": "bcde08c1212b_1",
      "chapter": "sh-async",
      "heading": "Asynchronous Scheduler",
      "text": "In the latter case, a well-performing trial which reaches the\nfull 10 epochs, first needs to train 1, then 2, then 4, then 8 epochs, each\ntime starting from scratch. This type of pause-and-resume scheduling can be\nimplemented efficiently by checkpointing the training state after each epoch,\nbut we avoid this extra complexity here. After the experiment has finished,\nwe can retrieve and plot results. ```{.python .input  n=59}\nd2l.set_figsize()\ne = load_experiment(tuner.name)\ne.plot()\n```"
    },
    {
      "chunk_id": "3bf092e8f69b_0",
      "chapter": "sh-async",
      "heading": "Visualize the Optimization Process",
      "text": "Once more, we visualize the learning curves of every trial (each color in the plot represents a trial). Compare this to\nasynchronous random search in :numref:`sec_rs_async`. As we have seen for\nsuccessive halving in :numref:`sec_mf_hpo`, most of the trials are stopped\nat 1 or 2 epochs ($r_{\\mathrm{min}}$ or $\\eta * r_{\\mathrm{min}}$). However, trials do not stop\nat the same point, because they require different amount of time per epoch. If\nwe ran standard successive halving instead of ASHA, we would need to synchronize\nour workers, before we can promote configurations to the next rung level.\n\n```{.python .input  n=60}\nd2l.set_figsize([6, 2.5])\nresults = e.results\nfor trial_id in results.trial_id.unique():\n    df = results[results[\"trial_id\"] == trial_id]\n    d2l.plt.plot(\n        df[\"st_tuner_time\"],\n        df[\"validation_error\"],\n        marker=\"o\"\n    )\nd2l.plt.xlabel(\"wall-clock time\")\nd2l.plt.ylabel(\"objective function\")\n```"
    },
    {
      "chunk_id": "e69d936f5423_0",
      "chapter": "sh-async",
      "heading": "Summary",
      "text": "Compared to random search, successive halving is not quite as trivial to run in\nan asynchronous distributed setting. To avoid synchronisation points, we promote\nconfigurations as quickly as possible to the next rung level, even if this means\npromoting some wrong ones. In practice, this usually does not hurt much, and the\ngains of asynchronous versus synchronous scheduling are usually much higher\nthan the loss of the suboptimal decision making.\n\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12101)\n:end_tab:"
    },
    {
      "chunk_id": "4193a686e076_0",
      "chapter": "sh-intro",
      "heading": "sh-intro",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n```\n\n# Multi-Fidelity Hyperparameter Optimization\n:label:`sec_mf_hpo`\n\nTraining neural networks can be expensive even on moderate size datasets. Depending on the configuration space (:numref:`sec_intro_config_spaces`),\nhyperparameter optimization requires tens to hundreds of function evaluations\nto find a well-performing hyperparameter configuration. As we have seen in\n:numref:`sec_rs_async`, we can significantly speed up the overall wall-clock\ntime of HPO by exploiting parallel resources, but this does not reduce the total\namount of compute required. In this section, we will show how the evaluation of hyperparameter configurations\ncan be sped up. Methods such as random search allocate the same amount of\nresources (e.g., number of epochs, training data points) to each hyperparameter\nevaluation. :numref:`img_samples_lc` depicts learning curves of a set of neural\nnetworks trained with different hyperparameter configurations. After a few epochs we are\nalready able to visually distinguish between well-performing and suboptimal\nconfigurations. However, the learning curves are noisy, and we might still require\nthe full amount of 100 epochs to identify the best performing one. ![Learning curves of random hyperparameter configurations](../img/samples_lc.svg)\n:label:`img_samples_lc`\n\nMulti-fidelity hyperparameter optimization allocates more resources\nto promising configurations and stop evaluations of poorly performing ones early. This speeds up the optimization process, since we can try a larger number of\nconfigurations for the same total amount of resources. More formally, we expand our definition in :numref:`sec_definition_hpo`,\nsuch that our objective function $f(\\mathbf{x}, r)$ gets an additional input\n$r \\in [r_{\\mathrm{min}}, r_{max}]$, specifying the amount of resources that we are\nwilling to spend for the evaluation of configuration $\\mathbf{x}$."
    },
    {
      "chunk_id": "4193a686e076_1",
      "chapter": "sh-intro",
      "heading": "sh-intro",
      "text": "We assume that\nthe error $f(\\mathbf{x}, r)$ decreases with $r$, whereas the computational\ncost $c(\\mathbf{x}, r)$ increases. Typically, $r$ represents the number of\nepochs for training the neural network, but it could also be the training\nsubset size or the number of cross-validation folds. ```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport numpy as np\nfrom scipy import stats\nfrom collections import defaultdict\nd2l.set_figsize()\n```"
    },
    {
      "chunk_id": "4402d44ea98f_0",
      "chapter": "sh-intro",
      "heading": "Successive Halving",
      "text": ":label:`sec_mf_hpo_sh`\n\nOne of the simplest ways to adapt random search to the multi-fidelity setting is\n*successive halving* :cite:`jamieson-aistats16,karnin-icml13`. The basic\nidea is to start with $N$ configurations, for example randomly sampled from the\nconfiguration space, and to train each of them for $r_{\\mathrm{min}}$ epochs only. We\nthen discard a fraction of the worst performing trials and train the remaining\nones for longer. Iterating this process, fewer trials run for longer, until at\nleast one trial reaches $r_{max}$ epochs. More formally, consider a minimum budget $r_{\\mathrm{min}}$ (for example 1 epoch), a maximum\nbudget $r_{max}$, for example `max_epochs` in our previous example, and a halving\nconstant $\\eta\\in\\{2, 3, \\dots\\}$. For simplicity, assume that\n$r_{max} = r_{\\mathrm{min}} \\eta^K$, with $K \\in \\mathbb{I}$ . The number of initial\nconfigurations is then $N = \\eta^K$. Let us define the set of rungs\n$\\mathcal{R} = \\{ r_{\\mathrm{min}}, r_{\\mathrm{min}}\\eta, r_{\\mathrm{min}}\\eta^2, \\dots, r_{max} \\}$. One round of successive halving proceeds as follows. We start with running $N$\ntrials until the first rung $r_{\\mathrm{min}}$. Sorting the validation errors, we keep\nthe top $1 / \\eta$ fraction (which amounts to $\\eta^{K-1}$ configurations) and\ndiscard all the rest. The surviving trials are trained for the next rung\n($r_{\\mathrm{min}}\\eta$ epochs), and the process is repeated. At each rung, a\n$1 / \\eta$ fraction of trials survives and their training continues with a\n$\\eta$ times larger budget. With this particular choice of $N$, only a single\ntrial will be trained to the full budget $r_{max}$. Once such a round of\nsuccessive halving is done, we start the next one with a new set of initial\nconfigurations, iterating until the total budget is spent."
    },
    {
      "chunk_id": "4402d44ea98f_1",
      "chapter": "sh-intro",
      "heading": "Successive Halving",
      "text": "With this particular choice of $N$, only a single\ntrial will be trained to the full budget $r_{max}$. Once such a round of\nsuccessive halving is done, we start the next one with a new set of initial\nconfigurations, iterating until the total budget is spent. ![Learning curves of random hyperparameter configurations.](../img/sh.svg)\n\nWe subclass the `HPOScheduler` base class from :numref:`sec_api_hpo` in order to\nimplement successive halving, allowing for a generic `HPOSearcher` object to\nsample configurations (which, in our example below, will be a `RandomSearcher`). Additionally, the user has to pass the minimum resource $r_{\\mathrm{min}}$, the maximum\nresource $r_{max}$ and $\\eta$ as input. Inside our scheduler, we maintain a\nqueue of configurations that still need to be evaluated for the current rung\n$r_i$. We update the queue every time we jump to the next rung. ```{.python .input  n=2}\nclass SuccessiveHalvingScheduler(d2l.HPOScheduler):  #@save\n    def __init__(self, searcher, eta, r_min, r_max, prefact=1):\n        self.save_hyperparameters()\n        # Compute K, which is later used to determine the number of configurations\n        self.K = int(np.log(r_max / r_min) / np.log(eta))\n        # Define the rungs\n        self.rung_levels = [r_min * eta ** k for k in range(self.K + 1)]\n        if r_max not in self.rung_levels:\n            # The final rung should be r_max\n            self.rung_levels.append(r_max)\n            self.K += 1\n        # Bookkeeping\n        self.observed_error_at_rungs = defaultdict(list)\n        self.all_observed_error_at_rungs = defaultdict(list)\n        # Our processing queue\n        self.queue = []\n```\n\nIn the beginning our queue is empty, and we fill it with\n$n = \\textrm{prefact} \\cdot \\eta^{K}$ configurations, which are first evaluated on\nthe smallest rung $r_{\\mathrm{min}}$. Here, $\\textrm{prefact}$ allows us to reuse our\ncode in a different context. For the purpose of this section, we fix\n$\\textrm{prefact} = 1$."
    },
    {
      "chunk_id": "4402d44ea98f_2",
      "chapter": "sh-intro",
      "heading": "Successive Halving",
      "text": "Here, $\\textrm{prefact}$ allows us to reuse our\ncode in a different context. For the purpose of this section, we fix\n$\\textrm{prefact} = 1$. Every time resources become available and the `HPOTuner`\nobject queries the `suggest` function, we return an element from the queue. Once\nwe finish one round of successive halving, which means that we evaluated all\nsurviving configurations on the highest resource level $r_{max}$ and our queue\nis empty, we start the entire process again with a new, randomly sampled set\nof configurations. ```{.python .input  n=12}\n%%tab pytorch\n@d2l.add_to_class(SuccessiveHalvingScheduler)  #@save\ndef suggest(self):\n    if len(self.queue) == 0:\n        # Start a new round of successive halving\n        # Number of configurations for the first rung:\n        n0 = int(self.prefact * self.eta ** self.K)\n        for _ in range(n0):\n            config = self.searcher.sample_configuration()\n            config[\"max_epochs\"] = self.r_min  # Set r = r_min\n            self.queue.append(config)\n    # Return an element from the queue\n    return self.queue.pop()\n```\n\nWhen we collected a new data point, we first update the searcher module. Afterwards we check if we already collect all data points on the current rung. If so, we sort all configurations and push the top $\\frac{1}{\\eta}$\nconfigurations into the queue."
    },
    {
      "chunk_id": "4402d44ea98f_3",
      "chapter": "sh-intro",
      "heading": "Successive Halving",
      "text": "Afterwards we check if we already collect all data points on the current rung. If so, we sort all configurations and push the top $\\frac{1}{\\eta}$\nconfigurations into the queue. ```{.python .input  n=4}\n%%tab pytorch\n@d2l.add_to_class(SuccessiveHalvingScheduler)  #@save\ndef update(self, config: dict, error: float, info=None):\n    ri = int(config[\"max_epochs\"])  # Rung r_i\n    # Update our searcher, e.g if we use Bayesian optimization later\n    self.searcher.update(config, error, additional_info=info)\n    self.all_observed_error_at_rungs[ri].append((config, error))\n    if ri < self.r_max:\n        # Bookkeeping\n        self.observed_error_at_rungs[ri].append((config, error))\n        # Determine how many configurations should be evaluated on this rung\n        ki = self.K - self.rung_levels.index(ri)\n        ni = int(self.prefact * self.eta ** ki)\n        # If we observed all configuration on this rung r_i, we estimate the\n        # top 1 / eta configuration, add them to queue and promote them for\n        # the next rung r_{i+1}\n        if len(self.observed_error_at_rungs[ri]) >= ni:\n            kiplus1 = ki - 1\n            niplus1 = int(self.prefact * self.eta ** kiplus1)\n            best_performing_configurations = self.get_top_n_configurations(\n                rung_level=ri, n=niplus1\n            )\n            riplus1 = self.rung_levels[self.K - kiplus1]  # r_{i+1}\n            # Queue may not be empty: insert new entries at the beginning\n            self.queue = [\n                dict(config, max_epochs=riplus1)\n                for config in best_performing_configurations\n            ] + self.queue\n            self.observed_error_at_rungs[ri] = []  # Reset\n```\n\nConfigurations are sorted based on their observed performance on the current\nrung."
    },
    {
      "chunk_id": "4402d44ea98f_4",
      "chapter": "sh-intro",
      "heading": "Successive Halving",
      "text": "```{.python .input  n=4}\n%%tab pytorch\n\n@d2l.add_to_class(SuccessiveHalvingScheduler)  #@save\ndef get_top_n_configurations(self, rung_level, n):\n    rung = self.observed_error_at_rungs[rung_level]\n    if not rung:\n        return []\n    sorted_rung = sorted(rung, key=lambda x: x[1])\n    return [x[0] for x in sorted_rung[:n]]\n```\n\nLet us see how successive halving is doing on our neural network example. We\nwill use $r_{\\mathrm{min}} = 2$, $\\eta = 2$, $r_{max} = 10$, so that rung levels are\n$2, 4, 8, 10$. ```{.python .input  n=5}\nmin_number_of_epochs = 2\nmax_number_of_epochs = 10\neta = 2\nnum_gpus=1\n\nconfig_space = {\n    \"learning_rate\": stats.loguniform(1e-2, 1),\n    \"batch_size\": stats.randint(32, 256),\n}\ninitial_config = {\n    \"learning_rate\": 0.1,\n    \"batch_size\": 128,\n}\n```\n\nWe just replace the scheduler with our new `SuccessiveHalvingScheduler`. ```{.python .input  n=14}\nsearcher = d2l.RandomSearcher(config_space, initial_config=initial_config)\nscheduler = SuccessiveHalvingScheduler(\n    searcher=searcher,\n    eta=eta,\n    r_min=min_number_of_epochs,\n    r_max=max_number_of_epochs,\n)\ntuner = d2l.HPOTuner(\n    scheduler=scheduler,\n    objective=d2l.hpo_objective_lenet,\n)\ntuner.run(number_of_trials=30)\n```\n\nWe can visualize the learning curves of all configurations that we evaluated. Most of the configurations are stopped early and only the better performing\nconfigurations survive until $r_{max}$. Compare this to vanilla random search,\nwhich would allocate $r_{max}$ to every configuration."
    },
    {
      "chunk_id": "4402d44ea98f_5",
      "chapter": "sh-intro",
      "heading": "Successive Halving",
      "text": "Most of the configurations are stopped early and only the better performing\nconfigurations survive until $r_{max}$. Compare this to vanilla random search,\nwhich would allocate $r_{max}$ to every configuration. ```{.python .input  n=19}\nfor rung_index, rung in scheduler.all_observed_error_at_rungs.items():\n    errors = [xi[1] for xi in rung]\n    d2l.plt.scatter([rung_index] * len(errors), errors)\nd2l.plt.xlim(min_number_of_epochs - 0.5, max_number_of_epochs + 0.5)\nd2l.plt.xticks(\n    np.arange(min_number_of_epochs, max_number_of_epochs + 1),\n    np.arange(min_number_of_epochs, max_number_of_epochs + 1)\n)\nd2l.plt.ylabel(\"validation error\")\nd2l.plt.xlabel(\"epochs\")\n```\n\nFinally, note some slight complexity in our implementation of\n`SuccessiveHalvingScheduler`. Say that a worker is free to run a job, and\n`suggest` is called when the current rung has almost been completely filled, but\nanother worker is still busy with an evaluation. Since we lack the metric value\nfrom this worker, we cannot determine the top $1 / \\eta$ fraction to open up\nthe next rung. On the other hand, we want to assign a job to our free worker,\nso it does not remain idle. Our solution is to start a new round of successive\nhalving and assign our worker to the first trial there. However, once a rung is\ncompleted in `update`, we make sure to insert new configurations at the\nbeginning of the queue, so they take precedence over configurations from the\nnext round."
    },
    {
      "chunk_id": "b8ab12f75841_0",
      "chapter": "sh-intro",
      "heading": "Summary",
      "text": "In this section, we introduced the concept of multi-fidelity hyperparameter\noptimization, where we assume to have access to cheap-to-evaluate approximations\nof the objective function, such as validation error after a certain number of\nepochs of training as proxy to validation error after the full number of epochs.\nMulti-fidelity hyperparameter optimization allows to reduce the overall\ncomputation of the HPO instead of just reducing the wall-clock time.\n\nWe implemented and evaluated successive halving, a simple yet efficient\nmulti-fidelity HPO algorithm.\n\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12094)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Installation\n:label:`chap_installation`\n\nIn order to get up and running,\nwe will need an environment for running Python,\nthe Jupyter Notebook, the relevant libraries,\nand the code needed to run the book itself."
    },
    {
      "chunk_id": "e2416db8d5ad_0",
      "chapter": "index",
      "heading": "Installing Miniconda",
      "text": "Your simplest option is to install\n[Miniconda](https://conda.io/en/latest/miniconda.html).\nNote that the Python 3.x version is required.\nYou can skip the following steps\nif your machine already has conda installed.\n\nVisit the Miniconda website and determine\nthe appropriate version for your system\nbased on your Python 3.x version and machine architecture.\nSuppose that your Python version is 3.9\n(our tested version).\nIf you are using macOS,\nyou would download the bash script\nwhose name contains the strings \"MacOSX\",\nnavigate to the download location,\nand execute the installation as follows\n(taking Intel Macs as an example):\n\n```bash\n# The file name is subject to changes\nsh Miniconda3-py39_4.12.0-MacOSX-x86_64.sh -b\n```\n\n\nA Linux user\nwould download the file\nwhose name contains the strings \"Linux\"\nand execute the following at the download location:\n\n```bash\n# The file name is subject to changes\nsh Miniconda3-py39_4.12.0-Linux-x86_64.sh -b\n```\n\n\nA Windows user would download and install Miniconda by following its [online instructions](https://conda.io/en/latest/miniconda.html).\nOn Windows, you may search for `cmd` to open the Command Prompt (command-line interpreter) for running commands.\n\nNext, initialize the shell so we can run `conda` directly.\n\n```bash\n~/miniconda3/bin/conda init\n```\n\n\nThen close and reopen your current shell.\nYou should be able to create\na new environment as follows:\n\n```bash\nconda create --name d2l python=3.9 -y\n```\n\n\nNow we can activate the `d2l` environment:\n\n```bash\nconda activate d2l\n```"
    },
    {
      "chunk_id": "1479318cfd7f_0",
      "chapter": "index",
      "heading": "Installing the Deep Learning Framework and the `d2l` Package",
      "text": "Before installing any deep learning framework,\nplease first check whether or not\nyou have proper GPUs on your machine\n(the GPUs that power the display\non a standard laptop are not relevant for our purposes). For example,\nif your computer has NVIDIA GPUs and has installed [CUDA](https://developer.nvidia.com/cuda-downloads),\nthen you are all set. If your machine does not house any GPU,\nthere is no need to worry just yet. Your CPU provides more than enough horsepower\nto get you through the first few chapters. Just remember that you will want to access GPUs\nbefore running larger models. :begin_tab:`mxnet`\n\nTo install a GPU-enabled version of MXNet,\nwe need to find out what version of CUDA you have installed. You can check this by running `nvcc --version`\nor `cat /usr/local/cuda/version.txt`. Assume that you have installed CUDA 11.2,\nthen execute the following command:\n\n```bash\n# For macOS and Linux users\npip install mxnet-cu112==1.9.1\n\n# For Windows users\npip install mxnet-cu112==1.9.1 -f https://dist.mxnet.io/python\n```\n\n\nYou may change the last digits according to your CUDA version, e.g., `cu101` for\nCUDA 10.1 and `cu90` for CUDA 9.0."
    },
    {
      "chunk_id": "1479318cfd7f_1",
      "chapter": "index",
      "heading": "Installing the Deep Learning Framework and the `d2l` Package",
      "text": "If your machine has no NVIDIA GPUs\nor CUDA,\nyou can install the CPU version\nas follows:\n\n```bash\npip install mxnet==1.9.1\n```\n\n\n:end_tab:\n\n\n:begin_tab:`pytorch`\n\nYou can install PyTorch (the specified versions are tested at the time of writing) with either CPU or GPU support as follows:\n\n```bash\npip install torch==2.0.0 torchvision==0.15.1\n```\n\n\n:end_tab:\n\n:begin_tab:`tensorflow`\nYou can install TensorFlow with either CPU or GPU support as follows:\n\n```bash\npip install tensorflow==2.12.0 tensorflow-probability==0.20.0\n```\n\n\n:end_tab:\n\n:begin_tab:`jax`\nYou can install JAX and Flax with either CPU or GPU support as follows:\n\n```bash\n# GPU\npip install \"jax[cuda11_pip]==0.4.13\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html flax==0.7.0\n```\n\n\nIf your machine has no NVIDIA GPUs\nor CUDA,\nyou can install the CPU version\nas follows:\n\n```bash\n# CPU\npip install \"jax[cpu]==0.4.13\" flax==0.7.0\n```\n\n\n:end_tab:\n\n\nOur next step is to install\nthe `d2l` package that we developed\nin order to encapsulate\nfrequently used functions and classes\nfound throughout this book:\n\n```bash\npip install d2l==1.0.3\n```"
    },
    {
      "chunk_id": "9378191d4da7_0",
      "chapter": "index",
      "heading": "Downloading and Running the Code",
      "text": "Next, you will want to download the notebooks\nso that you can run each of the book's code blocks.\nSimply click on the \"Notebooks\" tab at the top\nof any HTML page on [the D2L.ai website](https://d2l.ai/)\nto download the code and then unzip it.\nAlternatively, you can fetch the notebooks\nfrom the command line as follows:\n\n:begin_tab:`mxnet`\n\n```bash\nmkdir d2l-en && cd d2l-en\ncurl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip\nunzip d2l-en.zip && rm d2l-en.zip\ncd mxnet\n```\n\n\n:end_tab:\n\n\n:begin_tab:`pytorch`\n\n```bash\nmkdir d2l-en && cd d2l-en\ncurl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip\nunzip d2l-en.zip && rm d2l-en.zip\ncd pytorch\n```\n\n\n:end_tab:\n\n:begin_tab:`tensorflow`\n\n```bash\nmkdir d2l-en && cd d2l-en\ncurl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip\nunzip d2l-en.zip && rm d2l-en.zip\ncd tensorflow\n```\n\n\n:end_tab:\n\n:begin_tab:`jax`\n\n```bash\nmkdir d2l-en && cd d2l-en\ncurl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip\nunzip d2l-en.zip && rm d2l-en.zip\ncd jax\n```\n\n\n:end_tab:\n\nIf you do not already have `unzip` installed, first run `sudo apt-get install unzip`.\nNow we can start the Jupyter Notebook server by running:\n\n```bash\njupyter notebook\n```\n\n\nAt this point, you can open http://localhost:8888\n(it may have already opened automatically) in your web browser.\nThen we can run the code for each section of the book.\nWhenever you open a new command line window,\nyou will need to execute `conda activate d2l`\nto activate the runtime environment\nbefore running the D2L notebooks,\nor updating your packages\n(either the deep learning framework\nor the `d2l` package).\nTo exit the environment,\nrun `conda deactivate`.\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/23)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/24)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/436)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17964)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Introduction\n:label:`chap_introduction`\n\nUntil recently, nearly every computer program\nthat you might have interacted with during\nan ordinary day\nwas coded up as a rigid set of rules\nspecifying precisely how it should behave. Say that we wanted to write an application\nto manage an e-commerce platform. After huddling around a whiteboard\nfor a few hours to ponder the problem,\nwe might settle on the broad strokes\nof a working solution, for example:\n(i) users interact with the application through an interface\nrunning in a web browser or mobile application;\n(ii) our application interacts with a commercial-grade database engine\nto keep track of each user's state and maintain records\nof historical transactions;\nand (iii) at the heart of our application,\nthe *business logic* (you might say, the *brains*) of our application\nspells out a set of rules that map every conceivable circumstance\nto the corresponding action that our program should take. To build the brains of our application,\nwe might enumerate all the common events\nthat our program should handle. For example, whenever a customer clicks\nto add an item to their shopping cart,\nour program should add an entry\nto the shopping cart database table,\nassociating that user's ID\nwith the requested product's ID. We might then attempt to step through\nevery possible corner case,\ntesting the appropriateness of our rules\nand making any necessary modifications. What happens if a user\ninitiates a purchase with an empty cart? While few developers ever get it\ncompletely right the first time\n(it might take some test runs to work out the kinks),\nfor the most part we can write such programs\nand confidently launch them\n*before* ever seeing a real customer. Our ability to manually design automated systems\nthat drive functioning products and systems,\noften in novel situations,\nis a remarkable cognitive feat. And when you are able to devise solutions\nthat work $100\\%$ of the time,\nyou typically should not be\nworrying about machine learning."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "And when you are able to devise solutions\nthat work $100\\%$ of the time,\nyou typically should not be\nworrying about machine learning. Fortunately for the growing community\nof machine learning scientists,\nmany tasks that we would like to automate\ndo not bend so easily to human ingenuity. Imagine huddling around the whiteboard\nwith the smartest minds you know,\nbut this time you are tackling\none of the following problems:\n\n* Write a program that predicts tomorrow's weather given geographic information, satellite images, and a trailing window of past weather. * Write a program that takes in a factoid question, expressed in free-form text, and  answers it correctly. * Write a program that, given an image, identifies every person depicted in it and draws outlines around each. * Write a program that presents users with products that they are likely to enjoy but unlikely, in the natural course of browsing, to encounter. For these problems,\neven elite programmers would struggle\nto code up solutions from scratch. The reasons can vary. Sometimes the program that we are looking for\nfollows a pattern that changes over time,\nso there is no fixed right answer! In such cases, any successful solution\nmust adapt gracefully to a changing world. At other times, the relationship (say between pixels,\nand abstract categories) may be too complicated,\nrequiring thousands or millions of computations\nand following unknown principles. In the case of image recognition,\nthe precise steps required to perform the task\nlie beyond our conscious understanding,\neven though our subconscious cognitive processes\nexecute the task effortlessly. *Machine learning* is the study of algorithms\nthat can learn from experience. As a machine learning algorithm accumulates more experience,\ntypically in the form of observational data\nor interactions with an environment,\nits performance improves."
    },
    {
      "chunk_id": "01f4e33118cb_2",
      "chapter": "index",
      "heading": "index",
      "text": "*Machine learning* is the study of algorithms\nthat can learn from experience. As a machine learning algorithm accumulates more experience,\ntypically in the form of observational data\nor interactions with an environment,\nits performance improves. Contrast this with our deterministic e-commerce platform,\nwhich follows the same business logic,\nno matter how much experience accrues,\nuntil the developers themselves learn and decide\nthat it is time to update the software. In this book, we will teach you\nthe fundamentals of machine learning,\nfocusing in particular on *deep learning*,\na powerful set of techniques\ndriving innovations in areas as diverse as computer vision,\nnatural language processing, healthcare, and genomics."
    },
    {
      "chunk_id": "fcae4b6743e7_0",
      "chapter": "index",
      "heading": "A Motivating Example",
      "text": "Before beginning writing, the authors of this book,\nlike much of the work force, had to become caffeinated. We hopped in the car and started driving. Using an iPhone, Alex called out \"Hey Siri\",\nawakening the phone's voice recognition system. Then Mu commanded \"directions to Blue Bottle coffee shop\". The phone quickly displayed the transcription of his command. It also recognized that we were asking for directions\nand launched the Maps application (app)\nto fulfill our request. Once launched, the Maps app identified a number of routes. Next to each route, the phone displayed a predicted transit time. While this story was fabricated for pedagogical convenience,\nit demonstrates that in the span of just a few seconds,\nour everyday interactions with a smart phone\ncan engage several machine learning models. Imagine just writing a program to respond to a *wake word*\nsuch as \"Alexa\", \"OK Google\", and \"Hey Siri\". Try coding it up in a room by yourself\nwith nothing but a computer and a code editor,\nas illustrated in :numref:`fig_wake_word`. How would you write such a program from first principles? Think about it... the problem is hard. Every second, the microphone will collect roughly\n44,000 samples. Each sample is a measurement of the amplitude of the sound wave. What rule could map reliably from a snippet of raw audio to confident predictions\n$\\{\\textrm{yes}, \\textrm{no}\\}$\nabout whether the snippet contains the wake word? If you are stuck, do not worry. We do not know how to write such a program from scratch either. That is why we use machine learning. ![Identify a wake word.](../img/wake-word.svg)\n:label:`fig_wake_word`\n\n\nHere is the trick. Often, even when we do not know how to tell a computer\nexplicitly how to map from inputs to outputs,\nwe are nonetheless capable of performing the cognitive feat ourselves. In other words, even if you do not know\nhow to program a computer to recognize the word \"Alexa\",\nyou yourself are able to recognize it."
    },
    {
      "chunk_id": "fcae4b6743e7_1",
      "chapter": "index",
      "heading": "A Motivating Example",
      "text": "In other words, even if you do not know\nhow to program a computer to recognize the word \"Alexa\",\nyou yourself are able to recognize it. Armed with this ability, we can collect a huge *dataset*\ncontaining examples of audio snippets and associated labels,\nindicating which snippets contain the wake word. In the currently dominant approach to machine learning,\nwe do not attempt to design a system\n*explicitly* to recognize wake words. Instead, we define a flexible program\nwhose behavior is determined by a number of *parameters*. Then we use the dataset to determine the best possible parameter values,\ni.e., those that improve the performance of our program\nwith respect to a chosen performance measure. You can think of the parameters as knobs that we can turn,\nmanipulating the behavior of the program. Once the parameters are fixed, we call the program a *model*. The set of all distinct programs (input--output mappings)\nthat we can produce just by manipulating the parameters\nis called a *family* of models. And the \"meta-program\" that uses our dataset\nto choose the parameters is called a *learning algorithm*. Before we can go ahead and engage the learning algorithm,\nwe have to define the problem precisely,\npinning down the exact nature of the inputs and outputs,\nand choosing an appropriate model family. In this case,\nour model receives a snippet of audio as *input*,\nand the model\ngenerates a selection among\n$\\{\\textrm{yes}, \\textrm{no}\\}$ as *output*. If all goes according to plan\nthe model's guesses will\ntypically be correct as to\nwhether the snippet contains the wake word. If we choose the right family of models,\nthere should exist one setting of the knobs\nsuch that the model fires \"yes\" every time it hears the word \"Alexa\". Because the exact choice of the wake word is arbitrary,\nwe will probably need a model family sufficiently rich that,\nvia another setting of the knobs, it could fire \"yes\"\nonly upon hearing the word \"Apricot\"."
    },
    {
      "chunk_id": "fcae4b6743e7_2",
      "chapter": "index",
      "heading": "A Motivating Example",
      "text": "Because the exact choice of the wake word is arbitrary,\nwe will probably need a model family sufficiently rich that,\nvia another setting of the knobs, it could fire \"yes\"\nonly upon hearing the word \"Apricot\". We expect that the same model family should be suitable\nfor \"Alexa\" recognition and \"Apricot\" recognition\nbecause they seem, intuitively, to be similar tasks. However, we might need a different family of models entirely\nif we want to deal with fundamentally different inputs or outputs,\nsay if we wanted to map from images to captions,\nor from English sentences to Chinese sentences. As you might guess, if we just set all of the knobs randomly,\nit is unlikely that our model will recognize \"Alexa\",\n\"Apricot\", or any other English word. In machine learning,\nthe *learning* is the process\nby which we discover the right setting of the knobs\nfor coercing the desired behavior from our model. In other words,\nwe *train* our model with data. As shown in :numref:`fig_ml_loop`, the training process usually looks like the following:\n\n1. Start off with a randomly initialized model that cannot do anything useful. 1. Grab some of your data (e.g., audio snippets and corresponding $\\{\\textrm{yes}, \\textrm{no}\\}$ labels). 1. Tweak the knobs to make the model perform better as assessed on those examples. 1. Repeat Steps 2 and 3 until the model is awesome. ![A typical training process.](../img/ml-loop.svg)\n:label:`fig_ml_loop`\n\nTo summarize, rather than code up a wake word recognizer,\nwe code up a program that can *learn* to recognize wake words,\nif presented with a large labeled dataset. You can think of this act of determining a program's behavior\nby presenting it with a dataset as *programming with data*. That is to say, we can \"program\" a cat detector\nby providing our machine learning system\nwith many examples of cats and dogs. This way the detector will eventually learn to emit\na very large positive number if it is a cat,\na very large negative number if it is a dog,\nand something closer to zero if it is not sure."
    },
    {
      "chunk_id": "fcae4b6743e7_3",
      "chapter": "index",
      "heading": "A Motivating Example",
      "text": "This way the detector will eventually learn to emit\na very large positive number if it is a cat,\na very large negative number if it is a dog,\nand something closer to zero if it is not sure. This barely scratches the surface of what machine learning can do. Deep learning, which we will explain in greater detail later,\nis just one among many popular methods\nfor solving machine learning problems."
    },
    {
      "chunk_id": "c1acd03c3ec0_0",
      "chapter": "index",
      "heading": "Key Components",
      "text": "In our wake word example, we described a dataset\nconsisting of audio snippets and binary labels,\nand we gave a hand-wavy sense of how we might train\na model to approximate a mapping from snippets to classifications.\nThis sort of problem,\nwhere we try to predict a designated unknown label\nbased on known inputs\ngiven a dataset consisting of examples\nfor which the labels are known,\nis called *supervised learning*.\nThis is just one among many kinds of machine learning problems.\nBefore we explore other varieties,\nwe would like to shed more light\non some core components that will follow us around,\nno matter what kind of machine learning problem we tackle:\n\n1. The *data* that we can learn from.\n1. A *model* of how to transform the data.\n1. An *objective function* that quantifies how well (or badly) the model is doing.\n1. An *algorithm* to adjust the model's parameters to optimize the objective function."
    },
    {
      "chunk_id": "030e5298c5de_0",
      "chapter": "index",
      "heading": "Data",
      "text": "It might go without saying that you cannot do data science without data. We could lose hundreds of pages pondering what precisely data *is*,\nbut for now, we will focus on the key properties\nof the datasets that we will be concerned with. Generally, we are concerned with a collection of examples. In order to work with data usefully, we typically\nneed to come up with a suitable numerical representation. Each *example* (or *data point*, *data instance*, *sample*)\ntypically consists of a set of attributes\ncalled *features* (sometimes called *covariates* or *inputs*),\nbased on which the model must make its predictions. In supervised learning problems,\nour goal is to predict the value of a special attribute,\ncalled the *label* (or *target*),\nthat is not part of the model's input. If we were working with image data,\neach example might consist of an\nindividual photograph (the features)\nand a number indicating the category\nto which the photograph belongs (the label). The photograph would be represented numerically\nas three grids of numerical values representing\nthe brightness of red, green, and blue light\nat each pixel location. For example, a $200\\times 200$ pixel color photograph\nwould consist of $200\\times200\\times3=120000$ numerical values. Alternatively, we might work with electronic health record data\nand tackle the task of predicting the likelihood\nthat a given patient  will survive the next 30 days. Here, our features might consist of a collection\nof readily available attributes\nand frequently recorded measurements,\nincluding age, vital signs, comorbidities,\ncurrent medications, and recent procedures. The label available for training would be a binary value\nindicating whether each patient in the historical data\nsurvived within the 30-day window. In such cases, when every example is characterized\nby the same number of numerical features,\nwe say that the inputs are fixed-length vectors\nand we call the (constant) length of the vectors\nthe *dimensionality* of the data."
    },
    {
      "chunk_id": "030e5298c5de_1",
      "chapter": "index",
      "heading": "Data",
      "text": "In such cases, when every example is characterized\nby the same number of numerical features,\nwe say that the inputs are fixed-length vectors\nand we call the (constant) length of the vectors\nthe *dimensionality* of the data. As you might imagine, fixed-length inputs can be convenient,\ngiving us one less complication to worry about. However, not all data can easily\nbe represented as *fixed-length* vectors. While we might expect microscope images\nto come from standard equipment,\nwe cannot expect images mined from the Internet\nall to have the same resolution or shape. For images, we might consider\ncropping them to a standard size,\nbut that strategy only gets us so far. We risk losing information in the cropped-out portions. Moreover, text data resists fixed-length\nrepresentations even more stubbornly. Consider the customer reviews left\non e-commerce sites such as Amazon, IMDb, and TripAdvisor. Some are short: \"it stinks!\". Others ramble for pages. One major advantage of deep learning over traditional methods\nis the comparative grace with which modern models\ncan handle *varying-length* data. Generally, the more data we have, the easier our job becomes. When we have more data, we can train more powerful models\nand rely less heavily on preconceived assumptions. The regime change from (comparatively) small to big data\nis a major contributor to the success of modern deep learning. To drive the point home, many of\nthe most exciting models in deep learning\ndo not work without large datasets. Some others might work in the small data regime,\nbut are no better than traditional approaches. Finally, it is not enough to have lots of data\nand to process it cleverly. We need the *right* data. If the data is full of mistakes,\nor if the chosen features are not predictive\nof the target quantity of interest,\nlearning is going to fail. The situation is captured well by the clich\u00e9:\n*garbage in, garbage out*. Moreover, poor predictive performance\nis not the only potential consequence."
    },
    {
      "chunk_id": "030e5298c5de_2",
      "chapter": "index",
      "heading": "Data",
      "text": "The situation is captured well by the clich\u00e9:\n*garbage in, garbage out*. Moreover, poor predictive performance\nis not the only potential consequence. In sensitive applications of machine learning,\nlike predictive policing, resume screening,\nand risk models used for lending,\nwe must be especially alert\nto the consequences of garbage data. One commonly occurring failure mode concerns datasets\nwhere some groups of people are unrepresented\nin the training data. Imagine applying a skin cancer recognition system\nthat had never seen black skin before. Failure can also occur when the data\ndoes not only under-represent some groups\nbut reflects societal prejudices. For example, if past hiring decisions\nare used to train a predictive model\nthat will be used to screen resumes\nthen machine learning models could inadvertently\ncapture and automate historical injustices. Note that this can all happen without the data scientist\nactively conspiring, or even being aware."
    },
    {
      "chunk_id": "14a3cdfe98e5_0",
      "chapter": "index",
      "heading": "Models",
      "text": "Most machine learning involves transforming the data in some sense.\nWe might want to build a system that ingests photos and predicts smiley-ness.\nAlternatively,\nwe might want to ingest a set of sensor readings\nand predict how normal vs. anomalous the readings are.\nBy *model*, we denote the computational machinery for ingesting data\nof one type,\nand spitting out predictions of a possibly different type.\nIn particular, we are interested in *statistical models*\nthat can be estimated from data.\nWhile simple models are perfectly capable of addressing\nappropriately simple problems,\nthe problems\nthat we focus on in this book stretch the limits of classical methods.\nDeep learning is differentiated from classical approaches\nprincipally by the set of powerful models that it focuses on.\nThese models consist of many successive transformations of the data\nthat are chained together top to bottom, thus the name *deep learning*.\nOn our way to discussing deep models,\nwe will also discuss some more traditional methods."
    },
    {
      "chunk_id": "6e6e14ebeb7e_0",
      "chapter": "index",
      "heading": "Objective Functions",
      "text": "Earlier, we introduced machine learning as learning from experience. By *learning* here,\nwe mean improving at some task over time. But who is to say what constitutes an improvement? You might imagine that we could propose updating our model,\nand some people might disagree on whether our proposal\nconstituted an improvement or not. In order to develop a formal mathematical system of learning machines,\nwe need to have formal measures of how good (or bad) our models are. In machine learning, and optimization more generally,\nwe call these *objective functions*. By convention, we usually define objective functions\nso that lower is better. This is merely a convention. You can take any function\nfor which higher is better, and turn it into a new function\nthat is qualitatively identical but for which lower is better\nby flipping the sign. Because we choose lower to be better, these functions are sometimes called\n*loss functions*. When trying to predict numerical values,\nthe most common loss function is *squared error*,\ni.e., the square of the difference between\nthe prediction and the ground truth target. For classification, the most common objective\nis to minimize error rate,\ni.e., the fraction of examples on which\nour predictions disagree with the ground truth. Some objectives (e.g., squared error) are easy to optimize,\nwhile others (e.g., error rate) are difficult to optimize directly,\nowing to non-differentiability or other complications. In these cases, it is common instead to optimize a *surrogate objective*. During optimization, we think of the loss\nas a function of the model's parameters,\nand treat the training dataset as a constant. We learn\nthe best values of our model's parameters\nby minimizing the loss incurred on a set\nconsisting of some number of examples collected for training. However, doing well on the training data\ndoes not guarantee that we will do well on unseen data."
    },
    {
      "chunk_id": "6e6e14ebeb7e_1",
      "chapter": "index",
      "heading": "Objective Functions",
      "text": "We learn\nthe best values of our model's parameters\nby minimizing the loss incurred on a set\nconsisting of some number of examples collected for training. However, doing well on the training data\ndoes not guarantee that we will do well on unseen data. So we will typically want to split the available data into two partitions:\nthe *training dataset* (or *training set*), for learning model parameters;\nand the *test dataset* (or *test set*), which is held out for evaluation. At the end of the day, we typically report\nhow our models perform on both partitions. You could think of training performance\nas analogous to the scores that a student achieves\non the practice exams used to prepare for some real final exam. Even if the results are encouraging,\nthat does not guarantee success on the final exam. Over the course of studying, the student\nmight begin to memorize the practice questions,\nappearing to master the topic but faltering\nwhen faced with previously unseen questions\non the actual final exam. When a model performs well on the training set\nbut fails to generalize to unseen data,\nwe say that it is *overfitting* to the training data."
    },
    {
      "chunk_id": "5b8f78bb8e7b_0",
      "chapter": "index",
      "heading": "Optimization Algorithms",
      "text": "Once we have got some data source and representation,\na model, and a well-defined objective function,\nwe need an algorithm capable of searching\nfor the best possible parameters for minimizing the loss function.\nPopular optimization algorithms for deep learning\nare based on an approach called *gradient descent*.\nIn brief, at each step, this method\nchecks to see, for each parameter,\nhow that training set loss would change\nif you perturbed that parameter by just a small amount.\nIt would then update the parameter\nin the direction that lowers the loss."
    },
    {
      "chunk_id": "eb6a15c8fe6e_0",
      "chapter": "index",
      "heading": "Kinds of Machine Learning Problems",
      "text": "The wake word problem in our motivating example\nis just one among many\nthat machine learning can tackle.\nTo motivate the reader further\nand provide us with some common language\nthat will follow us throughout the book,\nwe now provide a broad overview of the landscape\nof machine learning problems."
    },
    {
      "chunk_id": "ec19fa7f3dfe_0",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Supervised learning describes tasks\nwhere we are given a dataset\ncontaining both features and labels\nand \nasked to produce a model that predicts the labels when\ngiven input features. Each feature--label pair is called an example. Sometimes, when the context is clear,\nwe may use the term *examples*\nto refer to a collection of inputs,\neven when the corresponding labels are unknown. The supervision comes into play\nbecause, for choosing the parameters,\nwe (the supervisors) provide the model\nwith a dataset consisting of labeled examples. In probabilistic terms, we typically are interested in estimating\nthe conditional probability of a label given input features. While it is just one among several paradigms,\nsupervised learning accounts for the majority of successful\napplications of machine learning in industry. Partly that is because many important tasks\ncan be described crisply as estimating the probability\nof something unknown given a particular set of available data:\n\n* Predict cancer vs. not cancer, given a computer tomography image. * Predict the correct translation in French, given a sentence in English. * Predict the price of a stock next month based on this month's financial reporting data. While all supervised learning problems\nare captured by the simple description\n\"predicting the labels given input features\",\nsupervised learning itself can take diverse forms\nand require tons of modeling decisions,\ndepending on (among other considerations)\nthe type, size, and quantity of the inputs and outputs. For example, we use different models\nfor processing sequences of arbitrary lengths\nand fixed-length vector representations. We will visit many of these problems\nin depth throughout this book. Informally, the learning process looks something like the following. First, grab a big collection of examples for which the features are known\nand select from them a random subset,\nacquiring the ground truth labels for each."
    },
    {
      "chunk_id": "ec19fa7f3dfe_1",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Informally, the learning process looks something like the following. First, grab a big collection of examples for which the features are known\nand select from them a random subset,\nacquiring the ground truth labels for each. Sometimes these labels might be available data that have already been collected\n(e.g., did a patient die within the following year?)\nand other times we might need to employ human annotators to label the data,\n(e.g., assigning images to categories). Together, these inputs and corresponding labels comprise the training set. We feed the training dataset into a supervised learning algorithm,\na function that takes as input a dataset\nand outputs another function: the learned model. Finally, we can feed previously unseen inputs to the learned model,\nusing its outputs as predictions of the corresponding label. The full process is drawn in :numref:`fig_supervised_learning`. ![Supervised learning.](../img/supervised-learning.svg)\n:label:`fig_supervised_learning`\n\n#### Regression\n\nPerhaps the simplest supervised learning task\nto wrap your head around is *regression*. Consider, for example, a set of data harvested\nfrom a database of home sales. We might construct a table,\nin which each row corresponds to a different house,\nand each column corresponds to some relevant attribute,\nsuch as the square footage of a house,\nthe number of bedrooms, the number of bathrooms,\nand the number of minutes (walking) to the center of town. In this dataset, each example would be a specific house,\nand the corresponding feature vector would be one row in the table. If you live in New York or San Francisco,\nand you are not the CEO of Amazon, Google, Microsoft, or Facebook,\nthe (sq. footage, no. of bedrooms, no. of bathrooms, walking distance)\nfeature vector for your home might look something like: $[600, 1, 1, 60]$. However, if you live in Pittsburgh, it might look more like $[3000, 4, 3, 10]$. Fixed-length feature vectors like this are essential\nfor most classic machine learning algorithms."
    },
    {
      "chunk_id": "ec19fa7f3dfe_2",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "However, if you live in Pittsburgh, it might look more like $[3000, 4, 3, 10]$. Fixed-length feature vectors like this are essential\nfor most classic machine learning algorithms. What makes a problem a regression is actually\nthe form of the target. Say that you are in the market for a new home. You might want to estimate the fair market value of a house,\ngiven some features such as above. The data here might consist of historical home listings\nand the labels might be the observed sales prices. When labels take on arbitrary numerical values\n(even within some interval),\nwe call this a *regression* problem. The goal is to produce a model whose predictions\nclosely approximate the actual label values. Lots of practical problems are easily described as regression problems. Predicting the rating that a user will assign to a movie\ncan be thought of as a regression problem\nand if you designed a great algorithm\nto accomplish this feat in 2009,\nyou might have won the [1-million-dollar Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize). Predicting the length of stay for patients in the hospital\nis also a regression problem. A good rule of thumb is that any *how much?* or *how many?* problem\nis likely to be regression. For example:\n\n* How many hours will this surgery take? * How much rainfall will this town have in the next six hours? Even if you have never worked with machine learning before,\nyou have probably worked through a regression problem informally. Imagine, for example, that you had your drains repaired\nand that your contractor spent 3 hours\nremoving gunk from your sewage pipes. Then they sent you a bill of 350 dollars. Now imagine that your friend hired the same contractor for 2 hours\nand received a bill of 250 dollars. If someone then asked you how much to expect\non their upcoming gunk-removal invoice\nyou might make some reasonable assumptions,\nsuch as more hours worked costs more dollars. You might also assume that there is some base charge\nand that the contractor then charges per hour."
    },
    {
      "chunk_id": "ec19fa7f3dfe_3",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "You might also assume that there is some base charge\nand that the contractor then charges per hour. If these assumptions held true, then given these two data examples,\nyou could already identify the contractor's pricing structure:\n100 dollars per hour plus 50 dollars to show up at your house. If you followed that much, then you already understand\nthe high-level idea behind *linear* regression. In this case, we could produce the parameters\nthat exactly matched the contractor's prices. Sometimes this is not possible,\ne.g., if some of the variation\narises from factors beyond your two features. In these cases, we will try to learn models\nthat minimize the distance between our predictions and the observed values. In most of our chapters, we will focus on\nminimizing the squared error loss function. As we will see later, this loss corresponds to the assumption\nthat our data were corrupted by Gaussian noise. #### Classification\n\nWhile regression models are great\nfor addressing *how many?* questions,\nlots of problems do not fit comfortably in this template. Consider, for example, a bank that wants\nto develop a check scanning feature for its mobile app. Ideally, the customer would simply snap a photo of a check\nand the app would automatically recognize the text from the image. Assuming that we had some ability\nto segment out image patches\ncorresponding to each handwritten character,\nthen the primary remaining task would be\nto determine which character among some known set\nis depicted in each image patch. These kinds of *which one?* problems are called *classification*\nand require a different set of tools\nfrom those used for regression,\nalthough many techniques will carry over. In *classification*, we want our model to look at features,\ne.g., the pixel values in an image,\nand then predict to which *category*\n(sometimes called a *class*)\namong some discrete set of options,\nan example belongs. For handwritten digits, we might have ten classes,\ncorresponding to the digits 0 through 9."
    },
    {
      "chunk_id": "ec19fa7f3dfe_4",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "For handwritten digits, we might have ten classes,\ncorresponding to the digits 0 through 9. The simplest form of classification is when there are only two classes,\na problem which we call *binary classification*. For example, our dataset could consist of images of animals\nand our labels  might be the classes $\\textrm{\\{cat, dog\\}}$. Whereas in regression we sought a regressor to output a numerical value,\nin classification we seek a classifier,\nwhose output is the predicted class assignment. For reasons that we will get into as the book gets more technical,\nit can be difficult to optimize a model that can only output\na *firm* categorical assignment,\ne.g., either \"cat\" or \"dog\". In these cases, it is usually much easier to express\nour model in the language of probabilities. Given features of an example,\nour model assigns a probability\nto each possible class. Returning to our animal classification example\nwhere the classes are $\\textrm{\\{cat, dog\\}}$,\na classifier might see an image and output the probability\nthat the image is a cat as 0.9. We can interpret this number by saying that the classifier\nis 90\\% sure that the image depicts a cat. The magnitude of the probability for the predicted class\nconveys a notion of uncertainty. It is not the only one available\nand we will discuss others in chapters dealing with more advanced topics. When we have more than two possible classes,\nwe call the problem *multiclass classification*. Common examples include handwritten character recognition\n$\\textrm{\\{0, 1, 2, ... 9, a, b, c, ...\\}}$. While we attacked regression problems by trying\nto minimize the squared error loss function,\nthe common loss function for classification problems is called *cross-entropy*,\nwhose name will be demystified\nwhen we introduce information theory in later chapters. Note that the most likely class is not necessarily\nthe one that you are going to use for your decision. Assume that you find a beautiful mushroom in your backyard\nas shown in :numref:`fig_death_cap`."
    },
    {
      "chunk_id": "ec19fa7f3dfe_5",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Note that the most likely class is not necessarily\nthe one that you are going to use for your decision. Assume that you find a beautiful mushroom in your backyard\nas shown in :numref:`fig_death_cap`. ![Death cap---do not eat!](../img/death-cap.jpg)\n:width:`200px`\n:label:`fig_death_cap`\n\nNow, assume that you built a classifier and trained it\nto predict whether a mushroom is poisonous based on a photograph. Say our poison-detection classifier outputs\nthat the probability that\n:numref:`fig_death_cap` shows a death cap is 0.2. In other words, the classifier is 80\\% sure\nthat our mushroom is not a death cap. Still, you would have to be a fool to eat it. That is because the certain benefit of a delicious dinner\nis not worth a 20\\% risk of dying from it. In other words, the effect of the uncertain risk\noutweighs the benefit by far. Thus, in order to make a decision about whether to eat the mushroom,\nwe need to compute the expected detriment\nassociated with each action\nwhich depends both on the likely outcomes\nand the benefits or harms associated with each. In this case, the detriment incurred\nby eating the mushroom\nmight be $0.2 \\times \\infty + 0.8 \\times 0 = \\infty$,\nwhereas the loss of discarding it\nis $0.2 \\times 0 + 0.8 \\times 1 = 0.8$. Our caution was justified:\nas any mycologist would tell us,\nthe mushroom in :numref:`fig_death_cap`\nis actually a death cap. Classification can get much more complicated than just\nbinary or multiclass classification. For instance, there are some variants of classification\naddressing hierarchically structured classes. In such cases not all errors are equal---if\nwe must err, we might prefer to misclassify\nto a related class rather than a distant class. Usually, this is referred to as *hierarchical classification*. For inspiration, you might think of [Linnaeus](https://en.wikipedia.org/wiki/Carl_Linnaeus),\nwho organized fauna in a hierarchy."
    },
    {
      "chunk_id": "ec19fa7f3dfe_6",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Usually, this is referred to as *hierarchical classification*. For inspiration, you might think of [Linnaeus](https://en.wikipedia.org/wiki/Carl_Linnaeus),\nwho organized fauna in a hierarchy. In the case of animal classification,\nit might not be so bad to mistake\na poodle for a schnauzer,\nbut our model would pay a huge penalty\nif it confused a poodle with a dinosaur. Which hierarchy is relevant might depend\non how you plan to use the model. For example, rattlesnakes and garter snakes\nmight be close on the phylogenetic tree,\nbut mistaking a rattler for a garter could have fatal consequences. #### Tagging\n\nSome classification problems fit neatly\ninto the binary or multiclass classification setups. For example, we could train a normal binary classifier\nto distinguish cats from dogs. Given the current state of computer vision,\nwe can do this easily, with off-the-shelf tools. Nonetheless, no matter how accurate our model gets,\nwe might find ourselves in trouble when the classifier\nencounters an image of the *Town Musicians of Bremen*,\na popular German fairy tale featuring four animals\n(:numref:`fig_stackedanimals`). ![A donkey, a dog, a cat, and a rooster.](../img/stackedanimals.png)\n:width:`300px`\n:label:`fig_stackedanimals`\n\nAs you can see, the photo features a cat,\na rooster, a dog, and a donkey,\nwith some trees in the background. If we anticipate encountering such images,\nmulticlass classification might not be\nthe right problem formulation. Instead, we might want to give the model the option of\nsaying the image depicts a cat, a dog, a donkey,\n*and* a rooster. The problem of learning to predict classes that are\nnot mutually exclusive is called *multi-label classification*. Auto-tagging problems are typically best described\nin terms of multi-label classification. Think of the tags people might apply\nto posts on a technical blog,\ne.g., \"machine learning\", \"technology\", \"gadgets\",\n\"programming languages\", \"Linux\", \"cloud computing\", \"AWS\". A typical article might have 5--10 tags applied."
    },
    {
      "chunk_id": "ec19fa7f3dfe_7",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Think of the tags people might apply\nto posts on a technical blog,\ne.g., \"machine learning\", \"technology\", \"gadgets\",\n\"programming languages\", \"Linux\", \"cloud computing\", \"AWS\". A typical article might have 5--10 tags applied. Typically, tags will exhibit some correlation structure. Posts about \"cloud computing\" are likely to mention \"AWS\"\nand posts about \"machine learning\" are likely to mention \"GPUs\". Sometimes such tagging problems\ndraw on enormous label sets. The National Library of Medicine\nemploys many professional annotators\nwho associate each article to be indexed in PubMed\nwith a set of tags drawn from the\nMedical Subject Headings (MeSH) ontology,\na collection of roughly 28,000 tags. Correctly tagging articles is important\nbecause it allows researchers to conduct\nexhaustive reviews of the literature. This is a time-consuming process and typically there is a one-year lag between archiving and tagging. Machine learning can provide provisional tags\nuntil each article has a proper manual review. Indeed, for several years, the BioASQ organization\nhas [hosted competitions](http://bioasq.org/)\nfor this task. #### Search\n\nIn the field of information retrieval,\nwe often impose ranks on sets of items. Take web search for example. The goal is less to determine *whether*\na particular page is relevant for a query, \nbut rather which, among a set of relevant results,\nshould be shown most prominently\nto a particular user. One way of doing this might be\nto first assign a score\nto every element in the set\nand then to retrieve the top-rated elements. [PageRank](https://en.wikipedia.org/wiki/PageRank),\nthe original secret sauce behind the Google search engine,\nwas an early example of such a scoring system. Weirdly, the scoring provided by PageRank\ndid not depend on the actual query. Instead, they relied on a simple relevance filter\nto identify the set of relevant candidates\nand then used PageRank to prioritize\nthe more authoritative pages."
    },
    {
      "chunk_id": "ec19fa7f3dfe_8",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Weirdly, the scoring provided by PageRank\ndid not depend on the actual query. Instead, they relied on a simple relevance filter\nto identify the set of relevant candidates\nand then used PageRank to prioritize\nthe more authoritative pages. Nowadays, search engines use machine learning and behavioral models\nto obtain query-dependent relevance scores. There are entire academic conferences devoted to this subject. #### Recommender Systems\n:label:`subsec_recommender_systems`\n\nRecommender systems are another problem setting\nthat is related to search and ranking. The problems are similar insofar as the goal\nis to display a set of items relevant to the user. The main difference is the emphasis on *personalization*\nto specific users in the context of recommender systems. For instance, for movie recommendations,\nthe results page for a science fiction fan\nand the results page\nfor a connoisseur of Peter Sellers comedies\nmight differ significantly. Similar problems pop up in other recommendation settings,\ne.g., for retail products, music, and news recommendation. In some cases, customers provide explicit feedback,\ncommunicating how much they liked a particular product\n(e.g., the product ratings and reviews\non Amazon, IMDb, or Goodreads). In other cases, they provide implicit feedback,\ne.g., by skipping titles on a playlist,\nwhich might indicate \ndissatisfaction or maybe just\nindicate\nthat the song was inappropriate in context. In the simplest formulations,\nthese systems are trained\nto estimate some score,\nsuch as an expected star rating\nor the probability that a given user\nwill purchase a particular item. Given such a model, for any given user,\nwe could retrieve the set of objects with the largest scores,\nwhich could then be recommended to the user. Production systems are considerably more advanced\nand take detailed user activity and item characteristics\ninto account when computing such scores."
    },
    {
      "chunk_id": "ec19fa7f3dfe_9",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Production systems are considerably more advanced\nand take detailed user activity and item characteristics\ninto account when computing such scores. :numref:`fig_deeplearning_amazon` displays the deep learning books\nrecommended by Amazon based on personalization algorithms\ntuned to capture Aston's preferences. ![Deep learning books recommended by Amazon.](../img/deeplearning-amazon.jpg)\n:label:`fig_deeplearning_amazon`\n\nDespite their tremendous economic value,\nrecommender systems\nnaively built on top of predictive models\nsuffer some serious conceptual flaws. To start, we only observe *censored feedback*:\nusers preferentially rate movies\nthat they feel strongly about. For example, on a five-point scale,\nyou might notice that items receive\nmany one- and five-star ratings\nbut that there are conspicuously few three-star ratings. Moreover, current purchase habits are often a result\nof the recommendation algorithm currently in place,\nbut learning algorithms do not always take this detail into account. Thus it is possible for feedback loops to form\nwhere a recommender system preferentially pushes an item\nthat is then taken to be better (due to greater purchases)\nand in turn is recommended even more frequently. Many of these problems---about\nhow to deal with censoring,\nincentives, and feedback loops---are important open research questions. #### Sequence Learning\n\nSo far, we have looked at problems where we have\nsome fixed number of inputs and produce a fixed number of outputs. For example, we considered predicting house prices\ngiven a fixed set of features:\nsquare footage, number of bedrooms,\nnumber of bathrooms, and the transit time to downtown. We also discussed mapping from an image (of fixed dimension)\nto the predicted probabilities that it belongs\nto each among a fixed number of classes\nand predicting star ratings associated with purchases\nbased on the user ID and product ID alone. In these cases, once our model is trained,\nafter each test example is fed into our model,\nit is immediately forgotten."
    },
    {
      "chunk_id": "ec19fa7f3dfe_10",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "In these cases, once our model is trained,\nafter each test example is fed into our model,\nit is immediately forgotten. We assumed that successive observations were independent\nand thus there was no need to hold on to this context. But how should we deal with video snippets? In this case, each snippet might consist of a different number of frames. And our guess of what is going on in each frame might be much stronger\nif we take into account the previous or succeeding frames. The same goes for language. For example, one popular deep learning problem is machine translation:\nthe task of ingesting sentences in some source language\nand predicting their translations in another language. Such problems also occur in medicine. We might want a model to monitor patients in the intensive care unit\nand to fire off alerts whenever their risk of dying in the next 24 hours\nexceeds some threshold. Here, we would not throw away everything\nthat we know about the patient history every hour,\nbecause we might not want to make predictions based only\non the most recent measurements. Questions like these are among the most\nexciting applications of machine learning\nand they are instances of *sequence learning*. They require a model either to ingest sequences of inputs\nor to emit sequences of outputs (or both). Specifically, *sequence-to-sequence learning* considers problems\nwhere both inputs and outputs consist of variable-length sequences. Examples include machine translation\nand speech-to-text transcription. While it is impossible to consider\nall types of sequence transformations,\nthe following special cases are worth mentioning. **Tagging and Parsing**. This involves annotating a text sequence with attributes. Here, the inputs and outputs are *aligned*,\ni.e., they are of the same number\nand occur in a corresponding order. For instance, in *part-of-speech (PoS) tagging*,\nwe annotate every word in a sentence\nwith the corresponding part of speech,\ni.e., \"noun\" or \"direct object\"."
    },
    {
      "chunk_id": "ec19fa7f3dfe_11",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "For instance, in *part-of-speech (PoS) tagging*,\nwe annotate every word in a sentence\nwith the corresponding part of speech,\ni.e., \"noun\" or \"direct object\". Alternatively, we might want to know\nwhich groups of contiguous words refer to named entities,\nlike *people*, *places*, or *organizations*. In the cartoonishly simple example below,\nwe might just want to indicate whether or not any word in the sentence is part of a named entity (tagged as \"Ent\"). ```text\nTom has dinner in Washington with Sally\nEnt  -    -    -     Ent      -    Ent\n```\n\n\n**Automatic Speech Recognition**. With speech recognition, the input sequence\nis an audio recording of a speaker (:numref:`fig_speech`),\nand the output is a transcript of what the speaker said. The challenge is that there are many more audio frames\n(sound is typically sampled at 8kHz or 16kHz)\nthan text, i.e., there is no 1:1 correspondence between audio and text,\nsince thousands of samples may\ncorrespond to a single spoken word. These are sequence-to-sequence learning problems,\nwhere the output is much shorter than the input. While humans are remarkably good at recognizing speech,\neven from low-quality audio,\ngetting computers to perform the same feat\nis a formidable challenge. ![`-D-e-e-p- L-ea-r-ni-ng-` in an audio recording.](../img/speech.png)\n:width:`700px`\n:label:`fig_speech`\n\n**Text to Speech**. This is the inverse of automatic speech recognition. Here, the input is text and the output is an audio file. In this case, the output is much longer than the input. **Machine Translation**. Unlike the case of speech recognition,\nwhere corresponding inputs and outputs\noccur in the same order,\nin machine translation,\nunaligned data poses a new challenge. Here the input and output sequences\ncan have different lengths,\nand the corresponding regions\nof the respective sequences\nmay appear in a different order."
    },
    {
      "chunk_id": "ec19fa7f3dfe_12",
      "chapter": "index",
      "heading": "Supervised Learning",
      "text": "Here the input and output sequences\ncan have different lengths,\nand the corresponding regions\nof the respective sequences\nmay appear in a different order. Consider the following illustrative example\nof the peculiar tendency of Germans\nto place the verbs at the end of sentences:\n\n```text\nGerman:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut? English:          Have you already looked at this excellent textbook? Wrong alignment:  Have you yourself already this excellent textbook looked at? ```\n\n\nMany related problems pop up in other learning tasks. For instance, determining the order in which a user\nreads a webpage is a two-dimensional layout analysis problem. Dialogue problems exhibit all kinds of additional complications,\nwhere determining what to say next requires taking into account\nreal-world knowledge and the prior state of the conversation\nacross long temporal distances. Such topics are active areas of research."
    },
    {
      "chunk_id": "103ebebdf485_0",
      "chapter": "index",
      "heading": "Unsupervised and Self-Supervised Learning",
      "text": "The previous examples focused on supervised learning,\nwhere we feed the model a giant dataset\ncontaining both the features and corresponding label values. You could think of the supervised learner as having\nan extremely specialized job and an extremely dictatorial boss. The boss stands over the learner's shoulder and tells them exactly what to do\nin every situation until they learn to map from situations to actions. Working for such a boss sounds pretty lame. On the other hand, pleasing such a boss is pretty easy. You just recognize the pattern as quickly as possible\nand imitate the boss's actions. Considering the opposite situation,\nit could be frustrating to work for a boss\nwho has no idea what they want you to do. However, if you plan to be a data scientist,\nyou had better get used to it. The boss might just hand you a giant dump of data\nand tell you to *do some data science with it!*\nThis sounds vague because it is vague. We call this class of problems *unsupervised learning*,\nand the type and number of questions we can ask\nis limited only by our creativity. We will address unsupervised learning techniques\nin later chapters. To whet your appetite for now,\nwe describe a few of the following questions you might ask. * Can we find a small number of prototypes\nthat accurately summarize the data? Given a set of photos, can we group them into landscape photos,\npictures of dogs, babies, cats, and mountain peaks? Likewise, given a collection of users' browsing activities,\ncan we group them into users with similar behavior? This problem is typically known as *clustering*. * Can we find a small number of parameters\nthat accurately capture the relevant properties of the data? The trajectories of a ball are well described\nby velocity, diameter, and mass of the ball. Tailors have developed a small number of parameters\nthat describe human body shape fairly accurately\nfor the purpose of fitting clothes. These problems are referred to as *subspace estimation*."
    },
    {
      "chunk_id": "103ebebdf485_1",
      "chapter": "index",
      "heading": "Unsupervised and Self-Supervised Learning",
      "text": "Tailors have developed a small number of parameters\nthat describe human body shape fairly accurately\nfor the purpose of fitting clothes. These problems are referred to as *subspace estimation*. If the dependence is linear, it is called *principal component analysis*. * Is there a representation of (arbitrarily structured) objects\nin Euclidean space\nsuch that symbolic properties can be well matched? This can be used to describe entities and their relations,\nsuch as \"Rome\" $-$ \"Italy\" $+$ \"France\" $=$ \"Paris\". * Is there a description of the root causes\nof much of the data that we observe? For instance, if we have demographic data\nabout house prices, pollution, crime, location,\neducation, and salaries, can we discover\nhow they are related simply based on empirical data? The fields concerned with *causality* and\n*probabilistic graphical models* tackle such questions. * Another important and exciting recent development in unsupervised learning\nis the advent of *deep generative models*. These models estimate the density of the data,\neither explicitly or *implicitly*. Once trained, we can use a generative model\neither to score examples according to how likely they are,\nor to sample synthetic examples from the learned distribution. Early deep learning breakthroughs in generative modeling\ncame with the invention of *variational autoencoders* :cite:`Kingma.Welling.2014,rezende2014stochastic`\nand continued with the development of *generative adversarial networks* :cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`. More recent advances include normalizing flows :cite:`dinh2014nice,dinh2017density` and\ndiffusion models :cite:`sohl2015deep,song2019generative,ho2020denoising,song2021score`. A further development in unsupervised learning\nhas been the rise of *self-supervised learning*,\ntechniques that leverage some aspect of the unlabeled data\nto provide supervision."
    },
    {
      "chunk_id": "103ebebdf485_2",
      "chapter": "index",
      "heading": "Unsupervised and Self-Supervised Learning",
      "text": "A further development in unsupervised learning\nhas been the rise of *self-supervised learning*,\ntechniques that leverage some aspect of the unlabeled data\nto provide supervision. For text, we can train models\nto \"fill in the blanks\"\nby predicting randomly masked words\nusing their surrounding words (contexts)\nin big corpora without any labeling effort :cite:`Devlin.Chang.Lee.ea.2018`! For images, we may train models\nto tell the relative position\nbetween two cropped regions\nof the same image :cite:`Doersch.Gupta.Efros.2015`,\nto predict an occluded part of an image\nbased on the remaining portions of the image,\nor to predict whether two examples\nare perturbed versions of the same underlying image. Self-supervised models often learn representations\nthat are subsequently leveraged\nby fine-tuning the resulting models\non some downstream task of interest."
    },
    {
      "chunk_id": "60675096a331_0",
      "chapter": "index",
      "heading": "Interacting with an Environment",
      "text": "So far, we have not discussed where data actually comes from,\nor what actually happens when a machine learning model generates an output. That is because supervised learning and unsupervised learning\ndo not address these issues in a very sophisticated way. In each case, we grab a big pile of data upfront,\nthen set our pattern recognition machines in motion\nwithout ever interacting with the environment again. Because all the learning takes place\nafter the algorithm is disconnected from the environment,\nthis is sometimes called *offline learning*. For example, supervised learning assumes\nthe simple interaction pattern\ndepicted in :numref:`fig_data_collection`. ![Collecting data for supervised learning from an environment.](../img/data-collection.svg)\n:label:`fig_data_collection`\n\nThis simplicity of offline learning has its charms. The upside is that we can worry\nabout pattern recognition in isolation,\nwith no concern about complications arising\nfrom interactions with a dynamic environment. But this problem formulation is limiting. If you grew up reading Asimov's Robot novels,\nthen you probably picture artificially intelligent agents\ncapable not only of making predictions,\nbut also of taking actions in the world. We want to think about intelligent *agents*,\nnot just predictive models. This means that we need to think about choosing *actions*,\nnot just making predictions. In contrast to mere predictions,\nactions actually impact the environment. If we want to train an intelligent agent,\nwe must account for the way its actions might\nimpact the future observations of the agent, and so offline learning is inappropriate. Considering the interaction with an environment\nopens a whole set of new modeling questions. The following are just a few examples. * Does the environment remember what we did previously? * Does the environment want to help us, e.g., a user reading text into a speech recognizer? * Does the environment want to beat us, e.g., spammers adapting their emails to evade spam filters?"
    },
    {
      "chunk_id": "60675096a331_1",
      "chapter": "index",
      "heading": "Interacting with an Environment",
      "text": "* Does the environment remember what we did previously? * Does the environment want to help us, e.g., a user reading text into a speech recognizer? * Does the environment want to beat us, e.g., spammers adapting their emails to evade spam filters? * Does the environment have shifting dynamics? For example, would future data always resemble the past or would the patterns change over time, either naturally or in response to our automated tools? These questions raise the problem of *distribution shift*,\nwhere training and test data are different. An example of this, that many of us may have met, is when taking exams written by a lecturer,\nwhile the homework was composed by their teaching assistants. Next, we briefly describe reinforcement learning,\na rich framework for posing learning problems in which\nan agent interacts with an environment."
    },
    {
      "chunk_id": "026dd62869d6_0",
      "chapter": "index",
      "heading": "Reinforcement Learning",
      "text": "If you are interested in using machine learning\nto develop an agent that interacts with an environment\nand takes actions, then you are probably going to wind up\nfocusing on *reinforcement learning*. This might include applications to robotics,\nto dialogue systems,\nand even to developing artificial intelligence (AI)\nfor video games. *Deep reinforcement learning*, which applies\ndeep learning to reinforcement learning problems,\nhas surged in popularity. The breakthrough deep Q-network, that beat humans\nat Atari games using only the visual input :cite:`mnih2015human`,\nand the AlphaGo program, which dethroned the world champion\nat the board game Go :cite:`Silver.Huang.Maddison.ea.2016`,\nare two prominent examples. Reinforcement learning gives a very general statement of a problem\nin which an agent interacts with an environment over a series of time steps. At each time step, the agent receives some *observation*\nfrom the environment and must choose an *action*\nthat is subsequently transmitted back to the environment\nvia some mechanism (sometimes called an *actuator*), when, after each loop, \nthe agent receives a reward from the environment. This process is illustrated in :numref:`fig_rl-environment`. The agent then receives a subsequent observation,\nand chooses a subsequent action, and so on. The behavior of a reinforcement learning agent is governed by a *policy*. In brief, a *policy* is just a function that maps\nfrom observations of the environment to actions. The goal of reinforcement learning is to produce good policies. ![The interaction between reinforcement learning and an environment.](../img/rl-environment.svg)\n:label:`fig_rl-environment`\n\nIt is hard to overstate the generality\nof the reinforcement learning framework. For example, supervised learning\ncan be recast as reinforcement learning. Say we had a classification problem. We could create a reinforcement learning agent\nwith one action corresponding to each class."
    },
    {
      "chunk_id": "026dd62869d6_1",
      "chapter": "index",
      "heading": "Reinforcement Learning",
      "text": "For example, supervised learning\ncan be recast as reinforcement learning. Say we had a classification problem. We could create a reinforcement learning agent\nwith one action corresponding to each class. We could then create an environment which gave a reward\nthat was exactly equal to the loss function\nfrom the original supervised learning problem. Further, reinforcement learning\ncan also address many problems\nthat supervised learning cannot. For example, in supervised learning,\nwe always expect that the training input\ncomes associated with the correct label. But in reinforcement learning,\nwe do not assume that, for each observation\nthe environment tells us the optimal action. In general, we just get some reward. Moreover, the environment may not even tell us\nwhich actions led to the reward. Consider the game of chess. The only real reward signal comes at the end of the game\nwhen we either win, earning a reward of, say, $1$,\nor when we lose, receiving a reward of, say, $-1$. So reinforcement learners must deal\nwith the *credit assignment* problem:\ndetermining which actions to credit or blame for an outcome. The same goes for an employee\nwho gets a promotion on October 11. That promotion likely reflects a number\nof well-chosen actions over the previous year. Getting promoted in the future requires figuring out\nwhich actions along the way led to the earlier promotions. Reinforcement learners may also have to deal\nwith the problem of partial observability. That is, the current observation might not\ntell you everything about your current state. Say your cleaning robot found itself trapped\nin one of many identical closets in your house. Rescuing the robot involves inferring\nits precise location which might require considering earlier observations prior to it entering the closet. Finally, at any given point, reinforcement learners\nmight know of one good policy,\nbut there might be many other better policies\nthat the agent has never tried."
    },
    {
      "chunk_id": "026dd62869d6_2",
      "chapter": "index",
      "heading": "Reinforcement Learning",
      "text": "Finally, at any given point, reinforcement learners\nmight know of one good policy,\nbut there might be many other better policies\nthat the agent has never tried. The reinforcement learner must constantly choose\nwhether to *exploit* the best (currently) known strategy as a policy,\nor to *explore* the space of strategies,\npotentially giving up some short-term reward\nin exchange for knowledge. The general reinforcement learning problem\nhas a very general setting. Actions affect subsequent observations. Rewards are only observed when they correspond to the chosen actions. The environment may be either fully or partially observed. Accounting for all this complexity at once may be asking too much. Moreover, not every practical problem exhibits all this complexity. As a result, researchers have studied a number of\nspecial cases of reinforcement learning problems. When the environment is fully observed,\nwe call the reinforcement learning problem a *Markov decision process*. When the state does not depend on the previous actions,\nwe call it a *contextual bandit problem*. When there is no state, just a set of available actions\nwith initially unknown rewards, we have the classic *multi-armed bandit problem*."
    },
    {
      "chunk_id": "1c2b13544c2f_0",
      "chapter": "index",
      "heading": "Roots",
      "text": "We have just reviewed a small subset of problems\nthat machine learning can address. For a diverse set of machine learning problems,\ndeep learning provides powerful tools for their solution. Although many deep learning methods are recent inventions,\nthe core ideas behind learning from data\nhave been studied for centuries. In fact, humans have held the desire to analyze data\nand to predict future outcomes for \nages, and it is this desire that is at the root of much of natural science and mathematics. Two examples are the Bernoulli distribution, named after\n[Jacob Bernoulli (1655--1705)](https://en.wikipedia.org/wiki/Jacob_Bernoulli),\nand the Gaussian distribution discovered\nby [Carl Friedrich Gauss (1777--1855)](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss). Gauss invented, for instance, the least mean squares algorithm,\nwhich is still used today for a multitude of problems\nfrom insurance calculations to medical diagnostics. Such tools enhanced the experimental approach\nin the natural sciences---for instance, Ohm's law\nrelating current and voltage in a resistor\nis perfectly described by a linear model. Even in the middle ages, mathematicians\nhad a keen intuition of estimates. For instance, the geometry book of [Jacob K\u00f6bel (1460--1533)](https://www.maa.org/press/periodicals/convergence/mathematical-treasures-jacob-kobels-geometry)\nillustrates averaging the length of 16 adult men's feet\nto estimate the typical foot length in the population (:numref:`fig_koebel`). ![Estimating the length of a foot.](../img/koebel.jpg)\n:width:`500px`\n:label:`fig_koebel`\n\n\nAs a group of individuals exited a church,\n16 adult men were asked to line up in a row\nand have their feet measured. The sum of these measurements was then divided by 16\nto obtain an estimate for what now is called one foot. This \"algorithm\" was later improved\nto deal with misshapen feet;\nThe two men with the shortest and longest feet were sent away,\naveraging only over the remainder. This is among the earliest examples\nof a trimmed mean estimate."
    },
    {
      "chunk_id": "1c2b13544c2f_1",
      "chapter": "index",
      "heading": "Roots",
      "text": "This \"algorithm\" was later improved\nto deal with misshapen feet;\nThe two men with the shortest and longest feet were sent away,\naveraging only over the remainder. This is among the earliest examples\nof a trimmed mean estimate. Statistics really took off with the availability and collection of data. One of its pioneers, [Ronald Fisher (1890--1962)](https://en.wikipedia.org/wiki/Ronald_Fisher),\ncontributed significantly to its theory\nand also its applications in genetics. Many of his algorithms (such as linear discriminant analysis)\nand concepts (such as the Fisher information matrix)\nstill hold a prominent place\nin the foundations of modern statistics. Even his data resources had a lasting impact. The Iris dataset that Fisher released in 1936\nis still sometimes used to demonstrate\nmachine learning algorithms. Fisher was also a proponent of eugenics,\nwhich should remind us that the morally dubious use of data science\nhas as long and enduring a history as its productive use\nin industry and the natural sciences. Other influences for machine learning\ncame from the information theory of\n[Claude Shannon (1916--2001)](https://en.wikipedia.org/wiki/Claude_Shannon)\nand the theory of computation proposed by\n[Alan Turing (1912--1954)](https://en.wikipedia.org/wiki/Alan_Turing). Turing posed the question \"can machines think?\u201d\nin his famous paper *Computing Machinery and Intelligence* :cite:`Turing.1950`. Describing what is now known as the Turing test, he proposed that a machine\ncan be considered *intelligent* if it is difficult\nfor a human evaluator to distinguish between the replies\nfrom a machine and those of a human, based purely on textual interactions. Further influences came from neuroscience and psychology. After all, humans clearly exhibit intelligent behavior. Many scholars have asked whether one could explain\nand possibly reverse engineer this capacity. One of the first biologically inspired algorithms\nwas formulated by [Donald Hebb (1904--1985)](https://en.wikipedia.org/wiki/Donald_O._Hebb)."
    },
    {
      "chunk_id": "1c2b13544c2f_2",
      "chapter": "index",
      "heading": "Roots",
      "text": "Many scholars have asked whether one could explain\nand possibly reverse engineer this capacity. One of the first biologically inspired algorithms\nwas formulated by [Donald Hebb (1904--1985)](https://en.wikipedia.org/wiki/Donald_O._Hebb). In his groundbreaking book *The Organization of Behavior* :cite:`Hebb.1949`,\nhe posited that neurons learn by positive reinforcement. This became known as the Hebbian learning rule. These ideas inspired later work, such as\nRosenblatt's perceptron learning algorithm,\nand laid the foundations of many stochastic gradient descent algorithms\nthat underpin deep learning today:\nreinforce desirable behavior and diminish undesirable behavior\nto obtain good settings of the parameters in a neural network. Biological inspiration is what gave *neural networks* their name. For over a century (dating back to the models of Alexander Bain, 1873,\nand James Sherrington, 1890), researchers have tried to assemble\ncomputational circuits that resemble networks of interacting neurons. Over time, the interpretation of biology has become less literal,\nbut the name stuck. At its heart lie a few key principles\nthat can be found in most networks today:\n\n* The alternation of linear and nonlinear processing units, often referred to as *layers*. * The use of the chain rule (also known as *backpropagation*) for adjusting parameters in the entire network at once. After initial rapid progress, research in neural networks\nlanguished from around 1995 until 2005. This was mainly due to two reasons. First, training a network is computationally very expensive. While random-access memory was plentiful at the end of the past century,\ncomputational power was scarce. Second, datasets were relatively small. In fact, Fisher's Iris dataset from 1936\nwas still a popular tool for testing the efficacy of algorithms. The MNIST dataset with its 60,000 handwritten digits was considered huge."
    },
    {
      "chunk_id": "1c2b13544c2f_3",
      "chapter": "index",
      "heading": "Roots",
      "text": "Second, datasets were relatively small. In fact, Fisher's Iris dataset from 1936\nwas still a popular tool for testing the efficacy of algorithms. The MNIST dataset with its 60,000 handwritten digits was considered huge. Given the scarcity of data and computation,\nstrong statistical tools such as kernel methods,\ndecision trees, and graphical models\nproved empirically superior in many applications. Moreover, unlike neural networks,\nthey did not require weeks to train\nand provided predictable results\nwith strong theoretical guarantees."
    },
    {
      "chunk_id": "f4e4d4ed5178_0",
      "chapter": "index",
      "heading": "The Road to Deep Learning",
      "text": "Much of this changed with the availability\nof massive amounts of data,\nthanks to the World Wide Web,\nthe advent of companies serving\nhundreds of millions of users online,\na dissemination of low-cost, high-quality sensors,\ninexpensive data storage (Kryder's law),\nand cheap computation (Moore's law). In particular, the landscape of computation in deep learning\nwas revolutionized by advances in GPUs that were originally engineered for computer gaming. Suddenly algorithms and models\nthat seemed computationally infeasible\nwere within reach. This is best illustrated in :numref:`tab_intro_decade`. :Dataset vs. computer memory and computational power\n:label:`tab_intro_decade`\n\n|Decade|Dataset|Memory|Floating point calculations per second|\n|:--|:-|:-|:-|\n|1970|100 (Iris)|1 KB|100 KF (Intel 8080)|\n|1980|1 K (house prices in Boston)|100 KB|1 MF (Intel 80186)|\n|1990|10 K (optical character recognition)|10 MB|10 MF (Intel 80486)|\n|2000|10 M (web pages)|100 MB|1 GF (Intel Core)|\n|2010|10 G (advertising)|1 GB|1 TF (NVIDIA C2050)|\n|2020|1 T (social network)|100 GB|1 PF (NVIDIA DGX-2)|\n\n\nNote that random-access memory has not kept pace with the growth in data. At the same time, increases in computational power\nhave outpaced the growth in datasets. This means that statistical models\nneed to become more memory efficient,\nand so they are free to spend more computer cycles\noptimizing parameters, thanks to\nthe increased compute budget. Consequently, the sweet spot in machine learning and statistics\nmoved from (generalized) linear models and kernel methods\nto deep neural networks. This is also one of the reasons why many of the mainstays\nof deep learning, such as multilayer perceptrons\n:cite:`McCulloch.Pitts.1943`, convolutional neural networks\n:cite:`LeCun.Bottou.Bengio.ea.1998`, long short-term memory\n:cite:`Hochreiter.Schmidhuber.1997`,\nand Q-Learning :cite:`Watkins.Dayan.1992`,\nwere essentially \"rediscovered\" in the past decade,\nafter lying comparatively dormant for considerable time."
    },
    {
      "chunk_id": "f4e4d4ed5178_1",
      "chapter": "index",
      "heading": "The Road to Deep Learning",
      "text": "The recent progress in statistical models, applications, and algorithms\nhas sometimes been likened to the Cambrian explosion:\na moment of rapid progress in the evolution of species. Indeed, the state of the art is not just a mere consequence\nof available resources applied to decades-old algorithms. Note that the list of ideas below barely scratches the surface\nof what has helped researchers achieve tremendous progress\nover the past decade. * Novel methods for capacity control, such as *dropout*\n  :cite:`Srivastava.Hinton.Krizhevsky.ea.2014`,\n  have helped to mitigate overfitting. Here, noise is injected :cite:`Bishop.1995`\n  throughout the neural network during training. * *Attention mechanisms* solved a second problem\n  that had plagued statistics for over a century:\n  how to increase the memory and complexity of a system without\n  increasing the number of learnable parameters. Researchers found an elegant solution\n  by using what can only be viewed as\n  a *learnable pointer structure* :cite:`Bahdanau.Cho.Bengio.2014`. Rather than having to remember an entire text sequence, e.g.,\n  for machine translation in a fixed-dimensional representation,\n  all that needed to be stored was a pointer to the intermediate state\n  of the translation process. This allowed for significantly\n  increased accuracy for long sequences, since the model\n  no longer needed to remember the entire sequence before\n  commencing the generation of a new one. * Built solely on attention mechanisms,\n  the *Transformer* architecture :cite:`Vaswani.Shazeer.Parmar.ea.2017` has demonstrated superior *scaling* behavior: it performs better with an increase in dataset size, model size, and amount of training compute :cite:`kaplan2020scaling`."
    },
    {
      "chunk_id": "f4e4d4ed5178_2",
      "chapter": "index",
      "heading": "The Road to Deep Learning",
      "text": "This architecture has demonstrated compelling success in a wide range of areas,\n  such as natural language processing :cite:`Devlin.Chang.Lee.ea.2018,brown2020language`, computer vision :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021,liu2021swin`, speech recognition :cite:`gulati2020conformer`, reinforcement learning :cite:`chen2021decision`, and graph neural networks :cite:`dwivedi2020generalization`. For example, a single Transformer pretrained on modalities\n  as diverse as text, images, joint torques, and button presses\n  can play Atari, caption images, chat,\n  and control a robot :cite:`reed2022generalist`. * Modeling probabilities of text sequences, *language models* can predict text given other text. Scaling up the data, model, and compute has unlocked a growing number of capabilities of language models to perform desired tasks via human-like text generation based on input text :cite:`brown2020language,rae2021scaling,hoffmann2022training,chowdhery2022palm,openai2023gpt4,anil2023palm,touvron2023llama,touvron2023llama2`. For instance, aligning language models with human intent :cite:`ouyang2022training`, OpenAI's [ChatGPT](https://chat.openai.com/) allows users to interact with it in a conversational way to solve problems, such as code debugging and creative writing. * Multi-stage designs, e.g., via the memory networks\n  :cite:`Sukhbaatar.Weston.Fergus.ea.2015`\n  and the neural programmer-interpreter :cite:`Reed.De-Freitas.2015`\n  permitted statistical modelers to describe iterative approaches to reasoning. These tools allow for an internal state of the deep neural network\n  to be modified repeatedly,\n  thus carrying out subsequent steps\n  in a chain of reasoning, just as a processor\n  can modify memory for a computation. * A key development in *deep generative modeling* was the invention\n  of *generative adversarial networks*\n  :cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`."
    },
    {
      "chunk_id": "f4e4d4ed5178_3",
      "chapter": "index",
      "heading": "The Road to Deep Learning",
      "text": "* A key development in *deep generative modeling* was the invention\n  of *generative adversarial networks*\n  :cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`. Traditionally, statistical methods for density estimation\n  and generative models focused on finding proper probability distributions\n  and (often approximate) algorithms for sampling from them. As a result, these algorithms were largely limited by the lack of\n  flexibility inherent in the statistical models. The crucial innovation in generative adversarial networks was to replace the sampler\n  by an arbitrary algorithm with differentiable parameters. These are then adjusted in such a way that the discriminator\n  (effectively a two-sample test) cannot distinguish fake from real data. Through the ability to use arbitrary algorithms to generate data,\n  density estimation was opened up to a wide variety of techniques. Examples of galloping zebras :cite:`Zhu.Park.Isola.ea.2017`\n  and of fake celebrity faces :cite:`Karras.Aila.Laine.ea.2017`\n  are each testimony to this progress. Even amateur doodlers can produce\n  photorealistic images just based on sketches describing the layout of a scene :cite:`Park.Liu.Wang.ea.2019`. * Furthermore, while the diffusion process gradually adds random noise to data samples, *diffusion models* :cite:`sohl2015deep,ho2020denoising` learn the denoising process to gradually construct data samples from random noise, reversing the diffusion process. They have started to replace generative adversarial networks in more recent deep generative models, such as in DALL-E 2 :cite:`ramesh2022hierarchical` and Imagen :cite:`saharia2022photorealistic` for creative art and image generation based on text descriptions. * In many cases, a single GPU is insufficient for processing the large amounts of data available for training. Over the past decade the ability to build parallel and\n  distributed training algorithms has improved significantly."
    },
    {
      "chunk_id": "f4e4d4ed5178_4",
      "chapter": "index",
      "heading": "The Road to Deep Learning",
      "text": "* In many cases, a single GPU is insufficient for processing the large amounts of data available for training. Over the past decade the ability to build parallel and\n  distributed training algorithms has improved significantly. One of the key challenges in designing scalable algorithms\n  is that the workhorse of deep learning optimization,\n  stochastic gradient descent, relies on relatively\n  small minibatches of data to be processed. At the same time, small batches limit the efficiency of GPUs. Hence, training on 1,024 GPUs with a minibatch size of,\n  say, 32 images per batch amounts to an aggregate minibatch\n  of about 32,000 images. Work, first by :citet:`Li.2017`\n  and subsequently by :citet:`You.Gitman.Ginsburg.2017`\n  and :citet:`Jia.Song.He.ea.2018` pushed the size up to 64,000 observations,\n  reducing training time for the ResNet-50 model\n  on the ImageNet dataset to less than 7 minutes. By comparison, training times were initially of the order of days. * The ability to parallelize computation\n  has also contributed to progress in *reinforcement learning*. This has led to significant progress in computers achieving\n  superhuman performance on tasks like Go, Atari games,\n  Starcraft, and in physics simulations (e.g., using MuJoCo)\n  where environment simulators are available. See, e.g., :citet:`Silver.Huang.Maddison.ea.2016` for a description\n  of such achievements in AlphaGo. In a nutshell,\n  reinforcement learning works best\n  if plenty of (state, action, reward) tuples are available. Simulation provides such an avenue. * Deep learning frameworks have played a crucial role\n  in disseminating ideas. The first generation of open-source frameworks\n  for neural network modeling consisted of\n  [Caffe](https://github.com/BVLC/caffe),\n  [Torch](https://github.com/torch), and\n  [Theano](https://github.com/Theano/Theano). Many seminal papers were written using these tools."
    },
    {
      "chunk_id": "f4e4d4ed5178_5",
      "chapter": "index",
      "heading": "The Road to Deep Learning",
      "text": "Many seminal papers were written using these tools. These have now been superseded by\n  [TensorFlow](https://github.com/tensorflow/tensorflow) (often used via its high-level API [Keras](https://github.com/keras-team/keras)), [CNTK](https://github.com/Microsoft/CNTK), [Caffe 2](https://github.com/caffe2/caffe2), and [Apache MXNet](https://github.com/apache/incubator-mxnet). The third generation of frameworks consists\n  of so-called *imperative* tools for deep learning,\n  a trend that was arguably ignited by [Chainer](https://github.com/chainer/chainer),\n  which used a syntax similar to Python NumPy to describe models. This idea was adopted by both [PyTorch](https://github.com/pytorch/pytorch),\n  the [Gluon API](https://github.com/apache/incubator-mxnet) of MXNet,\n  and [JAX](https://github.com/google/jax). The division of labor between system researchers building better tools\nand statistical modelers building better neural networks\nhas greatly simplified things. For instance,\ntraining a linear logistic regression model\nused to be a nontrivial homework problem,\nworthy to give to new machine learning\nPh.D. students at Carnegie Mellon University in 2014. By now, this task can be accomplished\nwith under 10 lines of code,\nputting it firmly within the reach of any programmer."
    },
    {
      "chunk_id": "8fcac99f2c9a_0",
      "chapter": "index",
      "heading": "Success Stories",
      "text": "Artificial intelligence has a long history of delivering results\nthat would be difficult to accomplish otherwise. For instance, mail sorting systems\nusing optical character recognition\nhave been deployed since the 1990s. This is, after all, the source\nof the famous MNIST dataset\nof handwritten digits. The same applies to reading checks for bank deposits and scoring\ncreditworthiness of applicants. Financial transactions are checked for fraud automatically. This forms the backbone of many e-commerce payment systems,\nsuch as PayPal, Stripe, AliPay, WeChat, Apple, Visa, and MasterCard. Computer programs for chess have been competitive for decades. Machine learning feeds search, recommendation, personalization,\nand ranking on the Internet. In other words, machine learning is pervasive, albeit often hidden from sight. It is only recently that AI\nhas been in the limelight, mostly due to\nsolutions to problems\nthat were considered intractable previously\nand that are directly related to consumers. Many of such advances are attributed to deep learning. * Intelligent assistants, such as Apple's Siri,\n  Amazon's Alexa, and Google's assistant,\n  are able to respond to spoken requests\n  with a reasonable degree of accuracy. This includes menial jobs, like turning on light switches,\n  and more complex tasks, such as arranging barber's appointments\n  and offering phone support dialog. This is likely the most noticeable sign\n  that AI is affecting our lives. * A key ingredient in digital assistants\n  is their ability to recognize speech accurately. The accuracy of such systems has gradually\n  increased to the point\n  of achieving parity with humans\n  for certain applications :cite:`Xiong.Wu.Alleva.ea.2018`. * Object recognition has likewise come a long way. Identifying the object in a picture\n  was a fairly challenging task in 2010. On the ImageNet benchmark researchers from NEC Labs\n  and University of Illinois at Urbana-Champaign\n  achieved a top-five error rate of 28% :cite:`Lin.Lv.Zhu.ea.2010`."
    },
    {
      "chunk_id": "8fcac99f2c9a_1",
      "chapter": "index",
      "heading": "Success Stories",
      "text": "Identifying the object in a picture\n  was a fairly challenging task in 2010. On the ImageNet benchmark researchers from NEC Labs\n  and University of Illinois at Urbana-Champaign\n  achieved a top-five error rate of 28% :cite:`Lin.Lv.Zhu.ea.2010`. By 2017, this error rate was reduced to 2.25% :cite:`Hu.Shen.Sun.2018`. Similarly, stunning results have been achieved\n  for identifying birdsong and for diagnosing skin cancer. * Prowess in games used to provide\n  a measuring stick for human ability. Starting from TD-Gammon, a program for playing backgammon\n  using temporal difference reinforcement learning,\n  algorithmic and computational progress\n  has led to algorithms for a wide range of applications. Compared with backgammon, chess has\n  a much more complex state space and set of actions. DeepBlue beat Garry Kasparov using massive parallelism,\n  special-purpose hardware and efficient search\n  through the game tree :cite:`Campbell.Hoane-Jr.Hsu.2002`. Go is more difficult still, due to its huge state space. AlphaGo reached human parity in 2015,\n  using deep learning combined with Monte Carlo tree sampling :cite:`Silver.Huang.Maddison.ea.2016`. The challenge in Poker was that the state space is large\n  and only partially observed\n  (we do not know the opponents' cards). Libratus exceeded human performance in Poker\n  using efficiently structured strategies :cite:`Brown.Sandholm.2017`. * Another indication of progress in AI\n  is the advent of self-driving vehicles. While full autonomy is not yet within reach,\n  excellent progress has been made in this direction,\n  with companies such as Tesla, NVIDIA,\n  and Waymo shipping products\n  that enable partial autonomy. What makes full autonomy so challenging\n  is that proper driving requires\n  the ability to perceive, to reason\n  and to incorporate rules into a system. At present, deep learning is used primarily\n  in the visual aspect of these problems. The rest is heavily tuned by engineers. This barely scratches the surface\nof significant applications of machine learning."
    },
    {
      "chunk_id": "8fcac99f2c9a_2",
      "chapter": "index",
      "heading": "Success Stories",
      "text": "At present, deep learning is used primarily\n  in the visual aspect of these problems. The rest is heavily tuned by engineers. This barely scratches the surface\nof significant applications of machine learning. For instance, robotics, logistics, computational biology,\nparticle physics, and astronomy\nowe some of their most impressive recent advances\nat least in parts to machine learning, which is thus becoming\na ubiquitous tool for engineers and scientists. Frequently, questions about a coming AI apocalypse\nand the plausibility of a *singularity*\nhave been raised in non-technical articles. The fear is that somehow machine learning systems\nwill become sentient and make decisions,\nindependently of their programmers,\nthat directly impact the lives of humans. To some extent, AI already affects\nthe livelihood of humans in direct ways:\ncreditworthiness is assessed automatically,\nautopilots mostly navigate vehicles, decisions about\nwhether to grant bail use statistical data as input. More frivolously, we can ask Alexa to switch on the coffee machine. Fortunately, we are far from a sentient AI system\nthat could deliberately manipulate its human creators. First, AI systems are engineered,\ntrained, and deployed\nin a specific, goal-oriented manner. While their behavior might give the illusion\nof general intelligence, it is a combination of rules, heuristics\nand statistical models that underlie the design. Second, at present, there are simply no tools for *artificial general intelligence*\nthat are able to improve themselves,\nreason about themselves, and that are able to modify,\nextend, and improve their own architecture\nwhile trying to solve general tasks. A much more pressing concern is how AI is being used in our daily lives. It is likely that many routine tasks, currently fulfilled by humans, can and will be automated. Farm robots will likely reduce the costs for organic farmers\nbut they will also automate harvesting operations."
    },
    {
      "chunk_id": "8fcac99f2c9a_3",
      "chapter": "index",
      "heading": "Success Stories",
      "text": "It is likely that many routine tasks, currently fulfilled by humans, can and will be automated. Farm robots will likely reduce the costs for organic farmers\nbut they will also automate harvesting operations. This phase of the industrial revolution\nmay have profound consequences for large swaths of society,\nsince menial jobs provide much employment \nin many countries. Furthermore, statistical models, when applied without care,\ncan lead to racial, gender, or age bias and raise\nreasonable concerns about procedural fairness\nif automated to drive consequential decisions. It is important to ensure that these algorithms are used with care. With what we know today, this strikes us as a much more pressing concern\nthan the potential of malevolent superintelligence for destroying humanity."
    },
    {
      "chunk_id": "14d5de11a3a6_0",
      "chapter": "index",
      "heading": "The Essence of Deep Learning",
      "text": "Thus far, we have talked in broad terms about machine learning. Deep learning is the subset of machine learning\nconcerned with models based on many-layered neural networks. It is *deep* in precisely the sense that its models\nlearn many *layers* of transformations. While this might sound narrow,\ndeep learning has given rise\nto a dizzying array of models, techniques,\nproblem formulations, and applications. Many intuitions have been developed\nto explain the benefits of depth. Arguably, all machine learning\nhas many layers of computation,\nthe first consisting of feature processing steps. What differentiates deep learning is that\nthe operations learned at each of the many layers\nof representations are learned jointly from data. The problems that we have discussed so far,\nsuch as learning from the raw audio signal,\nthe raw pixel values of images,\nor mapping between sentences of arbitrary lengths and\ntheir counterparts in foreign languages,\nare those where deep learning excels\nand traditional methods falter. It turns out that these many-layered models\nare capable of addressing low-level perceptual data\nin a way that previous tools could not. Arguably the most significant commonality\nin deep learning methods is *end-to-end training*. That is, rather than assembling a system\nbased on components that are individually tuned,\none builds the system and then tunes their performance jointly. For instance, in computer vision scientists\nused to separate the process of *feature engineering*\nfrom the process of building machine learning models. The Canny edge detector :cite:`Canny.1987`\nand Lowe's SIFT feature extractor :cite:`Lowe.2004`\nreigned supreme for over a decade as algorithms\nfor mapping images into feature vectors. In bygone days, the crucial part of applying machine learning to these problems\nconsisted of coming up with manually-engineered ways\nof transforming the data into some form amenable to shallow models."
    },
    {
      "chunk_id": "14d5de11a3a6_1",
      "chapter": "index",
      "heading": "The Essence of Deep Learning",
      "text": "In bygone days, the crucial part of applying machine learning to these problems\nconsisted of coming up with manually-engineered ways\nof transforming the data into some form amenable to shallow models. Unfortunately, there is only so much that humans can accomplish\nby ingenuity in comparison with a consistent evaluation\nover millions of choices carried out automatically by an algorithm. When deep learning took over,\nthese feature extractors were replaced\nby automatically tuned filters that yielded superior accuracy. Thus, one key advantage of deep learning is that it replaces\nnot only the shallow models at the end of traditional learning pipelines,\nbut also the labor-intensive process of feature engineering. Moreover, by replacing much of the domain-specific preprocessing,\ndeep learning has eliminated many of the boundaries\nthat previously separated computer vision, speech recognition,\nnatural language processing, medical informatics, and other application areas,\nthereby offering a unified set of tools for tackling diverse problems. Beyond end-to-end training, we are experiencing a transition\nfrom parametric statistical descriptions to fully nonparametric models. When data is scarce, one needs to rely on simplifying assumptions about reality\nin order to obtain useful models. When data is abundant, these can be replaced\nby nonparametric models that better fit the data. To some extent, this mirrors the progress\nthat physics experienced in the middle of the previous century\nwith the availability of computers. Rather than solving by hand parametric approximations of how electrons behave,\none can now resort to numerical simulations of the associated partial differential equations. This has led to much more accurate models,\nalbeit often at the expense of interpretation. Another difference from previous work is the acceptance of suboptimal solutions,\ndealing with nonconvex nonlinear optimization problems,\nand the willingness to try things before proving them."
    },
    {
      "chunk_id": "14d5de11a3a6_2",
      "chapter": "index",
      "heading": "The Essence of Deep Learning",
      "text": "Another difference from previous work is the acceptance of suboptimal solutions,\ndealing with nonconvex nonlinear optimization problems,\nand the willingness to try things before proving them. This new-found empiricism in dealing with statistical problems,\ncombined with a rapid influx of talent has led\nto rapid progress in the development of practical algorithms,\nalbeit in many cases at the expense of modifying\nand re-inventing tools that existed for decades. In the end, the deep learning community prides itself\non sharing tools across academic and corporate boundaries,\nreleasing many excellent libraries, statistical models,\nand trained networks as open source. It is in this spirit that the notebooks forming this book\nare freely available for distribution and use. We have worked hard to lower the barriers of access\nfor anyone wishing to learn about deep learning\nand we hope that our readers will benefit from this."
    },
    {
      "chunk_id": "91d22918ddd9_0",
      "chapter": "index",
      "heading": "Summary",
      "text": "Machine learning studies how computer systems\ncan leverage experience (often data)\nto improve performance at specific tasks.\nIt combines ideas from statistics, data mining, and optimization.\nOften, it is used as a means of implementing AI solutions.\nAs a class of machine learning, representational learning\nfocuses on how to automatically find\nthe appropriate way to represent data.\nConsidered as multi-level representation learning\nthrough learning many layers of transformations,\ndeep learning replaces not only the shallow models\nat the end of traditional machine learning pipelines,\nbut also the labor-intensive process of feature engineering.\nMuch of the recent progress in deep learning\nhas been triggered by an abundance of data\narising from cheap sensors and Internet-scale applications,\nand by significant progress in computation, mostly through GPUs.\nFurthermore, the availability of efficient deep learning frameworks\nhas made design and implementation of whole system optimization significantly easier,\nand this is a key component in obtaining high performance."
    },
    {
      "chunk_id": "020f0e0a8156_0",
      "chapter": "index",
      "heading": "Exercises",
      "text": "1. Which parts of code that you are currently writing could be \"learned\",\n   i.e., improved by learning and automatically determining design choices\n   that are made in your code?\n   Does your code include heuristic design choices?\n   What data might you need to learn the desired behavior?\n1. Which problems that you encounter have many examples for their solution,\n   yet no specific way for automating them?\n   These may be prime candidates for using deep learning.\n1. Describe the relationships between algorithms, data, and computation. How do characteristics of the data and the current available computational resources influence the appropriateness of various algorithms?\n1. Name some settings where end-to-end training is not currently the default approach but where it might be useful.\n\n[Discussions](https://discuss.d2l.ai/t/22)"
    },
    {
      "chunk_id": "71de81040111_0",
      "chapter": "classification",
      "heading": "classification",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# The Base Classification Model\n:label:`sec_classification`\n\nYou may have noticed that the implementations from scratch and the concise implementation using framework functionality were quite similar in the case of regression. The same is true for classification. Since many models in this book deal with classification, it is worth adding functionalities to support this setting specifically. This section provides a base class for classification models to simplify future code.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx, gluon\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom functools import partial\nfrom jax import numpy as jnp\nimport jax\nimport optax\n```"
    },
    {
      "chunk_id": "7aa2377f0f9d_0",
      "chapter": "classification",
      "heading": "The `Classifier` Class",
      "text": ":begin_tab:`pytorch, mxnet, tensorflow`\nWe define the `Classifier` class below. In the `validation_step` we report both the loss value and the classification accuracy on a validation batch. We draw an update for every `num_val_batches` batches. This has the benefit of generating the averaged loss and accuracy on the whole validation data. These average numbers are not exactly correct if the final batch contains fewer examples, but we ignore this minor difference to keep the code simple. :end_tab:\n\n\n:begin_tab:`jax`\nWe define the `Classifier` class below. In the `validation_step` we report both the loss value and the classification accuracy on a validation batch. We draw an update for every `num_val_batches` batches. This has the benefit of generating the averaged loss and accuracy on the whole validation data. These average numbers are not exactly correct if the last batch contains fewer examples, but we ignore this minor difference to keep the code simple. We also redefine the `training_step` method for JAX since all models that will\nsubclass `Classifier` later will have a loss that returns auxiliary data. This auxiliary data can be used for models with batch normalization\n(to be explained in :numref:`sec_batch_norm`), while in all other cases\nwe will make the loss also return a placeholder (empty dictionary) to\nrepresent the auxiliary data."
    },
    {
      "chunk_id": "7aa2377f0f9d_1",
      "chapter": "classification",
      "heading": "The `Classifier` Class",
      "text": "This auxiliary data can be used for models with batch normalization\n(to be explained in :numref:`sec_batch_norm`), while in all other cases\nwe will make the loss also return a placeholder (empty dictionary) to\nrepresent the auxiliary data. :end_tab:\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass Classifier(d2l.Module):  #@save\n    \"\"\"The base class of classification models.\"\"\"\n    def validation_step(self, batch):\n        Y_hat = self(*batch[:-1])\n        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n```\n\n```{.python .input}\n%%tab jax\nclass Classifier(d2l.Module):  #@save\n    \"\"\"The base class of classification models.\"\"\"\n    def training_step(self, params, batch, state):\n        # Here value is a tuple since models with BatchNorm layers require\n        # the loss to return auxiliary data\n        value, grads = jax.value_and_grad(\n            self.loss, has_aux=True)(params, batch[:-1], batch[-1], state)\n        l, _ = value\n        self.plot(\"loss\", l, train=True)\n        return value, grads\n\n    def validation_step(self, params, batch, state):\n        # Discard the second returned value. It is used for training models\n        # with BatchNorm layers since loss also returns auxiliary data\n        l, _ = self.loss(params, batch[:-1], batch[-1], state)\n        self.plot('loss', l, train=False)\n        self.plot('acc', self.accuracy(params, batch[:-1], batch[-1], state),\n                  train=False)\n```\n\nBy default we use a stochastic gradient descent optimizer, operating on minibatches, just as we did in the context of linear regression."
    },
    {
      "chunk_id": "7aa2377f0f9d_2",
      "chapter": "classification",
      "heading": "The `Classifier` Class",
      "text": "```{.python .input}\n%%tab mxnet\n@d2l.add_to_class(d2l.Module)  #@save\ndef configure_optimizers(self):\n    params = self.parameters()\n    if isinstance(params, list):\n        return d2l.SGD(params, self.lr)\n    return gluon.Trainer(params, 'sgd', {'learning_rate': self.lr})\n```\n\n```{.python .input}\n%%tab pytorch\n@d2l.add_to_class(d2l.Module)  #@save\ndef configure_optimizers(self):\n    return torch.optim.SGD(self.parameters(), lr=self.lr)\n```\n\n```{.python .input}\n%%tab tensorflow\n@d2l.add_to_class(d2l.Module)  #@save\ndef configure_optimizers(self):\n    return tf.keras.optimizers.SGD(self.lr)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.Module)  #@save\ndef configure_optimizers(self):\n    return optax.sgd(self.lr)\n```"
    },
    {
      "chunk_id": "d1233479f4ed_0",
      "chapter": "classification",
      "heading": "Accuracy",
      "text": "Given the predicted probability distribution `y_hat`,\nwe typically choose the class with the highest predicted probability\nwhenever we must output a hard prediction. Indeed, many applications require that we make a choice. For instance, Gmail must categorize an email into \"Primary\", \"Social\", \"Updates\", \"Forums\", or \"Spam\". It might estimate probabilities internally,\nbut at the end of the day it has to choose one among the classes. When predictions are consistent with the label class `y`, they are correct. The classification accuracy is the fraction of all predictions that are correct. Although it can be difficult to optimize accuracy directly (it is not differentiable),\nit is often the performance measure that we care about the most. It is often *the*\nrelevant quantity in benchmarks. As such, we will nearly always report it when training classifiers. Accuracy is computed as follows. First, if `y_hat` is a matrix,\nwe assume that the second dimension stores prediction scores for each class. We use `argmax` to obtain the predicted class by the index for the largest entry in each row. Then we [**compare the predicted class with the ground truth `y` elementwise.**]\nSince the equality operator `==` is sensitive to data types,\nwe convert `y_hat`'s data type to match that of `y`. The result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions."
    },
    {
      "chunk_id": "d1233479f4ed_1",
      "chapter": "classification",
      "heading": "Accuracy",
      "text": "The result is a tensor containing entries of 0 (false) and 1 (true). Taking the sum yields the number of correct predictions. ```{.python .input  n=9}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(Classifier)  #@save\ndef accuracy(self, Y_hat, Y, averaged=True):\n    \"\"\"Compute the number of correct predictions.\"\"\"\n    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n    preds = d2l.astype(d2l.argmax(Y_hat, axis=1), Y.dtype)\n    compare = d2l.astype(preds == d2l.reshape(Y, -1), d2l.float32)\n    return d2l.reduce_mean(compare) if averaged else compare\n```\n\n```{.python .input  n=9}\n%%tab jax\n@d2l.add_to_class(Classifier)  #@save\n@partial(jax.jit, static_argnums=(0, 5))\ndef accuracy(self, params, X, Y, state, averaged=True):\n    \"\"\"Compute the number of correct predictions.\"\"\"\n    Y_hat = state.apply_fn({'params': params,\n                            'batch_stats': state.batch_stats},  # BatchNorm Only\n                           *X)\n    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n    preds = d2l.astype(d2l.argmax(Y_hat, axis=1), Y.dtype)\n    compare = d2l.astype(preds == d2l.reshape(Y, -1), d2l.float32)\n    return d2l.reduce_mean(compare) if averaged else compare\n```\n\n```{.python .input  n=10}\n%%tab mxnet\n\n@d2l.add_to_class(d2l.Module)  #@save\ndef get_scratch_params(self):\n    params = []\n    for attr in dir(self):\n        a = getattr(self, attr)\n        if isinstance(a, np.ndarray):\n            params.append(a)\n        if isinstance(a, d2l.Module):\n            params.extend(a.get_scratch_params())\n    return params\n\n@d2l.add_to_class(d2l.Module)  #@save\ndef parameters(self):\n    params = self.collect_params()\n    return params if isinstance(params, gluon.parameter.ParameterDict) and len(\n        params.keys()) else self.get_scratch_params()\n```"
    },
    {
      "chunk_id": "8f18eb6b577d_0",
      "chapter": "classification",
      "heading": "Summary",
      "text": "Classification is a sufficiently common problem that it warrants its own convenience functions. Of central importance in classification is the *accuracy* of the classifier. Note that while we often care primarily about accuracy, we train classifiers to optimize a variety of other objectives for statistical and computational reasons. However, regardless of which loss function was minimized during training, it is useful to have a convenience method for assessing the accuracy of our classifier empirically."
    },
    {
      "chunk_id": "334325ee7ae6_0",
      "chapter": "classification",
      "heading": "Exercises",
      "text": "1. Denote by $L_\\textrm{v}$ the validation loss, and let $L_\\textrm{v}^\\textrm{q}$ be its quick and dirty estimate computed by the loss function averaging in this section. Lastly, denote by $l_\\textrm{v}^\\textrm{b}$ the loss on the last minibatch. Express $L_\\textrm{v}$ in terms of $L_\\textrm{v}^\\textrm{q}$, $l_\\textrm{v}^\\textrm{b}$, and the sample and minibatch sizes.\n1. Show that the quick and dirty estimate $L_\\textrm{v}^\\textrm{q}$ is unbiased. That is, show that $E[L_\\textrm{v}] = E[L_\\textrm{v}^\\textrm{q}]$. Why would you still want to use $L_\\textrm{v}$ instead?\n1. Given a multiclass classification loss, denoting by $l(y,y')$ the penalty of estimating $y'$ when we see $y$ and given a probabilty $p(y \\mid x)$, formulate the rule for an optimal selection of $y'$. Hint: express the expected loss, using $l$ and $p(y \\mid x)$.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/6808)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/6809)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/6810)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17981)\n:end_tab:"
    },
    {
      "chunk_id": "f251d49fabc8_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "environment-and-distribution-shift",
      "text": "# Environment and Distribution Shift\n:label:`sec_environment-and-distribution-shift`\n\nIn the previous sections, we worked through\na number of hands-on applications of machine learning,\nfitting models to a variety of datasets. And yet, we never stopped to contemplate\neither where data came from in the first place\nor what we ultimately plan to do\nwith the outputs from our models. Too often, machine learning developers\nin possession of data rush to develop models\nwithout pausing to consider these fundamental issues. Many failed machine learning deployments\ncan be traced back to this failure. Sometimes models appear to perform marvelously\nas measured by test set accuracy\nbut fail catastrophically in deployment\nwhen the distribution of data suddenly shifts. More insidiously, sometimes the very deployment of a model\ncan be the catalyst that perturbs the data distribution. Say, for example, that we trained a model\nto predict who will repay rather than default on a loan,\nfinding that an applicant's choice of footwear\nwas associated with the risk of default\n(Oxfords indicate repayment, sneakers indicate default). We might be inclined \nthereafter to grant a loan\nto any applicant wearing Oxfords\nand to deny all applicants wearing sneakers. In this case, our ill-considered leap from\npattern recognition to decision-making\nand our failure to critically consider the environment\nmight have disastrous consequences. For starters, as soon as we began\nmaking decisions based on footwear,\ncustomers would catch on and change their behavior. Before long, all applicants would be wearing Oxfords,\nwithout any coincident improvement in credit-worthiness. Take a minute to digest this because similar issues abound\nin many applications of machine learning:\nby introducing our model-based decisions to the environment,\nwe might break the model."
    },
    {
      "chunk_id": "f251d49fabc8_1",
      "chapter": "environment-and-distribution-shift",
      "heading": "environment-and-distribution-shift",
      "text": "Take a minute to digest this because similar issues abound\nin many applications of machine learning:\nby introducing our model-based decisions to the environment,\nwe might break the model. While we cannot possibly give these topics\na complete treatment in one section,\nwe aim here to expose some common concerns,\nand to stimulate the critical thinking\nrequired to detect such situations early,\nmitigate damage, and use machine learning responsibly. Some of the solutions are simple\n(ask for the \"right\" data),\nsome are technically difficult\n(implement a reinforcement learning system),\nand others require that we step outside the realm of\nstatistical prediction altogether and\ngrapple with difficult philosophical questions\nconcerning the ethical application of algorithms."
    },
    {
      "chunk_id": "833186d9b43f_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Types of Distribution Shift",
      "text": "To begin, we stick with the passive prediction setting\nconsidering the various ways that data distributions might shift\nand what might be done to salvage model performance.\nIn one classic setup, we assume that our training data\nwas sampled from some distribution $p_S(\\mathbf{x},y)$\nbut that our test data will consist\nof unlabeled examples drawn from\nsome different distribution $p_T(\\mathbf{x},y)$.\nAlready, we must confront a sobering reality.\nAbsent any assumptions on how $p_S$\nand $p_T$ relate to each other,\nlearning a robust classifier is impossible.\n\nConsider a binary classification problem,\nwhere we wish to distinguish between dogs and cats.\nIf the distribution can shift in arbitrary ways,\nthen our setup permits the pathological case\nin which the distribution over inputs remains\nconstant: $p_S(\\mathbf{x}) = p_T(\\mathbf{x})$,\nbut the labels are all flipped:\n$p_S(y \\mid \\mathbf{x}) = 1 - p_T(y \\mid \\mathbf{x})$.\nIn other words, if God can suddenly decide\nthat in the future all \"cats\" are now dogs\nand what we previously called \"dogs\" are now cats---without\nany change in the distribution of inputs $p(\\mathbf{x})$,\nthen we cannot possibly distinguish this setting\nfrom one in which the distribution did not change at all.\n\nFortunately, under some restricted assumptions\non the ways our data might change in the future,\nprincipled algorithms can detect shift\nand sometimes even adapt on the fly,\nimproving on the accuracy of the original classifier."
    },
    {
      "chunk_id": "949d37a5abfc_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Covariate Shift",
      "text": "Among categories of distribution shift,\ncovariate shift may be the most widely studied.\nHere, we assume that while the distribution of inputs\nmay change over time, the labeling function,\ni.e., the conditional distribution\n$P(y \\mid \\mathbf{x})$ does not change.\nStatisticians call this *covariate shift*\nbecause the problem arises due to a\nshift in the distribution of the covariates (features).\nWhile we can sometimes reason about distribution shift\nwithout invoking causality, we note that covariate shift\nis the natural assumption to invoke in settings\nwhere we believe that $\\mathbf{x}$ causes $y$.\n\nConsider the challenge of distinguishing cats and dogs.\nOur training data might consist of images of the kind in :numref:`fig_cat-dog-train`.\n\n![Training data for distinguishing cats and dogs (illustrations: Lafeez Hossain / 500px / Getty Images; ilkermetinkursova / iStock / Getty Images Plus; GlobalP / iStock / Getty Images Plus; Musthafa Aboobakuru / 500px / Getty Images).](../img/cat-dog-train.png)\n:label:`fig_cat-dog-train`\n\n\nAt test time we are asked to classify the images in :numref:`fig_cat-dog-test`.\n\n![Test data for distinguishing cats and dogs (illustrations: SIBAS_minich / iStock / Getty Images Plus; Ghrzuzudu / iStock / Getty Images Plus; id-work / DigitalVision Vectors / Getty Images; Yime / iStock / Getty Images Plus).](../img/cat-dog-test.png)\n:label:`fig_cat-dog-test`\n\nThe training set consists of photos,\nwhile the test set contains only cartoons.\nTraining on a dataset with substantially different\ncharacteristics from the test set\ncan spell trouble absent a coherent plan\nfor how to adapt to the new domain."
    },
    {
      "chunk_id": "a115c0b4838f_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Label Shift",
      "text": "*Label shift* describes the converse problem.\nHere, we assume that the label marginal $P(y)$\ncan change\nbut the class-conditional distribution\n$P(\\mathbf{x} \\mid y)$ remains fixed across domains.\nLabel shift is a reasonable assumption to make\nwhen we believe that $y$ causes $\\mathbf{x}$.\nFor example, we may want to predict diagnoses\ngiven their symptoms (or other manifestations),\neven as the relative prevalence of diagnoses\nare changing over time.\nLabel shift is the appropriate assumption here\nbecause diseases cause symptoms.\nIn some degenerate cases the label shift\nand covariate shift assumptions can hold simultaneously.\nFor example, when the label is deterministic,\nthe covariate shift assumption will be satisfied,\neven when $y$ causes $\\mathbf{x}$.\nInterestingly, in these cases,\nit is often advantageous to work with methods\nthat flow from the label shift assumption.\nThat is because these methods tend\nto involve manipulating objects that look like labels (often low-dimensional),\nas opposed to objects that look like inputs,\nwhich tend to be high-dimensional in deep learning."
    },
    {
      "chunk_id": "a85c5ce2dad3_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Concept Shift",
      "text": "We may also encounter the related problem of *concept shift*,\nwhich arises when the very definitions of labels can change.\nThis sounds weird---a *cat* is a *cat*, no?\nHowever, other categories are subject to changes in usage over time.\nDiagnostic criteria for mental illness,\nwhat passes for fashionable, and job titles,\nare all subject to considerable\namounts of concept shift.\nIt turns out that if we navigate around the United States,\nshifting the source of our data by geography,\nwe will find considerable concept shift regarding\nthe distribution of names for *soft drinks*\nas shown in :numref:`fig_popvssoda`.\n\n![Concept shift for soft drink names in the United States (CC-BY: Alan McConchie, PopVsSoda.com).](../img/popvssoda.png)\n:width:`400px`\n:label:`fig_popvssoda`\n\nIf we were to build a machine translation system,\nthe distribution $P(y \\mid \\mathbf{x})$ might be different\ndepending on our location.\nThis problem can be tricky to spot.\nWe might hope to exploit knowledge\nthat shift only takes place gradually\neither in a temporal or geographic sense."
    },
    {
      "chunk_id": "30eee0213e38_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Examples of Distribution Shift",
      "text": "Before delving into formalism and algorithms,\nwe can discuss some concrete situations\nwhere covariate or concept shift might not be obvious."
    },
    {
      "chunk_id": "56dc976e4097_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Medical Diagnostics",
      "text": "Imagine that you want to design an algorithm to detect cancer.\nYou collect data from healthy and sick people\nand you train your algorithm.\nIt works fine, giving you high accuracy\nand you conclude that you are ready\nfor a successful career in medical diagnostics.\n*Not so fast.*\n\nThe distributions that gave rise to the training data\nand those you will encounter in the wild might differ considerably.\nThis happened to an unfortunate startup\nthat some of we authors worked with years ago.\nThey were developing a blood test for a disease\nthat predominantly affects older men\nand hoped to study it using blood samples\nthat they had collected from patients.\nHowever, it is considerably more difficult\nto obtain blood samples from healthy men\nthan from sick patients already in the system.\nTo compensate, the startup solicited\nblood donations from students on a university campus\nto serve as healthy controls in developing their test.\nThen they asked whether we could help them\nto build a classifier for detecting the disease.\n\nAs we explained to them,\nit would indeed be easy to distinguish\nbetween the healthy and sick cohorts\nwith near-perfect accuracy.\nHowever, that is because the test subjects\ndiffered in age, hormone levels,\nphysical activity, diet, alcohol consumption,\nand many more factors unrelated to the disease.\nThis was unlikely to be the case with real patients.\nDue to their sampling procedure,\nwe could expect to encounter extreme covariate shift.\nMoreover, this case was unlikely to be\ncorrectable via conventional methods.\nIn short, they wasted a significant sum of money."
    },
    {
      "chunk_id": "8de4358dedb5_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Self-Driving Cars",
      "text": "Say a company wanted to leverage machine learning\nfor developing self-driving cars.\nOne key component here is a roadside detector.\nSince real annotated data is expensive to get,\nthey had the (smart and questionable) idea\nto use synthetic data from a game rendering engine\nas additional training data.\nThis worked really well on \"test data\"\ndrawn from the rendering engine.\nAlas, inside a real car it was a disaster.\nAs it turned out, the roadside had been rendered\nwith a very simplistic texture.\nMore importantly, *all* the roadside had been rendered\nwith the *same* texture and the roadside detector\nlearned about this \"feature\" very quickly.\n\nA similar thing happened to the US Army\nwhen they first tried to detect tanks in the forest.\nThey took aerial photographs of the forest without tanks,\nthen drove the tanks into the forest\nand took another set of pictures.\nThe classifier appeared to work *perfectly*.\nUnfortunately, it had merely learned\nhow to distinguish trees with shadows\nfrom trees without shadows---the first set\nof pictures was taken in the early morning,\nthe second set at noon."
    },
    {
      "chunk_id": "0f2a7339a6f2_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Nonstationary Distributions",
      "text": "A much more subtle situation arises\nwhen the distribution changes slowly\n(also known as *nonstationary distribution*)\nand the model is not updated adequately.\nBelow are some typical cases.\n\n* We train a computational advertising model and then fail to update it frequently (e.g., we forget to incorporate that an obscure new device called an iPad was just launched).\n* We build a spam filter. It works well at detecting all spam that we have seen so far. But then the spammers wise up and craft new messages that look unlike anything we have seen before.\n* We build a product recommendation system. It works throughout the winter but then continues to recommend Santa hats long after Christmas."
    },
    {
      "chunk_id": "7f39d063058d_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "More Anecdotes",
      "text": "* We build a face detector. It works well on all benchmarks. Unfortunately it fails on test data---the offending examples are close-ups where the face fills the entire image (no such data was in the training set).\n* We build a web search engine for the US market and want to deploy it in the UK.\n* We train an image classifier by compiling a large dataset where each among a large set of classes is equally represented in the dataset, say 1000 categories, represented by 1000 images each. Then we deploy the system in the real world, where the actual label distribution of photographs is decidedly non-uniform."
    },
    {
      "chunk_id": "54461ab0c597_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Correction of Distribution Shift",
      "text": "As we have discussed, there are many cases\nwhere training and test distributions\n$P(\\mathbf{x}, y)$ are different.\nIn some cases, we get lucky and the models work\ndespite covariate, label, or concept shift.\nIn other cases, we can do better by employing\nprincipled strategies to cope with the shift.\nThe remainder of this section grows considerably more technical.\nThe impatient reader could continue on to the next section\nas this material is not prerequisite to subsequent concepts."
    },
    {
      "chunk_id": "76f8a02c30b3_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Empirical Risk and  Risk",
      "text": ":label:`subsec_empirical-risk-and-risk`\n\nLet's first reflect on what exactly\nis happening during model training:\nwe iterate over features and associated labels\nof training data\n$\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)\\}$\nand update the parameters of a model $f$ after every minibatch.\nFor simplicity we do not consider regularization,\nso we largely minimize the loss on the training:\n\n$$\\mathop{\\mathrm{minimize}}_f \\frac{1}{n} \\sum_{i=1}^n l(f(\\mathbf{x}_i), y_i),$$\n:eqlabel:`eq_empirical-risk-min`\n\nwhere $l$ is the loss function\nmeasuring \"how bad\" the prediction $f(\\mathbf{x}_i)$ is given the associated label $y_i$.\nStatisticians call the term in :eqref:`eq_empirical-risk-min` *empirical risk*.\nThe *empirical risk* is an average loss over the training data\nfor approximating the *risk*,\nwhich is the\nexpectation of the loss over the entire population of data drawn from their true distribution\n$p(\\mathbf{x},y)$:\n\n$$E_{p(\\mathbf{x}, y)} [l(f(\\mathbf{x}), y)] = \\int\\int l(f(\\mathbf{x}), y) p(\\mathbf{x}, y) \\;d\\mathbf{x}dy.$$\n:eqlabel:`eq_true-risk`\n\nHowever, in practice we typically cannot obtain the entire population of data.\nThus, *empirical risk minimization*,\nwhich is minimizing the empirical risk in :eqref:`eq_empirical-risk-min`,\nis a practical strategy for machine learning,\nwith the hope of approximately\nminimizing the risk."
    },
    {
      "chunk_id": "0d3ec5064a42_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Covariate Shift Correction",
      "text": ":label:`subsec_covariate-shift-correction`\n\nAssume that we want to estimate\nsome dependency $P(y \\mid \\mathbf{x})$\nfor which we have labeled data $(\\mathbf{x}_i, y_i)$. Unfortunately, the observations $\\mathbf{x}_i$ are drawn\nfrom some *source distribution* $q(\\mathbf{x})$\nrather than the *target distribution* $p(\\mathbf{x})$. Fortunately,\nthe dependency assumption means\nthat the conditional distribution does not change: $p(y \\mid \\mathbf{x}) = q(y \\mid \\mathbf{x})$. If the source distribution $q(\\mathbf{x})$ is \"wrong\",\nwe can correct for that by using the following simple identity in the risk:\n\n$$\n\\begin{aligned}\n\\int\\int l(f(\\mathbf{x}), y) p(y \\mid \\mathbf{x})p(\\mathbf{x}) \\;d\\mathbf{x}dy =\n\\int\\int l(f(\\mathbf{x}), y) q(y \\mid \\mathbf{x})q(\\mathbf{x})\\frac{p(\\mathbf{x})}{q(\\mathbf{x})} \\;d\\mathbf{x}dy. \\end{aligned}\n$$\n\nIn other words, we need to reweigh each data example\nby the ratio of the\nprobability\nthat it would have been drawn from the correct distribution to that from the wrong one:\n\n$$\\beta_i \\stackrel{\\textrm{def}}{=} \\frac{p(\\mathbf{x}_i)}{q(\\mathbf{x}_i)}.$$\n\nPlugging in the weight $\\beta_i$ for\neach data example $(\\mathbf{x}_i, y_i)$\nwe can train our model using\n*weighted empirical risk minimization*:\n\n$$\\mathop{\\mathrm{minimize}}_f \\frac{1}{n} \\sum_{i=1}^n \\beta_i l(f(\\mathbf{x}_i), y_i).$$\n:eqlabel:`eq_weighted-empirical-risk-min`\n\n\n\nAlas, we do not know that ratio,\nso before we can do anything useful we need to estimate it. Many methods are available,\nincluding some fancy operator-theoretic approaches\nthat attempt to recalibrate the expectation operator directly\nusing a minimum-norm or a maximum entropy principle. Note that for any such approach, we need samples\ndrawn from both distributions---the \"true\" $p$, e.g.,\nby access to test data, and the one used\nfor generating the training set $q$ (the latter is trivially available). Note however, that we only need features $\\mathbf{x} \\sim p(\\mathbf{x})$;\nwe do not need to access labels $y \\sim p(y)$."
    },
    {
      "chunk_id": "0d3ec5064a42_1",
      "chapter": "environment-and-distribution-shift",
      "heading": "Covariate Shift Correction",
      "text": "Note however, that we only need features $\\mathbf{x} \\sim p(\\mathbf{x})$;\nwe do not need to access labels $y \\sim p(y)$. In this case, there exists a very effective approach\nthat will give almost as good results as the original: namely, logistic regression,\nwhich is a special case of softmax regression (see :numref:`sec_softmax`)\nfor binary classification. This is all that is needed to compute estimated probability ratios. We learn a classifier to distinguish\nbetween data drawn from $p(\\mathbf{x})$\nand data drawn from $q(\\mathbf{x})$. If it is impossible to distinguish\nbetween the two distributions\nthen it means that the associated instances\nare equally likely to come from\neither one of those two distributions. On the other hand, any instances\nthat can be well discriminated\nshould be significantly overweighted\nor underweighted accordingly. For simplicity's sake assume that we have\nan equal number of instances from both distributions\n$p(\\mathbf{x})$\nand $q(\\mathbf{x})$, respectively. Now denote by $z$ labels that are $1$\nfor data drawn from $p$ and $-1$ for data drawn from $q$. Then the probability in a mixed dataset is given by\n\n$$P(z=1 \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x})}{p(\\mathbf{x})+q(\\mathbf{x})} \\textrm{ and hence } \\frac{P(z=1 \\mid \\mathbf{x})}{P(z=-1 \\mid \\mathbf{x})} = \\frac{p(\\mathbf{x})}{q(\\mathbf{x})}.$$\n\nThus, if we use a logistic regression approach,\nwhere $P(z=1 \\mid \\mathbf{x})=\\frac{1}{1+\\exp(-h(\\mathbf{x}))}$ ($h$ is a parametrized function),\nit follows that\n\n$$\n\\beta_i = \\frac{1/(1 + \\exp(-h(\\mathbf{x}_i)))}{\\exp(-h(\\mathbf{x}_i))/(1 + \\exp(-h(\\mathbf{x}_i)))} = \\exp(h(\\mathbf{x}_i)). $$\n\nAs a result, we need to solve two problems:\nthe first, to distinguish between\ndata drawn from both distributions,\nand then a weighted empirical risk minimization problem\nin :eqref:`eq_weighted-empirical-risk-min`\nwhere we weigh terms by $\\beta_i$. Now we are ready to describe a correction algorithm."
    },
    {
      "chunk_id": "0d3ec5064a42_2",
      "chapter": "environment-and-distribution-shift",
      "heading": "Covariate Shift Correction",
      "text": "Now we are ready to describe a correction algorithm. Suppose that we have a training set $\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)\\}$ and an unlabeled test set $\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_m\\}$. For covariate shift,\nwe assume that $\\mathbf{x}_i$ for all $1 \\leq i \\leq n$ are drawn from some source distribution\nand $\\mathbf{u}_i$ for all $1 \\leq i \\leq m$\nare drawn from the target distribution. Here is a prototypical algorithm\nfor correcting covariate shift:\n\n1. Create a binary-classification training set: $\\{(\\mathbf{x}_1, -1), \\ldots, (\\mathbf{x}_n, -1), (\\mathbf{u}_1, 1), \\ldots, (\\mathbf{u}_m, 1)\\}$. 1. Train a binary classifier using logistic regression to get the function $h$. 1. Weigh training data using $\\beta_i = \\exp(h(\\mathbf{x}_i))$ or better $\\beta_i = \\min(\\exp(h(\\mathbf{x}_i)), c)$ for some constant $c$. 1. Use weights $\\beta_i$ for training on $\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)\\}$ in :eqref:`eq_weighted-empirical-risk-min`. Note that the above algorithm relies on a crucial assumption. For this scheme to work, we need that each data example\nin the target (e.g., test time) distribution\nhad nonzero probability of occurring at training time. If we find a point where $p(\\mathbf{x}) > 0$ but $q(\\mathbf{x}) = 0$,\nthen the corresponding importance weight should be infinity."
    },
    {
      "chunk_id": "67826ce6eb9b_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Label Shift Correction",
      "text": "Assume that we are dealing with a\nclassification task with $k$ categories. Using the same notation in :numref:`subsec_covariate-shift-correction`,\n$q$ and $p$ are the source distribution (e.g., training time) and target distribution (e.g., test time), respectively. Assume that the distribution of labels shifts over time:\n$q(y) \\neq p(y)$, but the class-conditional distribution\nstays the same: $q(\\mathbf{x} \\mid y)=p(\\mathbf{x} \\mid y)$. If the source distribution $q(y)$ is \"wrong\",\nwe can correct for that\naccording to\nthe following identity in the risk\nas defined in\n:eqref:`eq_true-risk`:\n\n$$\n\\begin{aligned}\n\\int\\int l(f(\\mathbf{x}), y) p(\\mathbf{x} \\mid y)p(y) \\;d\\mathbf{x}dy =\n\\int\\int l(f(\\mathbf{x}), y) q(\\mathbf{x} \\mid y)q(y)\\frac{p(y)}{q(y)} \\;d\\mathbf{x}dy. \\end{aligned}\n$$\n\n\n\nHere, our importance weights will correspond to the\nlabel likelihood ratios:\n\n$$\\beta_i \\stackrel{\\textrm{def}}{=} \\frac{p(y_i)}{q(y_i)}.$$\n\nOne nice thing about label shift is that\nif we have a reasonably good model\non the source distribution,\nthen we can get consistent estimates of these weights\nwithout ever having to deal with the ambient dimension. In deep learning, the inputs tend\nto be high-dimensional objects like images,\nwhile the labels are often simpler objects like categories. To estimate the target label distribution,\nwe first take our reasonably good off-the-shelf classifier\n(typically trained on the training data)\nand compute its \"confusion\" matrix using the validation set\n(also from the training distribution). The *confusion matrix*, $\\mathbf{C}$, is simply a $k \\times k$ matrix,\nwhere each column corresponds to the label category (ground truth)\nand each row corresponds to our model's predicted category. Each cell's value $c_{ij}$ is the fraction of total predictions on the validation set\nwhere the true label was $j$ and our model predicted $i$."
    },
    {
      "chunk_id": "67826ce6eb9b_1",
      "chapter": "environment-and-distribution-shift",
      "heading": "Label Shift Correction",
      "text": "Each cell's value $c_{ij}$ is the fraction of total predictions on the validation set\nwhere the true label was $j$ and our model predicted $i$. Now, we cannot calculate the confusion matrix\non the target data directly\nbecause we do not get to see the labels for the examples\nthat we see in the wild,\nunless we invest in a complex real-time annotation pipeline. What we can do, however, is average all of our model's predictions\nat test time together, yielding the mean model outputs $\\mu(\\hat{\\mathbf{y}}) \\in \\mathbb{R}^k$,\nwhere the $i^\\textrm{th}$ element $\\mu(\\hat{y}_i)$\nis the fraction of the total predictions on the test set\nwhere our model predicted $i$. It turns out that under some mild conditions---if\nour classifier was reasonably accurate in the first place,\nand if the target data contains only categories\nthat we have seen before,\nand if the label shift assumption holds in the first place\n(the strongest assumption here)---we can estimate the test set label distribution\nby solving a simple linear system\n\n$$\\mathbf{C} p(\\mathbf{y}) = \\mu(\\hat{\\mathbf{y}}),$$\n\nbecause as an estimate $\\sum_{j=1}^k c_{ij} p(y_j) = \\mu(\\hat{y}_i)$ holds for all $1 \\leq i \\leq k$,\nwhere $p(y_j)$ is the $j^\\textrm{th}$ element of the $k$-dimensional label distribution vector $p(\\mathbf{y})$. If our classifier is sufficiently accurate to begin with,\nthen the confusion matrix $\\mathbf{C}$ will be invertible,\nand we get a solution $p(\\mathbf{y}) = \\mathbf{C}^{-1} \\mu(\\hat{\\mathbf{y}})$. Because we observe the labels on the source data,\nit is easy to estimate the distribution $q(y)$. Then, for any training example $i$ with label $y_i$,\nwe can take the ratio of our estimated $p(y_i)/q(y_i)$\nto calculate the weight $\\beta_i$,\nand plug this into weighted empirical risk minimization\nin :eqref:`eq_weighted-empirical-risk-min`."
    },
    {
      "chunk_id": "db08701be9d4_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Concept Shift Correction",
      "text": "Concept shift is much harder to fix in a principled manner.\nFor instance, in a situation where suddenly the problem changes\nfrom distinguishing cats from dogs to one of\ndistinguishing white from black animals,\nit will be unreasonable to assume\nthat we can do much better than just collecting new labels\nand training from scratch.\nFortunately, in practice, such extreme shifts are rare.\nInstead, what usually happens is that the task keeps on changing slowly.\nTo make things more concrete, here are some examples:\n\n* In computational advertising, new products are launched,\nold products become less popular. This means that the distribution over ads and their popularity changes gradually and any click-through rate predictor needs to change gradually with it.\n* Traffic camera lenses degrade gradually due to environmental wear, affecting image quality progressively.\n* News content changes gradually (i.e., most of the news remains unchanged but new stories appear).\n\nIn such cases, we can use the same approach that we used for training networks to make them adapt to the change in the data. In other words, we use the existing network weights and simply perform a few update steps with the new data rather than training from scratch."
    },
    {
      "chunk_id": "3cccc5651439_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "A Taxonomy of Learning Problems",
      "text": "Armed with knowledge about how to deal with changes in distributions, we can now consider some other aspects of machine learning problem formulation."
    },
    {
      "chunk_id": "122feabf9925_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Batch Learning",
      "text": "In *batch learning*, we have access to training features and labels $\\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)\\}$, which we use to train a model $f(\\mathbf{x})$. Later on, we deploy this model to score new data $(\\mathbf{x}, y)$ drawn from the same distribution. This is the default assumption for any of the problems that we discuss here. For instance, we might train a cat detector based on lots of pictures of cats and dogs. Once we have trained it, we ship it as part of a smart catdoor computer vision system that lets only cats in. This is then installed in a customer's home and is never updated again (barring extreme circumstances)."
    },
    {
      "chunk_id": "7e4c54689c26_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Online Learning",
      "text": "Now imagine that the data $(\\mathbf{x}_i, y_i)$ arrives one sample at a time. More specifically, assume that we first observe $\\mathbf{x}_i$, then we need to come up with an estimate $f(\\mathbf{x}_i)$. Only once we have done this do we observe $y_i$ and so receive a reward or incur a loss, given our decision.\nMany real problems fall into this category. For example, we need to predict tomorrow's stock price, which allows us to trade based on that estimate and at the end of the day we find out whether our estimate made us a profit. In other words, in *online learning*, we have the following cycle where we are continuously improving our model given new observations:\n\n$$\\begin{aligned}&\\textrm{model } f_t \\longrightarrow \\textrm{data }  \\mathbf{x}_t \\longrightarrow \\textrm{estimate } f_t(\\mathbf{x}_t) \\longrightarrow\\\\ \\textrm{obs}&\\textrm{ervation } y_t \\longrightarrow \\textrm{loss } l(y_t, f_t(\\mathbf{x}_t)) \\longrightarrow \\textrm{model } f_{t+1}\\end{aligned}$$"
    },
    {
      "chunk_id": "777c8362b8a0_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Bandits",
      "text": "*Bandits* are a special case of the problem above. While in most learning problems we have a continuously parametrized function $f$ where we want to learn its parameters (e.g., a deep network), in a *bandit* problem we only have a finite number of arms that we can pull, i.e., a finite number of actions that we can take. It is not very surprising that for this simpler problem stronger theoretical guarantees in terms of optimality can be obtained. We list it mainly since this problem is often (confusingly) treated as if it were a distinct learning setting."
    },
    {
      "chunk_id": "9a926bc38ae2_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Control",
      "text": "In many cases the environment remembers what we did. Not necessarily in an adversarial manner but it will just remember and the response will depend on what happened before. For instance, a coffee boiler controller will observe different temperatures depending on whether it was heating the boiler previously. PID (proportional-integral-derivative) controller algorithms are a popular choice there.\nLikewise, a user's behavior on a news site will depend on what we showed them previously (e.g., they will read most news only once). Many such algorithms form a model of the environment in which they act so as to make their decisions appear less random.\nRecently,\ncontrol theory (e.g., PID variants) has also been used\nto automatically tune hyperparameters\nto achieve better disentangling and reconstruction quality,\nand improve the diversity of generated text and the reconstruction quality of generated images :cite:`Shao.Yao.Sun.ea.2020`."
    },
    {
      "chunk_id": "42f419d8f788_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Reinforcement Learning",
      "text": "In the more general case of an environment with memory, we may encounter situations where the environment is trying to cooperate with us (cooperative games, in particular for non-zero-sum games), or others where the environment will try to win. Chess, Go, Backgammon, or StarCraft are some of the cases in *reinforcement learning*. Likewise, we might want to build a good controller for autonomous cars. Other cars are likely to respond to the autonomous car's driving style in nontrivial ways, e.g., trying to avoid it, trying to cause an accident, or trying to cooperate with it."
    },
    {
      "chunk_id": "be78e219ec0d_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Considering the Environment",
      "text": "One key distinction between the different situations above is that a strategy that might have worked throughout in the case of a stationary environment, might not work throughout in an environment that can adapt. For instance, an arbitrage opportunity discovered by a trader is likely to disappear once it is exploited. The speed and manner at which the environment changes determines to a large extent the type of algorithms that we can bring to bear. For instance, if we know that things may only change slowly, we can force any estimate to change only slowly, too. If we know that the environment might change instantaneously, but only very infrequently, we can make allowances for that. These types of knowledge are crucial for the aspiring data scientist in dealing with concept shift, i.e., when the problem that is being solved can change over time."
    },
    {
      "chunk_id": "6c29410be386_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Fairness, Accountability, and Transparency in Machine Learning",
      "text": "Finally, it is important to remember\nthat when you deploy machine learning systems\nyou are not merely optimizing a predictive model---you\nare typically providing a tool that will\nbe used to (partially or fully) automate decisions. These technical systems can impact the lives\nof individuals who are subject to the resulting decisions. The leap from considering predictions to making decisions\nraises not only new technical questions,\nbut also a slew of ethical questions\nthat must be carefully considered. If we are deploying a medical diagnostic system,\nwe need to know for which populations\nit may work and for which it may not. Overlooking foreseeable risks to the welfare of\na subpopulation could cause us to administer inferior care. Moreover, once we contemplate decision-making systems,\nwe must step back and reconsider how we evaluate our technology. Among other consequences of this change of scope,\nwe will find that *accuracy* is seldom the right measure. For instance, when translating predictions into actions,\nwe will often want to take into account\nthe potential cost sensitivity of erring in various ways. If one way of misclassifying an image\ncould be perceived as a racial sleight of hand,\nwhile misclassification to a different category\nwould be harmless, then we might want to adjust\nour thresholds accordingly, accounting for societal values\nin designing the decision-making protocol. We also want to be careful about\nhow prediction systems can lead to feedback loops. For example, consider predictive policing systems,\nwhich allocate patrol officers\nto areas with high forecasted crime. It is easy to see how a worrying pattern can emerge:\n\n 1. Neighborhoods with more crime get more patrols. 1. Consequently, more crimes are discovered in these neighborhoods, entering the training data available for future iterations. 1. Exposed to more positives, the model predicts yet more crime in these neighborhoods. 1."
    },
    {
      "chunk_id": "6c29410be386_1",
      "chapter": "environment-and-distribution-shift",
      "heading": "Fairness, Accountability, and Transparency in Machine Learning",
      "text": "1. Consequently, more crimes are discovered in these neighborhoods, entering the training data available for future iterations. 1. Exposed to more positives, the model predicts yet more crime in these neighborhoods. 1. In the next iteration, the updated model targets the same neighborhood even more heavily leading to yet more crimes discovered, etc. Often, the various mechanisms by which\na model's predictions become coupled to its training data\nare unaccounted for in the modeling process. This can lead to what researchers call *runaway feedback loops*. Additionally, we want to be careful about\nwhether we are addressing the right problem in the first place. Predictive algorithms now play an outsize role\nin mediating the dissemination of information. Should the news that an individual encounters\nbe determined by the set of Facebook pages they have *Liked*? These are just a few among the many pressing ethical dilemmas\nthat you might encounter in a career in machine learning."
    },
    {
      "chunk_id": "11acd0aa4189_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Summary",
      "text": "In many cases training and test sets do not come from the same distribution. This is called distribution shift.\nThe risk is the expectation of the loss over the entire population of data drawn from their true distribution. However, this entire population is usually unavailable. Empirical risk is an average loss over the training data to approximate the risk. In practice, we perform empirical risk minimization.\n\nUnder the corresponding assumptions, covariate and label shift can be detected and corrected for at test time. Failure to account for this bias can become problematic at test time.\nIn some cases, the environment may remember automated actions and respond in surprising ways. We must account for this possibility when building models and continue to monitor live systems, open to the possibility that our models and the environment will become entangled in unanticipated ways."
    },
    {
      "chunk_id": "03f2e02a9494_0",
      "chapter": "environment-and-distribution-shift",
      "heading": "Exercises",
      "text": "1. What could happen when we change the behavior of a search engine? What might the users do? What about the advertisers?\n1. Implement a covariate shift detector. Hint: build a classifier.\n1. Implement a covariate shift corrector.\n1. Besides distribution shift, what else could affect how the empirical risk approximates the risk?\n\n\n[Discussions](https://discuss.d2l.ai/t/105)"
    },
    {
      "chunk_id": "7b6ae41297e6_0",
      "chapter": "generalization-classification",
      "heading": "generalization-classification",
      "text": "# Generalization in Classification\n\n:label:`chap_classification_generalization`\n\n\n\nSo far, we have focused on how to tackle multiclass classification problems\nby training (linear) neural networks with multiple outputs and softmax functions. Interpreting our model's outputs as probabilistic predictions,\nwe motivated and derived the cross-entropy loss function,\nwhich calculates the negative log likelihood\nthat our model (for a fixed set of parameters)\nassigns to the actual labels. And finally, we put these tools into practice\nby fitting our model to the training set. However, as always, our goal is to learn *general patterns*,\nas assessed empirically on previously unseen data (the test set). High accuracy on the training set means nothing. Whenever each of our inputs is unique\n(and indeed this is true for most high-dimensional datasets),\nwe can attain perfect accuracy on the training set\nby just memorizing the dataset on the first training epoch,\nand subsequently looking up the label whenever we see a new image. And yet, memorizing the exact labels\nassociated with the exact training examples\ndoes not tell us how to classify new examples. Absent further guidance, we might have to fall back\non random guessing whenever we encounter new examples. A number of burning questions demand immediate attention:\n\n1. How many test examples do we need to give a good estimate of the accuracy of our classifiers on the underlying population? 1. What happens if we keep evaluating models on the same test repeatedly? 1. Why should we expect that fitting our linear models to the training set\n   should fare any better than our naive memorization scheme? Whereas :numref:`sec_generalization_basics` introduced\nthe basics of overfitting and generalization\nin the context of linear regression,\nthis chapter will go a little deeper,\nintroducing some of the foundational ideas\nof statistical learning theory."
    },
    {
      "chunk_id": "7b6ae41297e6_1",
      "chapter": "generalization-classification",
      "heading": "generalization-classification",
      "text": "Whereas :numref:`sec_generalization_basics` introduced\nthe basics of overfitting and generalization\nin the context of linear regression,\nthis chapter will go a little deeper,\nintroducing some of the foundational ideas\nof statistical learning theory. It turns out that we often can guarantee generalization *a priori*:\nfor many models,\nand for any desired upper bound\non the generalization gap $\\epsilon$,\nwe can often determine some required number of samples $n$\nsuch that if our training set contains at least $n$\nsamples, our empirical error will lie\nwithin $\\epsilon$ of the true error,\n*for any data generating distribution*. Unfortunately, it also turns out\nthat while these sorts of guarantees provide\na profound set of intellectual building blocks,\nthey are of limited practical utility\nto the deep learning practitioner. In short, these guarantees suggest\nthat ensuring generalization\nof deep neural networks *a priori*\nrequires an absurd number of examples\n(perhaps trillions or more),\neven when we find that, on the tasks we care about,\ndeep neural networks typically generalize\nremarkably well with far fewer examples (thousands). Thus deep learning practitioners often forgo\n*a priori* guarantees altogether,\ninstead employing methods\nthat have generalized well\non similar problems in the past,\nand certifying generalization *post hoc*\nthrough empirical evaluations. When we get to :numref:`chap_perceptrons`,\nwe will revisit generalization\nand provide a light introduction\nto the vast scientific literature\nthat has sprung in attempts\nto explain why deep neural networks generalize in practice."
    },
    {
      "chunk_id": "17b3c8ab4901_0",
      "chapter": "generalization-classification",
      "heading": "The Test Set",
      "text": "Since we have already begun to rely on test sets\nas the gold standard method\nfor assessing generalization error,\nlet's get started by discussing\nthe properties of such error estimates. Let's focus on a fixed classifier $f$,\nwithout worrying about how it was obtained. Moreover suppose that we possess\na *fresh* dataset of examples $\\mathcal{D} = {(\\mathbf{x}^{(i)},y^{(i)})}_{i=1}^n$\nthat were not used to train the classifier $f$. The *empirical error* of our classifier $f$ on $\\mathcal{D}$\nis simply the fraction of instances\nfor which the prediction $f(\\mathbf{x}^{(i)})$\ndisagrees with the true label $y^{(i)}$,\nand is given by the following expression:\n\n$$\\epsilon_\\mathcal{D}(f) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}(f(\\mathbf{x}^{(i)}) \\neq y^{(i)}).$$\n\nBy contrast, the *population error*\nis the *expected* fraction\nof examples in the underlying population\n(some distribution $P(X,Y)$  characterized\nby probability density function $p(\\mathbf{x},y)$)\nfor which our classifier disagrees\nwith the true label:\n\n$$\\epsilon(f) =  E_{(\\mathbf{x}, y) \\sim P} \\mathbf{1}(f(\\mathbf{x}) \\neq y) =\n\\int\\int \\mathbf{1}(f(\\mathbf{x}) \\neq y) p(\\mathbf{x}, y) \\;d\\mathbf{x} dy.$$\n\nWhile $\\epsilon(f)$ is the quantity that we actually care about,\nwe cannot observe it directly,\njust as we cannot directly\nobserve the average height in a large population\nwithout measuring every single person. We can only estimate this quantity based on samples. Because our test set $\\mathcal{D}$\nis statistically representative\nof the underlying population,\nwe can view $\\epsilon_\\mathcal{D}(f)$ as a statistical\nestimator of the population error $\\epsilon(f)$. Moreover, because our quantity of interest $\\epsilon(f)$\nis an expectation (of the random variable $\\mathbf{1}(f(X) \\neq Y)$)\nand the corresponding estimator $\\epsilon_\\mathcal{D}(f)$\nis the sample average,\nestimating the population error\nis simply the classic problem of mean estimation,\nwhich you may recall from :numref:`sec_prob`."
    },
    {
      "chunk_id": "17b3c8ab4901_1",
      "chapter": "generalization-classification",
      "heading": "The Test Set",
      "text": "An important classical result from probability theory\ncalled the *central limit theorem* guarantees\nthat whenever we possess $n$ random samples $a_1, ..., a_n$\ndrawn from any distribution with mean $\\mu$ and standard deviation $\\sigma$,\nthen, as the number of samples $n$ approaches infinity,\nthe sample average $\\hat{\\mu}$ approximately\ntends towards a normal distribution centered\nat the true mean and with standard deviation $\\sigma/\\sqrt{n}$. Already, this tells us something important:\nas the number of examples grows large,\nour test error $\\epsilon_\\mathcal{D}(f)$\nshould approach the true error $\\epsilon(f)$\nat a rate of $\\mathcal{O}(1/\\sqrt{n})$. Thus, to estimate our test error twice as precisely,\nwe must collect four times as large a test set. To reduce our test error by a factor of one hundred,\nwe must collect ten thousand times as large a test set. In general, such a rate of $\\mathcal{O}(1/\\sqrt{n})$\nis often the best we can hope for in statistics. Now that we know something about the asymptotic rate\nat which our test error $\\epsilon_\\mathcal{D}(f)$ converges to the true error $\\epsilon(f)$,\nwe can zoom in on some important details. Recall that the random variable of interest\n$\\mathbf{1}(f(X) \\neq Y)$\ncan only take values $0$ and $1$\nand thus is a Bernoulli random variable,\ncharacterized by a parameter\nindicating the probability that it takes value $1$. Here, $1$ means that our classifier made an error,\nso the parameter of our random variable\nis actually the true error rate $\\epsilon(f)$. The variance $\\sigma^2$ of a Bernoulli\ndepends on its parameter (here, $\\epsilon(f)$)\naccording to the expression $\\epsilon(f)(1-\\epsilon(f))$. While $\\epsilon(f)$ is initially unknown,\nwe know that it cannot be greater than $1$. A little investigation of this function\nreveals that our variance is highest\nwhen the true error rate is close to $0.5$\nand can be far lower when it is\nclose to $0$ or close to $1$."
    },
    {
      "chunk_id": "17b3c8ab4901_2",
      "chapter": "generalization-classification",
      "heading": "The Test Set",
      "text": "A little investigation of this function\nreveals that our variance is highest\nwhen the true error rate is close to $0.5$\nand can be far lower when it is\nclose to $0$ or close to $1$. This tells us that the asymptotic standard deviation\nof our estimate $\\epsilon_\\mathcal{D}(f)$ of the error $\\epsilon(f)$\n(over the choice of the $n$ test samples)\ncannot be any greater than $\\sqrt{0.25/n}$. If we ignore the fact that this rate characterizes\nbehavior as the test set size approaches infinity\nrather than when we possess finite samples,\nthis tells us that if we want our test error $\\epsilon_\\mathcal{D}(f)$\nto approximate the population error $\\epsilon(f)$\nsuch that one standard deviation corresponds\nto an interval of $\\pm 0.01$,\nthen we should collect roughly 2500 samples. If we want to fit two standard deviations\nin that range and thus be 95% confident\nthat $\\epsilon_\\mathcal{D}(f) \\in \\epsilon(f) \\pm 0.01$,\nthen we will need 10,000 samples! This turns out to be the size of the test sets\nfor many popular benchmarks in machine learning. You might be surprised to find out that thousands\nof applied deep learning papers get published every year\nmaking a big deal out of error rate improvements of $0.01$ or less. Of course, when the error rates are much closer to $0$,\nthen an improvement of $0.01$ can indeed be a big deal. One pesky feature of our analysis thus far\nis that it really only tells us about asymptotics,\ni.e., how the relationship between $\\epsilon_\\mathcal{D}$ and $\\epsilon$\nevolves as our sample size goes to infinity."
    },
    {
      "chunk_id": "17b3c8ab4901_3",
      "chapter": "generalization-classification",
      "heading": "The Test Set",
      "text": "One pesky feature of our analysis thus far\nis that it really only tells us about asymptotics,\ni.e., how the relationship between $\\epsilon_\\mathcal{D}$ and $\\epsilon$\nevolves as our sample size goes to infinity. Fortunately, because our random variable is bounded,\nwe can obtain valid finite sample bounds\nby applying an inequality due to Hoeffding (1963):\n\n$$P(\\epsilon_\\mathcal{D}(f) - \\epsilon(f) \\geq t) < \\exp\\left( - 2n t^2 \\right).$$\n\nSolving for the smallest dataset size\nthat would allow us to conclude\nwith 95% confidence that the distance $t$\nbetween our estimate $\\epsilon_\\mathcal{D}(f)$\nand the true error rate $\\epsilon(f)$\ndoes not exceed $0.01$,\nyou will find that roughly 15,000 examples are required\nas compared to the 10,000 examples suggested\nby the asymptotic analysis above. If you go deeper into statistics\nyou will find that this trend holds generally. Guarantees that hold even in finite samples\nare typically slightly more conservative. Note that in the scheme of things,\nthese numbers are not so far apart,\nreflecting the general usefulness\nof asymptotic analysis for giving\nus ballpark figures even if they are not\nguarantees we can take to court."
    },
    {
      "chunk_id": "cbf8d7c93871_0",
      "chapter": "generalization-classification",
      "heading": "Test Set Reuse",
      "text": "In some sense, you are now set up to succeed\nat conducting empirical machine learning research. Nearly all practical models are developed\nand validated based on test set performance\nand you are now a master of the test set. For any fixed classifier $f$,\nyou know how to evaluate its test error $\\epsilon_\\mathcal{D}(f)$,\nand know precisely what can (and cannot)\nbe said about its population error $\\epsilon(f)$. So let's say that you take this knowledge\nand prepare to train your first model $f_1$. Knowing just how confident you need to be\nin the performance of your classifier's error rate\nyou apply our analysis above to determine\nan appropriate number of examples\nto set aside for the test set. Moreover, let's assume that you took the lessons from\n:numref:`sec_generalization_basics` to heart\nand made sure to preserve the sanctity of the test set\nby conducting all of your preliminary analysis,\nhyperparameter tuning, and even selection\namong multiple competing model architectures\non a validation set. Finally you evaluate your model $f_1$\non the test set and report an unbiased\nestimate of the population error\nwith an associated confidence interval. So far everything seems to be going well. However, that night you wake up at 3am\nwith a brilliant idea for a new modeling approach. The next day, you code up your new model,\ntune its hyperparameters on the validation set\nand not only are you getting your new model $f_2$ to work\nbut its error rate appears to be much lower than $f_1$'s. However, the thrill of discovery suddenly fades\nas you prepare for the final evaluation. You do not have a test set! Even though the original test set $\\mathcal{D}$\nis still sitting on your server,\nyou now face two formidable problems. First, when you collected your test set,\nyou determined the required level of precision\nunder the assumption that you were evaluating\na single classifier $f$."
    },
    {
      "chunk_id": "cbf8d7c93871_1",
      "chapter": "generalization-classification",
      "heading": "Test Set Reuse",
      "text": "First, when you collected your test set,\nyou determined the required level of precision\nunder the assumption that you were evaluating\na single classifier $f$. However, if you get into the business\nof evaluating multiple classifiers $f_1, ..., f_k$\non the same test set,\nyou must consider the problem of false discovery. Before, you might have been 95% sure\nthat $\\epsilon_\\mathcal{D}(f) \\in \\epsilon(f) \\pm 0.01$\nfor a single classifier $f$\nand thus the probability of a misleading result\nwas a mere 5%. With $k$ classifiers in the mix,\nit can be hard to guarantee\nthat there is not even one among them\nwhose test set performance is misleading. With 20 classifiers under consideration,\nyou might have no power at all\nto rule out the possibility\nthat at least one among them\nreceived a misleading score. This problem relates to multiple hypothesis testing,\nwhich despite a vast literature in statistics,\nremains a persistent problem plaguing scientific research. If that is not enough to worry you,\nthere is a special reason to distrust\nthe results that you get on subsequent evaluations. Recall that our analysis of test set performance\nrested on the assumption that the classifier\nwas chosen absent any contact with the test set\nand thus we could view the test set\nas drawn randomly from the underlying population. Here, not only are you testing multiple functions,\nthe subsequent function $f_2$ was chosen\nafter you observed the test set performance of $f_1$. Once information from the test set has leaked to the modeler,\nit can never be a true test set again in the strictest sense. This problem is called *adaptive overfitting* and has recently emerged\nas a topic of intense interest to learning theorists and statisticians\n:cite:`dwork2015preserving`. Fortunately, while it is possible\nto leak all information out of a holdout set,\nand the theoretical worst case scenarios are bleak,\nthese analyses may be too conservative."
    },
    {
      "chunk_id": "cbf8d7c93871_2",
      "chapter": "generalization-classification",
      "heading": "Test Set Reuse",
      "text": "Fortunately, while it is possible\nto leak all information out of a holdout set,\nand the theoretical worst case scenarios are bleak,\nthese analyses may be too conservative. In practice, take care to create real test sets,\nto consult them as infrequently as possible,\nto account for multiple hypothesis testing\nwhen reporting confidence intervals,\nand to dial up your vigilance more aggressively\nwhen the stakes are high and your dataset size is small. When running a series of benchmark challenges,\nit is often good practice to maintain\nseveral test sets so that after each round,\nthe old test set can be demoted to a validation set."
    },
    {
      "chunk_id": "5288dc79b5b8_0",
      "chapter": "generalization-classification",
      "heading": "Statistical Learning Theory",
      "text": "Put simply, *test sets are all that we really have*,\nand yet this fact seems strangely unsatisfying. First, we seldom possess a *true test set*---unless\nwe are the ones creating the dataset,\nsomeone else has probably already evaluated\ntheir own classifier on our ostensible \"test set\". And even when we have first dibs,\nwe soon find ourselves frustrated, wishing we could\nevaluate our subsequent modeling attempts\nwithout the gnawing feeling\nthat we cannot trust our numbers. Moreover, even a true test set can only tell us *post hoc*\nwhether a classifier has in fact generalized to the population,\nnot whether we have any reason to expect *a priori*\nthat it should generalize. With these misgivings in mind,\nyou might now be sufficiently primed\nto see the appeal of *statistical learning theory*,\nthe mathematical subfield of machine learning\nwhose practitioners aim to elucidate the\nfundamental principles that explain\nwhy/when models trained on empirical data\ncan/will generalize to unseen data. One of the primary aims\nof statistical learning researchers\nhas been to bound the generalization gap,\nrelating the properties of the model class\nto the number of samples in the dataset. Learning theorists aim to bound the difference\nbetween the *empirical error* $\\epsilon_\\mathcal{S}(f_\\mathcal{S})$\nof a learned classifier $f_\\mathcal{S}$,\nboth trained and evaluated\non the training set $\\mathcal{S}$,\nand the true error $\\epsilon(f_\\mathcal{S})$\nof that same classifier on the underlying population. This might look similar to the evaluation problem\nthat we just addressed but there is a major difference. Earlier, the classifier $f$ was fixed\nand we only needed a dataset\nfor evaluative purposes. And indeed, any fixed classifier does generalize:\nits error on a (previously unseen) dataset\nis an unbiased estimate of the population error. But what can we say when a classifier\nis trained and evaluated on the same dataset? Can we ever be confident that the training error\nwill be close to the testing error?"
    },
    {
      "chunk_id": "5288dc79b5b8_1",
      "chapter": "generalization-classification",
      "heading": "Statistical Learning Theory",
      "text": "But what can we say when a classifier\nis trained and evaluated on the same dataset? Can we ever be confident that the training error\nwill be close to the testing error? Suppose that our learned classifier $f_\\mathcal{S}$ must be chosen\nfrom some pre-specified set of functions $\\mathcal{F}$. Recall from our discussion of test sets\nthat while it is easy to estimate\nthe error of a single classifier,\nthings get hairy when we begin\nto consider collections of classifiers. Even if the empirical error\nof any one (fixed) classifier\nwill be close to its true error\nwith high probability,\nonce we consider a collection of classifiers,\nwe need to worry about the possibility\nthat *just one* of them\nwill receive a badly estimated error. The worry is that we might pick such a classifier\nand thereby grossly underestimate\nthe population error. Moreover, even for linear models,\nbecause their parameters are continuously valued,\nwe are typically choosing from\nan infinite class of functions ($|\\mathcal{F}| = \\infty$). One ambitious solution to the problem\nis to develop analytic tools\nfor proving uniform convergence, i.e.,\nthat with high probability,\nthe empirical error rate for every classifier in the class $f\\in\\mathcal{F}$\nwill *simultaneously* converge to its true error rate. In other words, we seek a theoretical principle\nthat would allow us to state that\nwith probability at least $1-\\delta$\n(for some small $\\delta$)\nno classifier's error rate $\\epsilon(f)$\n(among all classifiers in the class $\\mathcal{F}$)\nwill be misestimated by more\nthan some  small amount $\\alpha$. Clearly, we cannot make such statements\nfor all model classes $\\mathcal{F}$. Recall the class of memorization machines\nthat always achieve empirical error $0$\nbut never outperform random guessing\non the underlying population. In a sense the class of memorizers is too flexible. No such a uniform convergence result could possibly hold. On the other hand, a fixed classifier is useless---it\ngeneralizes perfectly, but fits neither\nthe training data nor the test data."
    },
    {
      "chunk_id": "5288dc79b5b8_2",
      "chapter": "generalization-classification",
      "heading": "Statistical Learning Theory",
      "text": "In a sense the class of memorizers is too flexible. No such a uniform convergence result could possibly hold. On the other hand, a fixed classifier is useless---it\ngeneralizes perfectly, but fits neither\nthe training data nor the test data. The central question of learning\nhas thus historically been framed as a trade-off\nbetween more flexible (higher variance) model classes\nthat better fit the training data but risk overfitting,\nversus more rigid (higher bias) model classes\nthat generalize well but risk underfitting. A central question in learning theory\nhas been to develop the appropriate\nmathematical analysis to quantify\nwhere a model sits along this spectrum,\nand to provide the associated guarantees. In a series of seminal papers,\nVapnik and Chervonenkis extended\nthe theory on the convergence\nof relative frequencies\nto more general classes of functions\n:cite:`VapChe64,VapChe68,VapChe71,VapChe74b,VapChe81,VapChe91`. One of the key contributions of this line of work\nis the Vapnik--Chervonenkis (VC) dimension,\nwhich measures (one notion of)\nthe complexity (flexibility) of a model class. Moreover, one of their key results bounds\nthe difference between the empirical error\nand the population error as a function\nof the VC dimension and the number of samples:\n\n$$P\\left(R[p, f] - R_\\textrm{emp}[\\mathbf{X}, \\mathbf{Y}, f] < \\alpha\\right) \\geq 1-\\delta\n\\ \\textrm{ for }\\ \\alpha \\geq c \\sqrt{(\\textrm{VC} - \\log \\delta)/n}.$$\n\nHere $\\delta > 0$ is the probability that the bound is violated,\n$\\alpha$ is the upper bound on the generalization gap,\nand $n$ is the dataset size. Lastly, $c > 0$ is a constant that depends\nonly on the scale of the loss that can be incurred. One use of the bound might be to plug in desired\nvalues of $\\delta$ and $\\alpha$\nto determine how many samples to collect. The VC dimension quantifies the largest\nnumber of data points for which we can assign\nany arbitrary (binary) labeling\nand for each find some model $f$ in the class\nthat agrees with that labeling."
    },
    {
      "chunk_id": "5288dc79b5b8_3",
      "chapter": "generalization-classification",
      "heading": "Statistical Learning Theory",
      "text": "The VC dimension quantifies the largest\nnumber of data points for which we can assign\nany arbitrary (binary) labeling\nand for each find some model $f$ in the class\nthat agrees with that labeling. For example, linear models on $d$-dimensional inputs\nhave VC dimension $d+1$. It is easy to see that a line can assign\nany possible labeling to three points in two dimensions,\nbut not to four. Unfortunately, the theory tends to be\noverly pessimistic for more complex models\nand obtaining this guarantee typically requires\nfar more examples than are actually needed\nto achieve the desired error rate. Note also that fixing the model class and $\\delta$,\nour error rate again decays\nwith the usual $\\mathcal{O}(1/\\sqrt{n})$ rate. It seems unlikely that we could do better in terms of $n$. However, as we vary the model class,\nVC dimension can present\na pessimistic picture\nof the generalization gap."
    },
    {
      "chunk_id": "0e5f2b12bd47_0",
      "chapter": "generalization-classification",
      "heading": "Summary",
      "text": "The most straightforward way to evaluate a model\nis to consult a test set comprised of previously unseen data. Test set evaluations provide an unbiased estimate of the true error\nand converge at the desired $\\mathcal{O}(1/\\sqrt{n})$ rate as the test set grows. We can provide approximate confidence intervals\nbased on exact asymptotic distributions\nor valid finite sample confidence intervals\nbased on (more conservative) finite sample guarantees. Indeed test set evaluation is the bedrock\nof modern machine learning research. However, test sets are seldom true test sets\n(used by multiple researchers again and again). Once the same test set is used\nto evaluate multiple models,\ncontrolling for false discovery can be difficult. This can cause huge problems in theory. In practice, the significance of the problem\ndepends on the size of the holdout sets in question\nand whether they are merely being used to choose hyperparameters\nor if they are leaking information more directly. Nevertheless, it is good practice to curate real test sets (or multiple)\nand to be as conservative as possible about how often they are used. Hoping to provide a more satisfying solution,\nstatistical learning theorists have developed methods\nfor guaranteeing uniform convergence over a model class. If indeed every model's empirical error simultaneously\nconverges to its true error,\nthen we are free to choose the model that performs\nbest, minimizing the training error,\nknowing that it too will perform similarly well\non the holdout data. Crucially, any one of such results must depend\non some property of the model class. Vladimir Vapnik and Alexey Chernovenkis\nintroduced the VC dimension,\npresenting uniform convergence results\nthat hold for all models in a VC class. The training errors for all models in the class\nare (simultaneously) guaranteed\nto be close to their true errors,\nand guaranteed to grow even closer\nat $\\mathcal{O}(1/\\sqrt{n})$ rates."
    },
    {
      "chunk_id": "0e5f2b12bd47_1",
      "chapter": "generalization-classification",
      "heading": "Summary",
      "text": "The training errors for all models in the class\nare (simultaneously) guaranteed\nto be close to their true errors,\nand guaranteed to grow even closer\nat $\\mathcal{O}(1/\\sqrt{n})$ rates. Following the revolutionary discovery of VC dimension,\nnumerous alternative complexity measures have been proposed,\neach facilitating an analogous generalization guarantee. See :citet:`boucheron2005theory` for a detailed discussion\nof several advanced ways of measuring function complexity. Unfortunately, while these complexity measures\nhave become broadly useful tools in statistical theory,\nthey turn out to be powerless\n(as straightforwardly applied)\nfor explaining why deep neural networks generalize. Deep neural networks often have millions of parameters (or more),\nand can easily assign random labels to large collections of points. Nevertheless, they generalize well on practical problems\nand, surprisingly, they often generalize better,\nwhen they are larger and deeper,\ndespite incurring higher VC dimensions. In the next chapter, we will revisit generalization\nin the context of deep learning."
    },
    {
      "chunk_id": "bb16568788ba_0",
      "chapter": "generalization-classification",
      "heading": "Exercises",
      "text": "1. If we wish to estimate the error of a fixed model $f$\n   to within $0.0001$ with probability greater than 99.9%,\n   how many samples do we need?\n1. Suppose that somebody else possesses a labeled test set\n   $\\mathcal{D}$ and only makes available the unlabeled inputs (features).\n   Now suppose that you can only access the test set labels\n   by running a model $f$ (with no restrictions placed on the model class)\n   on each of the unlabeled inputs\n   and receiving the corresponding error $\\epsilon_\\mathcal{D}(f)$.\n   How many models would you need to evaluate\n   before you leak the entire test set\n   and thus could appear to have error $0$,\n   regardless of your true error?\n1. What is the VC dimension of the class of fifth-order polynomials?\n1. What is the VC dimension of axis-aligned rectangles on two-dimensional data?\n\n[Discussions](https://discuss.d2l.ai/t/6829)"
    },
    {
      "chunk_id": "f873122ba379_0",
      "chapter": "image-classification-dataset",
      "heading": "image-classification-dataset",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# The Image Classification Dataset\n:label:`sec_fashion_mnist`\n\n(~~The MNIST dataset is one of the widely used dataset for image classification, while it is too simple as a benchmark dataset. We will use the similar, but more complex Fashion-MNIST dataset ~~)\n\nOne widely used dataset for image classification is the  [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) :cite:`LeCun.Bottou.Bengio.ea.1998` of handwritten digits. At the time of its release in the 1990s it posed a formidable challenge to most machine learning algorithms, consisting of 60,000 images of $28 \\times 28$ pixels resolution (plus a test dataset of 10,000 images). To put things into perspective, back in 1995, a Sun SPARCStation 5 with a whopping 64MB of RAM and a blistering 5 MFLOPs was considered state of the art equipment for machine learning at AT&T Bell Laboratories. Achieving high accuracy on digit recognition was a key component in automating letter sorting for the USPS in the 1990s. Deep networks such as LeNet-5 :cite:`LeCun.Jackel.Bottou.ea.1995`, support vector machines with invariances :cite:`Scholkopf.Burges.Vapnik.1996`, and tangent distance classifiers :cite:`Simard.LeCun.Denker.ea.1998` all could reach error rates below 1%. For over a decade, MNIST served as *the* point of reference for comparing machine learning algorithms. While it had a good run as a benchmark dataset,\neven simple models by today's standards achieve classification accuracy over 95%,\nmaking it unsuitable for distinguishing between strong models and weaker ones. Even more, the dataset allows for *very* high levels of accuracy, not typically seen in many classification problems. This skewed algorithmic development towards specific families of algorithms that can take advantage of clean datasets, such as active set methods and boundary-seeking active set algorithms. Today, MNIST serves as more of a sanity check than as a benchmark."
    },
    {
      "chunk_id": "f873122ba379_1",
      "chapter": "image-classification-dataset",
      "heading": "image-classification-dataset",
      "text": "This skewed algorithmic development towards specific families of algorithms that can take advantage of clean datasets, such as active set methods and boundary-seeking active set algorithms. Today, MNIST serves as more of a sanity check than as a benchmark. ImageNet :cite:`Deng.Dong.Socher.ea.2009` poses a much \nmore relevant challenge. Unfortunately, ImageNet is too large for many of the examples and illustrations in this book, as it would take too long to train to make the examples interactive. As a substitute we will focus our discussion in the coming sections on the qualitatively similar, but much smaller Fashion-MNIST\ndataset :cite:`Xiao.Rasul.Vollgraf.2017` which was released in 2017. It contains images of 10 categories of clothing at $28 \\times 28$ pixels resolution. ```{.python .input}\n%%tab mxnet\n%matplotlib inline\nimport time\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, npx\nfrom mxnet.gluon.data.vision import transforms\nnpx.set_np()\n\nd2l.use_svg_display()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nimport time\nfrom d2l import torch as d2l\nimport torch\nimport torchvision\nfrom torchvision import transforms\n\nd2l.use_svg_display()\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nimport time\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n\nd2l.use_svg_display()\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nd2l.use_svg_display()\n```"
    },
    {
      "chunk_id": "29a15e9cbcea_0",
      "chapter": "image-classification-dataset",
      "heading": "Loading the Dataset",
      "text": "Since the Fashion-MNIST dataset is so useful, all major frameworks provide preprocessed versions of it. We can  [**download and read it into memory using built-in framework utilities.**]\n\n```{.python .input}\n%%tab mxnet\nclass FashionMNIST(d2l.DataModule):  #@save\n    \"\"\"The Fashion-MNIST dataset.\"\"\"\n    def __init__(self, batch_size=64, resize=(28, 28)):\n        super().__init__()\n        self.save_hyperparameters()\n        trans = transforms.Compose([transforms.Resize(resize),\n                                    transforms.ToTensor()])\n        self.train = gluon.data.vision.FashionMNIST(\n            train=True).transform_first(trans)\n        self.val = gluon.data.vision.FashionMNIST(\n            train=False).transform_first(trans)\n```\n\n```{.python .input}\n%%tab pytorch\nclass FashionMNIST(d2l.DataModule):  #@save\n    \"\"\"The Fashion-MNIST dataset.\"\"\"\n    def __init__(self, batch_size=64, resize=(28, 28)):\n        super().__init__()\n        self.save_hyperparameters()\n        trans = transforms.Compose([transforms.Resize(resize),\n                                    transforms.ToTensor()])\n        self.train = torchvision.datasets.FashionMNIST(\n            root=self.root, train=True, transform=trans, download=True)\n        self.val = torchvision.datasets.FashionMNIST(\n            root=self.root, train=False, transform=trans, download=True)\n```\n\n```{.python .input}\n%%tab tensorflow, jax\nclass FashionMNIST(d2l.DataModule):  #@save\n    \"\"\"The Fashion-MNIST dataset.\"\"\"\n    def __init__(self, batch_size=64, resize=(28, 28)):\n        super().__init__()\n        self.save_hyperparameters()\n        self.train, self.val = tf.keras.datasets.fashion_mnist.load_data()\n```\n\nFashion-MNIST consists of images from 10 categories, each represented\nby 6000 images in the training dataset and by 1000 in the test dataset. A *test dataset* is used for evaluating model performance (it must not be used for training). Consequently the training set and the test set\ncontain 60,000 and 10,000 images, respectively."
    },
    {
      "chunk_id": "29a15e9cbcea_1",
      "chapter": "image-classification-dataset",
      "heading": "Loading the Dataset",
      "text": "A *test dataset* is used for evaluating model performance (it must not be used for training). Consequently the training set and the test set\ncontain 60,000 and 10,000 images, respectively. ```{.python .input}\n%%tab mxnet, pytorch\ndata = FashionMNIST(resize=(32, 32))\nlen(data.train), len(data.val)\n```\n\n```{.python .input}\n%%tab tensorflow, jax\ndata = FashionMNIST(resize=(32, 32))\nlen(data.train[0]), len(data.val[0])\n```\n\nThe images are grayscale and upscaled to $32 \\times 32$ pixels in resolution above. This is similar to the original MNIST dataset which consisted of (binary) black and white images. Note, though, that most modern image data has three channels (red, green, blue) and that hyperspectral images can have in excess of 100 channels (the HyMap sensor has 126 channels). By convention we store an image as a $c \\times h \\times w$ tensor, where $c$ is the number of color channels, $h$ is the height and $w$ is the width. ```{.python .input}\n%%tab all\ndata.train[0][0].shape\n```\n\n[~~Two utility functions to visualize the dataset~~]\n\nThe categories of Fashion-MNIST have human-understandable names. The following convenience method converts between numeric labels and their names. ```{.python .input}\n%%tab all\n@d2l.add_to_class(FashionMNIST)  #@save\ndef text_labels(self, indices):\n    \"\"\"Return text labels.\"\"\"\n    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n              'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [labels[int(i)] for i in indices]\n```"
    },
    {
      "chunk_id": "92d697fac4d4_0",
      "chapter": "image-classification-dataset",
      "heading": "Reading a Minibatch",
      "text": "To make our life easier when reading from the training and test sets,\nwe use the built-in data iterator rather than creating one from scratch. Recall that at each iteration, a data iterator\n[**reads a minibatch of data with size `batch_size`.**]\nWe also randomly shuffle the examples for the training data iterator. ```{.python .input}\n%%tab mxnet\n@d2l.add_to_class(FashionMNIST)  #@save\ndef get_dataloader(self, train):\n    data = self.train if train else self.val\n    return gluon.data.DataLoader(data, self.batch_size, shuffle=train,\n                                 num_workers=self.num_workers)\n```\n\n```{.python .input}\n%%tab pytorch\n@d2l.add_to_class(FashionMNIST)  #@save\ndef get_dataloader(self, train):\n    data = self.train if train else self.val\n    return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\n                                       num_workers=self.num_workers)\n```\n\n```{.python .input}\n%%tab tensorflow, jax\n@d2l.add_to_class(FashionMNIST)  #@save\ndef get_dataloader(self, train):\n    data = self.train if train else self.val\n    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n                            tf.cast(y, dtype='int32'))\n    resize_fn = lambda X, y: (tf.image.resize_with_pad(X, *self.resize), y)\n    shuffle_buf = len(data[0]) if train else 1\n    if tab.selected('tensorflow'):\n        return tf.data.Dataset.from_tensor_slices(process(*data)).batch(\n            self.batch_size).map(resize_fn).shuffle(shuffle_buf)\n    if tab.selected('jax'):\n        return tfds.as_numpy(\n            tf.data.Dataset.from_tensor_slices(process(*data)).batch(\n                self.batch_size).map(resize_fn).shuffle(shuffle_buf))\n```\n\nTo see how this works, let's load a minibatch of images by invoking the `train_dataloader` method. It contains 64 images. ```{.python .input}\n%%tab all\nX, y = next(iter(data.train_dataloader()))\nprint(X.shape, X.dtype, y.shape, y.dtype)\n```\n\nLet's look at the time it takes to read the images. Even though it is a built-in loader, it is not blazingly fast."
    },
    {
      "chunk_id": "92d697fac4d4_1",
      "chapter": "image-classification-dataset",
      "heading": "Reading a Minibatch",
      "text": "It contains 64 images. ```{.python .input}\n%%tab all\nX, y = next(iter(data.train_dataloader()))\nprint(X.shape, X.dtype, y.shape, y.dtype)\n```\n\nLet's look at the time it takes to read the images. Even though it is a built-in loader, it is not blazingly fast. Nonetheless, this is sufficient since processing images with a deep network takes quite a bit longer. Hence it is good enough that training a network will not be I/O constrained. ```{.python .input}\n%%tab all\ntic = time.time()\nfor X, y in data.train_dataloader():\n    continue\nf'{time.time() - tic:.2f} sec'\n```"
    },
    {
      "chunk_id": "0d273389e9a4_0",
      "chapter": "image-classification-dataset",
      "heading": "Visualization",
      "text": "We will often be using the Fashion-MNIST dataset. A convenience function `show_images` can be used to visualize the images and the associated labels. \nSkipping implementation details, we just show the interface below: we only need to know how to invoke `d2l.show_images` rather than how it works\nfor such utility functions.\n\n```{.python .input}\n%%tab all\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n    \"\"\"Plot a list of images.\"\"\"\n    raise NotImplementedError\n```\n\nLet's put it to good use. In general, it is a good idea to visualize and inspect data that you are training on. \nHumans are very good at spotting oddities and because of that, visualization serves as an additional safeguard against mistakes and errors in the design of experiments. Here are [**the images and their corresponding labels**] (in text)\nfor the first few examples in the training dataset.\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(FashionMNIST)  #@save\ndef visualize(self, batch, nrows=1, ncols=8, labels=[]):\n    X, y = batch\n    if not labels:\n        labels = self.text_labels(y)\n    if tab.selected('mxnet', 'pytorch'):\n        d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\n    if tab.selected('tensorflow'):\n        d2l.show_images(tf.squeeze(X), nrows, ncols, titles=labels)\n    if tab.selected('jax'):\n        d2l.show_images(jnp.squeeze(X), nrows, ncols, titles=labels)\n\nbatch = next(iter(data.val_dataloader()))\ndata.visualize(batch)\n```\n\nWe are now ready to work with the Fashion-MNIST dataset in the sections that follow."
    },
    {
      "chunk_id": "ad1adcc77a19_0",
      "chapter": "image-classification-dataset",
      "heading": "Summary",
      "text": "We now have a slightly more realistic dataset to use for classification. Fashion-MNIST is an apparel classification dataset consisting of images representing 10 categories. We will use this dataset in subsequent sections and chapters to evaluate various network designs, from a simple linear model to advanced residual networks. As we commonly do with images, we read them as a tensor of shape (batch size, number of channels, height, width). For now, we only have one channel as the images are grayscale (the visualization above uses a false color palette for improved visibility). \n\nLastly, data iterators are a key component for efficient performance. For instance, we might use GPUs for efficient image decompression, video transcoding, or other preprocessing. Whenever possible, you should rely on well-implemented data iterators that exploit high-performance computing to avoid slowing down your training loop."
    },
    {
      "chunk_id": "1c265d89bc0a_0",
      "chapter": "image-classification-dataset",
      "heading": "Exercises",
      "text": "1. Does reducing the `batch_size` (for instance, to 1) affect the reading performance?\n1. The data iterator performance is important. Do you think the current implementation is fast enough? Explore various options to improve it. Use a system profiler to find out where the bottlenecks are.\n1. Check out the framework's online API documentation. Which other datasets are available?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/48)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/49)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/224)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17980)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Linear Neural Networks for Classification\n:label:`chap_classification`\n\nNow that you have worked through all of the mechanics\nyou are ready to apply the skills you have learned to broader kinds of tasks.\nEven as we pivot towards classification,\nmost of the plumbing remains the same:\nloading the data, passing it through the model,\ngenerating output, calculating the loss,\ntaking gradients with respect to weights,\nand updating the model.\nHowever, the precise form of the targets,\nthe parametrization of the output layer,\nand the choice of loss function will adapt\nto suit the *classification* setting.\n\n```toc\n:maxdepth: 2\n\nsoftmax-regression\nimage-classification-dataset\nclassification\nsoftmax-regression-scratch\nsoftmax-regression-concise\ngeneralization-classification\nenvironment-and-distribution-shift\n```"
    },
    {
      "chunk_id": "a6e249cbbdc4_0",
      "chapter": "softmax-regression-concise",
      "heading": "softmax-regression-concise",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Concise Implementation of Softmax Regression\n:label:`sec_softmax_concise`\n\n\n\nJust as high-level deep learning frameworks\nmade it easier to implement linear regression\n(see :numref:`sec_linear_concise`),\nthey are similarly convenient here.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, init, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom functools import partial\nimport jax\nfrom jax import numpy as jnp\nimport optax\n```"
    },
    {
      "chunk_id": "74e5664b4167_0",
      "chapter": "softmax-regression-concise",
      "heading": "Defining the Model",
      "text": "As in :numref:`sec_linear_concise`, \nwe construct our fully connected layer \nusing the built-in layer. The built-in `__call__` method then invokes `forward` \nwhenever we need to apply the network to some input. :begin_tab:`mxnet`\nEven though the input `X` is a fourth-order tensor, \nthe built-in `Dense` layer \nwill automatically convert `X` into a second-order tensor \nby keeping the dimensionality along the first axis unchanged. :end_tab:\n\n:begin_tab:`pytorch`\nWe use a `Flatten` layer to convert the fourth-order tensor `X` to second order \nby keeping the dimensionality along the first axis unchanged. :end_tab:\n\n:begin_tab:`tensorflow`\nWe use a `Flatten` layer to convert the fourth-order tensor `X` \nby keeping the dimension along the first axis unchanged. :end_tab:\n\n:begin_tab:`jax`\nFlax allows users to write the network class in a more compact way\nusing `@nn.compact` dectorator. With `@nn.compact`, one\ncan simply write all network logic inside a single \u201cforward pass\u201d\nmethod, without needing to define the standard `setup` method in\nthe dataclass."
    },
    {
      "chunk_id": "74e5664b4167_1",
      "chapter": "softmax-regression-concise",
      "heading": "Defining the Model",
      "text": "With `@nn.compact`, one\ncan simply write all network logic inside a single \u201cforward pass\u201d\nmethod, without needing to define the standard `setup` method in\nthe dataclass. :end_tab:\n\n```{.python .input}\n%%tab pytorch\nclass SoftmaxRegression(d2l.Classifier):  #@save\n    \"\"\"The softmax regression model.\"\"\"\n    def __init__(self, num_outputs, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = nn.Sequential(nn.Flatten(),\n                                 nn.LazyLinear(num_outputs))\n\n    def forward(self, X):\n        return self.net(X)\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow\nclass SoftmaxRegression(d2l.Classifier):  #@save\n    \"\"\"The softmax regression model.\"\"\"\n    def __init__(self, num_outputs, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Dense(num_outputs)\n            self.net.initialize()\n        if tab.selected('tensorflow'):\n            self.net = tf.keras.models.Sequential()\n            self.net.add(tf.keras.layers.Flatten())\n            self.net.add(tf.keras.layers.Dense(num_outputs))\n\n    def forward(self, X):\n        return self.net(X)\n```\n\n```{.python .input}\n%%tab jax\nclass SoftmaxRegression(d2l.Classifier):  #@save\n    num_outputs: int\n    lr: float\n\n    @nn.compact\n    def __call__(self, X):\n        X = X.reshape((X.shape[0], -1))  # Flatten\n        X = nn.Dense(self.num_outputs)(X)\n        return X\n```"
    },
    {
      "chunk_id": "dee1ad915133_0",
      "chapter": "softmax-regression-concise",
      "heading": "Softmax Revisited",
      "text": ":label:`subsec_softmax-implementation-revisited`\n\nIn :numref:`sec_softmax_scratch` we calculated our model's output\nand applied the cross-entropy loss. While this is perfectly\nreasonable mathematically, it is risky computationally, because of\nnumerical underflow and overflow in the exponentiation. Recall that the softmax function computes probabilities via\n$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$. If some of the $o_k$ are very large, i.e., very positive,\nthen $\\exp(o_k)$ might be larger than the largest number\nwe can have for certain data types. This is called *overflow*. Likewise,\nif every argument is a very large negative number, we will get *underflow*. For instance, single precision floating point numbers approximately\ncover the range of $10^{-38}$ to $10^{38}$. As such, if the largest term in $\\mathbf{o}$\nlies outside the interval $[-90, 90]$, the result will not be stable. A way round this problem is to subtract $\\bar{o} \\stackrel{\\textrm{def}}{=} \\max_k o_k$ from\nall entries:\n\n$$\n\\hat y_j = \\frac{\\exp o_j}{\\sum_k \\exp o_k} =\n\\frac{\\exp(o_j - \\bar{o}) \\exp \\bar{o}}{\\sum_k \\exp (o_k - \\bar{o}) \\exp \\bar{o}} =\n\\frac{\\exp(o_j - \\bar{o})}{\\sum_k \\exp (o_k - \\bar{o})}. $$\n\nBy construction we know that $o_j - \\bar{o} \\leq 0$ for all $j$. As such, for a $q$-class\nclassification problem, the denominator is contained in the interval $[1, q]$. Moreover, the\nnumerator never exceeds $1$, thus preventing numerical overflow. Numerical underflow only\noccurs when $\\exp(o_j - \\bar{o})$ numerically evaluates as $0$. Nonetheless, a few steps down\nthe road we might find ourselves in trouble when we want to compute $\\log \\hat{y}_j$ as $\\log 0$. In particular, in backpropagation,\nwe might find ourselves faced with a screenful\nof the dreaded `NaN` (Not a Number) results. Fortunately, we are saved by the fact that\neven though we are computing exponential functions,\nwe ultimately intend to take their log\n(when calculating the cross-entropy loss)."
    },
    {
      "chunk_id": "dee1ad915133_1",
      "chapter": "softmax-regression-concise",
      "heading": "Softmax Revisited",
      "text": "Fortunately, we are saved by the fact that\neven though we are computing exponential functions,\nwe ultimately intend to take their log\n(when calculating the cross-entropy loss). By combining softmax and cross-entropy,\nwe can escape the numerical stability issues altogether. We have:\n\n$$\n\\log \\hat{y}_j =\n\\log \\frac{\\exp(o_j - \\bar{o})}{\\sum_k \\exp (o_k - \\bar{o})} =\no_j - \\bar{o} - \\log \\sum_k \\exp (o_k - \\bar{o}). $$\n\nThis avoids both overflow and underflow. We will want to keep the conventional softmax function handy\nin case we ever want to evaluate the output probabilities by our model. But instead of passing softmax probabilities into our new loss function,\nwe just\n[**pass the logits and compute the softmax and its log\nall at once inside the cross-entropy loss function,**]\nwhich does smart things like the [\"LogSumExp trick\"](https://en.wikipedia.org/wiki/LogSumExp)."
    },
    {
      "chunk_id": "dee1ad915133_2",
      "chapter": "softmax-regression-concise",
      "heading": "Softmax Revisited",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(d2l.Classifier)  #@save\ndef loss(self, Y_hat, Y, averaged=True):\n    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n    Y = d2l.reshape(Y, (-1,))\n    if tab.selected('mxnet'):\n        fn = gluon.loss.SoftmaxCrossEntropyLoss()\n        l = fn(Y_hat, Y)\n        return l.mean() if averaged else l\n    if tab.selected('pytorch'):\n        return F.cross_entropy(\n            Y_hat, Y, reduction='mean' if averaged else 'none')\n    if tab.selected('tensorflow'):\n        fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        return fn(Y, Y_hat)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.Classifier)  #@save\n@partial(jax.jit, static_argnums=(0, 5))\ndef loss(self, params, X, Y, state, averaged=True):\n    # To be used later (e.g., for batch norm)\n    Y_hat = state.apply_fn({'params': params}, *X,\n                           mutable=False, rngs=None)\n    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n    Y = d2l.reshape(Y, (-1,))\n    fn = optax.softmax_cross_entropy_with_integer_labels\n    # The returned empty dictionary is a placeholder for auxiliary data,\n    # which will be used later (e.g., for batch norm)\n    return (fn(Y_hat, Y).mean(), {}) if averaged else (fn(Y_hat, Y), {})\n```"
    },
    {
      "chunk_id": "efa1e941f0d0_0",
      "chapter": "softmax-regression-concise",
      "heading": "Training",
      "text": "Next we train our model. We use Fashion-MNIST images, flattened to 784-dimensional feature vectors.\n\n```{.python .input}\n%%tab all\ndata = d2l.FashionMNIST(batch_size=256)\nmodel = SoftmaxRegression(num_outputs=10, lr=0.1)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n```\n\nAs before, this algorithm converges to a solution\nthat is reasonably accurate,\nalbeit this time with fewer lines of code than before."
    },
    {
      "chunk_id": "05fae4c5f123_0",
      "chapter": "softmax-regression-concise",
      "heading": "Summary",
      "text": "High-level APIs are very convenient at hiding from their user potentially dangerous aspects, such as numerical stability. Moreover, they allow users to design models concisely with very few lines of code. This is both a blessing and a curse. The obvious benefit is that it makes things highly accessible, even to engineers who never took a single class of statistics in their life (in fact, they are part of the target audience of the book). But hiding the sharp edges also comes with a price: a disincentive to add new and different components on your own, since there is little muscle memory for doing it. Moreover, it makes it more difficult to *fix* things whenever the protective padding of\na framework fails to cover all the corner cases entirely. Again, this is due to lack of familiarity.\n\nAs such, we strongly urge you to review *both* the bare bones and the elegant versions of many of the implementations that follow. While we emphasize ease of understanding, the implementations are nonetheless usually quite performant (convolutions are the big exception here). It is our intention to allow you to build on these when you invent something new that no framework can give you."
    },
    {
      "chunk_id": "5dcc945c2880_0",
      "chapter": "softmax-regression-concise",
      "heading": "Exercises",
      "text": "1. Deep learning uses many different number formats, including FP64 double precision (used extremely rarely),\nFP32 single precision, BFLOAT16 (good for compressed representations), FP16 (very unstable), TF32 (a new format from NVIDIA), and INT8. Compute the smallest and largest argument of the exponential function for which the result does not lead to numerical underflow or overflow.\n1. INT8 is a very limited format consisting of nonzero numbers from $1$ to $255$. How could you extend its dynamic range without using more bits? Do standard multiplication and addition still work?\n1. Increase the number of epochs for training. Why might the validation accuracy decrease after a while? How could we fix this?\n1. What happens as you increase the learning rate? Compare the loss curves for several learning rates. Which one works better? When?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/52)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/53)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/260)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17983)\n:end_tab:"
    },
    {
      "chunk_id": "ea0b6efa810f_0",
      "chapter": "softmax-regression-scratch",
      "heading": "softmax-regression-scratch",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Softmax Regression Implementation from Scratch\n:label:`sec_softmax_scratch`\n\nBecause softmax regression is so fundamental,\nwe believe that you ought to know\nhow to implement it yourself.\nHere, we limit ourselves to defining the\nsoftmax-specific aspects of the model\nand reuse the other components\nfrom our linear regression section,\nincluding the training loop.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx, gluon\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\nfrom functools import partial\n```"
    },
    {
      "chunk_id": "44af6135ad8b_0",
      "chapter": "softmax-regression-scratch",
      "heading": "The Softmax",
      "text": "Let's begin with the most important part:\nthe mapping from scalars to probabilities. For a refresher, recall the operation of the sum operator\nalong specific dimensions in a tensor,\nas discussed in :numref:`subsec_lin-alg-reduction`\nand :numref:`subsec_lin-alg-non-reduction`. [**Given a matrix `X` we can sum over all elements (by default) or only\nover elements in the same axis.**]\nThe `axis` variable lets us compute row and column sums:\n\n```{.python .input}\n%%tab all\nX = d2l.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nd2l.reduce_sum(X, 0, keepdims=True), d2l.reduce_sum(X, 1, keepdims=True)\n```\n\nComputing the softmax requires three steps:\n(i) exponentiation of each term;\n(ii) a sum over each row to compute the normalization constant for each example;\n(iii) division of each row by its normalization constant,\nensuring that the result sums to 1:\n\n(**\n$$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}.$$\n**)\n\nThe (logarithm of the) denominator\nis called the (log) *partition function*. It was introduced in [statistical physics](https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics))\nto sum over all possible states in a thermodynamic ensemble. The implementation is straightforward:\n\n```{.python .input}\n%%tab all\ndef softmax(X):\n    X_exp = d2l.exp(X)\n    partition = d2l.reduce_sum(X_exp, 1, keepdims=True)\n    return X_exp / partition  # The broadcasting mechanism is applied here\n```\n\nFor any input `X`, [**we turn each element\ninto a nonnegative number. Each row sums up to 1,**]\nas is required for a probability. Caution: the code above is *not* robust against very large or very small arguments. While it is sufficient to illustrate what is happening, you should *not* use this code verbatim for any serious purpose. Deep learning frameworks have such protections built in and we will be using the built-in softmax going forward."
    },
    {
      "chunk_id": "44af6135ad8b_1",
      "chapter": "softmax-regression-scratch",
      "heading": "The Softmax",
      "text": "While it is sufficient to illustrate what is happening, you should *not* use this code verbatim for any serious purpose. Deep learning frameworks have such protections built in and we will be using the built-in softmax going forward. ```{.python .input}\n%%tab mxnet\nX = d2l.rand(2, 5)\nX_prob = softmax(X)\nX_prob, d2l.reduce_sum(X_prob, 1)\n```\n\n```{.python .input}\n%%tab tensorflow, pytorch\nX = d2l.rand((2, 5))\nX_prob = softmax(X)\nX_prob, d2l.reduce_sum(X_prob, 1)\n```\n\n```{.python .input}\n%%tab jax\nX = jax.random.uniform(jax.random.PRNGKey(d2l.get_seed()), (2, 5))\nX_prob = softmax(X)\nX_prob, d2l.reduce_sum(X_prob, 1)\n```"
    },
    {
      "chunk_id": "235d01df99a0_0",
      "chapter": "softmax-regression-scratch",
      "heading": "The Model",
      "text": "We now have everything that we need\nto implement [**the softmax regression model.**]\nAs in our linear regression example,\neach instance will be represented\nby a fixed-length vector. Since the raw data here consists\nof $28 \\times 28$ pixel images,\n[**we flatten each image,\ntreating them as vectors of length 784.**]\nIn later chapters, we will introduce\nconvolutional neural networks,\nwhich exploit the spatial structure\nin a more satisfying way. In softmax regression,\nthe number of outputs from our network\nshould be equal to the number of classes. (**Since our dataset has 10 classes,\nour network has an output dimension of 10.**)\nConsequently, our weights constitute a $784 \\times 10$ matrix\nplus a $1 \\times 10$ row vector for the biases. As with linear regression,\nwe initialize the weights `W`\nwith Gaussian noise. The biases are initialized as zeros."
    },
    {
      "chunk_id": "235d01df99a0_1",
      "chapter": "softmax-regression-scratch",
      "heading": "The Model",
      "text": "As with linear regression,\nwe initialize the weights `W`\nwith Gaussian noise. The biases are initialized as zeros. ```{.python .input}\n%%tab mxnet\nclass SoftmaxRegressionScratch(d2l.Classifier):\n    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.W = np.random.normal(0, sigma, (num_inputs, num_outputs))\n        self.b = np.zeros(num_outputs)\n        self.W.attach_grad()\n        self.b.attach_grad()\n\n    def collect_params(self):\n        return [self.W, self.b]\n```\n\n```{.python .input}\n%%tab pytorch\nclass SoftmaxRegressionScratch(d2l.Classifier):\n    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),\n                              requires_grad=True)\n        self.b = torch.zeros(num_outputs, requires_grad=True)\n\n    def parameters(self):\n        return [self.W, self.b]\n```\n\n```{.python .input}\n%%tab tensorflow\nclass SoftmaxRegressionScratch(d2l.Classifier):\n    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.W = tf.random.normal((num_inputs, num_outputs), 0, sigma)\n        self.b = tf.zeros(num_outputs)\n        self.W = tf.Variable(self.W)\n        self.b = tf.Variable(self.b)\n```\n\n```{.python .input}\n%%tab jax\nclass SoftmaxRegressionScratch(d2l.Classifier):\n    num_inputs: int\n    num_outputs: int\n    lr: float\n    sigma: float = 0.01\n\n    def setup(self):\n        self.W = self.param('W', nn.initializers.normal(self.sigma),\n                            (self.num_inputs, self.num_outputs))\n        self.b = self.param('b', nn.initializers.zeros, self.num_outputs)\n```\n\nThe code below defines how the network\nmaps each input to an output. Note that we flatten each $28 \\times 28$ pixel image in the batch\ninto a vector using `reshape`\nbefore passing the data through our model."
    },
    {
      "chunk_id": "235d01df99a0_2",
      "chapter": "softmax-regression-scratch",
      "heading": "The Model",
      "text": "Note that we flatten each $28 \\times 28$ pixel image in the batch\ninto a vector using `reshape`\nbefore passing the data through our model. ```{.python .input}\n%%tab all\n@d2l.add_to_class(SoftmaxRegressionScratch)\ndef forward(self, X):\n    X = d2l.reshape(X, (-1, self.W.shape[0]))\n    return softmax(d2l.matmul(X, self.W) + self.b)\n```"
    },
    {
      "chunk_id": "9b921ffab45b_0",
      "chapter": "softmax-regression-scratch",
      "heading": "The Cross-Entropy Loss",
      "text": "Next we need to implement the cross-entropy loss function\n(introduced in :numref:`subsec_softmax-regression-loss-func`). This may be the most common loss function\nin all of deep learning. At the moment, applications of deep learning\neasily cast as classification problems\nfar outnumber those better treated as regression problems. Recall that cross-entropy takes the negative log-likelihood\nof the predicted probability assigned to the true label. For efficiency we avoid Python for-loops and use indexing instead. In particular, the one-hot encoding in $\\mathbf{y}$\nallows us to select the matching terms in $\\hat{\\mathbf{y}}$. To see this in action we [**create sample data `y_hat`\nwith 2 examples of predicted probabilities over 3 classes and their corresponding labels `y`.**]\nThe correct labels are $0$ and $2$ respectively (i.e., the first and third class). [**Using `y` as the indices of the probabilities in `y_hat`,**]\nwe can pick out terms efficiently. ```{.python .input}\n%%tab mxnet, pytorch, jax\ny = d2l.tensor([0, 2])\ny_hat = d2l.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny_hat[[0, 1], y]\n```\n\n```{.python .input}\n%%tab tensorflow\ny_hat = tf.constant([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny = tf.constant([0, 2])\ntf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))\n```\n\n:begin_tab:`pytorch, mxnet, tensorflow`\nNow we can (**implement the cross-entropy loss function**) by averaging over the logarithms of the selected probabilities. :end_tab:\n\n:begin_tab:`jax`\nNow we can (**implement the cross-entropy loss function**) by averaging over the logarithms of the selected probabilities. Note that to make use of `jax.jit` to speed up JAX implementations, and\nto make sure `loss` is a pure function, the `cross_entropy` function is re-defined\ninside the `loss` to avoid usage of any global variables or functions\nwhich may render the `loss` function impure."
    },
    {
      "chunk_id": "9b921ffab45b_1",
      "chapter": "softmax-regression-scratch",
      "heading": "The Cross-Entropy Loss",
      "text": "We refer interested readers to the [JAX documentation](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions) on `jax.jit` and pure functions. :end_tab:\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\ndef cross_entropy(y_hat, y):\n    return -d2l.reduce_mean(d2l.log(y_hat[list(range(len(y_hat))), y]))\n\ncross_entropy(y_hat, y)\n```\n\n```{.python .input}\n%%tab tensorflow\ndef cross_entropy(y_hat, y):\n    return -tf.reduce_mean(tf.math.log(tf.boolean_mask(\n        y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))))\n\ncross_entropy(y_hat, y)\n```\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(SoftmaxRegressionScratch)\ndef loss(self, y_hat, y):\n    return cross_entropy(y_hat, y)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(SoftmaxRegressionScratch)\n@partial(jax.jit, static_argnums=(0))\ndef loss(self, params, X, y, state):\n    def cross_entropy(y_hat, y):\n        return -d2l.reduce_mean(d2l.log(y_hat[list(range(len(y_hat))), y]))\n    y_hat = state.apply_fn({'params': params}, *X)\n    # The returned empty dictionary is a placeholder for auxiliary data,\n    # which will be used later (e.g., for batch norm)\n    return cross_entropy(y_hat, y), {}\n```"
    },
    {
      "chunk_id": "32029f8fe767_0",
      "chapter": "softmax-regression-scratch",
      "heading": "Training",
      "text": "We reuse the `fit` method defined in :numref:`sec_linear_scratch` to [**train the model with 10 epochs.**]\nNote that the number of epochs (`max_epochs`),\nthe minibatch size (`batch_size`),\nand learning rate (`lr`)\nare adjustable hyperparameters.\nThat means that while these values are not\nlearned during our primary training loop,\nthey still influence the performance\nof our model, both vis-\u00e0-vis training\nand generalization performance.\nIn practice you will want to choose these values\nbased on the *validation* split of the data\nand then, ultimately, to evaluate your final model\non the *test* split.\nAs discussed in :numref:`subsec_generalization-model-selection`,\nwe will regard the test data of Fashion-MNIST\nas the validation set, thus\nreporting validation loss and validation accuracy\non this split.\n\n```{.python .input}\n%%tab all\ndata = d2l.FashionMNIST(batch_size=256)\nmodel = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "2543d7450905_0",
      "chapter": "softmax-regression-scratch",
      "heading": "Prediction",
      "text": "Now that training is complete,\nour model is ready to [**classify some images.**]\n\n```{.python .input}\n%%tab all\nX, y = next(iter(data.val_dataloader()))\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    preds = d2l.argmax(model(X), axis=1)\nif tab.selected('jax'):\n    preds = d2l.argmax(model.apply({'params': trainer.state.params}, X), axis=1)\npreds.shape\n```\n\nWe are more interested in the images we label *incorrectly*. We visualize them by\ncomparing their actual labels\n(first line of text output)\nwith the predictions from the model\n(second line of text output).\n\n```{.python .input}\n%%tab all\nwrong = d2l.astype(preds, y.dtype) != y\nX, y, preds = X[wrong], y[wrong], preds[wrong]\nlabels = [a+'\\n'+b for a, b in zip(\n    data.text_labels(y), data.text_labels(preds))]\ndata.visualize([X, y], labels=labels)\n```"
    },
    {
      "chunk_id": "7fe23dec1ad2_0",
      "chapter": "softmax-regression-scratch",
      "heading": "Summary",
      "text": "By now we are starting to get some experience\nwith solving linear regression\nand classification problems.\nWith it, we have reached what would arguably be\nthe state of the art of 1960--1970s of statistical modeling.\nIn the next section, we will show you how to leverage\ndeep learning frameworks to implement this model\nmuch more efficiently."
    },
    {
      "chunk_id": "2aac35926f5d_0",
      "chapter": "softmax-regression-scratch",
      "heading": "Exercises",
      "text": "1. In this section, we directly implemented the softmax function based on the mathematical definition of the softmax operation. As discussed in :numref:`sec_softmax` this can cause numerical instabilities.\n    1. Test whether `softmax` still works correctly if an input has a value of $100$.\n    1. Test whether `softmax` still works correctly if the largest of all inputs is smaller than $-100$.\n    1. Implement a fix by looking at the value relative to the largest entry in the argument.\n1. Implement a `cross_entropy` function that follows the definition of the cross-entropy loss function $\\sum_i y_i \\log \\hat{y}_i$.\n    1. Try it out in the code example of this section.\n    1. Why do you think it runs more slowly?\n    1. Should you use it? When would it make sense to?\n    1. What do you need to be careful of? Hint: consider the domain of the logarithm.\n1. Is it always a good idea to return the most likely label? For example, would you do this for medical diagnosis? How would you try to address this?\n1. Assume that we want to use softmax regression to predict the next word based on some features. What are some problems that might arise from a large vocabulary?\n1. Experiment with the hyperparameters of the code in this section. In particular:\n    1. Plot how the validation loss changes as you change the learning rate.\n    1. Do the validation and training loss change as you change the minibatch size? How large or small do you need to go before you see an effect?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/50)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/51)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/225)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17982)\n:end_tab:"
    },
    {
      "chunk_id": "511280b6b60a_0",
      "chapter": "softmax-regression",
      "heading": "softmax-regression",
      "text": "# Softmax Regression\n:label:`sec_softmax`\n\nIn :numref:`sec_linear_regression`, we introduced linear regression,\nworking through implementations from scratch in :numref:`sec_linear_scratch`\nand again using high-level APIs of a deep learning framework\nin :numref:`sec_linear_concise` to do the heavy lifting. Regression is the hammer we reach for when\nwe want to answer *how much?* or *how many?* questions. If you want to predict the number of dollars (price)\nat which a house will be sold,\nor the number of wins a baseball team might have,\nor the number of days that a patient\nwill remain hospitalized before being discharged,\nthen you are probably looking for a regression model. However, even within regression models,\nthere are important distinctions. For instance, the price of a house\nwill never be negative and changes might often be *relative* to its baseline price. As such, it might be more effective to regress\non the logarithm of the price. Likewise, the number of days a patient spends in hospital\nis a *discrete nonnegative* random variable. As such, least mean squares might not be an ideal approach either. This sort of time-to-event modeling\ncomes with a host of other complications that are dealt with\nin a specialized subfield called *survival modeling*. The point here is not to overwhelm you but just\nto let you know that there is a lot more to estimation\nthan simply minimizing squared errors. And more broadly, there is a lot more to supervised learning than regression. In this section, we focus on *classification* problems\nwhere we put aside *how much?* questions\nand instead focus on *which category?* questions. * Does this email belong in the spam folder or the inbox? * Is this customer more likely to sign up\n  or not to sign up for a subscription service? * Does this image depict a donkey, a dog, a cat, or a rooster? * Which movie is Aston most likely to watch next? * Which section of the book are you going to read next?"
    },
    {
      "chunk_id": "511280b6b60a_1",
      "chapter": "softmax-regression",
      "heading": "softmax-regression",
      "text": "* Does this image depict a donkey, a dog, a cat, or a rooster? * Which movie is Aston most likely to watch next? * Which section of the book are you going to read next? Colloquially, machine learning practitioners\noverload the word *classification*\nto describe two subtly different problems:\n(i) those where we are interested only in\nhard assignments of examples to categories (classes);\nand (ii) those where we wish to make soft assignments,\ni.e., to assess the probability that each category applies. The distinction tends to get blurred, in part,\nbecause often, even when we only care about hard assignments,\nwe still use models that make soft assignments. Even more, there are cases where more than one label might be true. For instance, a news article might simultaneously cover\nthe topics of entertainment, business, and space flight,\nbut not the topics of medicine or sports. Thus, categorizing it into one of the above categories\non their own would not be very useful. This problem is commonly known as [multi-label classification](https://en.wikipedia.org/wiki/Multi-label_classification). See :citet:`Tsoumakas.Katakis.2007` for an overview\nand :citet:`Huang.Xu.Yu.2015`\nfor an effective algorithm when tagging images."
    },
    {
      "chunk_id": "a4143529f02d_0",
      "chapter": "softmax-regression",
      "heading": "Classification",
      "text": ":label:`subsec_classification-problem`\n\nTo get our feet wet, let's start with\na simple image classification problem.\nHere, each input consists of a $2\\times2$ grayscale image.\nWe can represent each pixel value with a single scalar,\ngiving us four features $x_1, x_2, x_3, x_4$.\nFurther, let's assume that each image belongs to one\namong the categories \"cat\", \"chicken\", and \"dog\".\n\nNext, we have to choose how to represent the labels.\nWe have two obvious choices.\nPerhaps the most natural impulse would be\nto choose $y \\in \\{1, 2, 3\\}$,\nwhere the integers represent\n$\\{\\textrm{dog}, \\textrm{cat}, \\textrm{chicken}\\}$ respectively.\nThis is a great way of *storing* such information on a computer.\nIf the categories had some natural ordering among them,\nsay if we were trying to predict\n$\\{\\textrm{baby}, \\textrm{toddler}, \\textrm{adolescent}, \\textrm{young adult}, \\textrm{adult}, \\textrm{geriatric}\\}$,\nthen it might even make sense to cast this as\nan [ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression) problem\nand keep the labels in this format.\nSee :citet:`Moon.Smola.Chang.ea.2010` for an overview\nof different types of ranking loss functions\nand :citet:`Beutel.Murray.Faloutsos.ea.2014` for a Bayesian approach\nthat addresses responses with more than one mode.\n\nIn general, classification problems do not come\nwith natural orderings among the classes.\nFortunately, statisticians long ago invented a simple way\nto represent categorical data: the *one-hot encoding*.\nA one-hot encoding is a vector\nwith as many components as we have categories.\nThe component corresponding to a particular instance's category is set to 1\nand all other components are set to 0.\nIn our case, a label $y$ would be a three-dimensional vector,\nwith $(1, 0, 0)$ corresponding to \"cat\", $(0, 1, 0)$ to \"chicken\",\nand $(0, 0, 1)$ to \"dog\":\n\n$$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}.$$"
    },
    {
      "chunk_id": "1a8647a59482_0",
      "chapter": "softmax-regression",
      "heading": "Linear Model",
      "text": "In order to estimate the conditional probabilities\nassociated with all the possible classes,\nwe need a model with multiple outputs, one per class.\nTo address classification with linear models,\nwe will need as many affine functions as we have outputs.\nStrictly speaking, we only need one fewer,\nsince the final category has to be the difference\nbetween $1$ and the sum of the other categories,\nbut for reasons of symmetry\nwe use a slightly redundant parametrization.\nEach output corresponds to its own affine function.\nIn our case, since we have 4 features and 3 possible output categories,\nwe need 12 scalars to represent the weights ($w$ with subscripts),\nand 3 scalars to represent the biases ($b$ with subscripts). This yields:\n\n$$\n\\begin{aligned}\no_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\no_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\no_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n\\end{aligned}\n$$\n\nThe corresponding neural network diagram\nis shown in :numref:`fig_softmaxreg`.\nJust as in linear regression,\nwe use a single-layer neural network.\nAnd since the calculation of each output, $o_1, o_2$, and $o_3$,\ndepends on every input, $x_1$, $x_2$, $x_3$, and $x_4$,\nthe output layer can also be described as a *fully connected layer*.\n\n![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)\n:label:`fig_softmaxreg`\n\nFor a more concise notation we use vectors and matrices:\n$\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$ is\nmuch better suited for mathematics and code.\nNote that we have gathered all of our weights into a $3 \\times 4$ matrix and all biases\n$\\mathbf{b} \\in \\mathbb{R}^3$ in a vector."
    },
    {
      "chunk_id": "a1f73fd6d264_0",
      "chapter": "softmax-regression",
      "heading": "The Softmax",
      "text": ":label:`subsec_softmax_operation`\n\nAssuming a suitable loss function,\nwe could try, directly, to minimize the difference\nbetween $\\mathbf{o}$ and the labels $\\mathbf{y}$. While it turns out that treating classification\nas a vector-valued regression problem works surprisingly well,\nit is nonetheless unsatisfactory in the following ways:\n\n* There is no guarantee that the outputs $o_i$ sum up to $1$ in the way we expect probabilities to behave. * There is no guarantee that the outputs $o_i$ are even nonnegative, even if their outputs sum up to $1$, or that they do not exceed $1$. Both aspects render the estimation problem difficult to solve\nand the solution very brittle to outliers. For instance, if we assume that there\nis a positive linear dependency\nbetween the number of bedrooms and the likelihood\nthat someone will buy a house,\nthe probability might exceed $1$\nwhen it comes to buying a mansion! As such, we need a mechanism to \"squish\" the outputs. There are many ways we might accomplish this goal. For instance, we could assume that the outputs\n$\\mathbf{o}$ are corrupted versions of $\\mathbf{y}$,\nwhere the corruption occurs by means of adding noise $\\boldsymbol{\\epsilon}$\ndrawn from a normal distribution. In other words, $\\mathbf{y} = \\mathbf{o} + \\boldsymbol{\\epsilon}$,\nwhere $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This is the so-called [probit model](https://en.wikipedia.org/wiki/Probit_model),\nfirst introduced by :citet:`Fechner.1860`. While appealing, it does not work quite as well\nnor lead to a particularly nice optimization problem,\nwhen compared to the softmax. Another way to accomplish this goal\n(and to ensure nonnegativity) is to use\nan exponential function $P(y = i) \\propto \\exp o_i$. This does indeed satisfy the requirement\nthat the conditional class probability\nincreases with increasing $o_i$, it is monotonic,\nand all probabilities are nonnegative. We can then transform these values so that they add up to $1$\nby dividing each by their sum. This process is called *normalization*."
    },
    {
      "chunk_id": "a1f73fd6d264_1",
      "chapter": "softmax-regression",
      "heading": "The Softmax",
      "text": "We can then transform these values so that they add up to $1$\nby dividing each by their sum. This process is called *normalization*. Putting these two pieces together\ngives us the *softmax* function:\n\n$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\textrm{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}.$$\n:eqlabel:`eq_softmax_y_and_o`\n\nNote that the largest coordinate of $\\mathbf{o}$\ncorresponds to the most likely class according to $\\hat{\\mathbf{y}}$. Moreover, because the softmax operation\npreserves the ordering among its arguments,\nwe do not need to compute the softmax\nto determine which class has been assigned the highest probability. Thus,\n\n$$\n\\operatorname*{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j. $$\n\n\nThe idea of a softmax dates back to :citet:`Gibbs.1902`,\nwho adapted ideas from physics. Dating even further back, Boltzmann,\nthe father of modern statistical physics,\nused this trick to model a distribution\nover energy states in gas molecules. In particular, he discovered that the prevalence\nof a state of energy in a thermodynamic ensemble,\nsuch as the molecules in a gas,\nis proportional to $\\exp(-E/kT)$. Here, $E$ is the energy of a state,\n$T$ is the temperature, and $k$ is the Boltzmann constant. When statisticians talk about increasing or decreasing\nthe \"temperature\" of a statistical system,\nthey refer to changing $T$\nin order to favor lower or higher energy states. Following Gibbs' idea, energy equates to error. Energy-based models :cite:`Ranzato.Boureau.Chopra.ea.2007`\nuse this point of view when describing\nproblems in deep learning."
    },
    {
      "chunk_id": "58bba3f44e32_0",
      "chapter": "softmax-regression",
      "heading": "Vectorization",
      "text": ":label:`subsec_softmax_vectorization`\n\nTo improve computational efficiency,\nwe vectorize calculations in minibatches of data.\nAssume that we are given a minibatch $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$\nof $n$ examples with dimensionality (number of inputs) $d$.\nMoreover, assume that we have $q$ categories in the output.\nThen the weights satisfy $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$\nand the bias satisfies $\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$.\n\n$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$\n:eqlabel:`eq_minibatch_softmax_reg`\n\nThis accelerates the dominant operation into\na matrix--matrix product $\\mathbf{X} \\mathbf{W}$.\nMoreover, since each row in $\\mathbf{X}$ represents a data example,\nthe softmax operation itself can be computed *rowwise*:\nfor each row of $\\mathbf{O}$, exponentiate all entries\nand then normalize them by the sum.\nNote, though, that care must be taken\nto avoid exponentiating and taking logarithms of large numbers,\nsince this can cause numerical overflow or underflow.\nDeep learning frameworks take care of this automatically."
    },
    {
      "chunk_id": "ebb7e8e06830_0",
      "chapter": "softmax-regression",
      "heading": "Loss Function",
      "text": ":label:`subsec_softmax-regression-loss-func`\n\nNow that we have a mapping from features $\\mathbf{x}$\nto probabilities $\\mathbf{\\hat{y}}$,\nwe need a way to optimize the accuracy of this mapping.\nWe will rely on maximum likelihood estimation,\nthe very same method that we encountered\nwhen providing a probabilistic justification\nfor the mean squared error loss in\n:numref:`subsec_normal_distribution_and_squared_loss`."
    },
    {
      "chunk_id": "0b4ed2647e2b_0",
      "chapter": "softmax-regression",
      "heading": "Log-Likelihood",
      "text": "The softmax function gives us a vector $\\hat{\\mathbf{y}}$,\nwhich we can interpret as the (estimated) conditional probabilities\nof each class, given any input $\\mathbf{x}$,\nsuch as $\\hat{y}_1$ = $P(y=\\textrm{cat} \\mid \\mathbf{x})$. In the following we assume that for a dataset\nwith features $\\mathbf{X}$ the labels $\\mathbf{Y}$\nare represented using a one-hot encoding label vector. We can compare the estimates with reality\nby checking how probable the actual classes are\naccording to our model, given the features:\n\n$$\nP(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}). $$\n\nWe are allowed to use the factorization\nsince we assume that each label is drawn independently\nfrom its respective distribution $P(\\mathbf{y}\\mid\\mathbf{x}^{(i)})$. Since maximizing the product of terms is awkward,\nwe take the negative logarithm to obtain the equivalent problem\nof minimizing the negative log-likelihood:\n\n$$\n-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n$$\n\nwhere for any pair of label $\\mathbf{y}$\nand model prediction $\\hat{\\mathbf{y}}$\nover $q$ classes, the loss function $l$ is\n\n$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n:eqlabel:`eq_l_cross_entropy`\n\nFor reasons explained later on,\nthe loss function in :eqref:`eq_l_cross_entropy`\nis commonly called the *cross-entropy loss*. Since $\\mathbf{y}$ is a one-hot vector of length $q$,\nthe sum over all its coordinates $j$ vanishes for all but one term. Note that the loss $l(\\mathbf{y}, \\hat{\\mathbf{y}})$\nis bounded from below by $0$\nwhenever $\\hat{\\mathbf{y}}$ is a probability vector:\nno single entry is larger than $1$,\nhence their negative logarithm cannot be lower than $0$;\n$l(\\mathbf{y}, \\hat{\\mathbf{y}}) = 0$ only if we predict\nthe actual label with *certainty*."
    },
    {
      "chunk_id": "0b4ed2647e2b_1",
      "chapter": "softmax-regression",
      "heading": "Log-Likelihood",
      "text": "This can never happen for any finite setting of the weights\nbecause taking a softmax output towards $1$\nrequires taking the corresponding input $o_i$ to infinity\n(or all other outputs $o_j$ for $j \\neq i$ to negative infinity). Even if our model could assign an output probability of $0$,\nany error made when assigning such high confidence\nwould incur infinite loss ($-\\log 0 = \\infty$)."
    },
    {
      "chunk_id": "a53d8e491e2b_0",
      "chapter": "softmax-regression",
      "heading": "Softmax and Cross-Entropy Loss",
      "text": ":label:`subsec_softmax_and_derivatives`\n\nSince the softmax function\nand the corresponding cross-entropy loss are so common,\nit is worth understanding a bit better how they are computed. Plugging :eqref:`eq_softmax_y_and_o` into the definition of the loss\nin :eqref:`eq_l_cross_entropy`\nand using the definition of the softmax we obtain\n\n$$\n\\begin{aligned}\nl(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j \\\\\n&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j. \\end{aligned}\n$$\n\nTo understand a bit better what is going on,\nconsider the derivative with respect to any logit $o_j$. We get\n\n$$\n\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j. $$\n\nIn other words, the derivative is the difference\nbetween the probability assigned by our model,\nas expressed by the softmax operation,\nand what actually happened, as expressed\nby elements in the one-hot label vector. In this sense, it is very similar\nto what we saw in regression,\nwhere the gradient was the difference\nbetween the observation $y$ and estimate $\\hat{y}$. This is not a coincidence. In any exponential family model,\nthe gradients of the log-likelihood are given by precisely this term. This fact makes computing gradients easy in practice. Now consider the case where we observe not just a single outcome\nbut an entire distribution over outcomes. We can use the same representation as before for the label $\\mathbf{y}$. The only difference is that rather\nthan a vector containing only binary entries,\nsay $(0, 0, 1)$, we now have a generic probability vector,\nsay $(0.1, 0.2, 0.7)$. The math that we used previously to define the loss $l$\nin :eqref:`eq_l_cross_entropy`\nstill works well,\njust that the interpretation is slightly more general. It is the expected value of the loss for a distribution over labels."
    },
    {
      "chunk_id": "a53d8e491e2b_1",
      "chapter": "softmax-regression",
      "heading": "Softmax and Cross-Entropy Loss",
      "text": "The math that we used previously to define the loss $l$\nin :eqref:`eq_l_cross_entropy`\nstill works well,\njust that the interpretation is slightly more general. It is the expected value of the loss for a distribution over labels. This loss is called the *cross-entropy loss* and it is\none of the most commonly used losses for classification problems. We can demystify the name by introducing just the basics of information theory. In a nutshell, it measures the number of bits needed to encode what we see, $\\mathbf{y}$,\nrelative to what we predict that should happen, $\\hat{\\mathbf{y}}$. We provide a very basic explanation in the following. For further\ndetails on information theory see\n:citet:`Cover.Thomas.1999` or :citet:`mackay2003information`."
    },
    {
      "chunk_id": "aecad11fb2dc_0",
      "chapter": "softmax-regression",
      "heading": "Information Theory Basics",
      "text": ":label:`subsec_info_theory_basics`\n\nMany deep learning papers use intuition and terms from information theory.\nTo make sense of them, we need some common language.\nThis is a survival guide.\n*Information theory* deals with the problem\nof encoding, decoding, transmitting,\nand manipulating information (also known as data)."
    },
    {
      "chunk_id": "63628d409e5c_0",
      "chapter": "softmax-regression",
      "heading": "Entropy",
      "text": "The central idea in information theory is to quantify the\namount of information contained in data.\nThis places a  limit on our ability to compress data.\nFor a distribution $P$ its *entropy*, $H[P]$, is defined as:\n\n$$H[P] = \\sum_j - P(j) \\log P(j).$$\n:eqlabel:`eq_softmax_reg_entropy`\n\nOne of the fundamental theorems of information theory states\nthat in order to encode data drawn randomly from the distribution $P$,\nwe need at least $H[P]$ \"nats\" to encode it :cite:`Shannon.1948`.\nIf you wonder what a \"nat\" is, it is the equivalent of bit\nbut when using a code with base $e$ rather than one with base 2.\nThus, one nat is $\\frac{1}{\\log(2)} \\approx 1.44$ bit."
    },
    {
      "chunk_id": "425a9e0b0b0b_0",
      "chapter": "softmax-regression",
      "heading": "Surprisal",
      "text": "You might be wondering what compression has to do with prediction.\nImagine that we have a stream of data that we want to compress.\nIf it is always easy for us to predict the next token,\nthen this data is easy to compress.\nTake the extreme example where every token in the stream\nalways takes the same value.\nThat is a very boring data stream!\nAnd not only it is boring, but it is also easy to predict.\nBecause the tokens are always the same,\nwe do not have to transmit any information\nto communicate the contents of the stream.\nEasy to predict, easy to compress.\n\nHowever if we cannot perfectly predict every event,\nthen we might sometimes be surprised.\nOur surprise is greater when an event is assigned lower probability.\nClaude Shannon settled on $\\log \\frac{1}{P(j)} = -\\log P(j)$\nto quantify one's *surprisal* at observing an event $j$\nhaving assigned it a (subjective) probability $P(j)$.\nThe entropy defined in :eqref:`eq_softmax_reg_entropy`\nis then the *expected surprisal*\nwhen one assigned the correct probabilities\nthat truly match the data-generating process."
    },
    {
      "chunk_id": "be2a920ce80c_0",
      "chapter": "softmax-regression",
      "heading": "Cross-Entropy Revisited",
      "text": "So if entropy is the level of surprise experienced\nby someone who knows the true probability,\nthen you might be wondering, what is cross-entropy?\nThe cross-entropy *from* $P$ *to* $Q$, denoted $H(P, Q)$,\nis the expected surprisal of an observer with subjective probabilities $Q$\nupon seeing data that was actually generated according to probabilities $P$.\nThis is given by $H(P, Q) \\stackrel{\\textrm{def}}{=} \\sum_j - P(j) \\log Q(j)$.\nThe lowest possible cross-entropy is achieved when $P=Q$.\nIn this case, the cross-entropy from $P$ to $Q$ is $H(P, P)= H(P)$.\n\nIn short, we can think of the cross-entropy classification objective\nin two ways: (i) as maximizing the likelihood of the observed data;\nand (ii) as minimizing our surprisal (and thus the number of bits)\nrequired to communicate the labels."
    },
    {
      "chunk_id": "9e59900032f8_0",
      "chapter": "softmax-regression",
      "heading": "Summary and Discussion",
      "text": "In this section, we encountered the first nontrivial loss function,\nallowing us to optimize over *discrete* output spaces. Key in its design was that we took a probabilistic approach,\ntreating discrete categories as instances of draws from a probability distribution. As a side effect, we encountered the softmax,\na convenient activation function that transforms\noutputs of an ordinary neural network layer\ninto valid discrete probability distributions. We saw that the derivative of the cross-entropy loss\nwhen combined with softmax\nbehaves very similarly\nto the derivative of squared error;\nnamely by taking the difference between\nthe expected behavior and its prediction. And, while we were only able to\nscratch the very surface of it,\nwe encountered exciting connections\nto statistical physics and information theory. While this is enough to get you on your way,\nand hopefully enough to whet your appetite,\nwe hardly dived deep here. Among other things, we skipped over computational considerations. Specifically, for any fully connected layer with $d$ inputs and $q$ outputs,\nthe parametrization and computational cost is $\\mathcal{O}(dq)$,\nwhich can be prohibitively high in practice. Fortunately, this cost of transforming $d$ inputs into $q$ outputs\ncan be reduced through approximation and compression. For instance Deep Fried Convnets :cite:`Yang.Moczulski.Denil.ea.2015`\nuses a combination of permutations,\nFourier transforms, and scaling\nto reduce the cost from quadratic to log-linear. Similar techniques work for more advanced\nstructural matrix approximations :cite:`sindhwani2015structured`. Lastly, we can use quaternion-like decompositions\nto reduce the cost to $\\mathcal{O}(\\frac{dq}{n})$,\nagain if we are willing to trade off a small amount of accuracy\nfor computational and storage cost :cite:`Zhang.Tay.Zhang.ea.2021`\nbased on a compression factor $n$. This is an active area of research."
    },
    {
      "chunk_id": "9e59900032f8_1",
      "chapter": "softmax-regression",
      "heading": "Summary and Discussion",
      "text": "This is an active area of research. What makes it challenging is that\nwe do not necessarily strive\nfor the most compact representation\nor the smallest number of floating point operations\nbut rather for the solution\nthat can be executed most efficiently on modern GPUs."
    },
    {
      "chunk_id": "affc26290de5_0",
      "chapter": "softmax-regression",
      "heading": "Exercises",
      "text": "1. We can explore the connection between exponential families and softmax in some more depth. 1. Compute the second derivative of the cross-entropy loss $l(\\mathbf{y},\\hat{\\mathbf{y}})$ for softmax. 1. Compute the variance of the distribution given by $\\mathrm{softmax}(\\mathbf{o})$ and show that it matches the second derivative computed above. 1. Assume that we have three classes which occur with equal probability, i.e., the probability vector is $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$. 1. What is the problem if we try to design a binary code for it? 1. Can you design a better code? Hint: what happens if we try to encode two independent observations? What if we encode $n$ observations jointly? 1. When encoding signals transmitted over a physical wire, engineers do not always use binary codes. For instance, [PAM-3](https://en.wikipedia.org/wiki/Ternary_signal) uses three signal levels $\\{-1, 0, 1\\}$ as opposed to two levels $\\{0, 1\\}$. How many ternary units do you need to transmit an integer in the range $\\{0, \\ldots, 7\\}$? Why might this be a better idea in terms of electronics? 1. The [Bradley--Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) uses\na logistic model to capture preferences. For a user to choose between apples and oranges one\nassumes scores $o_{\\textrm{apple}}$ and $o_{\\textrm{orange}}$. Our requirements are that larger scores should lead to a higher likelihood in choosing the associated item and that\nthe item with the largest score is the most likely one to be chosen :cite:`Bradley.Terry.1952`. 1. Prove that softmax satisfies this requirement. 1. What happens if you want to allow for a default option of choosing neither apples nor oranges? Hint: now the user has three choices. 1. Softmax gets its name from the following mapping: $\\textrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))$. 1. Prove that $\\textrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$. 1. How small can you make the difference between both functions?"
    },
    {
      "chunk_id": "affc26290de5_1",
      "chapter": "softmax-regression",
      "heading": "Exercises",
      "text": "1. Softmax gets its name from the following mapping: $\\textrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))$. 1. Prove that $\\textrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$. 1. How small can you make the difference between both functions? Hint: without loss of\n    generality you can set $b = 0$ and $a \\geq b$. 1. Prove that this holds for $\\lambda^{-1} \\textrm{RealSoftMax}(\\lambda a, \\lambda b)$, provided that $\\lambda > 0$. 1. Show that for $\\lambda \\to \\infty$ we have $\\lambda^{-1} \\textrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$. 1. Construct an analogous softmin function. 1. Extend this to more than two numbers. 1. The function $g(\\mathbf{x}) \\stackrel{\\textrm{def}}{=} \\log \\sum_i \\exp x_i$ is sometimes also referred to as the [log-partition function](https://en.wikipedia.org/wiki/Partition_function_(mathematics)). 1. Prove that the function is convex. Hint: to do so, use the fact that the first derivative amounts to the probabilities from the softmax function and show that the second derivative is the variance. 1. Show that $g$ is translation invariant, i.e., $g(\\mathbf{x} + b) = g(\\mathbf{x})$. 1. What happens if some of the coordinates $x_i$ are very large? What happens if they're all very small? 1. Show that if we choose $b = \\mathrm{max}_i x_i$ we end up with a numerically stable implementation. 1. Assume that we have some probability distribution $P$. Suppose we pick another distribution $Q$ with $Q(i) \\propto P(i)^\\alpha$ for $\\alpha > 0$. 1. Which choice of $\\alpha$ corresponds to doubling the temperature? Which choice corresponds to halving it? 1. What happens if we let the temperature approach $0$? 1. What happens if we let the temperature approach $\\infty$? [Discussions](https://discuss.d2l.ai/t/46)"
    },
    {
      "chunk_id": "199e8a055b0c_0",
      "chapter": "generalization",
      "heading": "generalization",
      "text": "# Generalization\n:label:`sec_generalization_basics`\n\nConsider two college students diligently\npreparing for their final exam. Commonly, this preparation will consist\nof practicing and testing their abilities\nby taking exams administered in previous years. Nonetheless, doing well on past exams is no guarantee\nthat they will excel when it matters. For instance, imagine a student, Extraordinary Ellie,\nwhose preparation consisted entirely\nof memorizing the answers\nto previous years' exam questions. Even if Ellie were endowed\nwith an extraordinary memory,\nand thus could perfectly recall the answer\nto any *previously seen* question,\nshe might nevertheless freeze\nwhen faced with a new (*previously unseen*) question. By comparison, imagine another student,\nInductive Irene, with comparably poor\nmemorization skills,\nbut a knack for picking up patterns. Note that if the exam truly consisted of\nrecycled questions from a previous year,\nEllie would handily outperform Irene. Even if Irene's inferred patterns\nyielded 90% accurate predictions,\nthey could never compete with\nEllie's 100% recall. However, even if the exam consisted\nentirely of fresh questions,\nIrene might maintain her 90% average. As machine learning scientists,\nour goal is to discover *patterns*. But how can we be sure that we have\ntruly discovered a *general* pattern\nand not simply memorized our data? Most of the time, our predictions are only useful\nif our model discovers such a pattern. We do not want to predict yesterday's stock prices, but tomorrow's. We do not need to recognize\nalready diagnosed diseases\nfor previously seen patients,\nbut rather previously undiagnosed\nailments in previously unseen patients. This problem---how to discover patterns that *generalize*---is\nthe fundamental problem of machine learning,\nand arguably of all of statistics. We might cast this problem as just one slice\nof a far grander question\nthat engulfs all of science:\nwhen are we ever justified\nin making the leap from particular observations\nto more general statements?"
    },
    {
      "chunk_id": "199e8a055b0c_1",
      "chapter": "generalization",
      "heading": "generalization",
      "text": "We might cast this problem as just one slice\nof a far grander question\nthat engulfs all of science:\nwhen are we ever justified\nin making the leap from particular observations\nto more general statements? In real life, we must fit our models\nusing a finite collection of data. The typical scales of that data\nvary wildly across domains. For many important medical problems,\nwe can only access a few thousand data points. When studying rare diseases,\nwe might be lucky to access hundreds. By contrast, the largest public datasets\nconsisting of labeled photographs,\ne.g., ImageNet :cite:`Deng.Dong.Socher.ea.2009`,\ncontain millions of images. And some unlabeled image collections\nsuch as the Flickr YFC100M dataset\ncan be even larger, containing\nover 100 million images :cite:`thomee2016yfcc100m`. However, even at this extreme scale,\nthe number of available data points\nremains infinitesimally small\ncompared to the space of all possible images\nat a megapixel resolution. Whenever we work with finite samples,\nwe must keep in mind the risk\nthat we might fit our training data,\nonly to discover that we failed\nto discover a generalizable pattern. The phenomenon of fitting closer to our training data\nthan to the underlying distribution is called *overfitting*,\nand techniques for combatting overfitting\nare often called *regularization* methods. While it is no substitute for a proper introduction\nto statistical learning theory (see :citet:`Vapnik98,boucheron2005theory`),\nwe will give you just enough intuition to get going. We will revisit generalization in many chapters\nthroughout the book,\nexploring both what is known about\nthe principles underlying generalization\nin various models,\nand also heuristic techniques\nthat have been found (empirically)\nto yield improved generalization\non tasks of practical interest."
    },
    {
      "chunk_id": "aa08558f894e_0",
      "chapter": "generalization",
      "heading": "Training Error and Generalization Error",
      "text": "In the standard supervised learning setting,\nwe assume that the training data and the test data\nare drawn *independently* from *identical* distributions. This is commonly called the *IID assumption*. While this assumption is strong,\nit is worth noting that, absent any such assumption,\nwe would be dead in the water. Why should we believe that training data\nsampled from distribution $P(X,Y)$\nshould tell us how to make predictions on\ntest data generated by a *different distribution* $Q(X,Y)$? Making such leaps turns out to require\nstrong assumptions about how $P$ and $Q$ are related. Later on we will discuss some assumptions\nthat allow for shifts in distribution\nbut first we need to understand the IID case,\nwhere $P(\\cdot) = Q(\\cdot)$. To begin with, we need to differentiate between\nthe *training error* $R_\\textrm{emp}$,\nwhich is a *statistic*\ncalculated on the training dataset,\nand the *generalization error* $R$,\nwhich is an *expectation* taken\nwith respect to the underlying distribution. You can think of the generalization error as\nwhat you would see  if you applied your model\nto an infinite stream of additional data examples\ndrawn from the same underlying data distribution. Formally the training error is expressed as a *sum* (with the same notation as :numref:`sec_linear_regression`):\n\n$$R_\\textrm{emp}[\\mathbf{X}, \\mathbf{y}, f] = \\frac{1}{n} \\sum_{i=1}^n l(\\mathbf{x}^{(i)}, y^{(i)}, f(\\mathbf{x}^{(i)})),$$\n\n\nwhile the generalization error is expressed as an integral:\n\n$$R[p, f] = E_{(\\mathbf{x}, y) \\sim P} [l(\\mathbf{x}, y, f(\\mathbf{x}))] =\n\\int \\int l(\\mathbf{x}, y, f(\\mathbf{x})) p(\\mathbf{x}, y) \\;d\\mathbf{x} dy.$$\n\nProblematically, we can never calculate\nthe generalization error $R$ exactly. Nobody ever tells us the precise form\nof the density function $p(\\mathbf{x}, y)$. Moreover, we cannot sample an infinite stream of data points."
    },
    {
      "chunk_id": "aa08558f894e_1",
      "chapter": "generalization",
      "heading": "Training Error and Generalization Error",
      "text": "Nobody ever tells us the precise form\nof the density function $p(\\mathbf{x}, y)$. Moreover, we cannot sample an infinite stream of data points. Thus, in practice, we must *estimate* the generalization error\nby applying our model to an independent test set\nconstituted of a random selection of examples\n$\\mathbf{X}'$ and labels $\\mathbf{y}'$\nthat were withheld from our training set. This consists of applying the same formula\nthat was used for calculating the empirical training error\nbut to a test set $\\mathbf{X}', \\mathbf{y}'$. Crucially, when we evaluate our classifier on the test set,\nwe are working with a *fixed* classifier\n(it does not depend on the sample of the test set),\nand thus estimating its error\nis simply the problem of mean estimation. However the same cannot be said\nfor the training set. Note that the model we wind up with\ndepends explicitly on the selection of the training set\nand thus the training error will in general\nbe a biased estimate of the true error\non the underlying population. The central question of generalization\nis then when should we expect our training error\nto be close to the population error\n(and thus the generalization error)."
    },
    {
      "chunk_id": "1e552de70c22_0",
      "chapter": "generalization",
      "heading": "Model Complexity",
      "text": "In classical theory, when we have\nsimple models and abundant data,\nthe training and generalization errors tend to be close. However, when we work with\nmore complex models and/or fewer examples,\nwe expect the training error to go down\nbut the generalization gap to grow. This should not be surprising. Imagine a model class so expressive that\nfor any dataset of $n$ examples,\nwe can find a set of parameters\nthat can perfectly fit arbitrary labels,\neven if randomly assigned. In this case, even if we fit our training data perfectly,\nhow can we conclude anything about the generalization error? For all we know, our generalization error\nmight be no better than random guessing. In general, absent any restriction on our model class,\nwe cannot conclude, based on fitting the training data alone,\nthat our model has discovered any generalizable pattern :cite:`vapnik1994measuring`. On the other hand, if our model class\nwas not capable of fitting arbitrary labels,\nthen it must have discovered a pattern. Learning-theoretic ideas about model complexity\nderived some inspiration from the ideas\nof Karl Popper, an influential philosopher of science,\nwho formalized the criterion of falsifiability. According to Popper, a theory\nthat can explain any and all observations\nis not a scientific theory at all! After all, what has it told us about the world\nif it has not ruled out any possibility? In short, what we want is a hypothesis\nthat *could not* explain any observations\nwe might conceivably make\nand yet nevertheless happens to be compatible\nwith those observations that we *in fact* make. Now what precisely constitutes an appropriate\nnotion of model complexity is a complex matter. Often, models with more parameters\nare able to fit a greater number\nof arbitrarily assigned labels. However, this is not necessarily true. For instance, kernel methods operate in spaces\nwith infinite numbers of parameters,\nyet their complexity is controlled\nby other means :cite:`Scholkopf.Smola.2002`."
    },
    {
      "chunk_id": "1e552de70c22_1",
      "chapter": "generalization",
      "heading": "Model Complexity",
      "text": "However, this is not necessarily true. For instance, kernel methods operate in spaces\nwith infinite numbers of parameters,\nyet their complexity is controlled\nby other means :cite:`Scholkopf.Smola.2002`. One notion of complexity that often proves useful\nis the range of values that the parameters can take. Here, a model whose parameters are permitted\nto take arbitrary values\nwould be more complex. We will revisit this idea in the next section,\nwhen we introduce *weight decay*,\nyour first practical regularization technique. Notably, it can be difficult to compare\ncomplexity among members of substantially different model classes\n(say, decision trees vs. neural networks). At this point, we must stress another important point\nthat we will revisit when introducing deep neural networks. When a model is capable of fitting arbitrary labels,\nlow training error does not necessarily\nimply low generalization error. *However, it does not necessarily\nimply high generalization error either!*\nAll we can say with confidence is that\nlow training error alone is not enough\nto certify low generalization error. Deep neural networks turn out to be just such models:\nwhile they generalize well in practice,\nthey are too powerful to allow us to conclude\nmuch on the basis of training error alone. In these cases we must rely more heavily\non our holdout data to certify generalization\nafter the fact. Error on the holdout data, i.e., validation set,\nis called the *validation error*."
    },
    {
      "chunk_id": "e1a2cf218c7e_0",
      "chapter": "generalization",
      "heading": "Underfitting or Overfitting?",
      "text": "When we compare the training and validation errors,\nwe want to be mindful of two common situations.\nFirst, we want to watch out for cases\nwhen our training error and validation error are both substantial\nbut there is a little gap between them.\nIf the model is unable to reduce the training error,\nthat could mean that our model is too simple\n(i.e., insufficiently expressive)\nto capture the pattern that we are trying to model.\nMoreover, since the *generalization gap* ($R_\\textrm{emp} - R$)\nbetween our training and generalization errors is small,\nwe have reason to believe that we could get away with a more complex model.\nThis phenomenon is known as *underfitting*.\n\nOn the other hand, as we discussed above,\nwe want to watch out for the cases\nwhen our training error is significantly lower\nthan our validation error, indicating severe *overfitting*.\nNote that overfitting is not always a bad thing.\nIn deep learning especially,\nthe best predictive models often perform\nfar better on training data than on holdout data.\nUltimately, we usually care about\ndriving the generalization error lower,\nand only care about the gap insofar\nas it becomes an obstacle to that end.\nNote that if the training error is zero,\nthen the generalization gap is precisely equal to the generalization error\nand we can make progress only by reducing the gap."
    },
    {
      "chunk_id": "73c2baa69c3a_0",
      "chapter": "generalization",
      "heading": "Polynomial Curve Fitting",
      "text": ":label:`subsec_polynomial-curve-fitting`\n\nTo illustrate some classical intuition\nabout overfitting and model complexity,\nconsider the following:\ngiven training data consisting of a single feature $x$\nand a corresponding real-valued label $y$,\nwe try to find the polynomial of degree $d$\n\n$$\\hat{y}= \\sum_{i=0}^d x^i w_i$$\n\nfor estimating the label $y$.\nThis is just a linear regression problem\nwhere our features are given by the powers of $x$,\nthe model's weights are given by $w_i$,\nand the bias is given by $w_0$ since $x^0 = 1$ for all $x$.\nSince this is just a linear regression problem,\nwe can use the squared error as our loss function.\n\n\nA higher-order polynomial function is more complex\nthan a lower-order polynomial function,\nsince the higher-order polynomial has more parameters\nand the model function's selection range is wider.\nFixing the training dataset,\nhigher-order polynomial functions should always\nachieve lower (at worst, equal) training error\nrelative to lower-degree polynomials.\nIn fact, whenever each data example\nhas a distinct value of $x$,\na polynomial function with degree\nequal to the number of data examples\ncan fit the training set perfectly.\nWe compare the relationship between polynomial degree (model complexity)\nand both underfitting and overfitting in :numref:`fig_capacity_vs_error`.\n\n![Influence of model complexity on underfitting and overfitting.](../img/capacity-vs-error.svg)\n:label:`fig_capacity_vs_error`"
    },
    {
      "chunk_id": "af15245a9d50_0",
      "chapter": "generalization",
      "heading": "Dataset Size",
      "text": "As the above bound already indicates,\nanother big consideration\nto bear in mind is dataset size.\nFixing our model, the fewer samples\nwe have in the training dataset,\nthe more likely (and more severely)\nwe are to encounter overfitting.\nAs we increase the amount of training data,\nthe generalization error typically decreases.\nMoreover, in general, more data never hurts.\nFor a fixed task and data distribution,\nmodel complexity should not increase\nmore rapidly than the amount of data.\nGiven more data, we might  attempt\nto fit a more complex model.\nAbsent sufficient data, simpler models\nmay be more difficult to beat.\nFor many tasks, deep learning\nonly outperforms linear models\nwhen many thousands of training examples are available.\nIn part, the current success of deep learning\nowes considerably to the abundance of massive datasets\narising from Internet companies, cheap storage,\nconnected devices, and the broad digitization of the economy."
    },
    {
      "chunk_id": "d22c779050a2_0",
      "chapter": "generalization",
      "heading": "Model Selection",
      "text": ":label:`subsec_generalization-model-selection`\n\nTypically, we select our final model\nonly after evaluating multiple models\nthat differ in various ways\n(different architectures, training objectives,\nselected features, data preprocessing,\nlearning rates, etc.). Choosing among many models is aptly\ncalled *model selection*. In principle, we should not touch our test set\nuntil after we have chosen all our hyperparameters. Were we to use the test data in the model selection process,\nthere is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data,\nthere is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know? See :citet:`ong2005learning` for an example of how\nthis can lead to absurd results even for models where the complexity\ncan be tightly controlled. Thus, we should never rely on the test data for model selection. And yet we cannot rely solely on the training data\nfor model selection either because\nwe cannot estimate the generalization error\non the very data that we use to train the model. In practical applications, the picture gets muddier. While ideally we would only touch the test data once,\nto assess the very best model or to compare\na small number of models with each other,\nreal-world test data is seldom discarded after just one use. We can seldom afford a new test set for each round of experiments. In fact, recycling benchmark data for decades\ncan have a significant impact on the\ndevelopment of algorithms,\ne.g., for [image classification](https://paperswithcode.com/sota/image-classification-on-imagenet)\nand [optical character recognition](https://paperswithcode.com/sota/image-classification-on-mnist). The common practice for addressing the problem of *training on the test set*\nis to split our data three ways,\nincorporating a *validation set*\nin addition to the training and test datasets. The result is a murky business where the boundaries\nbetween validation and test data are worryingly ambiguous."
    },
    {
      "chunk_id": "d22c779050a2_1",
      "chapter": "generalization",
      "heading": "Model Selection",
      "text": "The result is a murky business where the boundaries\nbetween validation and test data are worryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book\nwe are really working with what should rightly be called\ntraining data and validation data, with no true test sets. Therefore, the accuracy reported in each experiment of the book is really\nthe validation accuracy and not a true test set accuracy."
    },
    {
      "chunk_id": "00caf0e35edf_0",
      "chapter": "generalization",
      "heading": "Cross-Validation",
      "text": "When training data is scarce,\nwe might not even be able to afford to hold out\nenough data to constitute a proper validation set.\nOne popular solution to this problem is to employ\n$K$*-fold cross-validation*.\nHere, the original training data is split into $K$ non-overlapping subsets.\nThen model training and validation are executed $K$ times,\neach time training on $K-1$ subsets and validating\non a different subset (the one not used for training in that round).\nFinally, the training and validation errors are estimated\nby averaging over the results from the $K$ experiments."
    },
    {
      "chunk_id": "1ac02a2fbddb_0",
      "chapter": "generalization",
      "heading": "Summary",
      "text": "This section explored some of the  underpinnings\nof generalization in  machine learning.\nSome of these ideas become complicated\nand counterintuitive when we get to deeper models; here, models are capable of overfitting data badly,\nand the relevant notions of complexity\ncan be both implicit and counterintuitive\n(e.g., larger architectures with more parameters\ngeneralizing better).\nWe leave you with a few rules of thumb:\n\n1. Use validation sets (or $K$*-fold cross-validation*) for model selection;\n1. More complex models often require more data;\n1. Relevant notions of complexity include both the number of parameters and the range of values that they are allowed to take;\n1. Keeping all else equal, more data almost always leads to better generalization;\n1. This entire talk of generalization is all predicated on the IID assumption. If we relax this assumption, allowing for distributions to shift between the train and testing periods, then we cannot say anything about generalization absent a further (perhaps milder) assumption."
    },
    {
      "chunk_id": "6de836f93c5a_0",
      "chapter": "generalization",
      "heading": "Exercises",
      "text": "1. When can you solve the problem of polynomial regression exactly?\n1. Give at least five examples where dependent random variables make treating the problem as IID data inadvisable.\n1. Can you ever expect to see zero training error? Under which circumstances would you see zero generalization error?\n1. Why is $K$-fold cross-validation very expensive to compute?\n1. Why is the $K$-fold cross-validation error estimate biased?\n1. The VC dimension is defined as the maximum number of points that can be classified with arbitrary labels $\\{\\pm 1\\}$ by a function of a class of functions. Why might this not be a good idea for measuring how complex the class of functions is? Hint: consider the magnitude of the functions.\n1. Your manager gives you a difficult dataset on which your current algorithm does not perform so well. How would you justify to him that you need more data? Hint: you cannot increase the data but you can decrease it.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/96)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/97)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/234)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17978)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Linear Neural Networks for Regression\n:label:`chap_regression`\n\nBefore we worry about making our neural networks deep,\nit will be helpful to implement some shallow ones,\nfor which the inputs connect directly to the outputs.\nThis will prove important for a few reasons.\nFirst, rather than getting distracted by complicated architectures,\nwe can focus on the basics of neural network training,\nincluding parametrizing the output layer, handling data,\nspecifying a loss function, and training the model.\nSecond, this class of shallow networks happens\nto comprise the set of linear models,\nwhich subsumes many classical methods of statistical prediction,\nincluding linear and softmax regression.\nUnderstanding these classical tools is pivotal\nbecause they are widely used in many contexts\nand we will often need to use them as baselines\nwhen justifying the use of fancier architectures.\nThis chapter will focus narrowly on linear regression\nand the next one will extend our modeling repertoire\nby developing linear neural networks for classification.\n\n```toc\n:maxdepth: 2\n\nlinear-regression\noo-design\nsynthetic-regression-data\nlinear-regression-scratch\nlinear-regression-concise\ngeneralization\nweight-decay\n```"
    },
    {
      "chunk_id": "1a892cec9041_0",
      "chapter": "linear-regression-concise",
      "heading": "linear-regression-concise",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Concise Implementation of Linear Regression\n:label:`sec_linear_concise`\n\nDeep learning has witnessed a sort of Cambrian explosion\nover the past decade. The sheer number of techniques, applications and algorithms by far surpasses the\nprogress of previous decades. This is due to a fortuitous combination of multiple factors,\none of which is the powerful free tools\noffered by a number of open-source deep learning frameworks. Theano :cite:`Bergstra.Breuleux.Bastien.ea.2010`,\nDistBelief :cite:`Dean.Corrado.Monga.ea.2012`,\nand Caffe :cite:`Jia.Shelhamer.Donahue.ea.2014`\narguably represent the\nfirst generation of such models \nthat found widespread adoption. In contrast to earlier (seminal) works like\nSN2 (Simulateur Neuristique) :cite:`Bottou.Le-Cun.1988`,\nwhich provided a Lisp-like programming experience,\nmodern frameworks offer automatic differentiation\nand the convenience of Python. These frameworks allow us to automate and modularize\nthe repetitive work of implementing gradient-based learning algorithms. In :numref:`sec_linear_scratch`, we relied only on\n(i) tensors for data storage and linear algebra;\nand (ii) automatic differentiation for calculating gradients. In practice, because data iterators, loss functions, optimizers,\nand neural network layers\nare so common, modern libraries implement these components for us as well. In this section, (**we will show you how to implement\nthe linear regression model**) from :numref:`sec_linear_scratch`\n(**concisely by using high-level APIs**) of deep learning frameworks."
    },
    {
      "chunk_id": "1a892cec9041_1",
      "chapter": "linear-regression-concise",
      "heading": "linear-regression-concise",
      "text": "In this section, (**we will show you how to implement\nthe linear regression model**) from :numref:`sec_linear_scratch`\n(**concisely by using high-level APIs**) of deep learning frameworks. ```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport numpy as np\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport numpy as np\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\nimport optax\n```"
    },
    {
      "chunk_id": "357cac0dd236_0",
      "chapter": "linear-regression-concise",
      "heading": "Defining the Model",
      "text": "When we implemented linear regression from scratch\nin :numref:`sec_linear_scratch`,\nwe defined our model parameters explicitly\nand coded up the calculations to produce output\nusing basic linear algebra operations. You *should* know how to do this. But once your models get more complex,\nand once you have to do this nearly every day,\nyou will be glad of the assistance. The situation is similar to coding up your own blog from scratch. Doing it once or twice is rewarding and instructive,\nbut you would be a lousy web developer\nif you spent a month reinventing the wheel. For standard operations,\nwe can [**use a framework's predefined layers,**]\nwhich allow us to focus\non the layers used to construct the model\nrather than worrying about their implementation. Recall the architecture of a single-layer network\nas described in :numref:`fig_single_neuron`. The layer is called *fully connected*,\nsince each of its inputs is connected\nto each of its outputs\nby means of a matrix--vector multiplication. :begin_tab:`mxnet`\nIn Gluon, the fully connected layer is defined in the `Dense` class. Since we only want to generate a single scalar output,\nwe set that number to 1. It is worth noting that, for convenience,\nGluon does not require us to specify\nthe input shape for each layer. Hence we do not need to tell Gluon\nhow many inputs go into this linear layer. When we first pass data through our model,\ne.g., when we execute `net(X)` later,\nGluon will automatically infer the number of inputs to each layer and\nthus instantiate the correct model. We will describe how this works in more detail later. :end_tab:\n\n:begin_tab:`pytorch`\nIn PyTorch, the fully connected layer is defined in `Linear` and `LazyLinear` classes (available since version 1.8.0). The latter\nallows users to specify *merely*\nthe output dimension,\nwhile the former\nadditionally asks for\nhow many inputs go into this layer. Specifying input shapes is inconvenient and may require nontrivial calculations\n(such as in convolutional layers)."
    },
    {
      "chunk_id": "357cac0dd236_1",
      "chapter": "linear-regression-concise",
      "heading": "Defining the Model",
      "text": "The latter\nallows users to specify *merely*\nthe output dimension,\nwhile the former\nadditionally asks for\nhow many inputs go into this layer. Specifying input shapes is inconvenient and may require nontrivial calculations\n(such as in convolutional layers). Thus, for simplicity, we will use such \"lazy\" layers\nwhenever we can. :end_tab:\n\n:begin_tab:`tensorflow`\nIn Keras, the fully connected layer is defined in the `Dense` class. Since we only want to generate a single scalar output,\nwe set that number to 1. It is worth noting that, for convenience,\nKeras does not require us to specify\nthe input shape for each layer. We do not need to tell Keras\nhow many inputs go into this linear layer. When we first try to pass data through our model,\ne.g., when we execute `net(X)` later,\nKeras will automatically infer\nthe number of inputs to each layer. We will describe how this works in more detail later. :end_tab:\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass LinearRegression(d2l.Module):  #@save\n    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n    def __init__(self, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.net = nn.Dense(1)\n            self.net.initialize(init.Normal(sigma=0.01))\n        if tab.selected('tensorflow'):\n            initializer = tf.initializers.RandomNormal(stddev=0.01)\n            self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)\n        if tab.selected('pytorch'):\n            self.net = nn.LazyLinear(1)\n            self.net.weight.data.normal_(0, 0.01)\n            self.net.bias.data.fill_(0)\n```\n\n```{.python .input}\n%%tab jax\nclass LinearRegression(d2l.Module):  #@save\n    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n    lr: float\n\n    def setup(self):\n        self.net = nn.Dense(1, kernel_init=nn.initializers.normal(0.01))\n```\n\nIn the `forward` method we just invoke the built-in `__call__` method of the predefined layers to compute the outputs."
    },
    {
      "chunk_id": "357cac0dd236_2",
      "chapter": "linear-regression-concise",
      "heading": "Defining the Model",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(LinearRegression)  #@save\ndef forward(self, X):\n    return self.net(X)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(LinearRegression)  #@save\ndef forward(self, X):\n    return self.net(X)\n```"
    },
    {
      "chunk_id": "8fb2915e00f9_0",
      "chapter": "linear-regression-concise",
      "heading": "Defining the Loss Function",
      "text": ":begin_tab:`mxnet`\nThe `loss` module defines many useful loss functions.\nFor speed and convenience, we forgo implementing our own\nand choose the built-in `loss.L2Loss` instead.\nBecause the `loss` that it returns is\nthe squared error for each example,\nwe use `mean`to average the loss across over the minibatch.\n:end_tab:\n\n:begin_tab:`pytorch`\n[**The `MSELoss` class computes the mean squared error (without the $1/2$ factor in :eqref:`eq_mse`).**]\nBy default, `MSELoss` returns the average loss over examples.\nIt is faster (and easier to use) than implementing our own.\n:end_tab:\n\n:begin_tab:`tensorflow`\nThe `MeanSquaredError` class computes the mean squared error (without the $1/2$ factor in :eqref:`eq_mse`).\nBy default, it returns the average loss over examples.\n:end_tab:\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(LinearRegression)  #@save\ndef loss(self, y_hat, y):\n    if tab.selected('mxnet'):\n        fn = gluon.loss.L2Loss()\n        return fn(y_hat, y).mean()\n    if tab.selected('pytorch'):\n        fn = nn.MSELoss()\n        return fn(y_hat, y)\n    if tab.selected('tensorflow'):\n        fn = tf.keras.losses.MeanSquaredError()\n        return fn(y, y_hat)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(LinearRegression)  #@save\ndef loss(self, params, X, y, state):\n    y_hat = state.apply_fn({'params': params}, *X)\n    return d2l.reduce_mean(optax.l2_loss(y_hat, y))\n```"
    },
    {
      "chunk_id": "917b58c8599e_0",
      "chapter": "linear-regression-concise",
      "heading": "Defining the Optimization Algorithm",
      "text": ":begin_tab:`mxnet`\nMinibatch SGD is a standard tool\nfor optimizing neural networks\nand thus Gluon supports it alongside a number of\nvariations on this algorithm through its `Trainer` class.\nNote that Gluon's `Trainer` class stands\nfor the optimization algorithm,\nwhile the `Trainer` class we created in :numref:`sec_oo-design`\ncontains the training method,\ni.e., repeatedly call the optimizer\nto update the model parameters.\nWhen we instantiate `Trainer`,\nwe specify the parameters to optimize over,\nobtainable from our model `net` via `net.collect_params()`,\nthe optimization algorithm we wish to use (`sgd`),\nand a dictionary of hyperparameters\nrequired by our optimization algorithm.\n:end_tab:\n\n:begin_tab:`pytorch`\nMinibatch SGD is a standard tool\nfor optimizing neural networks\nand thus PyTorch supports it alongside a number of\nvariations on this algorithm in the `optim` module.\nWhen we (**instantiate an `SGD` instance,**)\nwe specify the parameters to optimize over,\nobtainable from our model via `self.parameters()`,\nand the learning rate (`self.lr`)\nrequired by our optimization algorithm.\n:end_tab:\n\n:begin_tab:`tensorflow`\nMinibatch SGD is a standard tool\nfor optimizing neural networks\nand thus Keras supports it alongside a number of\nvariations on this algorithm in the `optimizers` module.\n:end_tab:\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(LinearRegression)  #@save\ndef configure_optimizers(self):\n    if tab.selected('mxnet'):\n        return gluon.Trainer(self.collect_params(),\n                             'sgd', {'learning_rate': self.lr})\n    if tab.selected('pytorch'):\n        return torch.optim.SGD(self.parameters(), self.lr)\n    if tab.selected('tensorflow'):\n        return tf.keras.optimizers.SGD(self.lr)\n    if tab.selected('jax'):\n        return optax.sgd(self.lr)\n```"
    },
    {
      "chunk_id": "652d6f1fbbf3_0",
      "chapter": "linear-regression-concise",
      "heading": "Training",
      "text": "You might have noticed that expressing our model through\nhigh-level APIs of a deep learning framework\nrequires fewer lines of code.\nWe did not have to allocate parameters individually,\ndefine our loss function, or implement minibatch SGD.\nOnce we start working with much more complex models,\nthe advantages of the high-level API will grow considerably.\n\nNow that we have all the basic pieces in place,\n[**the training loop itself is the same\nas the one we implemented from scratch.**]\nSo we just call the `fit` method (introduced in :numref:`oo-design-training`),\nwhich relies on the implementation of the `fit_epoch` method\nin :numref:`sec_linear_scratch`,\nto train our model.\n\n```{.python .input}\n%%tab all\nmodel = LinearRegression(lr=0.03)\ndata = d2l.SyntheticRegressionData(w=d2l.tensor([2, -3.4]), b=4.2)\ntrainer = d2l.Trainer(max_epochs=3)\ntrainer.fit(model, data)\n```\n\nBelow, we\n[**compare the model parameters learned\nby training on finite data\nand the actual parameters**]\nthat generated our dataset.\nTo access parameters,\nwe access the weights and bias\nof the layer that we need.\nAs in our implementation from scratch,\nnote that our estimated parameters\nare close to their true counterparts.\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(LinearRegression)  #@save\ndef get_w_b(self):\n    if tab.selected('mxnet'):\n        return (self.net.weight.data(), self.net.bias.data())\n    if tab.selected('pytorch'):\n        return (self.net.weight.data, self.net.bias.data)\n    if tab.selected('tensorflow'):\n        return (self.get_weights()[0], self.get_weights()[1])\n\nw, b = model.get_w_b()\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(LinearRegression)  #@save\ndef get_w_b(self, state):\n    net = state.params['net']\n    return net['kernel'], net['bias']\n\nw, b = model.get_w_b(trainer.state)\n```\n\n```{.python .input}\nprint(f'error in estimating w: {data.w - d2l.reshape(w, data.w.shape)}')\nprint(f'error in estimating b: {data.b - b}')\n```"
    },
    {
      "chunk_id": "f6ff72c667fd_0",
      "chapter": "linear-regression-concise",
      "heading": "Summary",
      "text": "This section contains the first\nimplementation of a deep network (in this book)\nto tap into the conveniences afforded\nby modern deep learning frameworks,\nsuch as MXNet :cite:`Chen.Li.Li.ea.2015`, \nJAX :cite:`Frostig.Johnson.Leary.2018`, \nPyTorch :cite:`Paszke.Gross.Massa.ea.2019`, \nand Tensorflow :cite:`Abadi.Barham.Chen.ea.2016`. We used framework defaults for loading data, defining a layer,\na loss function, an optimizer and a training loop. Whenever the framework provides all necessary features,\nit is generally a good idea to use them,\nsince the library implementations of these components\ntend to be heavily optimized for performance\nand properly tested for reliability. At the same time, try not to forget\nthat these modules *can* be implemented directly. This is especially important for aspiring researchers\nwho wish to live on the leading edge of model development,\nwhere you will be inventing new components\nthat cannot possibly exist in any current library. :begin_tab:`mxnet`\nIn Gluon, the `data` module provides tools for data processing,\nthe `nn` module defines a large number of neural network layers,\nand the `loss` module defines many common loss functions. Moreover, the `initializer` gives access\nto many choices for parameter initialization. Conveniently for the user,\ndimensionality and storage are automatically inferred. A consequence of this lazy initialization is that\nyou must not attempt to access parameters\nbefore they have been instantiated (and initialized). :end_tab:\n\n:begin_tab:`pytorch`\nIn PyTorch, the `data` module provides tools for data processing,\nthe `nn` module defines a large number of neural network layers and common loss functions. We can initialize the parameters by replacing their values\nwith methods ending with `_`. Note that we need to specify the input dimensions of the network. While this is trivial for now, it can have significant knock-on effects\nwhen we want to design complex networks with many layers."
    },
    {
      "chunk_id": "f6ff72c667fd_1",
      "chapter": "linear-regression-concise",
      "heading": "Summary",
      "text": "Note that we need to specify the input dimensions of the network. While this is trivial for now, it can have significant knock-on effects\nwhen we want to design complex networks with many layers. Careful considerations of how to parametrize these networks\nis needed to allow portability. :end_tab:\n\n:begin_tab:`tensorflow`\nIn TensorFlow, the `data` module provides tools for data processing,\nthe `keras` module defines a large number of neural network layers and common loss functions. Moreover, the `initializers` module provides various methods for model parameter initialization. Dimensionality and storage for networks are automatically inferred\n(but be careful not to attempt to access parameters before they have been initialized). :end_tab:"
    },
    {
      "chunk_id": "294db797e580_0",
      "chapter": "linear-regression-concise",
      "heading": "Exercises",
      "text": "1. How would you need to change the learning rate if you replace the aggregate loss over the minibatch\n   with an average over the loss on the minibatch?\n1. Review the framework documentation to see which loss functions are provided. In particular,\n   replace the squared loss with Huber's robust loss function. That is, use the loss function\n   $$l(y,y') = \\begin{cases}|y-y'| -\\frac{\\sigma}{2} & \\textrm{ if } |y-y'| > \\sigma \\\\ \\frac{1}{2 \\sigma} (y-y')^2 & \\textrm{ otherwise}\\end{cases}$$\n1. How do you access the gradient of the weights of the model?\n1. What is the effect on the solution if you change the learning rate and the number of epochs? Does it keep on improving?\n1. How does the solution change as you vary the amount of data generated?\n    1. Plot the estimation error for $\\hat{\\mathbf{w}} - \\mathbf{w}$ and $\\hat{b} - b$ as a function of the amount of data. Hint: increase the amount of data logarithmically rather than linearly, i.e., 5, 10, 20, 50, ..., 10,000 rather than 1000, 2000, ..., 10,000.\n    2. Why is the suggestion in the hint appropriate?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/44)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/45)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/204)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17977)\n:end_tab:"
    },
    {
      "chunk_id": "b9085872d4a4_0",
      "chapter": "linear-regression-scratch",
      "heading": "linear-regression-scratch",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Linear Regression Implementation from Scratch\n:label:`sec_linear_scratch`\n\nWe are now ready to work through \na fully functioning implementation \nof linear regression. \nIn this section, \n(**we will implement the entire method from scratch,\nincluding (i) the model; (ii) the loss function;\n(iii) a minibatch stochastic gradient descent optimizer;\nand (iv) the training function \nthat stitches all of these pieces together.**)\nFinally, we will run our synthetic data generator\nfrom :numref:`sec_synthetic-regression-data`\nand apply our model\non the resulting dataset. \nWhile modern deep learning frameworks \ncan automate nearly all of this work,\nimplementing things from scratch is the only way\nto make sure that you really know what you are doing.\nMoreover, when it is time to customize models,\ndefining our own layers or loss functions,\nunderstanding how things work under the hood will prove handy.\nIn this section, we will rely only \non tensors and automatic differentiation.\nLater, we will introduce a more concise implementation,\ntaking advantage of the bells and whistles of deep learning frameworks \nwhile retaining the structure of what follows below.\n\n```{.python .input  n=2}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx\nnpx.set_np()\n```\n\n```{.python .input  n=3}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input  n=4}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input  n=5}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\nimport optax\n```"
    },
    {
      "chunk_id": "2ab2be1f6462_0",
      "chapter": "linear-regression-scratch",
      "heading": "Defining the Model",
      "text": "[**Before we can begin optimizing our model's parameters**] by minibatch SGD,\n(**we need to have some parameters in the first place.**)\nIn the following we initialize weights by drawing\nrandom numbers from a normal distribution with mean 0\nand a standard deviation of 0.01. The magic number 0.01 often works well in practice, \nbut you can specify a different value \nthrough the argument `sigma`. Moreover we set the bias to 0. Note that for object-oriented design\nwe add the code to the `__init__` method of a subclass of `d2l.Module` (introduced in :numref:`subsec_oo-design-models`)."
    },
    {
      "chunk_id": "2ab2be1f6462_1",
      "chapter": "linear-regression-scratch",
      "heading": "Defining the Model",
      "text": "Moreover we set the bias to 0. Note that for object-oriented design\nwe add the code to the `__init__` method of a subclass of `d2l.Module` (introduced in :numref:`subsec_oo-design-models`). ```{.python .input  n=6}\n%%tab pytorch, mxnet, tensorflow\nclass LinearRegressionScratch(d2l.Module):  #@save\n    \"\"\"The linear regression model implemented from scratch.\"\"\"\n    def __init__(self, num_inputs, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.w = d2l.normal(0, sigma, (num_inputs, 1))\n            self.b = d2l.zeros(1)\n            self.w.attach_grad()\n            self.b.attach_grad()\n        if tab.selected('pytorch'):\n            self.w = d2l.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n            self.b = d2l.zeros(1, requires_grad=True)\n        if tab.selected('tensorflow'):\n            w = tf.random.normal((num_inputs, 1), mean=0, stddev=0.01)\n            b = tf.zeros(1)\n            self.w = tf.Variable(w, trainable=True)\n            self.b = tf.Variable(b, trainable=True)\n```\n\n```{.python .input  n=7}\n%%tab jax\nclass LinearRegressionScratch(d2l.Module):  #@save\n    \"\"\"The linear regression model implemented from scratch.\"\"\"\n    num_inputs: int\n    lr: float\n    sigma: float = 0.01\n\n    def setup(self):\n        self.w = self.param('w', nn.initializers.normal(self.sigma),\n                            (self.num_inputs, 1))\n        self.b = self.param('b', nn.initializers.zeros, (1))\n```\n\nNext we must [**define our model,\nrelating its input and parameters to its output.**]\nUsing the same notation as :eqref:`eq_linreg-y-vec`\nfor our linear model we simply take the matrix--vector product\nof the input features $\\mathbf{X}$ \nand the model weights $\\mathbf{w}$,\nand add the offset $b$ to each example. The product $\\mathbf{Xw}$ is a vector and $b$ is a scalar. Because of the broadcasting mechanism \n(see :numref:`subsec_broadcasting`),\nwhen we add a vector and a scalar,\nthe scalar is added to each component of the vector."
    },
    {
      "chunk_id": "2ab2be1f6462_2",
      "chapter": "linear-regression-scratch",
      "heading": "Defining the Model",
      "text": "The product $\\mathbf{Xw}$ is a vector and $b$ is a scalar. Because of the broadcasting mechanism \n(see :numref:`subsec_broadcasting`),\nwhen we add a vector and a scalar,\nthe scalar is added to each component of the vector. The resulting `forward` method \nis registered in the `LinearRegressionScratch` class\nvia `add_to_class` (introduced in :numref:`oo-design-utilities`). ```{.python .input  n=8}\n%%tab all\n@d2l.add_to_class(LinearRegressionScratch)  #@save\ndef forward(self, X):\n    return d2l.matmul(X, self.w) + self.b\n```"
    },
    {
      "chunk_id": "91f5cafb15ea_0",
      "chapter": "linear-regression-scratch",
      "heading": "Defining the Loss Function",
      "text": "Since [**updating our model requires taking\nthe gradient of our loss function,**]\nwe ought to (**define the loss function first.**)\nHere we use the squared loss function\nin :eqref:`eq_mse`.\nIn the implementation, we need to transform the true value `y`\ninto the predicted value's shape `y_hat`.\nThe result returned by the following method\nwill also have the same shape as `y_hat`. \nWe also return the averaged loss value\namong all examples in the minibatch.\n\n```{.python .input  n=9}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(LinearRegressionScratch)  #@save\ndef loss(self, y_hat, y):\n    l = (y_hat - y) ** 2 / 2\n    return d2l.reduce_mean(l)\n```\n\n```{.python .input  n=10}\n%%tab jax\n@d2l.add_to_class(LinearRegressionScratch)  #@save\ndef loss(self, params, X, y, state):\n    y_hat = state.apply_fn({'params': params}, *X)  # X unpacked from a tuple\n    l = (y_hat - d2l.reshape(y, y_hat.shape)) ** 2 / 2\n    return d2l.reduce_mean(l)\n```"
    },
    {
      "chunk_id": "4d04406b1a23_0",
      "chapter": "linear-regression-scratch",
      "heading": "Defining the Optimization Algorithm",
      "text": "As discussed in :numref:`sec_linear_regression`,\nlinear regression has a closed-form solution. However, our goal here is to illustrate \nhow to train more general neural networks,\nand that requires that we teach you \nhow to use minibatch SGD. Hence we will take this opportunity\nto introduce your first working example of SGD. At each step, using a minibatch \nrandomly drawn from our dataset,\nwe estimate the gradient of the loss\nwith respect to the parameters. Next, we update the parameters\nin the direction that may reduce the loss. The following code applies the update, \ngiven a set of parameters, a learning rate `lr`. Since our loss is computed as an average over the minibatch, \nwe do not need to adjust the learning rate against the batch size. In later chapters we will investigate \nhow learning rates should be adjusted\nfor very large minibatches as they arise \nin distributed large-scale learning. For now, we can ignore this dependency. :begin_tab:`mxnet`\nWe define our `SGD` class, \na subclass of `d2l.HyperParameters` (introduced in :numref:`oo-design-utilities`),\nto have a similar API\nas the built-in SGD optimizer. We update the parameters in the `step` method. It accepts a `batch_size` argument that can be ignored. :end_tab:\n\n:begin_tab:`pytorch`\nWe define our `SGD` class,\na subclass of `d2l.HyperParameters` (introduced in :numref:`oo-design-utilities`),\nto have a similar API \nas the built-in SGD optimizer. We update the parameters in the `step` method. The `zero_grad` method sets all gradients to 0,\nwhich must be run before a backpropagation step. :end_tab:\n\n:begin_tab:`tensorflow`\nWe define our `SGD` class,\na subclass of `d2l.HyperParameters` (introduced in :numref:`oo-design-utilities`),\nto have a similar API\nas the built-in SGD optimizer. We update the parameters in the `apply_gradients` method. It accepts a list of parameter and gradient pairs."
    },
    {
      "chunk_id": "4d04406b1a23_1",
      "chapter": "linear-regression-scratch",
      "heading": "Defining the Optimization Algorithm",
      "text": "We update the parameters in the `apply_gradients` method. It accepts a list of parameter and gradient pairs. :end_tab:\n\n```{.python .input  n=11}\n%%tab mxnet, pytorch\nclass SGD(d2l.HyperParameters):  #@save\n    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n    def __init__(self, params, lr):\n        self.save_hyperparameters()\n\n    if tab.selected('mxnet'):\n        def step(self, _):\n            for param in self.params:\n                param -= self.lr * param.grad\n\n    if tab.selected('pytorch'):\n        def step(self):\n            for param in self.params:\n                param -= self.lr * param.grad\n\n        def zero_grad(self):\n            for param in self.params:\n                if param.grad is not None:\n                    param.grad.zero_()\n```\n\n```{.python .input  n=12}\n%%tab tensorflow\nclass SGD(d2l.HyperParameters):  #@save\n    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n    def __init__(self, lr):\n        self.save_hyperparameters()\n\n    def apply_gradients(self, grads_and_vars):\n        for grad, param in grads_and_vars:\n            param.assign_sub(self.lr * grad)\n```\n\n```{.python .input  n=13}\n%%tab jax\nclass SGD(d2l.HyperParameters):  #@save\n    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n    # The key transformation of Optax is the GradientTransformation\n    # defined by two methods, the init and the update. # The init initializes the state and the update transforms the gradients. # https://github.com/deepmind/optax/blob/master/optax/_src/transform.py\n    def __init__(self, lr):\n        self.save_hyperparameters()\n\n    def init(self, params):\n        # Delete unused params\n        del params\n        return optax.EmptyState\n\n    def update(self, updates, state, params=None):\n        del params\n        # When state.apply_gradients method is called to update flax's\n        # train_state object, it internally calls optax.apply_updates method\n        # adding the params to the update equation defined below."
    },
    {
      "chunk_id": "4d04406b1a23_2",
      "chapter": "linear-regression-scratch",
      "heading": "Defining the Optimization Algorithm",
      "text": "updates = jax.tree_util.tree_map(lambda g: -self.lr * g, updates)\n        return updates, state\n\n    def __call__():\n        return optax.GradientTransformation(self.init, self.update)\n```\n\nWe next define the `configure_optimizers` method, which returns an instance of the `SGD` class. ```{.python .input  n=14}\n%%tab all\n@d2l.add_to_class(LinearRegressionScratch)  #@save\ndef configure_optimizers(self):\n    if tab.selected('mxnet') or tab.selected('pytorch'):\n        return SGD([self.w, self.b], self.lr)\n    if tab.selected('tensorflow', 'jax'):\n        return SGD(self.lr)\n```"
    },
    {
      "chunk_id": "c528f2da68c5_0",
      "chapter": "linear-regression-scratch",
      "heading": "Training",
      "text": "Now that we have all of the parts in place\n(parameters, loss function, model, and optimizer),\nwe are ready to [**implement the main training loop.**]\nIt is crucial that you understand this code fully\nsince you will employ similar training loops\nfor every other deep learning model\ncovered in this book. In each *epoch*, we iterate through \nthe entire training dataset, \npassing once through every example\n(assuming that the number of examples \nis divisible by the batch size). In each *iteration*, we grab a minibatch of training examples,\nand compute its loss through the model's `training_step` method. Then we compute the gradients with respect to each parameter. Finally, we will call the optimization algorithm\nto update the model parameters. In summary, we will execute the following loop:\n\n* Initialize parameters $(\\mathbf{w}, b)$\n* Repeat until done\n    * Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n    * Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n \nRecall that the synthetic regression dataset \nthat we generated in :numref:``sec_synthetic-regression-data`` \ndoes not provide a validation dataset. In most cases, however, \nwe will want a validation dataset \nto measure our model quality. Here we pass the validation dataloader \nonce in each epoch to measure the model performance. Following our object-oriented design,\nthe `prepare_batch` and `fit_epoch` methods\nare registered in the `d2l.Trainer` class\n(introduced in :numref:`oo-design-training`)."
    },
    {
      "chunk_id": "c528f2da68c5_1",
      "chapter": "linear-regression-scratch",
      "heading": "Training",
      "text": "Following our object-oriented design,\nthe `prepare_batch` and `fit_epoch` methods\nare registered in the `d2l.Trainer` class\n(introduced in :numref:`oo-design-training`). ```{.python .input  n=15}\n%%tab all    \n@d2l.add_to_class(d2l.Trainer)  #@save\ndef prepare_batch(self, batch):\n    return batch\n```\n\n```{.python .input  n=16}\n%%tab pytorch\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef fit_epoch(self):\n    self.model.train()        \n    for batch in self.train_dataloader:        \n        loss = self.model.training_step(self.prepare_batch(batch))\n        self.optim.zero_grad()\n        with torch.no_grad():\n            loss.backward()\n            if self.gradient_clip_val > 0:  # To be discussed later\n                self.clip_gradients(self.gradient_clip_val, self.model)\n            self.optim.step()\n        self.train_batch_idx += 1\n    if self.val_dataloader is None:\n        return\n    self.model.eval()\n    for batch in self.val_dataloader:\n        with torch.no_grad():            \n            self.model.validation_step(self.prepare_batch(batch))\n        self.val_batch_idx += 1\n```\n\n```{.python .input  n=17}\n%%tab mxnet\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef fit_epoch(self):\n    for batch in self.train_dataloader:\n        with autograd.record():\n            loss = self.model.training_step(self.prepare_batch(batch))\n        loss.backward()\n        if self.gradient_clip_val > 0:\n            self.clip_gradients(self.gradient_clip_val, self.model)\n        self.optim.step(1)\n        self.train_batch_idx += 1\n    if self.val_dataloader is None:\n        return\n    for batch in self.val_dataloader:        \n        self.model.validation_step(self.prepare_batch(batch))\n        self.val_batch_idx += 1\n```\n\n```{.python .input  n=18}\n%%tab tensorflow\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef fit_epoch(self):\n    self.model.training = True\n    for batch in self.train_dataloader:            \n        with tf.GradientTape() as tape:\n            loss = self.model.training_step(self.prepare_batch(batch))\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        if self.gradient_clip_val > 0:\n            grads = self.clip_gradients(self.gradient_clip_val, grads)\n        self.optim.apply_gradients(zip(grads, self.model.trainable_variables))\n        self.train_batch_idx += 1\n    if self.val_dataloader is None:\n        return\n    self.model.training = False\n    for batch in self.val_dataloader:        \n        self.model.validation_step(self.prepare_batch(batch))\n        self.val_batch_idx += 1\n```\n\n```{.python .input  n=19}\n%%tab jax\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef fit_epoch(self):\n    self.model.training = True\n    if self.state.batch_stats:\n        # Mutable states will be used later (e.g., for batch norm)\n        for batch in self.train_dataloader:\n            (_, mutated_vars), grads = self.model.training_step(self.state.params,\n                                                           self.prepare_batch(batch),\n                                                           self.state)\n            self.state = self.state.apply_gradients(grads=grads)\n            # Can be ignored for models without Dropout Layers\n            self.state = self.state.replace(\n                dropout_rng=jax.random.split(self.state.dropout_rng)[0])\n            self.state = self.state.replace(batch_stats=mutated_vars['batch_stats'])\n            self.train_batch_idx += 1\n    else:\n        for batch in self.train_dataloader:\n            _, grads = self.model.training_step(self.state.params,\n                                                self.prepare_batch(batch),\n                                                self.state)\n            self.state = self.state.apply_gradients(grads=grads)\n            # Can be ignored for models without Dropout Layers\n            self.state = self.state.replace(\n                dropout_rng=jax.random.split(self.state.dropout_rng)[0])\n            self.train_batch_idx += 1\n\n    if self.val_dataloader is None:\n        return\n    self.model.training = False\n    for batch in self.val_dataloader:\n        self.model.validation_step(self.state.params,\n                                   self.prepare_batch(batch),\n                                   self.state)\n        self.val_batch_idx += 1\n```\n\nWe are almost ready to train the model,\nbut first we need some training data."
    },
    {
      "chunk_id": "c528f2da68c5_2",
      "chapter": "linear-regression-scratch",
      "heading": "Training",
      "text": "Here we use the `SyntheticRegressionData` class \nand pass in some ground truth parameters. Then we train our model with \nthe learning rate `lr=0.03` \nand set `max_epochs=3`. Note that in general, both the number of epochs \nand the learning rate are hyperparameters. In general, setting hyperparameters is tricky\nand we will usually want to use a three-way split,\none set for training, \na second for hyperparameter selection,\nand the third reserved for the final evaluation. We elide these details for now but will revise them\nlater. ```{.python .input  n=20}\n%%tab all\nmodel = LinearRegressionScratch(2, lr=0.03)\ndata = d2l.SyntheticRegressionData(w=d2l.tensor([2, -3.4]), b=4.2)\ntrainer = d2l.Trainer(max_epochs=3)\ntrainer.fit(model, data)\n```\n\nBecause we synthesized the dataset ourselves,\nwe know precisely what the true parameters are. Thus, we can [**evaluate our success in training\nby comparing the true parameters\nwith those that we learned**] through our training loop. Indeed they turn out to be very close to each other. ```{.python .input  n=21}\n%%tab pytorch\nwith torch.no_grad():\n    print(f'error in estimating w: {data.w - d2l.reshape(model.w, data.w.shape)}')\n    print(f'error in estimating b: {data.b - model.b}')\n```\n\n```{.python .input  n=22}\n%%tab mxnet, tensorflow\nprint(f'error in estimating w: {data.w - d2l.reshape(model.w, data.w.shape)}')\nprint(f'error in estimating b: {data.b - model.b}')\n```\n\n```{.python .input  n=23}\n%%tab jax\nparams = trainer.state.params\nprint(f\"error in estimating w: {data.w - d2l.reshape(params['w'], data.w.shape)}\")\nprint(f\"error in estimating b: {data.b - params['b']}\")\n```\n\nWe should not take the ability to exactly recover \nthe ground truth parameters for granted. In general, for deep models unique solutions\nfor the parameters do not exist,\nand even for linear models,\nexactly recovering the parameters\nis only possible when no feature \nis linearly dependent on the others."
    },
    {
      "chunk_id": "c528f2da68c5_3",
      "chapter": "linear-regression-scratch",
      "heading": "Training",
      "text": "In general, for deep models unique solutions\nfor the parameters do not exist,\nand even for linear models,\nexactly recovering the parameters\nis only possible when no feature \nis linearly dependent on the others. However, in machine learning, \nwe are often less concerned\nwith recovering true underlying parameters,\nbut rather with parameters \nthat lead to highly accurate prediction :cite:`Vapnik.1992`. Fortunately, even on difficult optimization problems,\nstochastic gradient descent can often find remarkably good solutions,\nowing partly to the fact that, for deep networks,\nthere exist many configurations of the parameters\nthat lead to highly accurate prediction."
    },
    {
      "chunk_id": "a25d1480d6ce_0",
      "chapter": "linear-regression-scratch",
      "heading": "Summary",
      "text": "In this section, we took a significant step \ntowards designing deep learning systems \nby implementing a fully functional \nneural network model and training loop.\nIn this process, we built a data loader, \na model, a loss function, an optimization procedure,\nand a visualization and monitoring tool. \nWe did this by composing a Python object \nthat contains all relevant components for training a model. \nWhile this is not yet a professional-grade implementation\nit is perfectly functional and code like this \ncould already help you to solve small problems quickly.\nIn the coming sections, we will see how to do this\nboth *more concisely* (avoiding boilerplate code)\nand *more efficiently* (using our GPUs to their full potential)."
    },
    {
      "chunk_id": "0775b8fc67e4_0",
      "chapter": "linear-regression-scratch",
      "heading": "Exercises",
      "text": "1. What would happen if we were to initialize the weights to zero. Would the algorithm still work? What if we\n   initialized the parameters with variance $1000$ rather than $0.01$? 1. Assume that you are [Georg Simon Ohm](https://en.wikipedia.org/wiki/Georg_Ohm) trying to come up\n   with a model for resistance that relates voltage and current. Can you use automatic\n   differentiation to learn the parameters of your model? 1. Can you use [Planck's Law](https://en.wikipedia.org/wiki/Planck%27s_law) to determine the temperature of an object\n   using spectral energy density? For reference, the spectral density $B$ of radiation emanating from a black body is\n   $B(\\lambda, T) = \\frac{2 hc^2}{\\lambda^5} \\cdot \\left(\\exp \\frac{h c}{\\lambda k T} - 1\\right)^{-1}$. Here\n   $\\lambda$ is the wavelength, $T$ is the temperature, $c$ is the speed of light, $h$ is Planck's constant, and $k$ is the\n   Boltzmann constant. You measure the energy for different wavelengths $\\lambda$ and you now need to fit the spectral\n   density curve to Planck's law. 1. What are the problems you might encounter if you wanted to compute the second derivatives of the loss? How would\n   you fix them? 1. Why is the `reshape` method needed in the `loss` function? 1. Experiment using different learning rates to find out how quickly the loss function value drops. Can you reduce the\n   error by increasing the number of epochs of training? 1. If the number of examples cannot be divided by the batch size, what happens to `data_iter` at the end of an epoch? 1. Try implementing a different loss function, such as the absolute value loss `(y_hat - d2l.reshape(y, y_hat.shape)).abs().sum()`. 1. Check what happens for regular data. 1. Check whether there is a difference in behavior if you actively perturb some entries, such as $y_5 = 10000$, of $\\mathbf{y}$. 1. Can you think of a cheap solution for combining the best aspects of squared loss and absolute value loss? Hint: how can you avoid really large gradient values? 1. Why do we need to reshuffle the dataset?"
    },
    {
      "chunk_id": "0775b8fc67e4_1",
      "chapter": "linear-regression-scratch",
      "heading": "Exercises",
      "text": "1. Can you think of a cheap solution for combining the best aspects of squared loss and absolute value loss? Hint: how can you avoid really large gradient values? 1. Why do we need to reshuffle the dataset? Can you design a case where a maliciously constructed dataset would break the optimization algorithm otherwise? :begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/42)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/43)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/201)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17976)\n:end_tab:"
    },
    {
      "chunk_id": "3bcc0dc39046_0",
      "chapter": "linear-regression",
      "heading": "linear-regression",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Linear Regression\n:label:`sec_linear_regression`\n\n*Regression* problems pop up whenever we want to predict a numerical value.\nCommon examples include predicting prices (of homes, stocks, etc.),\npredicting the length of stay (for patients in the hospital),\nforecasting demand (for retail sales), among numerous others.\nNot every prediction problem is one of classical regression.\nLater on, we will introduce classification problems,\nwhere the goal is to predict membership among a set of categories.\n\nAs a running example, suppose that we wish\nto estimate the prices of houses (in dollars)\nbased on their area (in square feet) and age (in years).\nTo develop a model for predicting house prices,\nwe need to get our hands on data,\nincluding the sales price, area, and age for each home.\nIn the terminology of machine learning,\nthe dataset is called a *training dataset* or *training set*,\nand each row (containing the data corresponding to one sale)\nis called an *example* (or *data point*, *instance*, *sample*).\nThe thing we are trying to predict (price)\nis called a *label* (or *target*).\nThe variables (age and area)\nupon which the predictions are based\nare called *features* (or *covariates*).\n\n```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import np\nimport time\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport math\nimport torch\nimport numpy as np\nimport time\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport math\nimport tensorflow as tf\nimport numpy as np\nimport time\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nfrom jax import numpy as jnp\nimport math\nimport time\n```"
    },
    {
      "chunk_id": "d33fa4e2a7af_0",
      "chapter": "linear-regression",
      "heading": "Basics",
      "text": "*Linear regression* is both the simplest\nand most popular among the standard tools\nfor tackling regression problems.\nDating back to the dawn of the 19th century :cite:`Legendre.1805,Gauss.1809`,\nlinear regression flows from a few simple assumptions.\nFirst, we assume that the relationship\nbetween features $\\mathbf{x}$ and target $y$\nis approximately linear,\ni.e., that the conditional mean $E[Y \\mid X=\\mathbf{x}]$\ncan be expressed as a weighted sum\nof the features $\\mathbf{x}$.\nThis setup allows that the target value\nmay still deviate from its expected value\non account of observation noise.\nNext, we can impose the assumption that any such noise\nis well behaved, following a Gaussian distribution.\nTypically, we will use $n$ to denote\nthe number of examples in our dataset.\nWe use superscripts to enumerate samples and targets,\nand subscripts to index coordinates.\nMore concretely,\n$\\mathbf{x}^{(i)}$ denotes the $i^{\\textrm{th}}$ sample\nand $x_j^{(i)}$ denotes its $j^{\\textrm{th}}$ coordinate."
    },
    {
      "chunk_id": "ba94085a0059_0",
      "chapter": "linear-regression",
      "heading": "Model",
      "text": ":label:`subsec_linear_model`\n\nAt the heart of every solution is a model\nthat describes how features can be transformed\ninto an estimate of the target. The assumption of linearity means that\nthe expected value of the target (price) can be expressed\nas a weighted sum of the features (area and age):\n\n$$\\textrm{price} = w_{\\textrm{area}} \\cdot \\textrm{area} + w_{\\textrm{age}} \\cdot \\textrm{age} + b.$$\n:eqlabel:`eq_price-area`\n\nHere $w_{\\textrm{area}}$ and $w_{\\textrm{age}}$\nare called *weights*, and $b$ is called a *bias*\n(or *offset* or *intercept*). The weights determine the influence of each feature on our prediction. The bias determines the value of the estimate when all features are zero. Even though we will never see any newly-built homes with precisely zero area,\nwe still need the bias because it allows us\nto express all linear functions of our features\n(rather than restricting us to lines that pass through the origin). Strictly speaking, :eqref:`eq_price-area` is an *affine transformation* of input features, which is characterized by a *linear transformation* of features via a weighted sum, combined with a *translation* via the added bias. Given a dataset, our goal is to choose\nthe weights $\\mathbf{w}$ and the bias $b$\nthat, on average, make our model's predictions\nfit the true prices observed in the data as closely as possible. In disciplines where it is common to focus\non datasets with just a few features,\nexplicitly expressing models long-form,\nas in :eqref:`eq_price-area`, is common. In machine learning, we usually work\nwith high-dimensional datasets,\nwhere it is more convenient to employ\ncompact linear algebra notation."
    },
    {
      "chunk_id": "ba94085a0059_1",
      "chapter": "linear-regression",
      "heading": "Model",
      "text": "In machine learning, we usually work\nwith high-dimensional datasets,\nwhere it is more convenient to employ\ncompact linear algebra notation. When our inputs consist of $d$ features,\nwe can assign each an index (between $1$ and $d$)\nand express our prediction $\\hat{y}$\n(in general the \"hat\" symbol denotes an estimate) as\n\n$$\\hat{y} = w_1  x_1 + \\cdots + w_d  x_d + b.$$\n\nCollecting all features into a vector $\\mathbf{x} \\in \\mathbb{R}^d$\nand all weights into a vector $\\mathbf{w} \\in \\mathbb{R}^d$,\nwe can express our model compactly via the dot product\nbetween $\\mathbf{w}$ and $\\mathbf{x}$:\n\n$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.$$\n:eqlabel:`eq_linreg-y`\n\nIn :eqref:`eq_linreg-y`, the vector $\\mathbf{x}$\ncorresponds to the features of a single example. We will often find it convenient\nto refer to features of our entire dataset of $n$ examples\nvia the *design matrix* $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$. Here, $\\mathbf{X}$ contains one row for every example\nand one column for every feature. For a collection of features $\\mathbf{X}$,\nthe predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$\ncan be expressed via the matrix--vector product:\n\n$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b,$$\n:eqlabel:`eq_linreg-y-vec`\n\nwhere broadcasting (:numref:`subsec_broadcasting`) is applied during the summation. Given features of a training dataset $\\mathbf{X}$\nand corresponding (known) labels $\\mathbf{y}$,\nthe goal of linear regression is to find\nthe weight vector $\\mathbf{w}$ and the bias term $b$\nsuch that, given features of a new data example\nsampled from the same distribution as $\\mathbf{X}$,\nthe new example's label will (in expectation)\nbe predicted with the smallest error. Even if we believe that the best model for\npredicting $y$ given $\\mathbf{x}$ is linear,\nwe would not expect to find a real-world dataset of $n$ examples where\n$y^{(i)}$ exactly equals $\\mathbf{w}^\\top \\mathbf{x}^{(i)}+b$\nfor all $1 \\leq i \\leq n$."
    },
    {
      "chunk_id": "ba94085a0059_2",
      "chapter": "linear-regression",
      "heading": "Model",
      "text": "Even if we believe that the best model for\npredicting $y$ given $\\mathbf{x}$ is linear,\nwe would not expect to find a real-world dataset of $n$ examples where\n$y^{(i)}$ exactly equals $\\mathbf{w}^\\top \\mathbf{x}^{(i)}+b$\nfor all $1 \\leq i \\leq n$. For example, whatever instruments we use to observe\nthe features $\\mathbf{X}$ and labels $\\mathbf{y}$, there might be a small amount of measurement error. Thus, even when we are confident\nthat the underlying relationship is linear,\nwe will incorporate a noise term to account for such errors. Before we can go about searching for the best *parameters*\n(or *model parameters*) $\\mathbf{w}$ and $b$,\nwe will need two more things:\n(i) a measure of the quality of some given model;\nand (ii) a procedure for updating the model to improve its quality."
    },
    {
      "chunk_id": "942211da092d_0",
      "chapter": "linear-regression",
      "heading": "Loss Function",
      "text": ":label:`subsec_linear-regression-loss-function`\n\nNaturally, fitting our model to the data requires\nthat we agree on some measure of *fitness*\n(or, equivalently, of *unfitness*). *Loss functions* quantify the distance\nbetween the *real* and *predicted* values of the target. The loss will usually be a nonnegative number\nwhere smaller values are better\nand perfect predictions incur a loss of 0. For regression problems, the most common loss function is the squared error. When our prediction for an example $i$ is $\\hat{y}^{(i)}$\nand the corresponding true label is $y^{(i)}$,\nthe *squared error* is given by:\n\n$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n:eqlabel:`eq_mse`\n\nThe constant $\\frac{1}{2}$ makes no real difference\nbut proves to be notationally convenient,\nsince it cancels out when we take the derivative of the loss. Because the training dataset is given to us,\nand thus is out of our control,\nthe empirical error is only a function of the model parameters. In :numref:`fig_fit_linreg`, we visualize the fit of a linear regression model\nin a problem with one-dimensional inputs. ![Fitting a linear regression model to one-dimensional data.](../img/fit-linreg.svg)\n:label:`fig_fit_linreg`\n\nNote that large differences between\nestimates $\\hat{y}^{(i)}$ and targets $y^{(i)}$\nlead to even larger contributions to the loss,\ndue to its quadratic form\n(this quadraticity can be a double-edge sword; while it encourages the model to avoid large errors\nit can also lead to excessive sensitivity to anomalous data)."
    },
    {
      "chunk_id": "942211da092d_1",
      "chapter": "linear-regression",
      "heading": "Loss Function",
      "text": "To measure the quality of a model on the entire dataset of $n$ examples,\nwe simply average (or equivalently, sum)\nthe losses on the training set:\n\n$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n\nWhen training the model, we seek parameters ($\\mathbf{w}^*, b^*$)\nthat minimize the total loss across all training examples:\n\n$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$"
    },
    {
      "chunk_id": "42e984e19dc0_0",
      "chapter": "linear-regression",
      "heading": "Analytic Solution",
      "text": "Unlike most of the models that we will cover,\nlinear regression presents us with\na surprisingly easy optimization problem.\nIn particular, we can find the optimal parameters\n(as assessed on the training data)\nanalytically by applying a simple formula as follows.\nFirst, we can subsume the bias $b$ into the parameter $\\mathbf{w}$\nby appending a column to the design matrix consisting of all 1s.\nThen our prediction problem is to minimize $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$.\nAs long as the design matrix $\\mathbf{X}$ has full rank\n(no feature is linearly dependent on the others),\nthen there will be just one critical point on the loss surface\nand it corresponds to the minimum of the loss over the entire domain.\nTaking the derivative of the loss with respect to $\\mathbf{w}$\nand setting it equal to zero yields:\n\n$$\\begin{aligned}\n    \\partial_{\\mathbf{w}} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 =\n    2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) = 0\n    \\textrm{ and hence }\n    \\mathbf{X}^\\top \\mathbf{y} = \\mathbf{X}^\\top \\mathbf{X} \\mathbf{w}.\n\\end{aligned}$$\n\nSolving for $\\mathbf{w}$ provides us with the optimal solution\nfor the optimization problem.\nNote that this solution \n\n$$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}$$\n\nwill only be unique\nwhen the matrix $\\mathbf X^\\top \\mathbf X$ is invertible,\ni.e., when the columns of the design matrix\nare linearly independent :cite:`Golub.Van-Loan.1996`.\n\n\n\nWhile simple problems like linear regression\nmay admit analytic solutions,\nyou should not get used to such good fortune.\nAlthough analytic solutions allow for nice mathematical analysis,\nthe requirement of an analytic solution is so restrictive\nthat it would exclude almost all exciting aspects of deep learning."
    },
    {
      "chunk_id": "d7108bba2f17_0",
      "chapter": "linear-regression",
      "heading": "Minibatch Stochastic Gradient Descent",
      "text": "Fortunately, even in cases where we cannot solve the models analytically,\nwe can still often train models effectively in practice. Moreover, for many tasks, those hard-to-optimize models\nturn out to be so much better that figuring out how to train them\nends up being well worth the trouble. The key technique for optimizing nearly every deep learning model,\nand which we will call upon throughout this book,\nconsists of iteratively reducing the error\nby updating the parameters in the direction\nthat incrementally lowers the loss function. This algorithm is called *gradient descent*. The most naive application of gradient descent\nconsists of taking the derivative of the loss function,\nwhich is an average of the losses computed\non every single example in the dataset. In practice, this can be extremely slow:\nwe must pass over the entire dataset before making a single update,\neven if the update steps might be very powerful :cite:`Liu.Nocedal.1989`. Even worse, if there is a lot of redundancy in the training data,\nthe benefit of a full update is limited. The other extreme is to consider only a single example at a time and to take\nupdate steps based on one observation at a time. The resulting algorithm, *stochastic gradient descent* (SGD)\ncan be an effective strategy :cite:`Bottou.2010`, even for large datasets. Unfortunately, SGD has drawbacks, both computational and statistical. One problem arises from the fact that processors are a lot faster\nmultiplying and adding numbers than they are\nat moving data from main memory to processor cache. It is up to an order of magnitude more efficient to\nperform a matrix--vector multiplication\nthan a corresponding number of vector--vector operations. This means that it can take a lot longer to process\none sample at a time compared to a full batch. A second problem is that some of the layers,\nsuch as batch normalization (to be described in :numref:`sec_batch_norm`),\nonly work well when we have access\nto more than one observation at a time."
    },
    {
      "chunk_id": "d7108bba2f17_1",
      "chapter": "linear-regression",
      "heading": "Minibatch Stochastic Gradient Descent",
      "text": "A second problem is that some of the layers,\nsuch as batch normalization (to be described in :numref:`sec_batch_norm`),\nonly work well when we have access\nto more than one observation at a time. The solution to both problems is to pick an intermediate strategy:\nrather than taking a full batch or only a single sample at a time,\nwe take a *minibatch* of observations :cite:`Li.Zhang.Chen.ea.2014`. The specific choice of the size of the said minibatch depends on many factors,\nsuch as the amount of memory, the number of accelerators,\nthe choice of layers, and the total dataset size. Despite all that, a number between 32 and 256,\npreferably a multiple of a large power of $2$, is a good start. This leads us to *minibatch stochastic gradient descent*. In its most basic form, in each iteration $t$,\nwe first randomly sample a minibatch $\\mathcal{B}_t$\nconsisting of a fixed number $|\\mathcal{B}|$ of training examples. We then compute the derivative (gradient) of the average loss\non the minibatch with respect to the model parameters. Finally, we multiply the gradient\nby a predetermined small positive value $\\eta$,\ncalled the *learning rate*,\nand subtract the resulting term from the current parameter values. We can express the update as follows:\n\n$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n\nIn summary, minibatch SGD proceeds as follows:\n(i) initialize the values of the model parameters, typically at random;\n(ii) iteratively sample random minibatches from the data,\nupdating the parameters in the direction of the negative gradient."
    },
    {
      "chunk_id": "d7108bba2f17_2",
      "chapter": "linear-regression",
      "heading": "Minibatch Stochastic Gradient Descent",
      "text": "For quadratic losses and affine transformations,\nthis has a closed-form expansion:\n\n$$\\begin{aligned} \\mathbf{w} & \\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_b l^{(i)}(\\mathbf{w}, b) &&  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$\n:eqlabel:`eq_linreg_batch_update`\n\nSince we pick a minibatch $\\mathcal{B}$\nwe need to normalize by its size $|\\mathcal{B}|$. Frequently minibatch size and learning rate are user-defined. Such tunable parameters that are not updated\nin the training loop are called *hyperparameters*. They can be tuned automatically by a number of techniques, such as Bayesian optimization\n:cite:`Frazier.2018`. In the end, the quality of the solution is\ntypically assessed on a separate *validation dataset* (or *validation set*). After training for some predetermined number of iterations\n(or until some other stopping criterion is met),\nwe record the estimated model parameters,\ndenoted $\\hat{\\mathbf{w}}, \\hat{b}$. Note that even if our function is truly linear and noiseless,\nthese parameters will not be the exact minimizers of the loss, nor even deterministic. Although the algorithm converges slowly towards the minimizers\nit typically will not find them exactly in a finite number of steps. Moreover, the minibatches $\\mathcal{B}$\nused for updating the parameters are chosen at random. This breaks determinism. Linear regression happens to be a learning problem\nwith a global minimum\n(whenever $\\mathbf{X}$ is full rank, or equivalently,\nwhenever $\\mathbf{X}^\\top \\mathbf{X}$ is invertible). However, the loss surfaces for deep networks contain many saddle points and minima."
    },
    {
      "chunk_id": "d7108bba2f17_3",
      "chapter": "linear-regression",
      "heading": "Minibatch Stochastic Gradient Descent",
      "text": "However, the loss surfaces for deep networks contain many saddle points and minima. Fortunately, we typically do not care about finding\nan exact set of parameters but merely any set of parameters\nthat leads to accurate predictions (and thus low loss). In practice, deep learning practitioners\nseldom struggle to find parameters\nthat minimize the loss *on training sets*\n:cite:`Izmailov.Podoprikhin.Garipov.ea.2018,Frankle.Carbin.2018`. The more formidable task is to find parameters\nthat lead to accurate predictions on previously unseen data,\na challenge called *generalization*. We return to these topics throughout the book."
    },
    {
      "chunk_id": "0e8ee05402d8_0",
      "chapter": "linear-regression",
      "heading": "Predictions",
      "text": "Given the model $\\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}$,\nwe can now make *predictions* for a new example,\ne.g., predicting the sales price of a previously unseen house\ngiven its area $x_1$ and age $x_2$.\nDeep learning practitioners have taken to calling the prediction phase *inference*\nbut this is a bit of a misnomer---*inference* refers broadly\nto any conclusion reached on the basis of evidence,\nincluding both the values of the parameters\nand the likely label for an unseen instance.\nIf anything, in the statistics literature\n*inference* more often denotes parameter inference\nand this overloading of terminology creates unnecessary confusion\nwhen deep learning practitioners talk to statisticians.\nIn the following we will stick to *prediction* whenever possible."
    },
    {
      "chunk_id": "69137fe447d6_0",
      "chapter": "linear-regression",
      "heading": "Vectorization for Speed",
      "text": "When training our models, we typically want to process\nwhole minibatches of examples simultaneously.\nDoing this efficiently requires that (**we**) (~~should~~)\n(**vectorize the calculations and leverage\nfast linear algebra libraries\nrather than writing costly for-loops in Python.**)\n\nTo see why this matters so much,\nlet's (**consider two methods for adding vectors.**)\nTo start, we instantiate two 10,000-dimensional vectors\ncontaining all 1s.\nIn the first method, we loop over the vectors with a Python for-loop.\nIn the second, we rely on a single call to `+`.\n\n```{.python .input}\n%%tab all\nn = 10000\na = d2l.ones(n)\nb = d2l.ones(n)\n```\n\nNow we can benchmark the workloads.\nFirst, [**we add them, one coordinate at a time,\nusing a for-loop.**]\n\n```{.python .input}\n%%tab mxnet, pytorch\nc = d2l.zeros(n)\nt = time.time()\nfor i in range(n):\n    c[i] = a[i] + b[i]\nf'{time.time() - t:.5f} sec'\n```\n\n```{.python .input}\n%%tab tensorflow\nc = tf.Variable(d2l.zeros(n))\nt = time.time()\nfor i in range(n):\n    c[i].assign(a[i] + b[i])\nf'{time.time() - t:.5f} sec'\n```\n\n```{.python .input}\n%%tab jax\n# JAX arrays are immutable, meaning that once created their contents\n# cannot be changed. For updating individual elements, JAX provides\n# an indexed update syntax that returns an updated copy\nc = d2l.zeros(n)\nt = time.time()\nfor i in range(n):\n    c = c.at[i].set(a[i] + b[i])\nf'{time.time() - t:.5f} sec'\n```\n\n(**Alternatively, we rely on the reloaded `+` operator to compute the elementwise sum.**)\n\n```{.python .input}\n%%tab all\nt = time.time()\nd = a + b\nf'{time.time() - t:.5f} sec'\n```\n\nThe second method is dramatically faster than the first.\nVectorizing code often yields order-of-magnitude speedups.\nMoreover, we push more of the mathematics to the library\nso we do not have to write as many calculations ourselves,\nreducing the potential for errors and increasing portability of the code."
    },
    {
      "chunk_id": "21aa6f8f26d2_0",
      "chapter": "linear-regression",
      "heading": "The Normal Distribution and Squared Loss",
      "text": ":label:`subsec_normal_distribution_and_squared_loss`\n\nSo far we have given a fairly functional motivation\nof the squared loss objective:\nthe optimal parameters return the conditional expectation $E[Y\\mid X]$\nwhenever the underlying pattern is truly linear,\nand the loss assigns large penalties for outliers. We can also provide a more formal motivation\nfor the squared loss objective\nby making probabilistic assumptions\nabout the distribution of noise. Linear regression was invented at the turn of the 19th century. While it has long been debated whether Gauss or Legendre\nfirst thought up the idea,\nit was Gauss who also discovered the normal distribution\n(also called the *Gaussian*). It turns out that the normal distribution\nand linear regression with squared loss\nshare a deeper connection than common parentage. To begin, recall that a normal distribution\nwith mean $\\mu$ and variance $\\sigma^2$ (standard deviation $\\sigma$)\nis given as\n\n$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right).$$\n\nBelow [**we define a function to compute the normal distribution**]. ```{.python .input}\n%%tab all\ndef normal(x, mu, sigma):\n    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n    if tab.selected('jax'):\n        return p * jnp.exp(-0.5 * (x - mu)**2 / sigma**2)\n    if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n        return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)\n```\n\nWe can now (**visualize the normal distributions**)."
    },
    {
      "chunk_id": "21aa6f8f26d2_1",
      "chapter": "linear-regression",
      "heading": "The Normal Distribution and Squared Loss",
      "text": "```{.python .input}\n%%tab mxnet\n# Use NumPy again for visualization\nx = np.arange(-7, 7, 0.01)\n\n# Mean and standard deviation pairs\nparams = [(0, 1), (0, 2), (3, 1)]\nd2l.plot(x.asnumpy(), [normal(x, mu, sigma).asnumpy() for mu, sigma in params], xlabel='x',\n         ylabel='p(x)', figsize=(4.5, 2.5),\n         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])\n```\n\n```{.python .input}\n\n%%tab pytorch, tensorflow, jax\nif tab.selected('jax'):\n    # Use JAX NumPy for visualization\n    x = jnp.arange(-7, 7, 0.01)\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    # Use NumPy again for visualization\n    x = np.arange(-7, 7, 0.01)\n\n# Mean and standard deviation pairs\nparams = [(0, 1), (0, 2), (3, 1)]\nd2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',\n         ylabel='p(x)', figsize=(4.5, 2.5),\n         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])\n```\n\nNote that changing the mean corresponds\nto a shift along the $x$-axis,\nand increasing the variance\nspreads the distribution out,\nlowering its peak. One way to motivate linear regression with squared loss\nis to assume that observations arise from noisy measurements,\nwhere the noise $\\epsilon$ follows the normal distribution \n$\\mathcal{N}(0, \\sigma^2)$:\n\n$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon \\textrm{ where } \\epsilon \\sim \\mathcal{N}(0, \\sigma^2).$$\n\nThus, we can now write out the *likelihood*\nof seeing a particular $y$ for a given $\\mathbf{x}$ via\n\n$$P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).$$\n\nAs such, the likelihood factorizes. According to *the principle of maximum likelihood*,\nthe best values of parameters $\\mathbf{w}$ and $b$ are those\nthat maximize the *likelihood* of the entire dataset:\n\n$$P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)} \\mid \\mathbf{x}^{(i)}).$$\n\nThe equality follows since all pairs $(\\mathbf{x}^{(i)}, y^{(i)})$\nwere drawn independently of each other."
    },
    {
      "chunk_id": "21aa6f8f26d2_2",
      "chapter": "linear-regression",
      "heading": "The Normal Distribution and Squared Loss",
      "text": "Estimators chosen according to the principle of maximum likelihood\nare called *maximum likelihood estimators*. While, maximizing the product of many exponential functions,\nmight look difficult,\nwe can simplify things significantly, without changing the objective,\nby maximizing the logarithm of the likelihood instead. For historical reasons, optimizations are more often expressed\nas minimization rather than maximization. So, without changing anything,\nwe can *minimize* the *negative log-likelihood*,\nwhich we can express as follows:\n\n$$-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2.$$\n\nIf we assume that $\\sigma$ is fixed,\nwe can ignore the first term,\nbecause it does not depend on $\\mathbf{w}$ or $b$. The second term is identical\nto the squared error loss introduced earlier,\nexcept for the multiplicative constant $\\frac{1}{\\sigma^2}$. Fortunately, the solution does not depend on $\\sigma$ either. It follows that minimizing the mean squared error\nis equivalent to the maximum likelihood estimation\nof a linear model under the assumption of additive Gaussian noise."
    },
    {
      "chunk_id": "15f7cf3fd9d8_0",
      "chapter": "linear-regression",
      "heading": "Linear Regression as a Neural Network",
      "text": "While linear models are not sufficiently rich\nto express the many complicated networks\nthat we will introduce in this book,\n(artificial) neural networks are rich enough\nto subsume linear models as networks\nin which every feature is represented by an input neuron,\nall of which are connected directly to the output.\n\n:numref:`fig_single_neuron` depicts\nlinear regression as a neural network.\nThe diagram highlights the connectivity pattern,\nsuch as how each input is connected to the output,\nbut not the specific values taken by the weights or biases.\n\n![Linear regression is a single-layer neural network.](../img/singleneuron.svg)\n:label:`fig_single_neuron`\n\nThe inputs are $x_1, \\ldots, x_d$.\nWe refer to $d$ as the *number of inputs*\nor the *feature dimensionality* in the input layer.\nThe output of the network is $o_1$.\nBecause we are just trying to predict\na single numerical value,\nwe have only one output neuron.\nNote that the input values are all *given*.\nThere is just a single *computed* neuron.\nIn summary, we can think of linear regression\nas a single-layer fully connected neural network.\nWe will encounter networks\nwith far more layers\nin later chapters."
    },
    {
      "chunk_id": "878f87238366_0",
      "chapter": "linear-regression",
      "heading": "Biology",
      "text": "Because linear regression predates computational neuroscience,\nit might seem anachronistic to describe\nlinear regression in terms of neural networks. Nonetheless, they were a natural place to start\nwhen the cyberneticists and neurophysiologists\nWarren McCulloch and Walter Pitts began to develop\nmodels of artificial neurons. Consider the cartoonish picture\nof a biological neuron in :numref:`fig_Neuron`,\nconsisting of *dendrites* (input terminals),\nthe *nucleus* (CPU), the *axon* (output wire),\nand the *axon terminals* (output terminals),\nenabling connections to other neurons via *synapses*. ![The real neuron (source: \"Anatomy and Physiology\" by the US National Cancer Institute's Surveillance, Epidemiology and End Results (SEER) Program).](../img/neuron.svg)\n:label:`fig_Neuron`\n\nInformation $x_i$ arriving from other neurons\n(or environmental sensors) is received in the dendrites. In particular, that information is weighted\nby *synaptic weights* $w_i$,\ndetermining the effect of the inputs,\ne.g., activation or inhibition via the product $x_i w_i$. The weighted inputs arriving from multiple sources\nare aggregated in the nucleus\nas a weighted sum $y = \\sum_i x_i w_i + b$,\npossibly subject to some nonlinear postprocessing via a function $\\sigma(y)$. This information is then sent via the axon to the axon terminals,\nwhere it reaches its destination\n(e.g., an actuator such as a muscle)\nor it is fed into another neuron via its dendrites. Certainly, the high-level idea that many such units\ncould be combined, provided they have the correct connectivity and learning algorithm,\nto produce far more interesting and complex behavior\nthan any one neuron alone could express\narises from our study of real biological neural systems. At the same time, most research in deep learning today\ndraws inspiration from a much wider source. We invoke :citet:`Russell.Norvig.2016`\nwho pointed out that although airplanes might have been *inspired* by birds,\nornithology has not been the primary driver\nof aeronautics innovation for some centuries."
    },
    {
      "chunk_id": "878f87238366_1",
      "chapter": "linear-regression",
      "heading": "Biology",
      "text": "We invoke :citet:`Russell.Norvig.2016`\nwho pointed out that although airplanes might have been *inspired* by birds,\nornithology has not been the primary driver\nof aeronautics innovation for some centuries. Likewise, inspiration in deep learning these days\ncomes in equal or greater measure\nfrom mathematics, linguistics, psychology,\nstatistics, computer science, and many other fields."
    },
    {
      "chunk_id": "32df7dcb4502_0",
      "chapter": "linear-regression",
      "heading": "Summary",
      "text": "In this section, we introduced\ntraditional linear regression,\nwhere the parameters of a linear function\nare chosen to minimize squared loss on the training set.\nWe also motivated this choice of objective\nboth via some practical considerations\nand through an interpretation\nof linear regression as maximimum likelihood estimation\nunder an assumption of linearity and Gaussian noise.\nAfter discussing both computational considerations\nand connections to statistics,\nwe showed how such linear models could be expressed\nas simple neural networks where the inputs\nare directly wired to the output(s).\nWhile we will soon move past linear models altogether,\nthey are sufficient to introduce most of the components\nthat all of our models require:\nparametric forms, differentiable objectives,\noptimization via minibatch stochastic gradient descent,\nand ultimately, evaluation on previously unseen data."
    },
    {
      "chunk_id": "20244f92520c_0",
      "chapter": "linear-regression",
      "heading": "Exercises",
      "text": "1. Assume that we have some data $x_1, \\ldots, x_n \\in \\mathbb{R}$. Our goal is to find a constant $b$ such that $\\sum_i (x_i - b)^2$ is minimized. 1. Find an analytic solution for the optimal value of $b$. 1. How does this problem and its solution relate to the normal distribution? 1. What if we change the loss from $\\sum_i (x_i - b)^2$ to $\\sum_i |x_i-b|$? Can you find the optimal solution for $b$? 1. Prove that the affine functions that can be expressed by $\\mathbf{x}^\\top \\mathbf{w} + b$ are equivalent to linear functions on $(\\mathbf{x}, 1)$. 1. Assume that you want to find quadratic functions of $\\mathbf{x}$, i.e., $f(\\mathbf{x}) = b + \\sum_i w_i x_i + \\sum_{j \\leq i} w_{ij} x_{i} x_{j}$. How would you formulate this in a deep network? 1. Recall that one of the conditions for the linear regression problem to be solvable was that the design matrix $\\mathbf{X}^\\top \\mathbf{X}$ has full rank. 1. What happens if this is not the case? 1. How could you fix it? What happens if you add a small amount of coordinate-wise independent Gaussian noise to all entries of $\\mathbf{X}$? 1. What is the expected value of the design matrix $\\mathbf{X}^\\top \\mathbf{X}$ in this case? 1. What happens with stochastic gradient descent when $\\mathbf{X}^\\top \\mathbf{X}$ does not have full rank? 1. Assume that the noise model governing the additive noise $\\epsilon$ is the exponential distribution. That is, $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$. 1. Write out the negative log-likelihood of the data under the model $-\\log P(\\mathbf y \\mid \\mathbf X)$. 1. Can you find a closed form solution? 1. Suggest a minibatch stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this? 1. Assume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work? 1."
    },
    {
      "chunk_id": "20244f92520c_1",
      "chapter": "linear-regression",
      "heading": "Exercises",
      "text": "Can you fix this? 1. Assume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work? 1. What happens if you want to use regression for realistic price estimation of houses or stock prices? 1. Show that the additive Gaussian noise assumption is not appropriate. Hint: can we have negative prices? What about fluctuations? 1. Why would regression to the logarithm of the price be much better, i.e., $y = \\log \\textrm{price}$? 1. What do you need to worry about when dealing with pennystock, i.e., stock with very low prices? Hint: can you trade at all possible prices? Why is this a bigger problem for cheap stock? For more information review the celebrated Black--Scholes model for option pricing :cite:`Black.Scholes.1973`. 1. Suppose we want to use regression to estimate the *number* of apples sold in a grocery store. 1. What are the problems with a Gaussian additive noise model? Hint: you are selling apples, not oil. 1. The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) captures distributions over counts. It is given by $p(k \\mid \\lambda) = \\lambda^k e^{-\\lambda}/k!$. Here $\\lambda$ is the rate function and $k$ is the number of events you see. Prove that $\\lambda$ is the expected value of counts $k$. 1. Design a loss function associated with the Poisson distribution. 1. Design a loss function for estimating $\\log \\lambda$ instead. :begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/40)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/258)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/259)\n:end_tab:"
    },
    {
      "chunk_id": "504f494116e0_0",
      "chapter": "oo-design",
      "heading": "oo-design",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Object-Oriented Design for Implementation\n:label:`sec_oo-design`\n\nIn our introduction to linear regression,\nwe walked through various components\nincluding\nthe data, the model, the loss function,\nand the optimization algorithm. Indeed,\nlinear regression is\none of the simplest machine learning models. Training it,\nhowever, uses many of the same components that other models in this book require. Therefore, \nbefore diving into the implementation details\nit is worth \ndesigning some of the APIs\nthat we use throughout. Treating components in deep learning\nas objects,\nwe can start by\ndefining classes for these objects\nand their interactions. This object-oriented design\nfor implementation\nwill greatly\nstreamline the presentation and you might even want to use it in your projects. Inspired by open-source libraries such as [PyTorch Lightning](https://www.pytorchlightning.ai/),\nat a high level\nwe wish to have three classes: \n(i) `Module` contains models, losses, and optimization methods; \n(ii) `DataModule` provides data loaders for training and validation; \n(iii) both classes are combined using the `Trainer` class, which allows us to\ntrain models on a variety of hardware platforms. Most code in this book adapts `Module` and `DataModule`. We will touch upon the `Trainer` class only when we discuss GPUs, CPUs, parallel training, and optimization algorithms."
    },
    {
      "chunk_id": "504f494116e0_1",
      "chapter": "oo-design",
      "heading": "oo-design",
      "text": "Most code in this book adapts `Module` and `DataModule`. We will touch upon the `Trainer` class only when we discuss GPUs, CPUs, parallel training, and optimization algorithms. ```{.python .input}\n%%tab mxnet\nimport time\nimport numpy as np\nfrom d2l import mxnet as d2l\nfrom mxnet.gluon import nn\n```\n\n```{.python .input}\n%%tab pytorch\nimport time\nimport numpy as np\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nimport time\nimport numpy as np\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom dataclasses import field\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom flax.training import train_state\nfrom jax import numpy as jnp\nimport numpy as np\nimport jax\nimport time\nfrom typing import Any\n```"
    },
    {
      "chunk_id": "904df5fe2158_0",
      "chapter": "oo-design",
      "heading": "Utilities",
      "text": ":label:`oo-design-utilities`\n\nWe need a few utilities to simplify object-oriented programming in Jupyter notebooks. One of the challenges is that class definitions tend to be fairly long blocks of code. Notebook readability demands short code fragments, interspersed with explanations, a requirement incompatible with the style of programming common for Python libraries. The first\nutility function allows us to register functions as methods in a class *after* the class has been created. In fact, we can do so *even after* we have created instances of the class! It allows us to split the implementation of a class into multiple code blocks. ```{.python .input}\n%%tab all\ndef add_to_class(Class):  #@save\n    \"\"\"Register functions as methods in created class.\"\"\"\n    def wrapper(obj):\n        setattr(Class, obj.__name__, obj)\n    return wrapper\n```\n\nLet's have a quick look at how to use it. We plan to implement a class `A` with a method `do`. Instead of having code for both `A` and `do` in the same code block, we can first declare the class `A` and create an instance `a`. ```{.python .input}\n%%tab all\nclass A:\n    def __init__(self):\n        self.b = 1\n\na = A()\n```\n\nNext we define the method `do` as we normally would, but not in class `A`'s scope. Instead, we decorate this method by `add_to_class` with class `A` as its argument. In doing so, the method is able to access the member variables of `A` just as we would expect had it been included as part of `A`'s definition. Let's see what happens when we invoke it for the instance `a`. ```{.python .input}\n%%tab all\n@add_to_class(A)\ndef do(self):\n    print('Class attribute \"b\" is', self.b)\n\na.do()\n```\n\nThe second one is a utility class that saves all arguments in a class's `__init__` method as class attributes. This allows us to extend constructor call signatures implicitly without additional code."
    },
    {
      "chunk_id": "904df5fe2158_1",
      "chapter": "oo-design",
      "heading": "Utilities",
      "text": "This allows us to extend constructor call signatures implicitly without additional code. ```{.python .input}\n%%tab all\nclass HyperParameters:  #@save\n    \"\"\"The base class of hyperparameters.\"\"\"\n    def save_hyperparameters(self, ignore=[]):\n        raise NotImplemented\n```\n\nWe defer its implementation into :numref:`sec_utils`. To use it, we define our class that inherits from `HyperParameters` and calls `save_hyperparameters` in the `__init__` method. ```{.python .input}\n%%tab all\n# Call the fully implemented HyperParameters class saved in d2l\nclass B(d2l.HyperParameters):\n    def __init__(self, a, b, c):\n        self.save_hyperparameters(ignore=['c'])\n        print('self.a =', self.a, 'self.b =', self.b)\n        print('There is no self.c =', not hasattr(self, 'c'))\n\nb = B(a=1, b=2, c=3)\n```\n\nThe final utility allows us to plot experiment progress interactively while it is going on. In deference to the much more powerful (and complex) [TensorBoard](https://www.tensorflow.org/tensorboard) we name it `ProgressBoard`. The  implementation is deferred to :numref:`sec_utils`. For now, let's simply see it in action. The `draw` method plots a point `(x, y)` in the figure, with `label` specified in the legend. The optional `every_n` smooths the line by only showing $1/n$ points in the figure. Their values are averaged from the $n$ neighbor points in the original figure. ```{.python .input}\n%%tab all\nclass ProgressBoard(d2l.HyperParameters):  #@save\n    \"\"\"The board that plots data points in animation.\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 ls=['-', '--', '-.', ':'], colors=['C0', 'C1', 'C2', 'C3'],\n                 fig=None, axes=None, figsize=(3.5, 2.5), display=True):\n        self.save_hyperparameters()\n\n    def draw(self, x, y, label, every_n=1):\n        raise NotImplemented\n```\n\nIn the following example, we draw `sin` and `cos` with a different smoothness."
    },
    {
      "chunk_id": "904df5fe2158_2",
      "chapter": "oo-design",
      "heading": "Utilities",
      "text": "If you run this code block, you will see the lines grow in animation. ```{.python .input}\n%%tab all\nboard = d2l.ProgressBoard('x')\nfor x in np.arange(0, 10, 0.1):\n    board.draw(x, np.sin(x), 'sin', every_n=2)\n    board.draw(x, np.cos(x), 'cos', every_n=10)\n```"
    },
    {
      "chunk_id": "1af3d3ed9468_0",
      "chapter": "oo-design",
      "heading": "Models",
      "text": ":label:`subsec_oo-design-models`\n\nThe `Module` class is the base class of all models we will implement. At the very least we need three methods. The first, `__init__`, stores the learnable parameters, the `training_step` method accepts a data batch to return the loss value, and finally, `configure_optimizers` returns the optimization method, or a list of them, that is used to update the learnable parameters. Optionally we can define `validation_step` to report the evaluation measures. Sometimes we put the code for computing the output into a separate `forward` method to make it more reusable. :begin_tab:`jax`\nWith the introduction of [dataclasses](https://docs.python.org/3/library/dataclasses.html)\nin Python 3.7, classes decorated with `@dataclass` automatically add magic\nmethods such as `__init__` and `__repr__`. The member variables are defined\nusing type annotations. All Flax modules are Python 3.7 dataclasses."
    },
    {
      "chunk_id": "1af3d3ed9468_1",
      "chapter": "oo-design",
      "heading": "Models",
      "text": "The member variables are defined\nusing type annotations. All Flax modules are Python 3.7 dataclasses. :end_tab:\n\n```{.python .input}\n%%tab pytorch\nclass Module(d2l.nn_Module, d2l.HyperParameters):  #@save\n    \"\"\"The base class of models.\"\"\"\n    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n        super().__init__()\n        self.save_hyperparameters()\n        self.board = ProgressBoard()\n\n    def loss(self, y_hat, y):\n        raise NotImplementedError\n\n    def forward(self, X):\n        assert hasattr(self, 'net'), 'Neural network is defined'\n        return self.net(X)\n\n    def plot(self, key, value, train):\n        \"\"\"Plot a point in animation.\"\"\"\n        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n        self.board.xlabel = 'epoch'\n        if train:\n            x = self.trainer.train_batch_idx / \\\n                self.trainer.num_train_batches\n            n = self.trainer.num_train_batches / \\\n                self.plot_train_per_epoch\n        else:\n            x = self.trainer.epoch + 1\n            n = self.trainer.num_val_batches / \\\n                self.plot_valid_per_epoch\n        self.board.draw(x, d2l.numpy(d2l.to(value, d2l.cpu())),\n                        ('train_' if train else 'val_') + key,\n                        every_n=int(n))\n\n    def training_step(self, batch):\n        l = self.loss(self(*batch[:-1]), batch[-1])\n        self.plot('loss', l, train=True)\n        return l\n\n    def validation_step(self, batch):\n        l = self.loss(self(*batch[:-1]), batch[-1])\n        self.plot('loss', l, train=False)\n\n    def configure_optimizers(self):\n        raise NotImplementedError\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow, jax\nclass Module(d2l.nn_Module, d2l.HyperParameters):  #@save\n    \"\"\"The base class of models.\"\"\"\n    if tab.selected('mxnet', 'tensorflow'):\n        def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n            super().__init__()\n            self.save_hyperparameters()\n            self.board = ProgressBoard()\n        if tab.selected('tensorflow'):\n            self.training = None\n\n    if tab.selected('jax'):\n        # No need for save_hyperparam when using Python dataclass\n        plot_train_per_epoch: int = field(default=2, init=False)\n        plot_valid_per_epoch: int = field(default=1, init=False)\n        # Use default_factory to make sure new plots are generated on each run\n        board: ProgressBoard = field(default_factory=lambda: ProgressBoard(),\n                                     init=False)\n\n    def loss(self, y_hat, y):\n        raise NotImplementedError\n\n    if tab.selected('mxnet', 'tensorflow'):\n        def forward(self, X):\n            assert hasattr(self, 'net'), 'Neural network is defined'\n            return self.net(X)\n\n    if tab.selected('tensorflow'):\n        def call(self, X, *args, **kwargs):\n            if kwargs and \"training\" in kwargs:\n                self.training = kwargs['training']\n            return self.forward(X, *args)\n\n    if tab.selected('jax'):\n        # JAX & Flax do not have a forward-method-like syntax."
    },
    {
      "chunk_id": "1af3d3ed9468_2",
      "chapter": "oo-design",
      "heading": "Models",
      "text": "Flax uses setup\n        # and built-in __call__ magic methods for forward pass."
    },
    {
      "chunk_id": "1af3d3ed9468_3",
      "chapter": "oo-design",
      "heading": "Models",
      "text": "Flax uses setup\n        # and built-in __call__ magic methods for forward pass. Adding here\n        # for consistency\n        def forward(self, X, *args, **kwargs):\n            assert hasattr(self, 'net'), 'Neural network is defined'\n            return self.net(X, *args, **kwargs)\n\n        def __call__(self, X, *args, **kwargs):\n            return self.forward(X, *args, **kwargs)\n\n    def plot(self, key, value, train):\n        \"\"\"Plot a point in animation.\"\"\"\n        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n        self.board.xlabel = 'epoch'\n        if train:\n            x = self.trainer.train_batch_idx / \\\n                self.trainer.num_train_batches\n            n = self.trainer.num_train_batches / \\\n                self.plot_train_per_epoch\n        else:\n            x = self.trainer.epoch + 1\n            n = self.trainer.num_val_batches / \\\n                self.plot_valid_per_epoch\n        if tab.selected('mxnet', 'tensorflow'):\n            self.board.draw(x, d2l.numpy(value), (\n                'train_' if train else 'val_') + key, every_n=int(n))\n        if tab.selected('jax'):\n            self.board.draw(x, d2l.to(value, d2l.cpu()),\n                            ('train_' if train else 'val_') + key,\n                            every_n=int(n))\n\n    if tab.selected('mxnet', 'tensorflow'):\n        def training_step(self, batch):\n            l = self.loss(self(*batch[:-1]), batch[-1])\n            self.plot('loss', l, train=True)\n            return l\n\n        def validation_step(self, batch):\n            l = self.loss(self(*batch[:-1]), batch[-1])\n            self.plot('loss', l, train=False)\n\n    if tab.selected('jax'):\n        def training_step(self, params, batch, state):\n            l, grads = jax.value_and_grad(self.loss)(params, batch[:-1],\n                                                     batch[-1], state)\n            self.plot(\"loss\", l, train=True)\n            return l, grads\n\n        def validation_step(self, params, batch, state):\n            l = self.loss(params, batch[:-1], batch[-1], state)\n            self.plot('loss', l, train=False)\n        \n        def apply_init(self, dummy_input, key):\n            \"\"\"To be defined later in :numref:`sec_lazy_init`\"\"\"\n            raise NotImplementedError\n\n    def configure_optimizers(self):\n        raise NotImplementedError\n```\n\n:begin_tab:`mxnet`\nYou may notice that `Module` is a subclass of `nn.Block`, the base class of neural networks in Gluon."
    },
    {
      "chunk_id": "1af3d3ed9468_4",
      "chapter": "oo-design",
      "heading": "Models",
      "text": "It provides convenient features for handling neural networks. For example, if we define a `forward` method, such as `forward(self, X)`, then for an instance `a` we can invoke this method by `a(X)`. This works since it calls the `forward` method in the built-in `__call__` method. You can find more details and examples about `nn.Block` in :numref:`sec_model_construction`. :end_tab:\n\n:begin_tab:`pytorch`\nYou may notice that `Module` is a subclass of `nn.Module`, the base class of neural networks in PyTorch. It provides convenient features for handling neural networks. For example, if we define a `forward` method, such as `forward(self, X)`, then for an instance `a` we can invoke this method by `a(X)`. This works since it calls the `forward` method in the built-in `__call__` method. You can find more details and examples about `nn.Module` in :numref:`sec_model_construction`. :end_tab:\n\n:begin_tab:`tensorflow`\nYou may notice that `Module` is a subclass of `tf.keras.Model`, the base class of neural networks in TensorFlow. It provides convenient features for handling neural networks. For example, it invokes the `call` method in the built-in `__call__` method. Here we redirect `call` to the `forward` method, saving its arguments as a class attribute. We do this to make our code more similar to other framework implementations. :end_tab:\n\n:begin_tab:`jax`\nYou may notice that `Module` is a subclass of `linen.Module`, the base class of neural networks in Flax. It provides convenient features for handling neural networks. For example, it handles the model parameters, provides the `nn.compact` decorator to simplify code, invokes the `__call__` method among other things. Here we also redirect `__call__` to the `forward` method. We do this to make our code more similar to other framework implementations. :end_tab:"
    },
    {
      "chunk_id": "a6b593dab45e_0",
      "chapter": "oo-design",
      "heading": "Data",
      "text": ":label:`oo-design-data`\n\nThe `DataModule` class is the base class for data. Quite frequently the `__init__` method is used to prepare the data. This includes downloading and preprocessing if needed. The `train_dataloader` returns the data loader for the training dataset. A data loader is a (Python) generator that yields a data batch each time it is used. This batch is then fed into the `training_step` method of `Module` to compute the loss. There is an optional `val_dataloader` to return the validation dataset loader. It behaves in the same manner, except that it yields data batches for the `validation_step` method in `Module`.\n\n```{.python .input}\n%%tab all\nclass DataModule(d2l.HyperParameters):  #@save\n    \"\"\"The base class of data.\"\"\"\n    if tab.selected('mxnet', 'pytorch'):\n        def __init__(self, root='../data', num_workers=4):\n            self.save_hyperparameters()\n\n    if tab.selected('tensorflow', 'jax'):\n        def __init__(self, root='../data'):\n            self.save_hyperparameters()\n\n    def get_dataloader(self, train):\n        raise NotImplementedError\n\n    def train_dataloader(self):\n        return self.get_dataloader(train=True)\n\n    def val_dataloader(self):\n        return self.get_dataloader(train=False)\n```"
    },
    {
      "chunk_id": "e5866675d39a_0",
      "chapter": "oo-design",
      "heading": "Training",
      "text": ":label:`oo-design-training`\n\n:begin_tab:`pytorch, mxnet, tensorflow`\nThe `Trainer` class trains the learnable parameters in the `Module` class with data specified in `DataModule`. The key method is `fit`, which accepts two arguments: `model`, an instance of `Module`, and `data`, an instance of `DataModule`. It then iterates over the entire dataset `max_epochs` times to train the model. As before, we will defer the implementation of this method to later chapters. :end_tab:\n\n:begin_tab:`jax`\nThe `Trainer` class trains the learnable parameters `params` with data specified in `DataModule`. The key method is `fit`, which accepts three arguments: `model`, an instance of `Module`, `data`, an instance of `DataModule`, and `key`, a JAX `PRNGKeyArray`. We make the `key` argument optional here to simplify the interface, but it is recommended to always pass and initialize the model parameters with a root key in JAX and Flax. It then iterates over the entire dataset `max_epochs` times to train the model. As before, we will defer the implementation of this method to later chapters."
    },
    {
      "chunk_id": "e5866675d39a_1",
      "chapter": "oo-design",
      "heading": "Training",
      "text": "It then iterates over the entire dataset `max_epochs` times to train the model. As before, we will defer the implementation of this method to later chapters. :end_tab:\n\n```{.python .input}\n%%tab all\nclass Trainer(d2l.HyperParameters):  #@save\n    \"\"\"The base class for training models with data.\"\"\"\n    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n        self.save_hyperparameters()\n        assert num_gpus == 0, 'No GPU support yet'\n\n    def prepare_data(self, data):\n        self.train_dataloader = data.train_dataloader()\n        self.val_dataloader = data.val_dataloader()\n        self.num_train_batches = len(self.train_dataloader)\n        self.num_val_batches = (len(self.val_dataloader)\n                                if self.val_dataloader is not None else 0)\n\n    def prepare_model(self, model):\n        model.trainer = self\n        model.board.xlim = [0, self.max_epochs]\n        self.model = model\n\n    if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n        def fit(self, model, data):\n            self.prepare_data(data)\n            self.prepare_model(model)\n            self.optim = model.configure_optimizers()\n            self.epoch = 0\n            self.train_batch_idx = 0\n            self.val_batch_idx = 0\n            for self.epoch in range(self.max_epochs):\n                self.fit_epoch()\n\n    if tab.selected('jax'):\n        def fit(self, model, data, key=None):\n            self.prepare_data(data)\n            self.prepare_model(model)\n            self.optim = model.configure_optimizers()\n\n            if key is None:\n                root_key = d2l.get_key()\n            else:\n                root_key = key\n            params_key, dropout_key = jax.random.split(root_key)\n            key = {'params': params_key, 'dropout': dropout_key}\n\n            dummy_input = next(iter(self.train_dataloader))[:-1]\n            variables = model.apply_init(dummy_input, key=key)\n            params = variables['params']\n\n            if 'batch_stats' in variables.keys():\n                # Here batch_stats will be used later (e.g., for batch norm)\n                batch_stats = variables['batch_stats']\n            else:\n                batch_stats = {}\n\n            # Flax uses optax under the hood for a single state obj TrainState."
    },
    {
      "chunk_id": "e5866675d39a_2",
      "chapter": "oo-design",
      "heading": "Training",
      "text": "# More will be discussed later in the dropout and batch\n            # normalization section\n            class TrainState(train_state.TrainState):\n                batch_stats: Any\n                dropout_rng: jax.random.PRNGKeyArray\n\n            self.state = TrainState.create(apply_fn=model.apply,\n                                           params=params,\n                                           batch_stats=batch_stats,\n                                           dropout_rng=dropout_key,\n                                           tx=model.configure_optimizers())\n            self.epoch = 0\n            self.train_batch_idx = 0\n            self.val_batch_idx = 0\n            for self.epoch in range(self.max_epochs):\n                self.fit_epoch()\n\n    def fit_epoch(self):\n        raise NotImplementedError\n```"
    },
    {
      "chunk_id": "d08fe8ed4d25_0",
      "chapter": "oo-design",
      "heading": "Summary",
      "text": "To highlight the object-oriented design\nfor our future deep learning implementation,\nthe above classes simply show how their objects \nstore data and interact with each other.\nWe will keep enriching implementations of these classes,\nsuch as via `@add_to_class`,\nin the rest of the book.\nMoreover,\nthese fully implemented classes\nare saved in the [D2L library](https://github.com/d2l-ai/d2l-en/tree/master/d2l),\na *lightweight toolkit* that makes structured modeling for deep learning easy. \nIn particular, it facilitates reusing many components between projects without changing much at all. For instance, we can replace just the optimizer, just the model, just the dataset, etc.;\nthis degree of modularity pays dividends throughout the book in terms of conciseness and simplicity (this is why we added it) and it can do the same for your own projects."
    },
    {
      "chunk_id": "4fca8d743518_0",
      "chapter": "oo-design",
      "heading": "Exercises",
      "text": "1. Locate full implementations of the above classes that are saved in the [D2L library](https://github.com/d2l-ai/d2l-en/tree/master/d2l). We strongly recommend that you look at the implementation in detail once you have gained some more familiarity with deep learning modeling.\n1. Remove the `save_hyperparameters` statement in the `B` class. Can you still print `self.a` and `self.b`? Optional: if you have dived into the full implementation of the `HyperParameters` class, can you explain why?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/6645)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/6646)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/6647)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17974)\n:end_tab:"
    },
    {
      "chunk_id": "26c5eb38f530_0",
      "chapter": "synthetic-regression-data",
      "heading": "synthetic-regression-data",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Synthetic Regression Data\n:label:`sec_synthetic-regression-data`\n\n\nMachine learning is all about extracting information from data.\nSo you might wonder, what could we possibly learn from synthetic data?\nWhile we might not care intrinsically about the patterns \nthat we ourselves baked into an artificial data generating model,\nsuch datasets are nevertheless useful for didactic purposes,\nhelping us to evaluate the properties of our learning \nalgorithms and to confirm that our implementations work as expected.\nFor example, if we create data for which the correct parameters are known *a priori*,\nthen we can check that our model can in fact recover them.\n\n```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx, gluon\nimport random\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nimport random\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport random\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport random\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n```"
    },
    {
      "chunk_id": "12a398e9c1b2_0",
      "chapter": "synthetic-regression-data",
      "heading": "Generating the Dataset",
      "text": "For this example, we will work in low dimension\nfor succinctness. The following code snippet generates 1000 examples\nwith 2-dimensional features drawn \nfrom a standard normal distribution. The resulting design matrix $\\mathbf{X}$\nbelongs to $\\mathbb{R}^{1000 \\times 2}$. We generate each label by applying \na *ground truth* linear function, \ncorrupting them via additive noise $\\boldsymbol{\\epsilon}$, \ndrawn independently and identically for each example:\n\n(**$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\boldsymbol{\\epsilon}.$$**)\n\nFor convenience we assume that $\\boldsymbol{\\epsilon}$ is drawn \nfrom a normal distribution with mean $\\mu= 0$ \nand standard deviation $\\sigma = 0.01$. Note that for object-oriented design\nwe add the code to the `__init__` method of a subclass of `d2l.DataModule` (introduced in :numref:`oo-design-data`). It is good practice to allow the setting of any additional hyperparameters. We accomplish this with `save_hyperparameters()`. The `batch_size` will be determined later."
    },
    {
      "chunk_id": "12a398e9c1b2_1",
      "chapter": "synthetic-regression-data",
      "heading": "Generating the Dataset",
      "text": "It is good practice to allow the setting of any additional hyperparameters. We accomplish this with `save_hyperparameters()`. The `batch_size` will be determined later. ```{.python .input}\n%%tab all\nclass SyntheticRegressionData(d2l.DataModule):  #@save\n    \"\"\"Synthetic data for linear regression.\"\"\"\n    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, \n                 batch_size=32):\n        super().__init__()\n        self.save_hyperparameters()\n        n = num_train + num_val\n        if tab.selected('pytorch') or tab.selected('mxnet'):                \n            self.X = d2l.randn(n, len(w))\n            noise = d2l.randn(n, 1) * noise\n        if tab.selected('tensorflow'):\n            self.X = tf.random.normal((n, w.shape[0]))\n            noise = tf.random.normal((n, 1)) * noise\n        if tab.selected('jax'):\n            key = jax.random.PRNGKey(0)\n            key1, key2 = jax.random.split(key)\n            self.X = jax.random.normal(key1, (n, w.shape[0]))\n            noise = jax.random.normal(key2, (n, 1)) * noise\n        self.y = d2l.matmul(self.X, d2l.reshape(w, (-1, 1))) + b + noise\n```\n\nBelow, we set the true parameters to $\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$. Later, we can check our estimated parameters against these *ground truth* values. ```{.python .input}\n%%tab all\ndata = SyntheticRegressionData(w=d2l.tensor([2, -3.4]), b=4.2)\n```\n\n[**Each row in `features` consists of a vector in $\\mathbb{R}^2$ and each row in `labels` is a scalar.**] Let's have a look at the first entry. ```{.python .input}\n%%tab all\nprint('features:', data.X[0],'\\nlabel:', data.y[0])\n```"
    },
    {
      "chunk_id": "2fe14b209012_0",
      "chapter": "synthetic-regression-data",
      "heading": "Reading the Dataset",
      "text": "Training machine learning models often requires multiple passes over a dataset, \ngrabbing one minibatch of examples at a time. This data is then used to update the model. To illustrate how this works, we \n[**implement the `get_dataloader` method,**] \nregistering it in the `SyntheticRegressionData` class via `add_to_class` (introduced in :numref:`oo-design-utilities`). It (**takes a batch size, a matrix of features,\nand a vector of labels, and generates minibatches of size `batch_size`.**)\nAs such, each minibatch consists of a tuple of features and labels. Note that we need to be mindful of whether we're in training or validation mode: \nin the former, we will want to read the data in random order, \nwhereas for the latter, being able to read data in a pre-defined order \nmay be important for debugging purposes. ```{.python .input}\n%%tab all\n@d2l.add_to_class(SyntheticRegressionData)\ndef get_dataloader(self, train):\n    if train:\n        indices = list(range(0, self.num_train))\n        # The examples are read in random order\n        random.shuffle(indices)\n    else:\n        indices = list(range(self.num_train, self.num_train+self.num_val))\n    for i in range(0, len(indices), self.batch_size):\n        if tab.selected('mxnet', 'pytorch', 'jax'):\n            batch_indices = d2l.tensor(indices[i: i+self.batch_size])\n            yield self.X[batch_indices], self.y[batch_indices]\n        if tab.selected('tensorflow'):\n            j = tf.constant(indices[i : i+self.batch_size])\n            yield tf.gather(self.X, j), tf.gather(self.y, j)\n```\n\nTo build some intuition, let's inspect the first minibatch of\ndata. Each minibatch of features provides us with both its size and the dimensionality of input features. Likewise, our minibatch of labels will have a matching shape given by `batch_size`."
    },
    {
      "chunk_id": "2fe14b209012_1",
      "chapter": "synthetic-regression-data",
      "heading": "Reading the Dataset",
      "text": "Each minibatch of features provides us with both its size and the dimensionality of input features. Likewise, our minibatch of labels will have a matching shape given by `batch_size`. ```{.python .input}\n%%tab all\nX, y = next(iter(data.train_dataloader()))\nprint('X shape:', X.shape, '\\ny shape:', y.shape)\n```\n\nWhile seemingly innocuous, the invocation \nof `iter(data.train_dataloader())` \nillustrates the power of Python's object-oriented design. Note that we added a method to the `SyntheticRegressionData` class\n*after* creating the `data` object. Nonetheless, the object benefits from \nthe *ex post facto* addition of functionality to the class. Throughout the iteration we obtain distinct minibatches\nuntil the entire dataset has been exhausted (try this). While the iteration implemented above is good for didactic purposes,\nit is inefficient in ways that might get us into trouble with real problems. For example, it requires that we load all the data in memory\nand that we perform lots of random memory access. The built-in iterators implemented in a deep learning framework\nare considerably more efficient and they can deal\nwith sources such as data stored in files, \ndata received via a stream, \nand data generated or processed on the fly. Next let's try to implement the same method using built-in iterators."
    },
    {
      "chunk_id": "c66d2d723627_0",
      "chapter": "synthetic-regression-data",
      "heading": "Concise Implementation of the Data Loader",
      "text": "Rather than writing our own iterator,\nwe can [**call the existing API in a framework to load data.**]\nAs before, we need a dataset with features `X` and labels `y`. Beyond that, we set `batch_size` in the built-in data loader \nand let it take care of shuffling examples  efficiently. :begin_tab:`jax`\nJAX is all about NumPy like API with device acceleration and the functional\ntransformations, so at least the current version doesn\u2019t include data loading\nmethods. With other  libraries we already have great data loaders out there,\nand JAX suggests using them instead. Here we will grab TensorFlow\u2019s data loader,\nand modify it slightly to make it work with JAX. :end_tab:\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(d2l.DataModule)  #@save\ndef get_tensorloader(self, tensors, train, indices=slice(0, None)):\n    tensors = tuple(a[indices] for a in tensors)\n    if tab.selected('mxnet'):\n        dataset = gluon.data.ArrayDataset(*tensors)\n        return gluon.data.DataLoader(dataset, self.batch_size,\n                                     shuffle=train)\n    if tab.selected('pytorch'):\n        dataset = torch.utils.data.TensorDataset(*tensors)\n        return torch.utils.data.DataLoader(dataset, self.batch_size,\n                                           shuffle=train)\n    if tab.selected('jax'):\n        # Use Tensorflow Datasets & Dataloader."
    },
    {
      "chunk_id": "c66d2d723627_1",
      "chapter": "synthetic-regression-data",
      "heading": "Concise Implementation of the Data Loader",
      "text": "JAX or Flax do not provide\n        # any dataloading functionality\n        shuffle_buffer = tensors[0].shape[0] if train else 1\n        return tfds.as_numpy(\n            tf.data.Dataset.from_tensor_slices(tensors).shuffle(\n                buffer_size=shuffle_buffer).batch(self.batch_size))\n\n    if tab.selected('tensorflow'):\n        shuffle_buffer = tensors[0].shape[0] if train else 1\n        return tf.data.Dataset.from_tensor_slices(tensors).shuffle(\n            buffer_size=shuffle_buffer).batch(self.batch_size)\n```\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(SyntheticRegressionData)  #@save\ndef get_dataloader(self, train):\n    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n    return self.get_tensorloader((self.X, self.y), train, i)\n```\n\nThe new data loader behaves just like the previous one, except that it is more efficient and has some added functionality. ```{.python .input  n=4}\n%%tab all\nX, y = next(iter(data.train_dataloader()))\nprint('X shape:', X.shape, '\\ny shape:', y.shape)\n```\n\nFor instance, the data loader provided by the framework API \nsupports the built-in `__len__` method, \nso we can query its length, \ni.e., the number of batches. ```{.python .input}\n%%tab all\nlen(data.train_dataloader())\n```"
    },
    {
      "chunk_id": "e4ae2c01af33_0",
      "chapter": "synthetic-regression-data",
      "heading": "Summary",
      "text": "Data loaders are a convenient way of abstracting out \nthe process of loading and manipulating data. \nThis way the same machine learning *algorithm* \nis capable of processing many different types and sources of data \nwithout the need for modification. \nOne of the nice things about data loaders \nis that they can be composed. \nFor instance, we might be loading images \nand then have a postprocessing filter \nthat crops them or modifies them in other ways. \nAs such, data loaders can be used \nto describe an entire data processing pipeline. \n\nAs for the model itself, the two-dimensional linear model \nis about the simplest we might encounter. \nIt lets us test out the accuracy of regression models \nwithout worrying about having insufficient amounts of data \nor an underdetermined system of equations. \nWe will put this to good use in the next section."
    },
    {
      "chunk_id": "f3c6ac297369_0",
      "chapter": "synthetic-regression-data",
      "heading": "Exercises",
      "text": "1. What will happen if the number of examples cannot be divided by the batch size. How would you change this behavior by specifying a different argument by using the framework's API?\n1. Suppose that we want to generate a huge dataset, where both the size of the parameter vector `w` and the number of examples `num_examples` are large.\n    1. What happens if we cannot hold all data in memory?\n    1. How would you shuffle the data if it is held on disk? Your task is to design an *efficient* algorithm that does not require too many random reads or writes. Hint: [pseudorandom permutation generators](https://en.wikipedia.org/wiki/Pseudorandom_permutation) allow you to design a reshuffle without the need to store the permutation table explicitly :cite:`Naor.Reingold.1999`. \n1. Implement a data generator that produces new data on the fly, every time the iterator is called. \n1. How would you design a random data generator that generates *the same* data each time it is called?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/6662)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/6663)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/6664)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17975)\n:end_tab:"
    },
    {
      "chunk_id": "75a9d9634d66_0",
      "chapter": "weight-decay",
      "heading": "weight-decay",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Weight Decay\n:label:`sec_weight_decay`\n\nNow that we have characterized the problem of overfitting,\nwe can introduce our first *regularization* technique. Recall that we can always mitigate overfitting\nby collecting more training data. However, that can be costly, time consuming,\nor entirely out of our control,\nmaking it impossible in the short run. For now, we can assume that we already have\nas much high-quality data as our resources permit\nand focus the tools at our disposal\nwhen the dataset is taken as a given. Recall that in our polynomial regression example\n(:numref:`subsec_polynomial-curve-fitting`)\nwe could limit our model's capacity\nby tweaking the degree\nof the fitted polynomial. Indeed, limiting the number of features\nis a popular technique for mitigating overfitting. However, simply tossing aside features\ncan be too blunt an instrument. Sticking with the polynomial regression\nexample, consider what might happen\nwith high-dimensional input. The natural extensions of polynomials\nto multivariate data are called *monomials*,\nwhich are simply products of powers of variables. The degree of a monomial is the sum of the powers. For example, $x_1^2 x_2$, and $x_3 x_5^2$\nare both monomials of degree 3. Note that the number of terms with degree $d$\nblows up rapidly as $d$ grows larger. Given $k$ variables, the number of monomials\nof degree $d$ is ${k - 1 + d} \\choose {k - 1}$. Even small changes in degree, say from $2$ to $3$,\ndramatically increase the complexity of our model. Thus we often need a more fine-grained tool\nfor adjusting function complexity."
    },
    {
      "chunk_id": "75a9d9634d66_1",
      "chapter": "weight-decay",
      "heading": "weight-decay",
      "text": "Even small changes in degree, say from $2$ to $3$,\ndramatically increase the complexity of our model. Thus we often need a more fine-grained tool\nfor adjusting function complexity. ```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nimport optax\n```"
    },
    {
      "chunk_id": "c73dac8fcfe5_0",
      "chapter": "weight-decay",
      "heading": "Norms and Weight Decay",
      "text": "(**Rather than directly manipulating the number of parameters,\n*weight decay*, operates by restricting the values \nthat the parameters can take.**)\nMore commonly called $\\ell_2$ regularization\noutside of deep learning circles\nwhen optimized by minibatch stochastic gradient descent,\nweight decay might be the most widely used technique\nfor regularizing parametric machine learning models. The technique is motivated by the basic intuition\nthat among all functions $f$,\nthe function $f = 0$\n(assigning the value $0$ to all inputs)\nis in some sense the *simplest*,\nand that we can measure the complexity\nof a function by the distance of its parameters from zero. But how precisely should we measure\nthe distance between a function and zero? There is no single right answer. In fact, entire branches of mathematics,\nincluding parts of functional analysis\nand the theory of Banach spaces,\nare devoted to addressing such issues. One simple interpretation might be\nto measure the complexity of a linear function\n$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$\nby some norm of its weight vector, e.g., $\\| \\mathbf{w} \\|^2$. Recall that we introduced the $\\ell_2$ norm and $\\ell_1$ norm,\nwhich are special cases of the more general $\\ell_p$ norm,\nin :numref:`subsec_lin-algebra-norms`. The most common method for ensuring a small weight vector\nis to add its norm as a penalty term\nto the problem of minimizing the loss. Thus we replace our original objective,\n*minimizing the prediction loss on the training labels*,\nwith new objective,\n*minimizing the sum of the prediction loss and the penalty term*. Now, if our weight vector grows too large,\nour learning algorithm might focus\non minimizing the weight norm $\\| \\mathbf{w} \\|^2$\nrather than minimizing the training error. That is exactly what we want. To illustrate things in code,\nwe revive our previous example\nfrom :numref:`sec_linear_regression` for linear regression."
    },
    {
      "chunk_id": "c73dac8fcfe5_1",
      "chapter": "weight-decay",
      "heading": "Norms and Weight Decay",
      "text": "That is exactly what we want. To illustrate things in code,\nwe revive our previous example\nfrom :numref:`sec_linear_regression` for linear regression. There, our loss was given by\n\n$$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n\nRecall that $\\mathbf{x}^{(i)}$ are the features,\n$y^{(i)}$ is the label for any data example $i$, and $(\\mathbf{w}, b)$\nare the weight and bias parameters, respectively. To penalize the size of the weight vector,\nwe must somehow add $\\| \\mathbf{w} \\|^2$ to the loss function,\nbut how should the model trade off the\nstandard loss for this new additive penalty? In practice, we characterize this trade-off\nvia the *regularization constant* $\\lambda$,\na nonnegative hyperparameter\nthat we fit using validation data:\n\n$$L(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2.$$\n\n\nFor $\\lambda = 0$, we recover our original loss function. For $\\lambda > 0$, we restrict the size of $\\| \\mathbf{w} \\|$. We divide by $2$ by convention:\nwhen we take the derivative of a quadratic function,\nthe $2$ and $1/2$ cancel out, ensuring that the expression\nfor the update looks nice and simple. The astute reader might wonder why we work with the squared\nnorm and not the standard norm (i.e., the Euclidean distance). We do this for computational convenience. By squaring the $\\ell_2$ norm, we remove the square root,\nleaving the sum of squares of\neach component of the weight vector. This makes the derivative of the penalty easy to compute: \nthe sum of derivatives equals the derivative of the sum. Moreover, you might ask why we work with the $\\ell_2$ norm\nin the first place and not, say, the $\\ell_1$ norm. In fact, other choices are valid and\npopular throughout statistics. While $\\ell_2$-regularized linear models constitute\nthe classic *ridge regression* algorithm,\n$\\ell_1$-regularized linear regression\nis a similarly fundamental method in statistics, \npopularly known as *lasso regression*."
    },
    {
      "chunk_id": "c73dac8fcfe5_2",
      "chapter": "weight-decay",
      "heading": "Norms and Weight Decay",
      "text": "While $\\ell_2$-regularized linear models constitute\nthe classic *ridge regression* algorithm,\n$\\ell_1$-regularized linear regression\nis a similarly fundamental method in statistics, \npopularly known as *lasso regression*. One reason to work with the $\\ell_2$ norm\nis that it places an outsize penalty\non large components of the weight vector. This biases our learning algorithm\ntowards models that distribute weight evenly\nacross a larger number of features. In practice, this might make them more robust\nto measurement error in a single variable. By contrast, $\\ell_1$ penalties lead to models\nthat concentrate weights on a small set of features\nby clearing the other weights to zero. This gives us an effective method for *feature selection*,\nwhich may be desirable for other reasons. For example, if our model only relies on a few features,\nthen we may not need to collect, store, or transmit data\nfor the other (dropped) features. Using the same notation in :eqref:`eq_linreg_batch_update`,\nminibatch stochastic gradient descent updates\nfor $\\ell_2$-regularized regression as follows:\n\n$$\\begin{aligned}\n\\mathbf{w} & \\leftarrow \\left(1- \\eta\\lambda \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$\n\nAs before, we update $\\mathbf{w}$ based on the amount\nby which our estimate differs from the observation. However, we also shrink the size of $\\mathbf{w}$ towards zero. That is why the method is sometimes called \"weight decay\":\ngiven the penalty term alone,\nour optimization algorithm *decays*\nthe weight at each step of training. In contrast to feature selection,\nweight decay offers us a mechanism for continuously adjusting the complexity of a function. Smaller values of $\\lambda$ correspond\nto less constrained $\\mathbf{w}$,\nwhereas larger values of $\\lambda$\nconstrain $\\mathbf{w}$ more considerably."
    },
    {
      "chunk_id": "c73dac8fcfe5_3",
      "chapter": "weight-decay",
      "heading": "Norms and Weight Decay",
      "text": "Smaller values of $\\lambda$ correspond\nto less constrained $\\mathbf{w}$,\nwhereas larger values of $\\lambda$\nconstrain $\\mathbf{w}$ more considerably. Whether we include a corresponding bias penalty $b^2$ \ncan vary across implementations, \nand may vary across layers of a neural network. Often, we do not regularize the bias term. Besides,\nalthough $\\ell_2$ regularization may not be equivalent to weight decay for other optimization algorithms,\nthe idea of regularization through\nshrinking the size of weights\nstill holds true."
    },
    {
      "chunk_id": "cc3425e68504_0",
      "chapter": "weight-decay",
      "heading": "High-Dimensional Linear Regression",
      "text": "We can illustrate the benefits of weight decay \nthrough a simple synthetic example.\n\nFirst, we [**generate some data as before**]:\n\n(**$$y = 0.05 + \\sum_{i = 1}^d 0.01 x_i + \\epsilon \\textrm{ where }\n\\epsilon \\sim \\mathcal{N}(0, 0.01^2).$$**)\n\nIn this synthetic dataset, our label is given \nby an underlying linear function of our inputs,\ncorrupted by Gaussian noise \nwith zero mean and standard deviation 0.01.\nFor illustrative purposes, \nwe can make the effects of overfitting pronounced,\nby increasing the dimensionality of our problem to $d = 200$\nand working with a small training set with only 20 examples.\n\n```{.python .input}\n%%tab all\nclass Data(d2l.DataModule):\n    def __init__(self, num_train, num_val, num_inputs, batch_size):\n        self.save_hyperparameters()                \n        n = num_train + num_val \n        if tab.selected('mxnet') or tab.selected('pytorch'):\n            self.X = d2l.randn(n, num_inputs)\n            noise = d2l.randn(n, 1) * 0.01\n        if tab.selected('tensorflow'):\n            self.X = d2l.normal((n, num_inputs))\n            noise = d2l.normal((n, 1)) * 0.01\n        if tab.selected('jax'):\n            self.X = jax.random.normal(jax.random.PRNGKey(0), (n, num_inputs))\n            noise = jax.random.normal(jax.random.PRNGKey(0), (n, 1)) * 0.01\n        w, b = d2l.ones((num_inputs, 1)) * 0.01, 0.05\n        self.y = d2l.matmul(self.X, w) + b + noise\n\n    def get_dataloader(self, train):\n        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n        return self.get_tensorloader([self.X, self.y], train, i)\n```"
    },
    {
      "chunk_id": "e7807cda2071_0",
      "chapter": "weight-decay",
      "heading": "Implementation from Scratch",
      "text": "Now, let's try implementing weight decay from scratch.\nSince minibatch stochastic gradient descent\nis our optimizer,\nwe just need to add the squared $\\ell_2$ penalty\nto the original loss function."
    },
    {
      "chunk_id": "ef82bf23e68d_0",
      "chapter": "weight-decay",
      "heading": "(**Defining $\\ell_2$ Norm Penalty**)",
      "text": "Perhaps the most convenient way of implementing this penalty\nis to square all terms in place and sum them.\n\n```{.python .input}\n%%tab all\ndef l2_penalty(w):\n    return d2l.reduce_sum(w**2) / 2\n```"
    },
    {
      "chunk_id": "ba1203b7f685_0",
      "chapter": "weight-decay",
      "heading": "Defining the Model",
      "text": "In the final model,\nthe linear regression and the squared loss have not changed since :numref:`sec_linear_scratch`,\nso we will just define a subclass of `d2l.LinearRegressionScratch`. The only change here is that our loss now includes the penalty term.\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass WeightDecayScratch(d2l.LinearRegressionScratch):\n    def __init__(self, num_inputs, lambd, lr, sigma=0.01):\n        super().__init__(num_inputs, lr, sigma)\n        self.save_hyperparameters()\n        \n    def loss(self, y_hat, y):\n        return (super().loss(y_hat, y) +\n                self.lambd * l2_penalty(self.w))\n```\n\n```{.python .input}\n%%tab jax\nclass WeightDecayScratch(d2l.LinearRegressionScratch):\n    lambd: int = 0\n        \n    def loss(self, params, X, y, state):\n        return (super().loss(params, X, y, state) +\n                self.lambd * l2_penalty(params['w']))\n```\n\nThe following code fits our model on the training set with 20 examples and evaluates it on the validation set with 100 examples.\n\n```{.python .input}\n%%tab all\ndata = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)\ntrainer = d2l.Trainer(max_epochs=10)\n\ndef train_scratch(lambd):    \n    model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)\n    model.board.yscale='log'\n    trainer.fit(model, data)\n    if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n        print('L2 norm of w:', float(l2_penalty(model.w)))\n    if tab.selected('jax'):\n        print('L2 norm of w:',\n              float(l2_penalty(trainer.state.params['w'])))\n```"
    },
    {
      "chunk_id": "182814de38f8_0",
      "chapter": "weight-decay",
      "heading": "[**Training without Regularization**]",
      "text": "We now run this code with `lambd = 0`,\ndisabling weight decay.\nNote that we overfit badly,\ndecreasing the training error but not the\nvalidation error---a textbook case of overfitting.\n\n```{.python .input}\n%%tab all\ntrain_scratch(0)\n```"
    },
    {
      "chunk_id": "bff985803ba5_0",
      "chapter": "weight-decay",
      "heading": "[**Using Weight Decay**]",
      "text": "Below, we run with substantial weight decay.\nNote that the training error increases\nbut the validation error decreases.\nThis is precisely the effect\nwe expect from regularization.\n\n```{.python .input}\n%%tab all\ntrain_scratch(3)\n```"
    },
    {
      "chunk_id": "b167345cac1b_0",
      "chapter": "weight-decay",
      "heading": "[**Concise Implementation**]",
      "text": "Because weight decay is ubiquitous\nin neural network optimization,\nthe deep learning framework makes it especially convenient,\nintegrating weight decay into the optimization algorithm itself\nfor easy use in combination with any loss function. Moreover, this integration serves a computational benefit,\nallowing implementation tricks to add weight decay to the algorithm,\nwithout any additional computational overhead. Since the weight decay portion of the update\ndepends only on the current value of each parameter,\nthe optimizer must touch each parameter once anyway. :begin_tab:`mxnet`\nBelow, we specify\nthe weight decay hyperparameter directly\nthrough `wd` when instantiating our `Trainer`. By default, Gluon decays both\nweights and biases simultaneously. Note that the hyperparameter `wd`\nwill be multiplied by `wd_mult`\nwhen updating model parameters. Thus, if we set `wd_mult` to zero,\nthe bias parameter $b$ will not decay. :end_tab:\n\n:begin_tab:`pytorch`\nBelow, we specify\nthe weight decay hyperparameter directly\nthrough `weight_decay` when instantiating our optimizer. By default, PyTorch decays both\nweights and biases simultaneously, but\nwe can configure the optimizer to handle different parameters\naccording to different policies. Here, we only set `weight_decay` for\nthe weights (the `net.weight` parameters), hence the \nbias (the `net.bias` parameter) will not decay. :end_tab:\n\n:begin_tab:`tensorflow`\nBelow, we create an $\\ell_2$ regularizer with\nthe weight decay hyperparameter `wd` and apply it to the layer's weights\nthrough the `kernel_regularizer` argument."
    },
    {
      "chunk_id": "b167345cac1b_1",
      "chapter": "weight-decay",
      "heading": "[**Concise Implementation**]",
      "text": ":end_tab:\n\n:begin_tab:`tensorflow`\nBelow, we create an $\\ell_2$ regularizer with\nthe weight decay hyperparameter `wd` and apply it to the layer's weights\nthrough the `kernel_regularizer` argument. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nclass WeightDecay(d2l.LinearRegression):\n    def __init__(self, wd, lr):\n        super().__init__(lr)\n        self.save_hyperparameters()\n        self.wd = wd\n        \n    def configure_optimizers(self):\n        self.collect_params('.*bias').setattr('wd_mult', 0)\n        return gluon.Trainer(self.collect_params(),\n                             'sgd', \n                             {'learning_rate': self.lr, 'wd': self.wd})\n```\n\n```{.python .input}\n%%tab pytorch\nclass WeightDecay(d2l.LinearRegression):\n    def __init__(self, wd, lr):\n        super().__init__(lr)\n        self.save_hyperparameters()\n        self.wd = wd\n\n    def configure_optimizers(self):\n        return torch.optim.SGD([\n            {'params': self.net.weight, 'weight_decay': self.wd},\n            {'params': self.net.bias}], lr=self.lr)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass WeightDecay(d2l.LinearRegression):\n    def __init__(self, wd, lr):\n        super().__init__(lr)\n        self.save_hyperparameters()\n        self.net = tf.keras.layers.Dense(\n            1, kernel_regularizer=tf.keras.regularizers.l2(wd),\n            kernel_initializer=tf.keras.initializers.RandomNormal(0, 0.01)\n        )\n        \n    def loss(self, y_hat, y):\n        return super().loss(y_hat, y) + self.net.losses\n```\n\n```{.python .input}\n%%tab jax\nclass WeightDecay(d2l.LinearRegression):\n    wd: int = 0\n    \n    def configure_optimizers(self):\n        # Weight Decay is not available directly within optax.sgd, but\n        # optax allows chaining several transformations together\n        return optax.chain(optax.additive_weight_decay(self.wd),\n                           optax.sgd(self.lr))\n```\n\n[**The plot looks similar to that when\nwe implemented weight decay from scratch**]."
    },
    {
      "chunk_id": "b167345cac1b_2",
      "chapter": "weight-decay",
      "heading": "[**Concise Implementation**]",
      "text": "However, this version runs faster\nand is easier to implement,\nbenefits that will become more\npronounced as you address larger problems\nand this work becomes more routine. ```{.python .input}\n%%tab all\nmodel = WeightDecay(wd=3, lr=0.01)\nmodel.board.yscale='log'\ntrainer.fit(model, data)\n\nif tab.selected('jax'):\n    print('L2 norm of w:', float(l2_penalty(model.get_w_b(trainer.state)[0])))\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    print('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))\n```\n\nSo far, we have touched upon one notion of\nwhat constitutes a simple linear function. However, even for simple nonlinear functions, the situation can be much more complex. To see this, the concept of [reproducing kernel Hilbert space (RKHS)](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)\nallows one to apply tools introduced\nfor linear functions in a nonlinear context. Unfortunately, RKHS-based algorithms\ntend to scale poorly to large, high-dimensional data. In this book we will often adopt the common heuristic\nwhereby weight decay is applied\nto all layers of a deep network."
    },
    {
      "chunk_id": "8fc424857500_0",
      "chapter": "weight-decay",
      "heading": "Summary",
      "text": "Regularization is a common method for dealing with overfitting. Classical regularization techniques add a penalty term to the loss function (when training) to reduce the complexity of the learned model.\nOne particular choice for keeping the model simple is using an $\\ell_2$ penalty. This leads to weight decay in the update steps of the minibatch stochastic gradient descent algorithm.\nIn practice, the weight decay functionality is provided in optimizers from deep learning frameworks.\nDifferent sets of parameters can have different update behaviors within the same training loop."
    },
    {
      "chunk_id": "b527fc82856b_0",
      "chapter": "weight-decay",
      "heading": "Exercises",
      "text": "1. Experiment with the value of $\\lambda$ in the estimation problem in this section. Plot training and validation accuracy as a function of $\\lambda$. What do you observe?\n1. Use a validation set to find the optimal value of $\\lambda$. Is it really the optimal value? Does this matter?\n1. What would the update equations look like if instead of $\\|\\mathbf{w}\\|^2$ we used $\\sum_i |w_i|$ as our penalty of choice ($\\ell_1$ regularization)?\n1. We know that $\\|\\mathbf{w}\\|^2 = \\mathbf{w}^\\top \\mathbf{w}$. Can you find a similar equation for matrices (see the Frobenius norm in :numref:`subsec_lin-algebra-norms`)?\n1. Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways might help us deal with overfitting?\n1. In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via $P(w \\mid x) \\propto P(x \\mid w) P(w)$. How can you identify $P(w)$ with regularization?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/98)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/99)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/236)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17979)\n:end_tab:"
    },
    {
      "chunk_id": "19c53eb65036_0",
      "chapter": "backprop",
      "heading": "backprop",
      "text": "# Forward Propagation, Backward Propagation, and Computational Graphs\n:label:`sec_backprop`\n\nSo far, we have trained our models\nwith minibatch stochastic gradient descent.\nHowever, when we implemented the algorithm,\nwe only worried about the calculations involved\nin *forward propagation* through the model.\nWhen it came time to calculate the gradients,\nwe just invoked the backpropagation function provided by the deep learning framework.\n\nThe automatic calculation of gradients\nprofoundly simplifies\nthe implementation of deep learning algorithms.\nBefore automatic differentiation,\neven small changes to complicated models required\nrecalculating complicated derivatives by hand.\nSurprisingly often, academic papers had to allocate\nnumerous pages to deriving update rules.\nWhile we must continue to rely on automatic differentiation\nso we can focus on the interesting parts,\nyou ought to know how these gradients\nare calculated under the hood\nif you want to go beyond a shallow\nunderstanding of deep learning.\n\nIn this section, we take a deep dive\ninto the details of *backward propagation*\n(more commonly called *backpropagation*).\nTo convey some insight for both the\ntechniques and their implementations,\nwe rely on some basic mathematics and computational graphs.\nTo start, we focus our exposition on\na one-hidden-layer MLP\nwith weight decay ($\\ell_2$ regularization, to be described in subsequent chapters)."
    },
    {
      "chunk_id": "576383f7b1df_0",
      "chapter": "backprop",
      "heading": "Forward Propagation",
      "text": "*Forward propagation* (or *forward pass*) refers to the calculation and storage\nof intermediate variables (including outputs)\nfor a neural network in order\nfrom the input layer to the output layer.\nWe now work step-by-step through the mechanics\nof a neural network with one hidden layer.\nThis may seem tedious but in the eternal words\nof funk virtuoso James Brown,\nyou must \"pay the cost to be the boss\".\n\n\nFor the sake of simplicity, let's assume\nthat the input example is $\\mathbf{x}\\in \\mathbb{R}^d$\nand that our hidden layer does not include a bias term.\nHere the intermediate variable is:\n\n$$\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x},$$\n\nwhere $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\nis the weight parameter of the hidden layer.\nAfter running the intermediate variable\n$\\mathbf{z}\\in \\mathbb{R}^h$ through the\nactivation function $\\phi$\nwe obtain our hidden activation vector of length $h$:\n\n$$\\mathbf{h}= \\phi (\\mathbf{z}).$$\n\nThe hidden layer output $\\mathbf{h}$\nis also an intermediate variable.\nAssuming that the parameters of the output layer\npossess only a weight of\n$\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$,\nwe can obtain an output layer variable\nwith a vector of length $q$:\n\n$$\\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h}.$$\n\nAssuming that the loss function is $l$\nand the example label is $y$,\nwe can then calculate the loss term\nfor a single data example,\n\n$$L = l(\\mathbf{o}, y).$$\n\nAs we will see the definition of $\\ell_2$ regularization\nto be introduced later,\ngiven the hyperparameter $\\lambda$,\nthe regularization term is\n\n$$s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_\\textrm{F}^2 + \\|\\mathbf{W}^{(2)}\\|_\\textrm{F}^2\\right),$$\n:eqlabel:`eq_forward-s`\n\nwhere the Frobenius norm of the matrix\nis simply the $\\ell_2$ norm applied\nafter flattening the matrix into a vector.\nFinally, the model's regularized loss\non a given data example is:\n\n$$J = L + s.$$\n\nWe refer to $J$ as the *objective function*\nin the following discussion."
    },
    {
      "chunk_id": "a6213418f5f6_0",
      "chapter": "backprop",
      "heading": "Computational Graph of Forward Propagation",
      "text": "Plotting *computational graphs* helps us visualize\nthe dependencies of operators\nand variables within the calculation.\n:numref:`fig_forward` contains the graph associated\nwith the simple network described above,\nwhere squares denote variables and circles denote operators.\nThe lower-left corner signifies the input\nand the upper-right corner is the output.\nNotice that the directions of the arrows\n(which illustrate data flow)\nare primarily rightward and upward.\n\n![Computational graph of forward propagation.](../img/forward.svg)\n:label:`fig_forward`"
    },
    {
      "chunk_id": "441ec45c18a0_0",
      "chapter": "backprop",
      "heading": "Backpropagation",
      "text": "*Backpropagation* refers to the method of calculating\nthe gradient of neural network parameters. In short, the method traverses the network in reverse order,\nfrom the output to the input layer,\naccording to the *chain rule* from calculus. The algorithm stores any intermediate variables\n(partial derivatives)\nrequired while calculating the gradient\nwith respect to some parameters. Assume that we have functions\n$\\mathsf{Y}=f(\\mathsf{X})$\nand $\\mathsf{Z}=g(\\mathsf{Y})$,\nin which the input and the output\n$\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}$\nare tensors of arbitrary shapes. By using the chain rule,\nwe can compute the derivative\nof $\\mathsf{Z}$ with respect to $\\mathsf{X}$ via\n\n$$\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{X}} = \\textrm{prod}\\left(\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{Y}}, \\frac{\\partial \\mathsf{Y}}{\\partial \\mathsf{X}}\\right).$$\n\nHere we use the $\\textrm{prod}$ operator\nto multiply its arguments\nafter the necessary operations,\nsuch as transposition and swapping input positions,\nhave been carried out. For vectors, this is straightforward:\nit is simply matrix--matrix multiplication. For higher dimensional tensors,\nwe use the appropriate counterpart. The operator $\\textrm{prod}$ hides all the notational overhead. Recall that\nthe parameters of the simple network with one hidden layer,\nwhose computational graph is in :numref:`fig_forward`,\nare $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$. The objective of backpropagation is to\ncalculate the gradients $\\partial J/\\partial \\mathbf{W}^{(1)}$\nand $\\partial J/\\partial \\mathbf{W}^{(2)}$. To accomplish this, we apply the chain rule\nand calculate, in turn, the gradient of\neach intermediate variable and parameter. The order of calculations are reversed\nrelative to those performed in forward propagation,\nsince we need to start with the outcome of the computational graph\nand work our way towards the parameters."
    },
    {
      "chunk_id": "441ec45c18a0_1",
      "chapter": "backprop",
      "heading": "Backpropagation",
      "text": "The order of calculations are reversed\nrelative to those performed in forward propagation,\nsince we need to start with the outcome of the computational graph\nand work our way towards the parameters. The first step is to calculate the gradients\nof the objective function $J=L+s$\nwith respect to the loss term $L$\nand the regularization term $s$:\n\n$$\\frac{\\partial J}{\\partial L} = 1 \\; \\textrm{and} \\; \\frac{\\partial J}{\\partial s} = 1.$$\n\nNext, we compute the gradient of the objective function\nwith respect to variable of the output layer $\\mathbf{o}$\naccording to the chain rule:\n\n$$\n\\frac{\\partial J}{\\partial \\mathbf{o}}\n= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\mathbf{o}}\\right)\n= \\frac{\\partial L}{\\partial \\mathbf{o}}\n\\in \\mathbb{R}^q. $$\n\nNext, we calculate the gradients\nof the regularization term\nwith respect to both parameters:\n\n$$\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}\n\\; \\textrm{and} \\;\n\\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}.$$\n\nNow we are able to calculate the gradient\n$\\partial J/\\partial \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$\nof the model parameters closest to the output layer. Using the chain rule yields:\n\n$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\textrm{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)= \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}.$$\n:eqlabel:`eq_backprop-J-h`\n\nTo obtain the gradient with respect to $\\mathbf{W}^{(1)}$\nwe need to continue backpropagation\nalong the output layer to the hidden layer."
    },
    {
      "chunk_id": "441ec45c18a0_2",
      "chapter": "backprop",
      "heading": "Backpropagation",
      "text": "The gradient with respect to the hidden layer output\n$\\partial J/\\partial \\mathbf{h} \\in \\mathbb{R}^h$ is given by\n\n\n$$\n\\frac{\\partial J}{\\partial \\mathbf{h}}\n= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}}\\right)\n= {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}. $$\n\nSince the activation function $\\phi$ applies elementwise,\ncalculating the gradient $\\partial J/\\partial \\mathbf{z} \\in \\mathbb{R}^h$\nof the intermediate variable $\\mathbf{z}$\nrequires that we use the elementwise multiplication operator,\nwhich we denote by $\\odot$:\n\n$$\n\\frac{\\partial J}{\\partial \\mathbf{z}}\n= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\\right)\n= \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'\\left(\\mathbf{z}\\right). $$\n\nFinally, we can obtain the gradient\n$\\partial J/\\partial \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\nof the model parameters closest to the input layer. According to the chain rule, we get\n\n$$\n\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{z}}, \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}}\\right) + \\textrm{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right)\n= \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}. $$"
    },
    {
      "chunk_id": "4af2b1ae1ba6_0",
      "chapter": "backprop",
      "heading": "Training Neural Networks",
      "text": "When training neural networks,\nforward and backward propagation depend on each other.\nIn particular, for forward propagation,\nwe traverse the computational graph in the direction of dependencies\nand compute all the variables on its path.\nThese are then used for backpropagation\nwhere the compute order on the graph is reversed.\n\nTake the aforementioned simple network as an illustrative example.\nOn the one hand,\ncomputing the regularization term :eqref:`eq_forward-s`\nduring forward propagation\ndepends on the current values of model parameters $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$.\nThey are given by the optimization algorithm according to backpropagation in the most recent iteration.\nOn the other hand,\nthe gradient calculation for the parameter\n:eqref:`eq_backprop-J-h` during backpropagation\ndepends on the current value of the hidden layer output $\\mathbf{h}$,\nwhich is given by forward propagation.\n\n\nTherefore when training neural networks, once model parameters are initialized,\nwe alternate forward propagation with backpropagation,\nupdating model parameters using gradients given by backpropagation.\nNote that backpropagation reuses the stored intermediate values from forward propagation to avoid duplicate calculations.\nOne of the consequences is that we need to retain\nthe intermediate values until backpropagation is complete.\nThis is also one of the reasons why training\nrequires significantly more memory than plain prediction.\nBesides, the size of such intermediate values is roughly\nproportional to the number of network layers and the batch size.\nThus,\ntraining deeper networks using larger batch sizes\nmore easily leads to *out-of-memory* errors."
    },
    {
      "chunk_id": "b88c9ae416f4_0",
      "chapter": "backprop",
      "heading": "Summary",
      "text": "Forward propagation sequentially calculates and stores intermediate variables within the computational graph defined by the neural network. It proceeds from the input to the output layer.\nBackpropagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order.\nWhen training deep learning models, forward propagation and backpropagation are interdependent,\nand training requires significantly more memory than prediction."
    },
    {
      "chunk_id": "24bd9cb5ccc1_0",
      "chapter": "backprop",
      "heading": "Exercises",
      "text": "1. Assume that the inputs $\\mathbf{X}$ to some scalar function $f$ are $n \\times m$ matrices. What is the dimensionality of the gradient of $f$ with respect to $\\mathbf{X}$?\n1. Add a bias to the hidden layer of the model described in this section (you do not need to include bias in the regularization term).\n    1. Draw the corresponding computational graph.\n    1. Derive the forward and backward propagation equations.\n1. Compute the memory footprint for training and prediction in the model described in this section.\n1. Assume that you want to compute second derivatives. What happens to the computational graph? How long do you expect the calculation to take?\n1. Assume that the computational graph is too large for your GPU.\n    1. Can you partition it over more than one GPU?\n    1. What are the advantages and disadvantages over training on a smaller minibatch?\n\n[Discussions](https://discuss.d2l.ai/t/102)"
    },
    {
      "chunk_id": "e181765b4e60_0",
      "chapter": "dropout",
      "heading": "dropout",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Dropout\n:label:`sec_dropout`\n\n\nLet's think briefly about what we\nexpect from a good predictive model. We want it to peform well on unseen data. Classical generalization theory\nsuggests that to close the gap between\ntrain and test performance,\nwe should aim for a simple model. Simplicity can come in the form\nof a small number of dimensions. We explored this when discussing the\nmonomial basis functions of linear models\nin :numref:`sec_generalization_basics`. Additionally, as we saw when discussing weight decay\n($\\ell_2$ regularization) in :numref:`sec_weight_decay`,\nthe (inverse) norm of the parameters also\nrepresents a useful measure of simplicity. Another useful notion of simplicity is smoothness,\ni.e., that the function should not be sensitive\nto small changes to its inputs. For instance, when we classify images,\nwe would expect that adding some random noise\nto the pixels should be mostly harmless. :citet:`Bishop.1995` formalized\nthis idea when he proved that training with input noise\nis equivalent to Tikhonov regularization. This work drew a clear mathematical connection\nbetween the requirement that a function be smooth (and thus simple),\nand the requirement that it be resilient\nto perturbations in the input. Then, :citet:`Srivastava.Hinton.Krizhevsky.ea.2014`\ndeveloped a clever idea for how to apply Bishop's idea\nto the internal layers of a network, too. Their idea, called *dropout*, involves\ninjecting noise while computing\neach internal layer during forward propagation,\nand it has become a standard technique\nfor training neural networks. The method is called *dropout* because we literally\n*drop out* some neurons during training. Throughout training, on each iteration,\nstandard dropout consists of zeroing out\nsome fraction of the nodes in each layer\nbefore calculating the subsequent layer. To be clear, we are imposing\nour own narrative with the link to Bishop."
    },
    {
      "chunk_id": "e181765b4e60_1",
      "chapter": "dropout",
      "heading": "dropout",
      "text": "Throughout training, on each iteration,\nstandard dropout consists of zeroing out\nsome fraction of the nodes in each layer\nbefore calculating the subsequent layer. To be clear, we are imposing\nour own narrative with the link to Bishop. The original paper on dropout\noffers intuition through a surprising\nanalogy to sexual reproduction. The authors argue that neural network overfitting\nis characterized by a state in which\neach layer relies on a specific\npattern of activations in the previous layer,\ncalling this condition *co-adaptation*. Dropout, they claim, breaks up co-adaptation\njust as sexual reproduction is argued to\nbreak up co-adapted genes. While such an justification of this theory is certainly up for debate,\nthe dropout technique itself has proved enduring,\nand various forms of dropout are implemented\nin most deep learning libraries. The key challenge is how to inject this noise. One idea is to inject it in an *unbiased* manner\nso that the expected value of each layer---while fixing\nthe others---equals the value it would have taken absent noise. In Bishop's work, he added Gaussian noise\nto the inputs to a linear model. At each training iteration, he added noise\nsampled from a distribution with mean zero\n$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ to the input $\\mathbf{x}$,\nyielding a perturbed point $\\mathbf{x}' = \\mathbf{x} + \\epsilon$. In expectation, $E[\\mathbf{x}'] = \\mathbf{x}$. In standard dropout regularization,\none zeros out some fraction of the nodes in each layer\nand then *debiases* each layer by normalizing\nby the fraction of nodes that were retained (not dropped out). In other words,\nwith *dropout probability* $p$,\neach intermediate activation $h$ is replaced by\na random variable $h'$ as follows:\n\n$$\n\\begin{aligned}\nh' =\n\\begin{cases}\n    0 & \\textrm{ with probability } p \\\\\n    \\frac{h}{1-p} & \\textrm{ otherwise}\n\\end{cases}\n\\end{aligned}\n$$\n\nBy design, the expectation remains unchanged, i.e., $E[h'] = h$."
    },
    {
      "chunk_id": "e181765b4e60_2",
      "chapter": "dropout",
      "heading": "dropout",
      "text": "```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom functools import partial\nimport jax\nfrom jax import numpy as jnp\nimport optax\n```"
    },
    {
      "chunk_id": "ad30b04cf261_0",
      "chapter": "dropout",
      "heading": "Dropout in Practice",
      "text": "Recall the MLP with a hidden layer and five hidden units\nfrom :numref:`fig_mlp`.\nWhen we apply dropout to a hidden layer,\nzeroing out each hidden unit with probability $p$,\nthe result can be viewed as a network\ncontaining only a subset of the original neurons.\nIn :numref:`fig_dropout2`, $h_2$ and $h_5$ are removed.\nConsequently, the calculation of the outputs\nno longer depends on $h_2$ or $h_5$\nand their respective gradient also vanishes\nwhen performing backpropagation.\nIn this way, the calculation of the output layer\ncannot be overly dependent on any\none element of $h_1, \\ldots, h_5$.\n\n![MLP before and after dropout.](../img/dropout2.svg)\n:label:`fig_dropout2`\n\nTypically, we disable dropout at test time.\nGiven a trained model and a new example,\nwe do not drop out any nodes\nand thus do not need to normalize.\nHowever, there are some exceptions:\nsome researchers use dropout at test time as a heuristic\nfor estimating the *uncertainty* of neural network predictions:\nif the predictions agree across many different dropout outputs,\nthen we might say that the network is more confident."
    },
    {
      "chunk_id": "90f52b5471be_0",
      "chapter": "dropout",
      "heading": "Implementation from Scratch",
      "text": "To implement the dropout function for a single layer,\nwe must draw as many samples\nfrom a Bernoulli (binary) random variable\nas our layer has dimensions,\nwhere the random variable takes value $1$ (keep)\nwith probability $1-p$ and $0$ (drop) with probability $p$. One easy way to implement this is to first draw samples\nfrom the uniform distribution $U[0, 1]$. Then we can keep those nodes for which the corresponding\nsample is greater than $p$, dropping the rest. In the following code, we (**implement a `dropout_layer` function\nthat drops out the elements in the tensor input `X`\nwith probability `dropout`**),\nrescaling the remainder as described above:\ndividing the survivors by `1.0-dropout`. ```{.python .input}\n%%tab mxnet\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    if dropout == 1: return np.zeros_like(X)\n    mask = np.random.uniform(0, 1, X.shape) > dropout\n    return mask.astype(np.float32) * X / (1.0 - dropout)\n```\n\n```{.python .input}\n%%tab pytorch\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    if dropout == 1: return torch.zeros_like(X)\n    mask = (torch.rand(X.shape) > dropout).float()\n    return mask * X / (1.0 - dropout)\n```\n\n```{.python .input}\n%%tab tensorflow\ndef dropout_layer(X, dropout):\n    assert 0 <= dropout <= 1\n    if dropout == 1: return tf.zeros_like(X)\n    mask = tf.random.uniform(\n        shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout\n    return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)\n```\n\n```{.python .input}\n%%tab jax\ndef dropout_layer(X, dropout, key=d2l.get_key()):\n    assert 0 <= dropout <= 1\n    if dropout == 1: return jnp.zeros_like(X)\n    mask = jax.random.uniform(key, X.shape) > dropout\n    return jnp.asarray(mask, dtype=jnp.float32) * X / (1.0 - dropout)\n```\n\nWe can [**test out the `dropout_layer` function on a few examples**]. In the following lines of code,\nwe pass our input `X` through the dropout operation,\nwith probabilities 0, 0.5, and 1, respectively."
    },
    {
      "chunk_id": "90f52b5471be_1",
      "chapter": "dropout",
      "heading": "Implementation from Scratch",
      "text": "In the following lines of code,\nwe pass our input `X` through the dropout operation,\nwith probabilities 0, 0.5, and 1, respectively. ```{.python .input}\n%%tab all\nif tab.selected('mxnet'):\n    X = np.arange(16).reshape(2, 8)\nif tab.selected('pytorch'):\n    X = torch.arange(16, dtype = torch.float32).reshape((2, 8))\nif tab.selected('tensorflow'):\n    X = tf.reshape(tf.range(16, dtype=tf.float32), (2, 8))\nif tab.selected('jax'):\n    X = jnp.arange(16, dtype=jnp.float32).reshape(2, 8)\nprint('dropout_p = 0:', dropout_layer(X, 0))\nprint('dropout_p = 0.5:', dropout_layer(X, 0.5))\nprint('dropout_p = 1:', dropout_layer(X, 1))\n```"
    },
    {
      "chunk_id": "ac59e3349937_0",
      "chapter": "dropout",
      "heading": "Defining the Model",
      "text": "The model below applies dropout to the output\nof each hidden layer (following the activation function). We can set dropout probabilities for each layer separately. A common choice is to set\na lower dropout probability closer to the input layer. We ensure that dropout is only active during training."
    },
    {
      "chunk_id": "ac59e3349937_1",
      "chapter": "dropout",
      "heading": "Defining the Model",
      "text": "We can set dropout probabilities for each layer separately. A common choice is to set\na lower dropout probability closer to the input layer. We ensure that dropout is only active during training. ```{.python .input}\n%%tab mxnet\nclass DropoutMLPScratch(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n                 dropout_1, dropout_2, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lin1 = nn.Dense(num_hiddens_1, activation='relu')\n        self.lin2 = nn.Dense(num_hiddens_2, activation='relu')\n        self.lin3 = nn.Dense(num_outputs)\n        self.initialize()\n\n    def forward(self, X):\n        H1 = self.lin1(X)\n        if autograd.is_training():\n            H1 = dropout_layer(H1, self.dropout_1)\n        H2 = self.lin2(H1)\n        if autograd.is_training():\n            H2 = dropout_layer(H2, self.dropout_2)\n        return self.lin3(H2)\n```\n\n```{.python .input}\n%%tab pytorch\nclass DropoutMLPScratch(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n                 dropout_1, dropout_2, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lin1 = nn.LazyLinear(num_hiddens_1)\n        self.lin2 = nn.LazyLinear(num_hiddens_2)\n        self.lin3 = nn.LazyLinear(num_outputs)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n        if self.training:  \n            H1 = dropout_layer(H1, self.dropout_1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training:\n            H2 = dropout_layer(H2, self.dropout_2)\n        return self.lin3(H2)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass DropoutMLPScratch(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n                 dropout_1, dropout_2, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lin1 = tf.keras.layers.Dense(num_hiddens_1, activation='relu')\n        self.lin2 = tf.keras.layers.Dense(num_hiddens_2, activation='relu')\n        self.lin3 = tf.keras.layers.Dense(num_outputs)\n\n    def forward(self, X):\n        H1 = self.lin1(tf.reshape(X, (X.shape[0], -1)))\n        if self.training:\n            H1 = dropout_layer(H1, self.dropout_1)\n        H2 = self.lin2(H1)\n        if self.training:\n            H2 = dropout_layer(H2, self.dropout_2)\n        return self.lin3(H2)\n```\n\n```{.python .input}\n%%tab jax\nclass DropoutMLPScratch(d2l.Classifier):\n    num_hiddens_1: int\n    num_hiddens_2: int\n    num_outputs: int\n    dropout_1: float\n    dropout_2: float\n    lr: float\n    training: bool = True\n\n    def setup(self):\n        self.lin1 = nn.Dense(self.num_hiddens_1)\n        self.lin2 = nn.Dense(self.num_hiddens_2)\n        self.lin3 = nn.Dense(self.num_outputs)\n        self.relu = nn.relu\n\n    def forward(self, X):\n        H1 = self.relu(self.lin1(X.reshape(X.shape[0], -1)))\n        if self.training:\n            H1 = dropout_layer(H1, self.dropout_1)\n        H2 = self.relu(self.lin2(H1))\n        if self.training:\n            H2 = dropout_layer(H2, self.dropout_2)\n        return self.lin3(H2)\n```"
    },
    {
      "chunk_id": "74f305c9b104_0",
      "chapter": "dropout",
      "heading": "[**Training**]",
      "text": "The following is similar to the training of MLPs described previously.\n\n```{.python .input}\n%%tab all\nhparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\nmodel = DropoutMLPScratch(**hparams)\ndata = d2l.FashionMNIST(batch_size=256)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "a2f1100eaef6_0",
      "chapter": "dropout",
      "heading": "[**Concise Implementation**]",
      "text": "With high-level APIs, all we need to do is add a `Dropout` layer\nafter each fully connected layer,\npassing in the dropout probability\nas the only argument to its constructor. During training, the `Dropout` layer will randomly\ndrop out outputs of the previous layer\n(or equivalently, the inputs to the subsequent layer)\naccording to the specified dropout probability. When not in training mode,\nthe `Dropout` layer simply passes the data through during testing."
    },
    {
      "chunk_id": "a2f1100eaef6_1",
      "chapter": "dropout",
      "heading": "[**Concise Implementation**]",
      "text": "When not in training mode,\nthe `Dropout` layer simply passes the data through during testing. ```{.python .input}\n%%tab mxnet\nclass DropoutMLP(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n                 dropout_1, dropout_2, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = nn.Sequential()\n        self.net.add(nn.Dense(num_hiddens_1, activation=\"relu\"),\n                     nn.Dropout(dropout_1),\n                     nn.Dense(num_hiddens_2, activation=\"relu\"),\n                     nn.Dropout(dropout_2),\n                     nn.Dense(num_outputs))\n        self.net.initialize()\n```\n\n```{.python .input}\n%%tab pytorch\nclass DropoutMLP(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n                 dropout_1, dropout_2, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = nn.Sequential(\n            nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(), \n            nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(), \n            nn.Dropout(dropout_2), nn.LazyLinear(num_outputs))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass DropoutMLP(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n                 dropout_1, dropout_2, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = tf.keras.models.Sequential([\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(num_hiddens_1, activation=tf.nn.relu),\n            tf.keras.layers.Dropout(dropout_1),\n            tf.keras.layers.Dense(num_hiddens_2, activation=tf.nn.relu),\n            tf.keras.layers.Dropout(dropout_2),\n            tf.keras.layers.Dense(num_outputs)])\n```\n\n```{.python .input}\n%%tab jax\nclass DropoutMLP(d2l.Classifier):\n    num_hiddens_1: int\n    num_hiddens_2: int\n    num_outputs: int\n    dropout_1: float\n    dropout_2: float\n    lr: float\n    training: bool = True\n\n    @nn.compact\n    def __call__(self, X):\n        x = nn.relu(nn.Dense(self.num_hiddens_1)(X.reshape((X.shape[0], -1))))\n        x = nn.Dropout(self.dropout_1, deterministic=not self.training)(x)\n        x = nn.relu(nn.Dense(self.num_hiddens_2)(x))\n        x = nn.Dropout(self.dropout_2, deterministic=not self.training)(x)\n        return nn.Dense(self.num_outputs)(x)\n```\n\n:begin_tab:`jax`\nNote that we need to redefine the loss function since a network\nwith a dropout layer needs a PRNGKey when using `Module.apply()`,\nand this RNG seed should be explicitly named `dropout`."
    },
    {
      "chunk_id": "a2f1100eaef6_2",
      "chapter": "dropout",
      "heading": "[**Concise Implementation**]",
      "text": "This key is\nused by the `dropout` layer in Flax to generate the random dropout\nmask internally. It is important to use a unique `dropout_rng` key\nwith every epoch in the training loop, otherwise the generated dropout\nmask will not be stochastic and different between the epoch runs. This `dropout_rng` can be stored in the\n`TrainState` object (in the `d2l.Trainer` class defined in\n:numref:`oo-design-training`) as an attribute and with every epoch\nit is replaced with a new `dropout_rng`. We already handled this with the\n`fit_epoch` method defined in :numref:`sec_linear_scratch`. :end_tab:\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.Classifier)  #@save\n@partial(jax.jit, static_argnums=(0, 5))\ndef loss(self, params, X, Y, state, averaged=True):\n    Y_hat = state.apply_fn({'params': params}, *X,\n                           mutable=False,  # To be used later (e.g., batch norm)\n                           rngs={'dropout': state.dropout_rng})\n    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n    Y = d2l.reshape(Y, (-1,))\n    fn = optax.softmax_cross_entropy_with_integer_labels\n    # The returned empty dictionary is a placeholder for auxiliary data,\n    # which will be used later (e.g., for batch norm)\n    return (fn(Y_hat, Y).mean(), {}) if averaged else (fn(Y_hat, Y), {})\n```\n\nNext, we [**train the model**]. ```{.python .input}\n%%tab all\nmodel = DropoutMLP(**hparams)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "5ecc4b91f6d2_0",
      "chapter": "dropout",
      "heading": "Summary",
      "text": "Beyond controlling the number of dimensions and the size of the weight vector, dropout is yet another tool for avoiding overfitting. Often tools are used jointly.\nNote that dropout is\nused only during training:\nit replaces an activation $h$ with a random variable with expected value $h$."
    },
    {
      "chunk_id": "409bf4b5fb14_0",
      "chapter": "dropout",
      "heading": "Exercises",
      "text": "1. What happens if you change the dropout probabilities for the first and second layers? In particular, what happens if you switch the ones for both layers? Design an experiment to answer these questions, describe your results quantitatively, and summarize the qualitative takeaways.\n1. Increase the number of epochs and compare the results obtained when using dropout with those when not using it.\n1. What is the variance of the activations in each hidden layer when dropout is and is not applied? Draw a plot to show how this quantity evolves over time for both models.\n1. Why is dropout not typically used at test time?\n1. Using the model in this section as an example, compare the effects of using dropout and weight decay. What happens when dropout and weight decay are used at the same time? Are the results additive? Are there diminished returns (or worse)? Do they cancel each other out?\n1. What happens if we apply dropout to the individual weights of the weight matrix rather than the activations?\n1. Invent another technique for injecting random noise at each layer that is different from the standard dropout technique. Can you develop a method that outperforms dropout on the Fashion-MNIST dataset (for a fixed architecture)?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/100)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/101)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/261)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17987)\n:end_tab:"
    },
    {
      "chunk_id": "2bfa0c2713ca_0",
      "chapter": "generalization-deep",
      "heading": "generalization-deep",
      "text": "# Generalization in Deep Learning\n\n\nIn :numref:`chap_regression` and :numref:`chap_classification`,\nwe tackled regression and classification problems\nby fitting linear models to training data. In both cases, we provided practical algorithms\nfor finding the parameters that maximized\nthe likelihood of the observed training labels. And then, towards the end of each chapter,\nwe recalled that fitting the training data\nwas only an intermediate goal. Our real quest all along was to discover *general patterns*\non the basis of which we can make accurate predictions\neven on new examples drawn from the same underlying population. Machine learning researchers are *consumers* of optimization algorithms. Sometimes, we must even develop new optimization algorithms. But at the end of the day, optimization is merely a means to an end. At its core, machine learning is a statistical discipline\nand we wish to optimize training loss only insofar\nas some statistical principle (known or unknown)\nleads the resulting models to generalize beyond the training set. On the bright side, it turns out that deep neural networks\ntrained by stochastic gradient descent generalize remarkably well\nacross myriad prediction problems, spanning computer vision;\nnatural language processing; time series data; recommender systems;\nelectronic health records; protein folding;\nvalue function approximation in video games\nand board games; and numerous other domains. On the downside, if you were looking\nfor a straightforward account\nof either the optimization story\n(why we can fit them to training data)\nor the generalization story\n(why the resulting models generalize to unseen examples),\nthen you might want to pour yourself a drink. While our procedures for optimizing linear models\nand the statistical properties of the solutions\nare both described well by a comprehensive body of theory,\nour understanding of deep learning\nstill resembles the wild west on both fronts."
    },
    {
      "chunk_id": "2bfa0c2713ca_1",
      "chapter": "generalization-deep",
      "heading": "generalization-deep",
      "text": "While our procedures for optimizing linear models\nand the statistical properties of the solutions\nare both described well by a comprehensive body of theory,\nour understanding of deep learning\nstill resembles the wild west on both fronts. Both the theory and practice of deep learning\nare rapidly evolving,\nwith theorists adopting new strategies\nto explain what's going on,\neven as practitioners continue\nto innovate at a blistering pace,\nbuilding arsenals of heuristics for training deep networks\nand a body of intuitions and folk knowledge\nthat provide guidance for deciding\nwhich techniques to apply in which situations. The summary of the present moment is that the theory of deep learning\nhas produced promising lines of attack and scattered fascinating results,\nbut still appears far from a comprehensive account\nof both (i) why we are able to optimize neural networks\nand (ii) how models learned by gradient descent\nmanage to generalize so well, even on high-dimensional tasks. However, in practice, (i) is seldom a problem\n(we can always find parameters that will fit all of our training data)\nand thus understanding generalization is far the bigger problem. On the other hand, even absent the comfort of a coherent scientific theory,\npractitioners have developed a large collection of techniques\nthat may help you to produce models that generalize well in practice. While no pithy summary can possibly do justice\nto the vast topic of generalization in deep learning,\nand while the overall state of research is far from resolved,\nwe hope, in this section, to present a broad overview\nof the state of research and practice."
    },
    {
      "chunk_id": "8e91e09c739f_0",
      "chapter": "generalization-deep",
      "heading": "Revisiting Overfitting and Regularization",
      "text": "According to the \"no free lunch\" theorem of :citet:`wolpert1995no`,\nany learning algorithm generalizes better on data with certain distributions, and worse with other distributions. Thus, given a finite training set,\na model relies on certain assumptions: \nto achieve human-level performance\nit may be useful to identify *inductive biases* \nthat reflect how humans think about the world. Such inductive biases show preferences \nfor solutions with certain properties. For example,\na deep MLP has an inductive bias\ntowards building up a complicated function by the composition of simpler functions. With machine learning models encoding inductive biases,\nour approach to training them\ntypically consists of two phases: (i) fit the training data;\nand (ii) estimate the *generalization error*\n(the true error on the underlying population)\nby evaluating the model on holdout data. The difference between our fit on the training data\nand our fit on the test data is called the *generalization gap* and when this is large,\nwe say that our models *overfit* to the training data. In extreme cases of overfitting,\nwe might exactly fit the training data,\neven when the test error remains significant. And in the classical view,\nthe interpretation is that our models are too complex,\nrequiring that we either shrink the number of features,\nthe number of nonzero parameters learned,\nor the size of the parameters as quantified. Recall the plot of model complexity compared with loss\n(:numref:`fig_capacity_vs_error`)\nfrom :numref:`sec_generalization_basics`. However deep learning complicates this picture in counterintuitive ways. First, for classification problems,\nour models are typically expressive enough\nto perfectly fit every training example,\neven in datasets consisting of millions\n:cite:`zhang2021understanding`."
    },
    {
      "chunk_id": "8e91e09c739f_1",
      "chapter": "generalization-deep",
      "heading": "Revisiting Overfitting and Regularization",
      "text": "First, for classification problems,\nour models are typically expressive enough\nto perfectly fit every training example,\neven in datasets consisting of millions\n:cite:`zhang2021understanding`. In the classical picture, we might think\nthat this setting lies on the far right extreme\nof the model complexity axis,\nand that any improvements in generalization error\nmust come by way of regularization,\neither by reducing the complexity of the model class,\nor by applying a penalty, severely constraining\nthe set of values that our parameters might take. But that is where things start to get weird. Strangely, for many deep learning tasks\n(e.g., image recognition and text classification)\nwe are typically choosing among model architectures,\nall of which can achieve arbitrarily low training loss\n(and zero training error). Because all models under consideration achieve zero training error,\n*the only avenue for further gains is to reduce overfitting*. Even stranger, it is often the case that\ndespite fitting the training data perfectly,\nwe can actually *reduce the generalization error*\nfurther by making the model *even more expressive*,\ne.g., adding layers, nodes, or training\nfor a larger number of epochs. Stranger yet, the pattern relating the generalization gap\nto the *complexity* of the model (as captured, for example, in the depth or width of the networks)\ncan be non-monotonic,\nwith greater complexity hurting at first\nbut subsequently helping in a so-called \"double-descent\" pattern\n:cite:`nakkiran2021deep`. Thus the deep learning practitioner possesses a bag of tricks,\nsome of which seemingly restrict the model in some fashion\nand others that seemingly make it even more expressive,\nand all of which, in some sense, are applied to mitigate overfitting. Complicating things even further,\nwhile the guarantees provided by classical learning theory\ncan be conservative even for classical models,\nthey appear powerless to explain why it is\nthat deep neural networks generalize in the first place."
    },
    {
      "chunk_id": "8e91e09c739f_2",
      "chapter": "generalization-deep",
      "heading": "Revisiting Overfitting and Regularization",
      "text": "Complicating things even further,\nwhile the guarantees provided by classical learning theory\ncan be conservative even for classical models,\nthey appear powerless to explain why it is\nthat deep neural networks generalize in the first place. Because deep neural networks are capable of fitting\narbitrary labels even for large datasets,\nand despite the use of familiar methods such as $\\ell_2$ regularization,\ntraditional complexity-based generalization bounds,\ne.g., those based on the VC dimension\nor Rademacher complexity of a hypothesis class\ncannot explain why neural networks generalize."
    },
    {
      "chunk_id": "bc4e3e3b5716_0",
      "chapter": "generalization-deep",
      "heading": "Inspiration from Nonparametrics",
      "text": "Approaching deep learning for the first time,\nit is tempting to think of them as parametric models. After all, the models *do* have millions of parameters. When we update the models, we update their parameters. When we save the models, we write their parameters to disk. However, mathematics and computer science are riddled\nwith counterintuitive changes of perspective,\nand surprising isomorphisms between seemingly different problems. While neural networks clearly *have* parameters,\nin some ways it can be more fruitful\nto think of them as behaving like nonparametric models. So what precisely makes a model nonparametric? While the name covers a diverse set of approaches,\none common theme is that nonparametric methods\ntend to have a level of complexity that grows\nas the amount of available data grows. Perhaps the simplest example of a nonparametric model\nis the $k$-nearest neighbor algorithm (we will cover more nonparametric models later, for example in :numref:`sec_attention-pooling`). Here, at training time,\nthe learner simply memorizes the dataset. Then, at prediction time,\nwhen confronted with a new point $\\mathbf{x}$,\nthe learner looks up the $k$ nearest neighbors\n(the $k$ points $\\mathbf{x}_i'$ that minimize\nsome distance $d(\\mathbf{x}, \\mathbf{x}_i')$). When $k=1$, this algorithm is called $1$-nearest neighbors,\nand the algorithm will always achieve a training error of zero. That however, does not mean that the algorithm will not generalize. In fact, it turns out that under some mild conditions,\nthe 1-nearest neighbor algorithm is consistent\n(eventually converging to the optimal predictor). Note that $1$-nearest neighbor requires that we specify\nsome distance function $d$, or equivalently,\nthat we specify some vector-valued basis function $\\phi(\\mathbf{x})$\nfor featurizing our data."
    },
    {
      "chunk_id": "bc4e3e3b5716_1",
      "chapter": "generalization-deep",
      "heading": "Inspiration from Nonparametrics",
      "text": "Note that $1$-nearest neighbor requires that we specify\nsome distance function $d$, or equivalently,\nthat we specify some vector-valued basis function $\\phi(\\mathbf{x})$\nfor featurizing our data. For any choice of the distance metric,\nwe will achieve zero training error\nand eventually reach an optimal predictor,\nbut different distance metrics $d$\nencode different inductive biases\nand with a finite amount of available data\nwill yield different predictors. Different choices of the distance metric $d$\nrepresent different assumptions about the underlying patterns\nand the performance of the different predictors\nwill depend on how compatible the assumptions\nare with the observed data. In a sense, because neural networks are over-parametrized,\npossessing many more parameters than are needed to fit the training data,\nthey tend to *interpolate* the training data (fitting it perfectly)\nand thus behave, in some ways, more like nonparametric models. More recent theoretical research has established\ndeep connection between large neural networks\nand nonparametric methods, notably kernel methods. In particular, :citet:`Jacot.Grabriel.Hongler.2018`\ndemonstrated that in the limit, as multilayer perceptrons\nwith randomly initialized weights grow infinitely wide,\nthey become equivalent to (nonparametric) kernel methods\nfor a specific choice of the kernel function\n(essentially, a distance function),\nwhich they call the neural tangent kernel. While current neural tangent kernel models may not fully explain\nthe behavior of modern deep networks,\ntheir success as an analytical tool\nunderscores the usefulness of nonparametric modeling\nfor understanding the behavior of over-parametrized deep networks."
    },
    {
      "chunk_id": "396ffc44f7b6_0",
      "chapter": "generalization-deep",
      "heading": "Early Stopping",
      "text": "While deep neural networks are capable of fitting arbitrary labels,\neven when labels are assigned incorrectly or randomly\n:cite:`zhang2021understanding`,\nthis capability only emerges over many iterations of training. A new line of work :cite:`Rolnick.Veit.Belongie.Shavit.2017`\nhas revealed that in the setting of label noise,\nneural networks tend to fit cleanly labeled data first\nand only subsequently to interpolate the mislabeled data. Moreover, it has been established that this phenomenon\ntranslates directly into a guarantee on generalization:\nwhenever a model has fitted the cleanly labeled data\nbut not randomly labeled examples included in the training set,\nit has in fact generalized :cite:`Garg.Balakrishnan.Kolter.Lipton.2021`. Together these findings help to motivate *early stopping*,\na classic technique for regularizing deep neural networks. Here, rather than directly constraining the values of the weights,\none constrains the number of epochs of training. The most common way to determine the stopping criterion\nis to monitor validation error throughout training\n(typically by checking once after each epoch)\nand to cut off training when the validation error\nhas not decreased by more than some small amount $\\epsilon$\nfor some number of epochs. This is sometimes called a *patience criterion*. As well as the potential to lead to better generalization\nin the setting of noisy labels,\nanother benefit of early stopping is the time saved. Once the patience criterion is met, one can terminate training. For large models that might require days of training\nsimultaneously across eight or more GPUs,\nwell-tuned early stopping can save researchers days of time\nand can save their employers many thousands of dollars. Notably, when there is no label noise and datasets are *realizable*\n(the classes are truly separable, e.g., distinguishing cats from dogs),\nearly stopping tends not to lead to significant improvements in generalization."
    },
    {
      "chunk_id": "396ffc44f7b6_1",
      "chapter": "generalization-deep",
      "heading": "Early Stopping",
      "text": "Notably, when there is no label noise and datasets are *realizable*\n(the classes are truly separable, e.g., distinguishing cats from dogs),\nearly stopping tends not to lead to significant improvements in generalization. On the other hand, when there is label noise,\nor intrinsic variability in the label\n(e.g., predicting mortality among patients),\nearly stopping is crucial. Training models until they interpolate noisy data is typically a bad idea."
    },
    {
      "chunk_id": "17293213dac3_0",
      "chapter": "generalization-deep",
      "heading": "Classical Regularization Methods for Deep Networks",
      "text": "In :numref:`chap_regression`, we described\nseveral  classical regularization techniques\nfor constraining the complexity of our models. In particular, :numref:`sec_weight_decay`\nintroduced a method called weight decay,\nwhich consists of adding a regularization term to the loss function\nin order to penalize large values of the weights. Depending on which weight norm is penalized\nthis technique is known either as ridge regularization (for $\\ell_2$ penalty)\nor lasso regularization (for an $\\ell_1$ penalty). In the classical analysis of these regularizers,\nthey are considered as sufficiently restrictive on the values\nthat the weights can take to prevent the model from fitting arbitrary labels. In deep learning implementations,\nweight decay remains a popular tool. However, researchers have noted\nthat typical strengths of $\\ell_2$ regularization\nare insufficient to prevent the networks\nfrom interpolating the data :cite:`zhang2021understanding` and thus the benefits if interpreted\nas regularization might only make sense\nin combination with the early stopping criterion. Absent early stopping, it is possible\nthat just like the number of layers\nor number of nodes (in deep learning)\nor the distance metric (in 1-nearest neighbor),\nthese methods may lead to better generalization\nnot because they meaningfully constrain\nthe power of the neural network\nbut rather because they somehow encode inductive biases\nthat are better compatible with the patterns\nfound in datasets of interests. Thus, classical regularizers remain popular\nin deep learning implementations,\neven if the theoretical rationale\nfor their efficacy may be radically different. Notably, deep learning researchers have also built\non techniques first popularized\nin classical regularization contexts,\nsuch as adding noise to model inputs."
    },
    {
      "chunk_id": "17293213dac3_1",
      "chapter": "generalization-deep",
      "heading": "Classical Regularization Methods for Deep Networks",
      "text": "Notably, deep learning researchers have also built\non techniques first popularized\nin classical regularization contexts,\nsuch as adding noise to model inputs. In the next section we will introduce\nthe famous dropout technique\n(invented by :citet:`Srivastava.Hinton.Krizhevsky.ea.2014`),\nwhich has become a mainstay of deep learning,\neven as the theoretical basis for its efficacy\nremains similarly mysterious."
    },
    {
      "chunk_id": "91f723acbf31_0",
      "chapter": "generalization-deep",
      "heading": "Summary",
      "text": "Unlike classical linear models,\nwhich tend to have fewer parameters than examples,\ndeep networks tend to be over-parametrized,\nand for most tasks are capable\nof perfectly fitting the training set.\nThis *interpolation regime* challenges\nmany hard fast-held intuitions.\nFunctionally, neural networks look like parametric models.\nBut thinking of them as nonparametric models\ncan sometimes be a more reliable source of intuition.\nBecause it is often the case that all deep networks under consideration\nare capable of fitting all of the training labels,\nnearly all gains must come by mitigating overfitting\n(closing the *generalization gap*).\nParadoxically, the interventions\nthat reduce the generalization gap\nsometimes appear to increase model complexity\nand at other times appear to decrease complexity.\nHowever, these methods seldom decrease complexity\nsufficiently for classical theory\nto explain the generalization of deep networks,\nand *why certain choices lead to improved generalization*\nremains for the most part a massive open question\ndespite the concerted efforts of many brilliant researchers."
    },
    {
      "chunk_id": "5c65bbfcd332_0",
      "chapter": "generalization-deep",
      "heading": "Exercises",
      "text": "1. In what sense do traditional complexity-based measures fail to account for generalization of deep neural networks?\n1. Why might *early stopping* be considered a regularization technique?\n1. How do researchers typically determine the stopping criterion?\n1. What important factor seems to differentiate cases when early stopping leads to big improvements in generalization?\n1. Beyond generalization, describe another benefit of early stopping.\n\n[Discussions](https://discuss.d2l.ai/t/7473)"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Multilayer Perceptrons\n:label:`chap_perceptrons`\n\nIn this chapter, we will introduce your first truly *deep* network.\nThe simplest deep networks are called *multilayer perceptrons*,\nand they consist of multiple layers of neurons\neach fully connected to those in the layer below\n(from which they receive input)\nand those above (which they, in turn, influence).\nAlthough automatic differentiation\nsignificantly simplifies the implementation of deep learning algorithms,\nwe will dive deep into how these gradients\nare calculated in deep networks.\nThen we will\nbe ready to\ndiscuss issues relating to numerical stability and parameter initialization\nthat are key to successfully training deep networks.\nWhen we train such high-capacity models we run the risk of overfitting. Thus, we will\nrevisit regularization and generalization\nfor deep networks.\nThroughout, we aim\nto give you a firm grasp not just of the concepts but also of the practice of using deep networks.\nAt the end of this chapter, we apply what we have introduced so far to a real case: house price\nprediction. We punt matters relating to the computational performance, scalability, and efficiency\nof our models to subsequent chapters.\n\n```toc\n:maxdepth: 2\n\nmlp\nmlp-implementation\nbackprop\nnumerical-stability-and-init\ngeneralization-deep\ndropout\nkaggle-house-price\n```"
    },
    {
      "chunk_id": "041cea9236b1_0",
      "chapter": "kaggle-house-price",
      "heading": "kaggle-house-price",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Predicting House Prices on Kaggle\n:label:`sec_kaggle_house`\n\nNow that we have introduced some basic tools\nfor building and training deep networks\nand regularizing them with techniques including\nweight decay and dropout,\nwe are ready to put all this knowledge into practice\nby participating in a Kaggle competition.\nThe house price prediction competition\nis a great place to start.\nThe data is fairly generic and do not exhibit exotic structure\nthat might require specialized models (as audio or video might).\nThis dataset, collected by :citet:`De-Cock.2011`,\ncovers house prices in Ames, Iowa from the period 2006--2010.\nIt is considerably larger than the famous [Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names) of Harrison and Rubinfeld (1978),\nboasting both more examples and more features.\n\n\nIn this section, we will walk you through details of\ndata preprocessing, model design, and hyperparameter selection.\nWe hope that through a hands-on approach,\nyou will gain some intuitions that will guide you\nin your career as a data scientist.\n\n```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, autograd, init, np, npx\nfrom mxnet.gluon import nn\nimport pandas as pd\n\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nimport pandas as pd\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport pandas as pd\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport pandas as pd\n```"
    },
    {
      "chunk_id": "3cc8c1a62140_0",
      "chapter": "kaggle-house-price",
      "heading": "Downloading Data",
      "text": "Throughout the book, we will train and test models\non various downloaded datasets.\nHere, we (**implement two utility functions**)\nfor downloading and extracting zip or tar files.\nAgain, we skip implementation details of\nsuch utility functions.\n\n```{.python .input  n=2}\n%%tab all\ndef download(url, folder, sha1_hash=None):\n    \"\"\"Download a file to folder and return the local filepath.\"\"\"\n\ndef extract(filename, folder):\n    \"\"\"Extract a zip/tar file into folder.\"\"\"\n```"
    },
    {
      "chunk_id": "3bacecd53c02_0",
      "chapter": "kaggle-house-price",
      "heading": "Kaggle",
      "text": "[Kaggle](https://www.kaggle.com) is a popular platform\nthat hosts machine learning competitions.\nEach competition centers on a dataset and many\nare sponsored by stakeholders who offer prizes\nto the winning solutions.\nThe platform helps users to interact\nvia forums and shared code,\nfostering both collaboration and competition.\nWhile leaderboard chasing often spirals out of control,\nwith researchers focusing myopically on preprocessing steps\nrather than asking fundamental questions,\nthere is also tremendous value in the objectivity of a platform\nthat facilitates direct quantitative comparisons\namong competing approaches as well as code sharing\nso that everyone can learn what did and did not work.\nIf you want to participate in a Kaggle competition,\nyou will first need to register for an account\n(see :numref:`fig_kaggle`).\n\n![The Kaggle website.](../img/kaggle.png)\n:width:`400px`\n:label:`fig_kaggle`\n\nOn the house price prediction competition page, as illustrated\nin :numref:`fig_house_pricing`,\nyou can find the dataset (under the \"Data\" tab),\nsubmit predictions, and see your ranking,\nThe URL is right here:\n\n> https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n\n![The house price prediction competition page.](../img/house-pricing.png)\n:width:`400px`\n:label:`fig_house_pricing`"
    },
    {
      "chunk_id": "ebd6f650b5f1_0",
      "chapter": "kaggle-house-price",
      "heading": "Accessing and Reading the Dataset",
      "text": "Note that the competition data is separated\ninto training and test sets. Each record includes the property value of the house\nand attributes such as street type, year of construction,\nroof type, basement condition, etc. The features consist of various data types. For example, the year of construction\nis represented by an integer,\nthe roof type by discrete categorical assignments,\nand other features by floating point numbers. And here is where reality complicates things:\nfor some examples, some data is altogether missing\nwith the missing value marked simply as \"na\". The price of each house is included\nfor the training set only\n(it is a competition after all). We will want to partition the training set\nto create a validation set,\nbut we only get to evaluate our models on the official test set\nafter uploading predictions to Kaggle. The \"Data\" tab on the competition tab\nin :numref:`fig_house_pricing`\nhas links for downloading the data. To get started, we will [**read in and process the data\nusing `pandas`**], which we introduced in :numref:`sec_pandas`. For convenience, we can download and cache\nthe Kaggle housing dataset. If a file corresponding to this dataset already exists in the cache directory and its SHA-1 matches `sha1_hash`, our code will use the cached file to avoid clogging up your Internet with redundant downloads."
    },
    {
      "chunk_id": "ebd6f650b5f1_1",
      "chapter": "kaggle-house-price",
      "heading": "Accessing and Reading the Dataset",
      "text": "If a file corresponding to this dataset already exists in the cache directory and its SHA-1 matches `sha1_hash`, our code will use the cached file to avoid clogging up your Internet with redundant downloads. ```{.python .input  n=30}\n%%tab all\nclass KaggleHouse(d2l.DataModule):\n    def __init__(self, batch_size, train=None, val=None):\n        super().__init__()\n        self.save_hyperparameters()\n        if self.train is None:\n            self.raw_train = pd.read_csv(d2l.download(\n                d2l.DATA_URL + 'kaggle_house_pred_train.csv', self.root,\n                sha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))\n            self.raw_val = pd.read_csv(d2l.download(\n                d2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,\n                sha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))\n```\n\nThe training dataset includes 1460 examples,\n80 features, and one label, while the validation data\ncontains 1459 examples and 80 features. ```{.python .input  n=31}\n%%tab all\ndata = KaggleHouse(batch_size=64)\nprint(data.raw_train.shape)\nprint(data.raw_val.shape)\n```"
    },
    {
      "chunk_id": "ca1dfa9c90fb_0",
      "chapter": "kaggle-house-price",
      "heading": "Data Preprocessing",
      "text": "Let's [**take a look at the first four and final two features\nas well as the label (SalePrice)**] from the first four examples. ```{.python .input  n=10}\n%%tab all\nprint(data.raw_train.iloc[:4, [0, 1, 2, 3, -3, -2, -1]])\n```\n\nWe can see that in each example, the first feature is the identifier. This helps the model determine each training example. While this is convenient, it does not carry\nany information for prediction purposes. Hence, we will remove it from the dataset\nbefore feeding the data into the model. Furthermore, given a wide variety of data types,\nwe will need to preprocess the data before we can start modeling. Let's start with the numerical features. First, we apply a heuristic,\n[**replacing all missing values\nby the corresponding feature's mean.**]\nThen, to put all features on a common scale,\nwe (***standardize* the data by\nrescaling features to zero mean and unit variance**):\n\n$$x \\leftarrow \\frac{x - \\mu}{\\sigma},$$\n\nwhere $\\mu$ and $\\sigma$ denote mean and standard deviation, respectively. To verify that this indeed transforms\nour feature (variable) such that it has zero mean and unit variance,\nnote that $E[\\frac{x-\\mu}{\\sigma}] = \\frac{\\mu - \\mu}{\\sigma} = 0$\nand that $E[(x-\\mu)^2] = (\\sigma^2 + \\mu^2) - 2\\mu^2+\\mu^2 = \\sigma^2$. Intuitively, we standardize the data\nfor two reasons. First, it proves convenient for optimization. Second, because we do not know *a priori*\nwhich features will be relevant,\nwe do not want to penalize coefficients\nassigned to one feature more than any other. [**Next we deal with discrete values.**]\nThese include features such as \"MSZoning\". (**We replace them by a one-hot encoding**)\nin the same way that we earlier transformed\nmulticlass labels into vectors (see :numref:`subsec_classification-problem`). For instance, \"MSZoning\" assumes the values \"RL\" and \"RM\". Dropping the \"MSZoning\" feature,\ntwo new indicator features\n\"MSZoning_RL\" and \"MSZoning_RM\" are created with values being either 0 or 1."
    },
    {
      "chunk_id": "ca1dfa9c90fb_1",
      "chapter": "kaggle-house-price",
      "heading": "Data Preprocessing",
      "text": "For instance, \"MSZoning\" assumes the values \"RL\" and \"RM\". Dropping the \"MSZoning\" feature,\ntwo new indicator features\n\"MSZoning_RL\" and \"MSZoning_RM\" are created with values being either 0 or 1. According to one-hot encoding,\nif the original value of \"MSZoning\" is \"RL\",\nthen \"MSZoning_RL\" is 1 and \"MSZoning_RM\" is 0. The `pandas` package does this automatically for us. ```{.python .input  n=32}\n%%tab all\n@d2l.add_to_class(KaggleHouse)\ndef preprocess(self):\n    # Remove the ID and label columns\n    label = 'SalePrice'\n    features = pd.concat(\n        (self.raw_train.drop(columns=['Id', label]),\n         self.raw_val.drop(columns=['Id'])))\n    # Standardize numerical columns\n    numeric_features = features.dtypes[features.dtypes!='object'].index\n    features[numeric_features] = features[numeric_features].apply(\n        lambda x: (x - x.mean()) / (x.std()))\n    # Replace NAN numerical features by 0\n    features[numeric_features] = features[numeric_features].fillna(0)\n    # Replace discrete features by one-hot encoding\n    features = pd.get_dummies(features, dummy_na=True)\n    # Save preprocessed features\n    self.train = features[:self.raw_train.shape[0]].copy()\n    self.train[label] = self.raw_train[label]\n    self.val = features[self.raw_train.shape[0]:].copy()\n```\n\nYou can see that this conversion increases\nthe number of features from 79 to 331 (excluding ID and label columns). ```{.python .input  n=33}\n%%tab all\ndata.preprocess()\ndata.train.shape\n```"
    },
    {
      "chunk_id": "3d3116f12cd3_0",
      "chapter": "kaggle-house-price",
      "heading": "Error Measure",
      "text": "To get started we will train a linear model with squared loss. Not surprisingly, our linear model will not lead to a competition-winning submission but it does provide a sanity check to see whether there is meaningful information in the data. If we cannot do better than random guessing here, then there might be a good chance that we have a data processing bug. And if things work, the linear model will serve as a baseline giving us some intuition about how close the simple model gets to the best reported models, giving us a sense of how much gain we should expect from fancier models. With house prices, as with stock prices,\nwe care about relative quantities\nmore than absolute quantities. Thus [**we tend to care more about\nthe relative error $\\frac{y - \\hat{y}}{y}$**]\nthan about the absolute error $y - \\hat{y}$. For instance, if our prediction is off by \\$100,000\nwhen estimating the price of a house in rural Ohio,\nwhere the value of a typical house is \\$125,000,\nthen we are probably doing a horrible job. On the other hand, if we err by this amount\nin Los Altos Hills, California,\nthis might represent a stunningly accurate prediction\n(there, the median house price exceeds \\$4 million). (**One way to address this problem is to\nmeasure the discrepancy in the logarithm of the price estimates.**)\nIn fact, this is also the official error measure\nused by the competition to evaluate the quality of submissions. After all, a small value $\\delta$ for $|\\log y - \\log \\hat{y}| \\leq \\delta$\ntranslates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$."
    },
    {
      "chunk_id": "3d3116f12cd3_1",
      "chapter": "kaggle-house-price",
      "heading": "Error Measure",
      "text": "After all, a small value $\\delta$ for $|\\log y - \\log \\hat{y}| \\leq \\delta$\ntranslates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$. This leads to the following root-mean-squared-error between the logarithm of the predicted price and the logarithm of the label price:\n\n$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}.$$\n\n```{.python .input  n=60}\n%%tab all\n@d2l.add_to_class(KaggleHouse)\ndef get_dataloader(self, train):\n    label = 'SalePrice'\n    data = self.train if train else self.val\n    if label not in data: return\n    get_tensor = lambda x: d2l.tensor(x.values.astype(float),\n                                      dtype=d2l.float32)\n    # Logarithm of prices \n    tensors = (get_tensor(data.drop(columns=[label])),  # X\n               d2l.reshape(d2l.log(get_tensor(data[label])), (-1, 1)))  # Y\n    return self.get_tensorloader(tensors, train)\n```"
    },
    {
      "chunk_id": "8354da7ef9db_0",
      "chapter": "kaggle-house-price",
      "heading": "$K$-Fold Cross-Validation",
      "text": "You might recall that we introduced [**cross-validation**]\nin :numref:`subsec_generalization-model-selection`, where we discussed how to deal\nwith model selection.\nWe will put this to good use to select the model design\nand to adjust the hyperparameters.\nWe first need a function that returns\nthe $i^\\textrm{th}$ fold of the data\nin a $K$-fold cross-validation procedure.\nIt proceeds by slicing out the $i^\\textrm{th}$ segment\nas validation data and returning the rest as training data.\nNote that this is not the most efficient way of handling data\nand we would definitely do something much smarter\nif our dataset was considerably larger.\nBut this added complexity might obfuscate our code unnecessarily\nso we can safely omit it here owing to the simplicity of our problem.\n\n```{.python .input}\n%%tab all\ndef k_fold_data(data, k):\n    rets = []\n    fold_size = data.train.shape[0] // k\n    for j in range(k):\n        idx = range(j * fold_size, (j+1) * fold_size)\n        rets.append(KaggleHouse(data.batch_size, data.train.drop(index=idx),  \n                                data.train.loc[idx]))    \n    return rets\n```\n\n[**The average validation error is returned**]\nwhen we train $K$ times in the $K$-fold cross-validation.\n\n```{.python .input}\n%%tab all\ndef k_fold(trainer, data, k, lr):\n    val_loss, models = [], []\n    for i, data_fold in enumerate(k_fold_data(data, k)):\n        model = d2l.LinearRegression(lr)\n        model.board.yscale='log'\n        if i != 0: model.board.display = False\n        trainer.fit(model, data_fold)\n        val_loss.append(float(model.board.data['val_loss'][-1].y))\n        models.append(model)\n    print(f'average validation log mse = {sum(val_loss)/len(val_loss)}')\n    return models\n```"
    },
    {
      "chunk_id": "2ed45400f5d8_0",
      "chapter": "kaggle-house-price",
      "heading": "[**Model Selection**]",
      "text": "In this example, we pick an untuned set of hyperparameters\nand leave it up to the reader to improve the model.\nFinding a good choice can take time,\ndepending on how many variables one optimizes over.\nWith a large enough dataset,\nand the normal sorts of hyperparameters,\n$K$-fold cross-validation tends to be\nreasonably resilient against multiple testing.\nHowever, if we try an unreasonably large number of options\nwe might find that our validation\nperformance is no longer representative of the true error.\n\n```{.python .input}\n%%tab all\ntrainer = d2l.Trainer(max_epochs=10)\nmodels = k_fold(trainer, data, k=5, lr=0.01)\n```\n\nNotice that sometimes the number of training errors\nfor a set of hyperparameters can be very low,\neven as the number of errors on $K$-fold cross-validation\ngrows considerably higher.\nThis indicates that we are overfitting.\nThroughout training you will want to monitor both numbers.\nLess overfitting might indicate that our data can support a more powerful model.\nMassive overfitting might suggest that we can gain\nby incorporating regularization techniques."
    },
    {
      "chunk_id": "f257508bef60_0",
      "chapter": "kaggle-house-price",
      "heading": "[**Submitting Predictions on Kaggle**]",
      "text": "Now that we know what a good choice of hyperparameters should be,\nwe might \ncalculate the average predictions \non the test set\nby all the $K$ models.\nSaving the predictions in a csv file\nwill simplify uploading the results to Kaggle.\nThe following code will generate a file called `submission.csv`.\n\n```{.python .input}\n%%tab all\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    preds = [model(d2l.tensor(data.val.values.astype(float), dtype=d2l.float32))\n             for model in models]\nif tab.selected('jax'):\n    preds = [model.apply({'params': trainer.state.params},\n             d2l.tensor(data.val.values.astype(float), dtype=d2l.float32))\n             for model in models]\n# Taking exponentiation of predictions in the logarithm scale\nensemble_preds = d2l.reduce_mean(d2l.exp(d2l.concat(preds, 1)), 1)\nsubmission = pd.DataFrame({'Id':data.raw_val.Id,\n                           'SalePrice':d2l.numpy(ensemble_preds)})\nsubmission.to_csv('submission.csv', index=False)\n```\n\nNext, as demonstrated in :numref:`fig_kaggle_submit2`,\nwe can submit our predictions on Kaggle\nand see how they compare with the actual house prices (labels)\non the test set.\nThe steps are quite simple:\n\n* Log in to the Kaggle website and visit the house price prediction competition page.\n* Click the \u201cSubmit Predictions\u201d or \u201cLate Submission\u201d button.\n* Click the \u201cUpload Submission File\u201d button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n* Click the \u201cMake Submission\u201d button at the bottom of the page to view your results.\n\n![Submitting data to Kaggle.](../img/kaggle-submit2.png)\n:width:`400px`\n:label:`fig_kaggle_submit2`"
    },
    {
      "chunk_id": "7de4b97f8e7b_0",
      "chapter": "kaggle-house-price",
      "heading": "Summary and Discussion",
      "text": "Real data often contains a mix of different data types and needs to be preprocessed.\nRescaling real-valued data to zero mean and unit variance is a good default. So is replacing missing values with their mean.\nFurthermore, transforming categorical features into indicator features allows us to treat them like one-hot vectors.\nWhen we tend to care more about\nthe relative error than about the absolute error,\nwe can \nmeasure the discrepancy in the logarithm of the prediction.\nTo select the model and adjust the hyperparameters,\nwe can use $K$-fold cross-validation ."
    },
    {
      "chunk_id": "dc54a9f546ae_0",
      "chapter": "kaggle-house-price",
      "heading": "Exercises",
      "text": "1. Submit your predictions for this section to Kaggle. How good are they?\n1. Is it always a good idea to replace missing values by a mean? Hint: can you construct a situation where the values are not missing at random?\n1. Improve the score by tuning the hyperparameters through $K$-fold cross-validation.\n1. Improve the score by improving the model (e.g., layers, weight decay, and dropout).\n1. What happens if we do not standardize the continuous numerical features as we have done in this section?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/106)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/107)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/237)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17988)\n:end_tab:"
    },
    {
      "chunk_id": "d95826f28069_0",
      "chapter": "mlp-implementation",
      "heading": "mlp-implementation",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Implementation of Multilayer Perceptrons\n:label:`sec_mlp-implementation`\n\nMultilayer perceptrons (MLPs) are not much more complex to implement than simple linear models. The key conceptual\ndifference is that we now concatenate multiple layers.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "9c05c9cffb67_0",
      "chapter": "mlp-implementation",
      "heading": "Implementation from Scratch",
      "text": "Let's begin again by implementing such a network from scratch."
    },
    {
      "chunk_id": "e12d94e42448_0",
      "chapter": "mlp-implementation",
      "heading": "Initializing Model Parameters",
      "text": "Recall that Fashion-MNIST contains 10 classes,\nand that each image consists of a $28 \\times 28 = 784$\ngrid of grayscale pixel values. As before we will disregard the spatial structure\namong the pixels for now,\nso we can think of this as a classification dataset\nwith 784 input features and 10 classes. To begin, we will [**implement an MLP\nwith one hidden layer and 256 hidden units.**]\nBoth the number of layers and their width are adjustable\n(they are considered hyperparameters). Typically, we choose the layer widths to be divisible by larger powers of 2. This is computationally efficient due to the way\nmemory is allocated and addressed in hardware. Again, we will represent our parameters with several tensors. Note that *for every layer*, we must keep track of\none weight matrix and one bias vector. As always, we allocate memory\nfor the gradients of the loss with respect to these parameters. :begin_tab:`mxnet`\nIn the code below, we first define and initialize the parameters\nand then enable gradient tracking. :end_tab:\n\n:begin_tab:`pytorch`\nIn the code below we use `nn.Parameter`\nto automatically register\na class attribute as a parameter to be tracked by `autograd` (:numref:`sec_autograd`). :end_tab:\n\n:begin_tab:`tensorflow`\nIn the code below we use `tf.Variable`\nto define the model parameter. :end_tab:\n\n:begin_tab:`jax`\nIn the code below we use `flax.linen.Module.param`\nto define the model parameter."
    },
    {
      "chunk_id": "e12d94e42448_1",
      "chapter": "mlp-implementation",
      "heading": "Initializing Model Parameters",
      "text": ":end_tab:\n\n:begin_tab:`tensorflow`\nIn the code below we use `tf.Variable`\nto define the model parameter. :end_tab:\n\n:begin_tab:`jax`\nIn the code below we use `flax.linen.Module.param`\nto define the model parameter. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nclass MLPScratch(d2l.Classifier):\n    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.W1 = np.random.randn(num_inputs, num_hiddens) * sigma\n        self.b1 = np.zeros(num_hiddens)\n        self.W2 = np.random.randn(num_hiddens, num_outputs) * sigma\n        self.b2 = np.zeros(num_outputs)\n        for param in self.get_scratch_params():\n            param.attach_grad()\n```\n\n```{.python .input}\n%%tab pytorch\nclass MLPScratch(d2l.Classifier):\n    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n        self.b1 = nn.Parameter(torch.zeros(num_hiddens))\n        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n        self.b2 = nn.Parameter(torch.zeros(num_outputs))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MLPScratch(d2l.Classifier):\n    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.W1 = tf.Variable(\n            tf.random.normal((num_inputs, num_hiddens)) * sigma)\n        self.b1 = tf.Variable(tf.zeros(num_hiddens))\n        self.W2 = tf.Variable(\n            tf.random.normal((num_hiddens, num_outputs)) * sigma)\n        self.b2 = tf.Variable(tf.zeros(num_outputs))\n```\n\n```{.python .input}\n%%tab jax\nclass MLPScratch(d2l.Classifier):\n    num_inputs: int\n    num_outputs: int\n    num_hiddens: int\n    lr: float\n    sigma: float = 0.01\n\n    def setup(self):\n        self.W1 = self.param('W1', nn.initializers.normal(self.sigma),\n                             (self.num_inputs, self.num_hiddens))\n        self.b1 = self.param('b1', nn.initializers.zeros, self.num_hiddens)\n        self.W2 = self.param('W2', nn.initializers.normal(self.sigma),\n                             (self.num_hiddens, self.num_outputs))\n        self.b2 = self.param('b2', nn.initializers.zeros, self.num_outputs)\n```"
    },
    {
      "chunk_id": "f357bd429817_0",
      "chapter": "mlp-implementation",
      "heading": "Model",
      "text": "To make sure we know how everything works,\nwe will [**implement the ReLU activation**] ourselves\nrather than invoking the built-in `relu` function directly.\n\n```{.python .input}\n%%tab mxnet\ndef relu(X):\n    return np.maximum(X, 0)\n```\n\n```{.python .input}\n%%tab pytorch\ndef relu(X):\n    a = torch.zeros_like(X)\n    return torch.max(X, a)\n```\n\n```{.python .input}\n%%tab tensorflow\ndef relu(X):\n    return tf.math.maximum(X, 0)\n```\n\n```{.python .input}\n%%tab jax\ndef relu(X):\n    return jnp.maximum(X, 0)\n```\n\nSince we are disregarding spatial structure,\nwe `reshape` each two-dimensional image into\na flat vector of length  `num_inputs`.\nFinally, we (**implement our model**)\nwith just a few lines of code. Since we use the framework built-in autograd this is all that it takes.\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(MLPScratch)\ndef forward(self, X):\n    X = d2l.reshape(X, (-1, self.num_inputs))\n    H = relu(d2l.matmul(X, self.W1) + self.b1)\n    return d2l.matmul(H, self.W2) + self.b2\n```"
    },
    {
      "chunk_id": "713680532bcb_0",
      "chapter": "mlp-implementation",
      "heading": "Training",
      "text": "Fortunately, [**the training loop for MLPs\nis exactly the same as for softmax regression.**] We define the model, data, and trainer, then finally invoke the `fit` method on model and data.\n\n```{.python .input}\n%%tab all\nmodel = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)\ndata = d2l.FashionMNIST(batch_size=256)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "f71ff2903f6d_0",
      "chapter": "mlp-implementation",
      "heading": "Concise Implementation",
      "text": "As you might expect, by relying on the high-level APIs, we can implement MLPs even more concisely."
    },
    {
      "chunk_id": "f357bd429817_0",
      "chapter": "mlp-implementation",
      "heading": "Model",
      "text": "Compared with our concise implementation\nof softmax regression implementation\n(:numref:`sec_softmax_concise`),\nthe only difference is that we add\n*two* fully connected layers where we previously added only *one*. The first is [**the hidden layer**],\nthe second is the output layer. ```{.python .input}\n%%tab mxnet\nclass MLP(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = nn.Sequential()\n        self.net.add(nn.Dense(num_hiddens, activation='relu'),\n                     nn.Dense(num_outputs))\n        self.net.initialize()\n```\n\n```{.python .input}\n%%tab pytorch\nclass MLP(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),\n                                 nn.ReLU(), nn.LazyLinear(num_outputs))\n```\n\n```{.python .input}\n%%tab tensorflow\nclass MLP(d2l.Classifier):\n    def __init__(self, num_outputs, num_hiddens, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.net = tf.keras.models.Sequential([\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(num_hiddens, activation='relu'),\n            tf.keras.layers.Dense(num_outputs)])\n```\n\n```{.python .input}\n%%tab jax\nclass MLP(d2l.Classifier):\n    num_outputs: int\n    num_hiddens: int\n    lr: float\n\n    @nn.compact\n    def __call__(self, X):\n        X = X.reshape((X.shape[0], -1))  # Flatten\n        X = nn.Dense(self.num_hiddens)(X)\n        X = nn.relu(X)\n        X = nn.Dense(self.num_outputs)(X)\n        return X\n```\n\nPreviously, we defined `forward` methods for models to transform input using the model parameters."
    },
    {
      "chunk_id": "f357bd429817_1",
      "chapter": "mlp-implementation",
      "heading": "Model",
      "text": "These operations are essentially a pipeline:\nyou take an input and\napply a transformation (e.g.,\nmatrix multiplication with weights followed by bias addition),\nthen repetitively use the output of the current transformation as\ninput to the next transformation. However, you may have noticed that \nno `forward` method is defined here. In fact, `MLP` inherits the `forward` method from the `Module` class (:numref:`subsec_oo-design-models`) to \nsimply invoke `self.net(X)` (`X` is input),\nwhich is now defined as a sequence of transformations\nvia the `Sequential` class. The `Sequential` class abstracts the forward process\nenabling us to focus on the transformations. We will further discuss how the `Sequential` class works in :numref:`subsec_model-construction-sequential`."
    },
    {
      "chunk_id": "713680532bcb_0",
      "chapter": "mlp-implementation",
      "heading": "Training",
      "text": "[**The training loop**] is exactly the same\nas when we implemented softmax regression.\nThis modularity enables us to separate\nmatters concerning the model architecture\nfrom orthogonal considerations.\n\n```{.python .input}\n%%tab all\nmodel = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "cb0d52d01949_0",
      "chapter": "mlp-implementation",
      "heading": "Summary",
      "text": "Now that we have more practice in designing deep networks, the step from a single to multiple layers of deep networks does not pose such a significant challenge any longer. In particular, we can reuse the training algorithm and data loader. Note, though, that implementing MLPs from scratch is nonetheless messy: naming and keeping track of the model parameters makes it difficult to extend models. For instance, imagine wanting to insert another layer between layers 42 and 43. This might now be layer 42b, unless we are willing to perform sequential renaming. Moreover, if we implement the network from scratch, it is much more difficult for the framework to perform meaningful performance optimizations.\n\nNonetheless, you have now reached the state of the art of the late 1980s when fully connected deep networks were the method of choice for neural network modeling. Our next conceptual step will be to consider images. Before we do so, we need to review a number of statistical basics and details on how to compute models efficiently."
    },
    {
      "chunk_id": "5dd7ad0ac1af_0",
      "chapter": "mlp-implementation",
      "heading": "Exercises",
      "text": "1. Change the number of hidden units `num_hiddens` and plot how its number affects the accuracy of the model. What is the best value of this hyperparameter?\n1. Try adding a hidden layer to see how it affects the results.\n1. Why is it a bad idea to insert a hidden layer with a single neuron? What could go wrong?\n1. How does changing the learning rate alter your results? With all other parameters fixed, which learning rate gives you the best results? How does this relate to the number of epochs?\n1. Let's optimize over all hyperparameters jointly, i.e., learning rate, number of epochs, number of hidden layers, and number of hidden units per layer.\n    1. What is the best result you can get by optimizing over all of them?\n    1. Why it is much more challenging to deal with multiple hyperparameters?\n    1. Describe an efficient strategy for optimizing over multiple parameters jointly.\n1. Compare the speed of the framework and the from-scratch implementation for a challenging problem. How does it change with the complexity of the network?\n1. Measure the speed of tensor--matrix multiplications for well-aligned and misaligned matrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and 1032.\n    1. How does this change between GPUs and CPUs?\n    1. Determine the memory bus width of your CPU and GPU.\n1. Try out different activation functions. Which one works best?\n1. Is there a difference between weight initializations of the network? Does it matter?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/92)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/93)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/227)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17985)\n:end_tab:"
    },
    {
      "chunk_id": "3923743c6d31_0",
      "chapter": "mlp",
      "heading": "mlp",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Multilayer Perceptrons\n:label:`sec_mlp`\n\nIn :numref:`sec_softmax`, we introduced\nsoftmax regression,\nimplementing the algorithm from scratch\n(:numref:`sec_softmax_scratch`) and using high-level APIs\n(:numref:`sec_softmax_concise`). This allowed us to\ntrain classifiers capable of recognizing\n10 categories of clothing from low-resolution images.\nAlong the way, we learned how to wrangle data,\ncoerce our outputs into a valid probability distribution,\napply an appropriate loss function,\nand minimize it with respect to our model's parameters.\nNow that we have mastered these mechanics\nin the context of simple linear models,\nwe can launch our exploration of deep neural networks,\nthe comparatively rich class of models\nwith which this book is primarily concerned.\n\n```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nfrom jax import grad, vmap\n```"
    },
    {
      "chunk_id": "4b91b2e36563_0",
      "chapter": "mlp",
      "heading": "Hidden Layers",
      "text": "We described affine transformations in\n:numref:`subsec_linear_model` as\nlinear transformations with added bias.\nTo begin, recall the model architecture\ncorresponding to our softmax regression example,\nillustrated in  :numref:`fig_softmaxreg`.\nThis model maps inputs directly to outputs\nvia a single affine transformation,\nfollowed by a softmax operation.\nIf our labels truly were related\nto the input data by a simple affine transformation,\nthen this approach would be sufficient.\nHowever, linearity (in affine transformations) is a *strong* assumption."
    },
    {
      "chunk_id": "5ef21cd051c0_0",
      "chapter": "mlp",
      "heading": "Limitations of Linear Models",
      "text": "For example, linearity implies the *weaker*\nassumption of *monotonicity*, i.e.,\nthat any increase in our feature must\neither always cause an increase in our model's output\n(if the corresponding weight is positive),\nor always cause a decrease in our model's output\n(if the corresponding weight is negative). Sometimes that makes sense. For example, if we were trying to predict\nwhether an individual will repay a loan,\nwe might reasonably assume that all other things being equal,\nan applicant with a higher income\nwould always be more likely to repay\nthan one with a lower income. While monotonic, this relationship likely\nis not linearly associated with the probability of\nrepayment. An increase in income from \\$0 to \\$50,000\nlikely corresponds to a bigger increase\nin likelihood of repayment\nthan an increase from \\$1 million to \\$1.05 million. One way to handle this might be to postprocess our outcome\nsuch that linearity becomes more plausible,\nby using the logistic map (and thus the logarithm of the probability of outcome). Note that we can easily come up with examples\nthat violate monotonicity. Say for example that we want to predict health as a function\nof body temperature. For individuals with a normal body temperature\nabove 37\u00b0C (98.6\u00b0F),\nhigher temperatures indicate greater risk. However, if the body temperatures drops\nbelow 37\u00b0C, lower temperatures indicate greater risk! Again, we might resolve the problem\nwith some clever preprocessing, such as using the distance from 37\u00b0C\nas a feature. But what about classifying images of cats and dogs? Should increasing the intensity\nof the pixel at location (13, 17)\nalways increase (or always decrease)\nthe likelihood that the image depicts a dog? Reliance on a linear model corresponds to the implicit\nassumption that the only requirement\nfor differentiating cats and dogs is to assess\nthe brightness of individual pixels. This approach is doomed to fail in a world\nwhere inverting an image preserves the category."
    },
    {
      "chunk_id": "5ef21cd051c0_1",
      "chapter": "mlp",
      "heading": "Limitations of Linear Models",
      "text": "This approach is doomed to fail in a world\nwhere inverting an image preserves the category. And yet despite the apparent absurdity of linearity here,\nas compared with our previous examples,\nit is less obvious that we could address the problem\nwith a simple preprocessing fix. That is, because the significance of any pixel\ndepends in complex ways on its context\n(the values of the surrounding pixels). While there might exist a representation of our data\nthat would take into account\nthe relevant interactions among our features,\non top of which a linear model would be suitable,\nwe simply do not know how to calculate it by hand. With deep neural networks, we used observational data\nto jointly learn both a representation via hidden layers\nand a linear predictor that acts upon that representation. This problem of nonlinearity has been studied for at least a\ncentury :cite:`Fisher.1928`. For instance, decision trees\nin their most basic form use a sequence of binary decisions to\ndecide upon class membership :cite:`quinlan2014c4`. Likewise, kernel\nmethods have been used for many decades to model nonlinear dependencies\n:cite:`Aronszajn.1950`. This has found its way into\nnonparametric spline models :cite:`Wahba.1990` and kernel methods\n:cite:`Scholkopf.Smola.2002`. It is also something that the brain solves\nquite naturally. After all, neurons feed into other neurons which,\nin turn, feed into other neurons again :cite:`Cajal.Azoulay.1894`. Consequently we have a sequence of relatively simple transformations."
    },
    {
      "chunk_id": "d4397a62eeec_0",
      "chapter": "mlp",
      "heading": "Incorporating Hidden Layers",
      "text": "We can overcome the limitations of linear models\nby incorporating one or more hidden layers.\nThe easiest way to do this is to stack\nmany fully connected layers on top of one another.\nEach layer feeds into the layer above it,\nuntil we generate outputs.\nWe can think of the first $L-1$ layers\nas our representation and the final layer\nas our linear predictor.\nThis architecture is commonly called\na *multilayer perceptron*,\noften abbreviated as *MLP* (:numref:`fig_mlp`).\n\n![An MLP with a hidden layer of five hidden units.](../img/mlp.svg)\n:label:`fig_mlp`\n\nThis MLP has four inputs, three outputs,\nand its hidden layer contains five hidden units.\nSince the input layer does not involve any calculations,\nproducing outputs with this network\nrequires implementing the computations\nfor both the hidden and output layers;\nthus, the number of layers in this MLP is two.\nNote that both layers are fully connected.\nEvery input influences every neuron in the hidden layer,\nand each of these in turn influences\nevery neuron in the output layer. Alas, we are not quite\ndone yet."
    },
    {
      "chunk_id": "e8c8fbe3c0f6_0",
      "chapter": "mlp",
      "heading": "From Linear to Nonlinear",
      "text": "As before, we denote by the matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$\na minibatch of $n$ examples where each example has $d$ inputs (features). For a one-hidden-layer MLP whose hidden layer has $h$ hidden units,\nwe denote by $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$\nthe outputs of the hidden layer, which are\n*hidden representations*. Since the hidden and output layers are both fully connected,\nwe have hidden-layer weights $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}$ and biases $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}$\nand output-layer weights $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times q}$ and biases $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{1 \\times q}$. This allows us to calculate the outputs $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$\nof the one-hidden-layer MLP as follows:\n\n$$\n\\begin{aligned}\n    \\mathbf{H} & = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}, \\\\\n    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}. \\end{aligned}\n$$\n\nNote that after adding the hidden layer,\nour model now requires us to track and update\nadditional sets of parameters. So what have we gained in exchange? You might be surprised to find out\nthat---in the model defined above---*we\ngain nothing for our troubles*! The reason is plain. The hidden units above are given by\nan affine function of the inputs,\nand the outputs (pre-softmax) are just\nan affine function of the hidden units. An affine function of an affine function\nis itself an affine function. Moreover, our linear model was already\ncapable of representing any affine function."
    },
    {
      "chunk_id": "e8c8fbe3c0f6_1",
      "chapter": "mlp",
      "heading": "From Linear to Nonlinear",
      "text": "An affine function of an affine function\nis itself an affine function. Moreover, our linear model was already\ncapable of representing any affine function. To see this formally we can just collapse out the hidden layer in the above definition,\nyielding an equivalent single-layer model with parameters\n$\\mathbf{W} = \\mathbf{W}^{(1)}\\mathbf{W}^{(2)}$ and $\\mathbf{b} = \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}$:\n\n$$\n\\mathbf{O} = (\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W}^{(1)}\\mathbf{W}^{(2)} + \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}. $$\n\nIn order to realize the potential of multilayer architectures,\nwe need one more key ingredient: a\nnonlinear *activation function* $\\sigma$\nto be applied to each hidden unit\nfollowing the affine transformation. For instance, a popular\nchoice is the ReLU (rectified linear unit) activation function :cite:`Nair.Hinton.2010`\n$\\sigma(x) = \\mathrm{max}(0, x)$ operating on its arguments elementwise. The outputs of activation functions $\\sigma(\\cdot)$\nare called *activations*. In general, with activation functions in place,\nit is no longer possible to collapse our MLP into a linear model:\n\n$$\n\\begin{aligned}\n    \\mathbf{H} & = \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}), \\\\\n    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\\\\\n\\end{aligned}\n$$\n\nSince each row in $\\mathbf{X}$ corresponds to an example in the minibatch,\nwith some abuse of notation, we define the nonlinearity\n$\\sigma$ to apply to its inputs in a rowwise fashion,\ni.e., one example at a time. Note that we used the same notation for softmax\nwhen we denoted a rowwise operation in :numref:`subsec_softmax_vectorization`. Quite frequently the activation functions we use apply not merely rowwise but\nelementwise. That means that after computing the linear portion of the layer,\nwe can calculate each activation\nwithout looking at the values taken by the other hidden units."
    },
    {
      "chunk_id": "e8c8fbe3c0f6_2",
      "chapter": "mlp",
      "heading": "From Linear to Nonlinear",
      "text": "Quite frequently the activation functions we use apply not merely rowwise but\nelementwise. That means that after computing the linear portion of the layer,\nwe can calculate each activation\nwithout looking at the values taken by the other hidden units. To build more general MLPs, we can continue stacking\nsuch hidden layers,\ne.g., $\\mathbf{H}^{(1)} = \\sigma_1(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})$\nand $\\mathbf{H}^{(2)} = \\sigma_2(\\mathbf{H}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)})$,\none atop another, yielding ever more expressive models."
    },
    {
      "chunk_id": "243eaa897373_0",
      "chapter": "mlp",
      "heading": "Universal Approximators",
      "text": "We know that the brain is capable of very sophisticated statistical analysis. As such,\nit is worth asking, just *how powerful* a deep network could be. This question\nhas been answered multiple times, e.g., in :citet:`Cybenko.1989` in the context\nof MLPs, and in :citet:`micchelli1984interpolation` in the context of reproducing kernel\nHilbert spaces in a way that could be seen as radial basis function (RBF) networks with a single hidden layer.\nThese (and related results) suggest that even with a single-hidden-layer network,\ngiven enough nodes (possibly absurdly many),\nand the right set of weights,\nwe can model any function.\nActually learning that function is the hard part, though.\nYou might think of your neural network\nas being a bit like the C programming language.\nThe language, like any other modern language,\nis capable of expressing any computable program.\nBut actually coming up with a program\nthat meets your specifications is the hard part.\n\nMoreover, just because a single-hidden-layer network\n*can* learn any function\ndoes not mean that you should try\nto solve all of your problems\nwith one. In fact, in this case kernel methods\nare way more effective, since they are capable of solving the problem\n*exactly* even in infinite dimensional spaces :cite:`Kimeldorf.Wahba.1971,Scholkopf.Herbrich.Smola.2001`.\nIn fact, we can approximate many functions\nmuch more compactly by using deeper (rather than wider) networks :cite:`Simonyan.Zisserman.2014`.\nWe will touch upon more rigorous arguments in subsequent chapters."
    },
    {
      "chunk_id": "2afb792aeaed_0",
      "chapter": "mlp",
      "heading": "Activation Functions",
      "text": ":label:`subsec_activation-functions`\n\nActivation functions decide whether a neuron should be activated or not by\ncalculating the weighted sum and further adding bias to it.\nThey are differentiable operators for transforming input signals to outputs,\nwhile most of them add nonlinearity.\nBecause activation functions are fundamental to deep learning,\n(**let's briefly survey some common ones**)."
    },
    {
      "chunk_id": "d4ad44a8e8b2_0",
      "chapter": "mlp",
      "heading": "ReLU Function",
      "text": "The most popular choice,\ndue to both simplicity of implementation and\nits good performance on a variety of predictive tasks,\nis the *rectified linear unit* (*ReLU*) :cite:`Nair.Hinton.2010`. [**ReLU provides a very simple nonlinear transformation**]. Given an element $x$, the function is defined\nas the maximum of that element and $0$:\n\n$$\\operatorname{ReLU}(x) = \\max(x, 0).$$\n\nInformally, the ReLU function retains only positive\nelements and discards all negative elements\nby setting the corresponding activations to 0. To gain some intuition, we can plot the function. As you can see, the activation function is piecewise linear. ```{.python .input}\n%%tab mxnet\nx = np.arange(-8.0, 8.0, 0.1)\nx.attach_grad()\nwith autograd.record():\n    y = npx.relu(x)\nd2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)\ny = tf.nn.relu(x)\nd2l.plot(x.numpy(), y.numpy(), 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.arange(-8.0, 8.0, 0.1)\ny = jax.nn.relu(x)\nd2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\nWhen the input is negative,\nthe derivative of the ReLU function is 0,\nand when the input is positive,\nthe derivative of the ReLU function is 1. Note that the ReLU function is not differentiable\nwhen the input takes value precisely equal to 0. In these cases, we default to the left-hand-side\nderivative and say that the derivative is 0 when the input is 0. We can get away with this because\nthe input may never actually be zero (mathematicians would\nsay that it is nondifferentiable on a set of measure zero). There is an old adage that if subtle boundary conditions matter,\nwe are probably doing (*real*) mathematics, not engineering."
    },
    {
      "chunk_id": "d4ad44a8e8b2_1",
      "chapter": "mlp",
      "heading": "ReLU Function",
      "text": "There is an old adage that if subtle boundary conditions matter,\nwe are probably doing (*real*) mathematics, not engineering. That conventional wisdom may apply here, or at least, the fact that\nwe are not performing constrained optimization :cite:`Mangasarian.1965,Rockafellar.1970`. We plot the derivative of the ReLU function below. ```{.python .input}\n%%tab mxnet\ny.backward()\nd2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab pytorch\ny.backward(torch.ones_like(x), retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab tensorflow\nwith tf.GradientTape() as t:\n    y = tf.nn.relu(x)\nd2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of relu',\n         figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab jax\ngrad_relu = vmap(grad(jax.nn.relu))\nd2l.plot(x, grad_relu(x), 'x', 'grad of relu', figsize=(5, 2.5))\n```\n\nThe reason for using ReLU is that\nits derivatives are particularly well behaved:\neither they vanish or they just let the argument through. This makes optimization better behaved\nand it mitigated the well-documented problem\nof vanishing gradients that plagued\nprevious versions of neural networks (more on this later). Note that there are many variants to the ReLU function,\nincluding the *parametrized ReLU* (*pReLU*) function :cite:`He.Zhang.Ren.ea.2015`. This variation adds a linear term to ReLU,\nso some information still gets through,\neven when the argument is negative:\n\n$$\\operatorname{pReLU}(x) = \\max(0, x) + \\alpha \\min(0, x).$$"
    },
    {
      "chunk_id": "b26672199a80_0",
      "chapter": "mlp",
      "heading": "Sigmoid Function",
      "text": "[**The *sigmoid function* transforms those inputs**]\nwhose values lie in the domain $\\mathbb{R}$,\n(**to outputs that lie on the interval (0, 1).**)\nFor that reason, the sigmoid is\noften called a *squashing function*:\nit squashes any input in the range (-inf, inf)\nto some value in the range (0, 1):\n\n$$\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$\n\nIn the earliest neural networks, scientists\nwere interested in modeling biological neurons\nthat either *fire* or *do not fire*. Thus the pioneers of this field,\ngoing all the way back to McCulloch and Pitts,\nthe inventors of the artificial neuron,\nfocused on thresholding units :cite:`McCulloch.Pitts.1943`. A thresholding activation takes value 0\nwhen its input is below some threshold\nand value 1 when the input exceeds the threshold. When attention shifted to gradient-based learning,\nthe sigmoid function was a natural choice\nbecause it is a smooth, differentiable\napproximation to a thresholding unit. Sigmoids are still widely used as\nactivation functions on the output units\nwhen we want to interpret the outputs as probabilities\nfor binary classification problems: you can think of the sigmoid as a special case of the softmax. However, the sigmoid has largely been replaced\nby the simpler and more easily trainable ReLU\nfor most use in hidden layers. Much of this has to do\nwith the fact that the sigmoid poses challenges for optimization\n:cite:`LeCun.Bottou.Orr.ea.1998` since its gradient vanishes for large positive *and* negative arguments. This can lead to plateaus that are difficult to escape from. Nonetheless sigmoids are important. In later chapters (e.g., :numref:`sec_lstm`) on recurrent neural networks,\nwe will describe architectures that leverage sigmoid units\nto control the flow of information across time. Below, we plot the sigmoid function. Note that when the input is close to 0,\nthe sigmoid function approaches\na linear transformation."
    },
    {
      "chunk_id": "b26672199a80_1",
      "chapter": "mlp",
      "heading": "Sigmoid Function",
      "text": "Below, we plot the sigmoid function. Note that when the input is close to 0,\nthe sigmoid function approaches\na linear transformation. ```{.python .input}\n%%tab mxnet\nwith autograd.record():\n    y = npx.sigmoid(x)\nd2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab pytorch\ny = torch.sigmoid(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab tensorflow\ny = tf.nn.sigmoid(x)\nd2l.plot(x.numpy(), y.numpy(), 'x', 'sigmoid(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab jax\ny = jax.nn.sigmoid(x)\nd2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))\n```\n\nThe derivative of the sigmoid function is given by the following equation:\n\n$$\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).$$\n\n\nThe derivative of the sigmoid function is plotted below. Note that when the input is 0,\nthe derivative of the sigmoid function\nreaches a maximum of 0.25. As the input diverges from 0 in either direction,\nthe derivative approaches 0. ```{.python .input}\n%%tab mxnet\ny.backward()\nd2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab pytorch\n# Clear out previous gradients\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab tensorflow\nwith tf.GradientTape() as t:\n    y = tf.nn.sigmoid(x)\nd2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of sigmoid',\n         figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab jax\ngrad_sigmoid = vmap(grad(jax.nn.sigmoid))\nd2l.plot(x, grad_sigmoid(x), 'x', 'grad of sigmoid', figsize=(5, 2.5))\n```"
    },
    {
      "chunk_id": "f92eb9ff42a4_0",
      "chapter": "mlp",
      "heading": "Tanh Function",
      "text": ":label:`subsec_tanh`\n\nLike the sigmoid function, [**the tanh (hyperbolic tangent)\nfunction also squashes its inputs**],\ntransforming them into elements on the interval (**between $-1$ and $1$**):\n\n$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$\n\nWe plot the tanh function below. Note that as input nears 0, the tanh function approaches a linear transformation. Although the shape of the function is similar to that of the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system :cite:`Kalman.Kwasny.1992`. ```{.python .input}\n%%tab mxnet\nwith autograd.record():\n    y = np.tanh(x)\nd2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab pytorch\ny = torch.tanh(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab tensorflow\ny = tf.nn.tanh(x)\nd2l.plot(x.numpy(), y.numpy(), 'x', 'tanh(x)', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab jax\ny = jax.nn.tanh(x)\nd2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))\n```\n\nThe derivative of the tanh function is:\n\n$$\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x).$$\n\nIt is plotted below. As the input nears 0,\nthe derivative of the tanh function approaches a maximum of 1. And as we saw with the sigmoid function,\nas input moves away from 0 in either direction,\nthe derivative of the tanh function approaches 0. ```{.python .input}\n%%tab mxnet\ny.backward()\nd2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab pytorch\n# Clear out previous gradients\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab tensorflow\nwith tf.GradientTape() as t:\n    y = tf.nn.tanh(x)\nd2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of tanh',\n         figsize=(5, 2.5))\n```\n\n```{.python .input}\n%%tab jax\ngrad_tanh = vmap(grad(jax.nn.tanh))\nd2l.plot(x, grad_tanh(x), 'x', 'grad of tanh', figsize=(5, 2.5))\n```"
    },
    {
      "chunk_id": "dce3d97a83ce_0",
      "chapter": "mlp",
      "heading": "Summary and Discussion",
      "text": "We now know how to incorporate nonlinearities\nto build expressive multilayer neural network architectures.\nAs a side note, your knowledge already\nputs you in command of a similar toolkit\nto a practitioner circa 1990.\nIn some ways, you have an advantage\nover anyone working back then,\nbecause you can leverage powerful\nopen-source deep learning frameworks\nto build models rapidly, using only a few lines of code.\nPreviously, training these networks\nrequired researchers to code up layers and derivatives\nexplicitly in C, Fortran, or even Lisp (in the case of LeNet).\n\nA secondary benefit is that ReLU is significantly more amenable to\noptimization than the sigmoid or the tanh function. One could argue\nthat this was one of the key innovations that helped the resurgence\nof deep learning over the past decade. Note, though, that research in\nactivation functions has not stopped.\nFor instance, \nthe GELU (Gaussian error linear unit)\nactivation function $x \\Phi(x)$ by :citet:`Hendrycks.Gimpel.2016` ($\\Phi(x)$\nis the standard Gaussian cumulative distribution function) \nand\nthe Swish activation\nfunction $\\sigma(x) = x \\operatorname{sigmoid}(\\beta x)$ as proposed in :citet:`Ramachandran.Zoph.Le.2017` can yield better accuracy\nin many cases."
    },
    {
      "chunk_id": "936b6897a0f9_0",
      "chapter": "mlp",
      "heading": "Exercises",
      "text": "1. Show that adding layers to a *linear* deep network, i.e., a network without\n   nonlinearity $\\sigma$ can never increase the expressive power of the network.\n   Give an example where it actively reduces it.\n1. Compute the derivative of the pReLU activation function.\n1. Compute the derivative of the Swish activation function $x \\operatorname{sigmoid}(\\beta x)$.\n1. Show that an MLP using only ReLU (or pReLU) constructs a\n   continuous piecewise linear function.\n1. Sigmoid and tanh are very similar.\n    1. Show that $\\operatorname{tanh}(x) + 1 = 2 \\operatorname{sigmoid}(2x)$.\n    1. Prove that the function classes parametrized by both nonlinearities are identical. Hint: affine layers have bias terms, too.\n1. Assume that we have a nonlinearity that applies to one minibatch at a time, such as the batch normalization :cite:`Ioffe.Szegedy.2015`. What kinds of problems do you expect this to cause?\n1. Provide an example where the gradients vanish for the sigmoid activation function.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/90)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/91)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/226)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17984)\n:end_tab:"
    },
    {
      "chunk_id": "3e2d4896a8ad_0",
      "chapter": "numerical-stability-and-init",
      "heading": "numerical-stability-and-init",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Numerical Stability and Initialization\n:label:`sec_numerical_stability`\n\n\nThus far, every model that we have implemented\nrequired that we initialize its parameters\naccording to some pre-specified distribution.\nUntil now, we took the initialization scheme for granted,\nglossing over the details of how these choices are made.\nYou might have even gotten the impression that these choices\nare not especially important.\nOn the contrary, the choice of initialization scheme\nplays a significant role in neural network learning,\nand it can be crucial for maintaining numerical stability.\nMoreover, these choices can be tied up in interesting ways\nwith the choice of the nonlinear activation function.\nWhich function we choose and how we initialize parameters\ncan determine how quickly our optimization algorithm converges.\nPoor choices here can cause us to encounter\nexploding or vanishing gradients while training.\nIn this section, we delve into these topics in greater detail\nand discuss some useful heuristics\nthat you will find useful\nthroughout your career in deep learning.\n\n```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nfrom jax import grad, vmap\n```"
    },
    {
      "chunk_id": "c09cd0a22826_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Vanishing and Exploding Gradients",
      "text": "Consider a deep network with $L$ layers,\ninput $\\mathbf{x}$ and output $\\mathbf{o}$. With each layer $l$ defined by a transformation $f_l$\nparametrized by weights $\\mathbf{W}^{(l)}$,\nwhose hidden layer output is $\\mathbf{h}^{(l)}$ (let $\\mathbf{h}^{(0)} = \\mathbf{x}$),\nour network can be expressed as:\n\n$$\\mathbf{h}^{(l)} = f_l (\\mathbf{h}^{(l-1)}) \\textrm{ and thus } \\mathbf{o} = f_L \\circ \\cdots \\circ f_1(\\mathbf{x}).$$\n\nIf all the hidden layer output and the input are vectors,\nwe can write the gradient of $\\mathbf{o}$ with respect to\nany set of parameters $\\mathbf{W}^{(l)}$ as follows:\n\n$$\\partial_{\\mathbf{W}^{(l)}} \\mathbf{o} = \\underbrace{\\partial_{\\mathbf{h}^{(L-1)}} \\mathbf{h}^{(L)}}_{ \\mathbf{M}^{(L)} \\stackrel{\\textrm{def}}{=}} \\cdots \\underbrace{\\partial_{\\mathbf{h}^{(l)}} \\mathbf{h}^{(l+1)}}_{ \\mathbf{M}^{(l+1)} \\stackrel{\\textrm{def}}{=}} \\underbrace{\\partial_{\\mathbf{W}^{(l)}} \\mathbf{h}^{(l)}}_{ \\mathbf{v}^{(l)} \\stackrel{\\textrm{def}}{=}}.$$\n\nIn other words, this gradient is\nthe product of $L-l$ matrices\n$\\mathbf{M}^{(L)} \\cdots \\mathbf{M}^{(l+1)}$\nand the gradient vector $\\mathbf{v}^{(l)}$. Thus we are susceptible to the same\nproblems of numerical underflow that often crop up\nwhen multiplying together too many probabilities. When dealing with probabilities, a common trick is to\nswitch into log-space, i.e., shifting\npressure from the mantissa to the exponent\nof the numerical representation. Unfortunately, our problem above is more serious:\ninitially the matrices $\\mathbf{M}^{(l)}$ may have a wide variety of eigenvalues. They might be small or large, and\ntheir product might be *very large* or *very small*. The risks posed by unstable gradients\ngo beyond numerical representation. Gradients of unpredictable magnitude\nalso threaten the stability of our optimization algorithms."
    },
    {
      "chunk_id": "c09cd0a22826_1",
      "chapter": "numerical-stability-and-init",
      "heading": "Vanishing and Exploding Gradients",
      "text": "They might be small or large, and\ntheir product might be *very large* or *very small*. The risks posed by unstable gradients\ngo beyond numerical representation. Gradients of unpredictable magnitude\nalso threaten the stability of our optimization algorithms. We may be facing parameter updates that are either\n(i) excessively large, destroying our model\n(the *exploding gradient* problem);\nor (ii) excessively small\n(the *vanishing gradient* problem),\nrendering learning impossible as parameters\nhardly move on each update."
    },
    {
      "chunk_id": "069c51730f0a_0",
      "chapter": "numerical-stability-and-init",
      "heading": "(**Vanishing Gradients**)",
      "text": "One frequent culprit causing the vanishing gradient problem\nis the choice of the activation function $\\sigma$\nthat is appended following each layer's linear operations. Historically, the sigmoid function\n$1/(1 + \\exp(-x))$ (introduced in :numref:`sec_mlp`)\nwas popular because it resembles a thresholding function. Since early artificial neural networks were inspired\nby biological neural networks,\nthe idea of neurons that fire either *fully* or *not at all*\n(like biological neurons) seemed appealing. Let's take a closer look at the sigmoid\nto see why it can cause vanishing gradients. ```{.python .input}\n%%tab mxnet\nx = np.arange(-8.0, 8.0, 0.1)\nx.attach_grad()\nwith autograd.record():\n    y = npx.sigmoid(x)\ny.backward()\n\nd2l.plot(x, [y, x.grad], legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.sigmoid(x)\ny.backward(torch.ones_like(x))\n\nd2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],\n         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.Variable(tf.range(-8.0, 8.0, 0.1))\nwith tf.GradientTape() as t:\n    y = tf.nn.sigmoid(x)\nd2l.plot(x.numpy(), [y.numpy(), t.gradient(y, x).numpy()],\n         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.arange(-8.0, 8.0, 0.1)\ny = jax.nn.sigmoid(x)\ngrad_sigmoid = vmap(grad(jax.nn.sigmoid))\nd2l.plot(x, [y, grad_sigmoid(x)],\n         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))\n```\n\nAs you can see, (**the sigmoid's gradient vanishes\nboth when its inputs are large and when they are small**). Moreover, when backpropagating through many layers,\nunless we are in the Goldilocks zone, where\nthe inputs to many of the sigmoids are close to zero,\nthe gradients of the overall product may vanish. When our network boasts many layers,\nunless we are careful, the gradient\nwill likely be cut off at some layer. Indeed, this problem used to plague deep network training."
    },
    {
      "chunk_id": "069c51730f0a_1",
      "chapter": "numerical-stability-and-init",
      "heading": "(**Vanishing Gradients**)",
      "text": "When our network boasts many layers,\nunless we are careful, the gradient\nwill likely be cut off at some layer. Indeed, this problem used to plague deep network training. Consequently, ReLUs, which are more stable\n(but less neurally plausible),\nhave emerged as the default choice for practitioners."
    },
    {
      "chunk_id": "fcb27dfec923_0",
      "chapter": "numerical-stability-and-init",
      "heading": "[**Exploding Gradients**]",
      "text": "The opposite problem, when gradients explode,\ncan be similarly vexing.\nTo illustrate this a bit better,\nwe draw 100 Gaussian random matrices\nand multiply them with some initial matrix.\nFor the scale that we picked\n(the choice of the variance $\\sigma^2=1$),\nthe matrix product explodes.\nWhen this happens because of the initialization\nof a deep network, we have no chance of getting\na gradient descent optimizer to converge.\n\n```{.python .input}\n%%tab mxnet\nM = np.random.normal(size=(4, 4))\nprint('a single matrix', M)\nfor i in range(100):\n    M = np.dot(M, np.random.normal(size=(4, 4)))\nprint('after multiplying 100 matrices', M)\n```\n\n```{.python .input}\n%%tab pytorch\nM = torch.normal(0, 1, size=(4, 4))\nprint('a single matrix \\n',M)\nfor i in range(100):\n    M = M @ torch.normal(0, 1, size=(4, 4))\nprint('after multiplying 100 matrices\\n', M)\n```\n\n```{.python .input}\n%%tab tensorflow\nM = tf.random.normal((4, 4))\nprint('a single matrix \\n', M)\nfor i in range(100):\n    M = tf.matmul(M, tf.random.normal((4, 4)))\nprint('after multiplying 100 matrices\\n', M.numpy())\n```\n\n```{.python .input}\n%%tab jax\nget_key = lambda: jax.random.PRNGKey(d2l.get_seed())  # Generate PRNG keys\nM = jax.random.normal(get_key(), (4, 4))\nprint('a single matrix \\n', M)\nfor i in range(100):\n    M = jnp.matmul(M, jax.random.normal(get_key(), (4, 4)))\nprint('after multiplying 100 matrices\\n', M)\n```"
    },
    {
      "chunk_id": "1a6c6fb00b3d_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Breaking the Symmetry",
      "text": "Another problem in neural network design\nis the symmetry inherent in their parametrization.\nAssume that we have a simple MLP\nwith one hidden layer and two units.\nIn this case, we could permute the weights $\\mathbf{W}^{(1)}$\nof the first layer and likewise permute\nthe weights of the output layer\nto obtain the same function.\nThere is nothing special differentiating\nthe first and second hidden units.\nIn other words, we have permutation symmetry\namong the hidden units of each layer.\n\nThis is more than just a theoretical nuisance.\nConsider the aforementioned one-hidden-layer MLP\nwith two hidden units.\nFor illustration,\nsuppose that the output layer transforms the two hidden units into only one output unit.\nImagine what would happen if we initialized\nall the parameters of the hidden layer\nas $\\mathbf{W}^{(1)} = c$ for some constant $c$.\nIn this case, during forward propagation\neither hidden unit takes the same inputs and parameters\nproducing the same activation\nwhich is fed to the output unit.\nDuring backpropagation,\ndifferentiating the output unit with respect to parameters $\\mathbf{W}^{(1)}$ gives a gradient all of whose elements take the same value.\nThus, after gradient-based iteration (e.g., minibatch stochastic gradient descent),\nall the elements of $\\mathbf{W}^{(1)}$ still take the same value.\nSuch iterations would\nnever *break the symmetry* on their own\nand we might never be able to realize\nthe network's expressive power.\nThe hidden layer would behave\nas if it had only a single unit.\nNote that while minibatch stochastic gradient descent would not break this symmetry,\ndropout regularization (to be introduced later) would!"
    },
    {
      "chunk_id": "c3028e36685b_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Parameter Initialization",
      "text": "One way of addressing---or at least mitigating---the\nissues raised above is through careful initialization.\nAs we will see later,\nadditional care during optimization\nand suitable regularization can further enhance stability."
    },
    {
      "chunk_id": "e6d45d77b5a3_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Default Initialization",
      "text": "In the previous sections, e.g., in :numref:`sec_linear_concise`,\nwe used a normal distribution\nto initialize the values of our weights.\nIf we do not specify the initialization method, the framework will\nuse a default random initialization method, which often works well in practice\nfor moderate problem sizes."
    },
    {
      "chunk_id": "5241eaf3cbe3_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Xavier Initialization",
      "text": ":label:`subsec_xavier`\n\nLet's look at the scale distribution of\nan output $o_{i}$ for some fully connected layer\n*without nonlinearities*. With $n_\\textrm{in}$ inputs $x_j$\nand their associated weights $w_{ij}$ for this layer,\nan output is given by\n\n$$o_{i} = \\sum_{j=1}^{n_\\textrm{in}} w_{ij} x_j.$$\n\nThe weights $w_{ij}$ are all drawn\nindependently from the same distribution. Furthermore, let's assume that this distribution\nhas zero mean and variance $\\sigma^2$. Note that this does not mean that the distribution has to be Gaussian,\njust that the mean and variance need to exist. For now, let's assume that the inputs to the layer $x_j$\nalso have zero mean and variance $\\gamma^2$\nand that they are independent of $w_{ij}$ and independent of each other. In this case, we can compute the mean of $o_i$:\n\n$$\n\\begin{aligned}\n    E[o_i] & = \\sum_{j=1}^{n_\\textrm{in}} E[w_{ij} x_j] \\\\&= \\sum_{j=1}^{n_\\textrm{in}} E[w_{ij}] E[x_j] \\\\&= 0, \\end{aligned}$$\n\nand the variance:\n\n$$\n\\begin{aligned}\n    \\textrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\\\\n        & = \\sum_{j=1}^{n_\\textrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\\n        & = \\sum_{j=1}^{n_\\textrm{in}} E[w^2_{ij}] E[x^2_j] \\\\\n        & = n_\\textrm{in} \\sigma^2 \\gamma^2. \\end{aligned}\n$$\n\nOne way to keep the variance fixed\nis to set $n_\\textrm{in} \\sigma^2 = 1$. Now consider backpropagation. There we face a similar problem,\nalbeit with gradients being propagated from the layers closer to the output. Using the same reasoning as for forward propagation,\nwe see that the gradients' variance can blow up\nunless $n_\\textrm{out} \\sigma^2 = 1$,\nwhere $n_\\textrm{out}$ is the number of outputs of this layer. This leaves us in a dilemma:\nwe cannot possibly satisfy both conditions simultaneously. Instead, we simply try to satisfy:\n\n$$\n\\begin{aligned}\n\\frac{1}{2} (n_\\textrm{in} + n_\\textrm{out}) \\sigma^2 = 1 \\textrm{ or equivalently }\n\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in} + n_\\textrm{out}}}."
    },
    {
      "chunk_id": "5241eaf3cbe3_1",
      "chapter": "numerical-stability-and-init",
      "heading": "Xavier Initialization",
      "text": "Instead, we simply try to satisfy:\n\n$$\n\\begin{aligned}\n\\frac{1}{2} (n_\\textrm{in} + n_\\textrm{out}) \\sigma^2 = 1 \\textrm{ or equivalently }\n\\sigma = \\sqrt{\\frac{2}{n_\\textrm{in} + n_\\textrm{out}}}. \\end{aligned}\n$$\n\nThis is the reasoning underlying the now-standard\nand practically beneficial *Xavier initialization*,\nnamed after the first author of its creators :cite:`Glorot.Bengio.2010`. Typically, the Xavier initialization\nsamples weights from a Gaussian distribution\nwith zero mean and variance\n$\\sigma^2 = \\frac{2}{n_\\textrm{in} + n_\\textrm{out}}$. We can also adapt this to\nchoose the variance when sampling weights\nfrom a uniform distribution. Note that the uniform distribution $U(-a, a)$ has variance $\\frac{a^2}{3}$. Plugging $\\frac{a^2}{3}$ into our condition on $\\sigma^2$\nprompts us to initialize according to\n\n$$U\\left(-\\sqrt{\\frac{6}{n_\\textrm{in} + n_\\textrm{out}}}, \\sqrt{\\frac{6}{n_\\textrm{in} + n_\\textrm{out}}}\\right).$$\n\nThough the assumption for nonexistence of nonlinearities\nin the above mathematical reasoning\ncan be easily violated in neural networks,\nthe Xavier initialization method\nturns out to work well in practice."
    },
    {
      "chunk_id": "a3e86dcd5323_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Beyond",
      "text": "The reasoning above barely scratches the surface\nof modern approaches to parameter initialization.\nA deep learning framework often implements over a dozen different heuristics.\nMoreover, parameter initialization continues to be\na hot area of fundamental research in deep learning.\nAmong these are heuristics specialized for\ntied (shared) parameters, super-resolution,\nsequence models, and other situations.\nFor instance,\n:citet:`Xiao.Bahri.Sohl-Dickstein.ea.2018` demonstrated the possibility of training\n10,000-layer neural networks without architectural tricks\nby using a carefully-designed initialization method.\n\nIf the topic interests you we suggest\na deep dive into this module's offerings,\nreading the papers that proposed and analyzed each heuristic,\nand then exploring the latest publications on the topic.\nPerhaps you will stumble across or even invent\na clever idea and contribute an implementation to deep learning frameworks."
    },
    {
      "chunk_id": "0ceb1b509b1b_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Summary",
      "text": "Vanishing and exploding gradients are common issues in deep networks. Great care in parameter initialization is required to ensure that gradients and parameters remain well controlled.\nInitialization heuristics are needed to ensure that the initial gradients are neither too large nor too small.\nRandom initialization is key to ensuring that symmetry is broken before optimization.\nXavier initialization suggests that, for each layer, variance of any output is not affected by the number of inputs, and variance of any gradient is not affected by the number of outputs.\nReLU activation functions mitigate the vanishing gradient problem. This can accelerate convergence."
    },
    {
      "chunk_id": "cec233a575bc_0",
      "chapter": "numerical-stability-and-init",
      "heading": "Exercises",
      "text": "1. Can you design other cases where a neural network might exhibit symmetry that needs breaking, besides the permutation symmetry in an MLP's layers?\n1. Can we initialize all weight parameters in linear regression or in softmax regression to the same value?\n1. Look up analytic bounds on the eigenvalues of the product of two matrices. What does this tell you about ensuring that gradients are well conditioned?\n1. If we know that some terms diverge, can we fix this after the fact? Look at the paper on layerwise adaptive rate scaling  for inspiration :cite:`You.Gitman.Ginsburg.2017`.\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/103)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/104)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/235)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17986)\n:end_tab:"
    },
    {
      "chunk_id": "8be27c1f70d7_0",
      "chapter": "finetuning-bert",
      "heading": "finetuning-bert",
      "text": "# Fine-Tuning BERT for Sequence-Level and Token-Level Applications\n:label:`sec_finetuning-bert`\n\n\nIn the previous sections of this chapter,\nwe have designed different models for natural language processing applications,\nsuch as based on RNNs, CNNs, attention, and MLPs.\nThese models are helpful when there is space or time constraint,\nhowever,\ncrafting a specific model for every natural language processing task\nis practically infeasible.\nIn :numref:`sec_bert`,\nwe introduced a pretraining model, BERT,\nthat requires minimal architecture changes\nfor a wide range of natural language processing tasks.\nOn the one hand,\nat the time of its proposal,\nBERT improved the state of the art on various natural language processing tasks.\nOn the other hand,\nas noted in :numref:`sec_bert-pretraining`,\nthe two versions of the original BERT model\ncome with 110 million and 340 million parameters.\nThus, when there are sufficient computational resources,\nwe may consider\nfine-tuning BERT for downstream natural language processing applications.\n\nIn the following,\nwe generalize a subset of natural language processing applications\nas sequence-level and token-level.\nOn the sequence level,\nwe introduce how to transform the BERT representation of the text input\nto the output label\nin single text classification\nand text pair classification or regression.\nOn the token level, we will briefly introduce new applications\nsuch as text tagging and question answering\nand shed light on how BERT can represent their inputs and get transformed into output labels.\nDuring fine-tuning,\nthe \"minimal architecture changes\" required by BERT across different applications\nare the extra fully connected layers.\nDuring supervised learning of a downstream application,\nparameters of the extra layers are learned from scratch while\nall the parameters in the pretrained BERT model are fine-tuned."
    },
    {
      "chunk_id": "30ce25bf1132_0",
      "chapter": "finetuning-bert",
      "heading": "Single Text Classification",
      "text": "*Single text classification* takes a single text sequence as input and outputs its classification result.\nBesides sentiment analysis that we have studied in this chapter,\nthe Corpus of Linguistic Acceptability (CoLA)\nis also a dataset for single text classification,\njudging whether a given sentence is grammatically acceptable or not :cite:`Warstadt.Singh.Bowman.2019`.\nFor instance, \"I should study.\" is acceptable but \"I should studying.\" is not.\n\n![Fine-tuning BERT for single text classification applications, such as sentiment analysis and testing linguistic acceptability. Suppose that the input single text has six tokens.](../img/bert-one-seq.svg)\n:label:`fig_bert-one-seq`\n\n:numref:`sec_bert` describes the input representation of BERT.\nThe BERT input sequence unambiguously represents both single text and text pairs,\nwhere the special classification token \n\u201c&lt;cls&gt;\u201d is used for sequence classification and \nthe special classification token \n\u201c&lt;sep&gt;\u201d marks the end of single text or separates a pair of text.\nAs shown in :numref:`fig_bert-one-seq`,\nin single text classification applications,\nthe BERT representation of the special classification token \n\u201c&lt;cls&gt;\u201d encodes the information of the entire input text sequence.\nAs the representation of the input single text,\nit will be fed into a small MLP consisting of fully connected (dense) layers\nto output the distribution of all the discrete label values."
    },
    {
      "chunk_id": "06528962d8b1_0",
      "chapter": "finetuning-bert",
      "heading": "Text Pair Classification or Regression",
      "text": "We have also examined natural language inference in this chapter.\nIt belongs to *text pair classification*,\na type of application classifying a pair of text.\n\nTaking a pair of text as input but outputting a continuous value,\n*semantic textual similarity* is a popular *text pair regression* task.\nThis task measures semantic similarity of sentences.\nFor instance, in the Semantic Textual Similarity Benchmark dataset,\nthe similarity score of a pair of sentences\nis an ordinal scale ranging from 0 (no meaning overlap) to 5 (meaning equivalence) :cite:`Cer.Diab.Agirre.ea.2017`.\nThe goal is to predict these scores.\nExamples from the Semantic Textual Similarity Benchmark dataset include (sentence 1, sentence 2, similarity score):\n\n* \"A plane is taking off.\", \"An air plane is taking off.\", 5.000;\n* \"A woman is eating something.\", \"A woman is eating meat.\", 3.000;\n* \"A woman is dancing.\", \"A man is talking.\", 0.000.\n\n\n![Fine-tuning BERT for text pair classification or regression applications, such as natural language inference and semantic textual similarity. Suppose that the input text pair has two and three tokens.](../img/bert-two-seqs.svg)\n:label:`fig_bert-two-seqs`\n\nComparing with single text classification in :numref:`fig_bert-one-seq`,\nfine-tuning BERT for text pair classification in :numref:`fig_bert-two-seqs` \nis different in the input representation.\nFor text pair regression tasks such as semantic textual similarity,\ntrivial changes can be applied such as outputting a continuous label value\nand using the mean squared loss: they are common for regression."
    },
    {
      "chunk_id": "3a9db99f13c7_0",
      "chapter": "finetuning-bert",
      "heading": "Text Tagging",
      "text": "Now let's consider token-level tasks, such as *text tagging*,\nwhere each token is assigned a label.\nAmong text tagging tasks,\n*part-of-speech tagging* assigns each word a part-of-speech tag (e.g., adjective and determiner)\naccording to the role of the word in the sentence.\nFor example,\naccording to the Penn Treebank II tag set,\nthe sentence \"John Smith 's car is new\"\nshould be tagged as\n\"NNP (noun, proper singular) NNP POS (possessive ending) NN (noun, singular or mass) VB (verb, base form) JJ (adjective)\".\n\n![Fine-tuning BERT for text tagging applications, such as part-of-speech tagging. Suppose that the input single text has six tokens.](../img/bert-tagging.svg)\n:label:`fig_bert-tagging`\n\nFine-tuning BERT for text tagging applications\nis illustrated in :numref:`fig_bert-tagging`.\nComparing with :numref:`fig_bert-one-seq`,\nthe only distinction lies in that\nin text tagging, the BERT representation of *every token* of the input text\nis fed into the same extra fully connected layers to output the label of the token,\nsuch as a part-of-speech tag."
    },
    {
      "chunk_id": "503a460d1f03_0",
      "chapter": "finetuning-bert",
      "heading": "Question Answering",
      "text": "As another token-level application,\n*question answering* reflects capabilities of reading comprehension. For example,\nthe Stanford Question Answering Dataset (SQuAD v1.1)\nconsists of reading passages and questions,\nwhere the answer to every question\nis just a segment of text (text span) from the passage that the question is about :cite:`Rajpurkar.Zhang.Lopyrev.ea.2016`. To explain,\nconsider a passage\n\"Some experts report that a mask's efficacy is inconclusive. However, mask makers insist that their products, such as N95 respirator masks, can guard against the virus.\"\nand a question \"Who say that N95 respirator masks can guard against the virus?\". The answer should be the text span \"mask makers\" in the passage. Thus, the goal in SQuAD v1.1 is to predict the start and end of the text span in the passage given a pair of question and passage. ![Fine-tuning BERT for question answering. Suppose that the input text pair has two and three tokens.](../img/bert-qa.svg)\n:label:`fig_bert-qa`\n\nTo fine-tune BERT for question answering,\nthe question and passage are packed as\nthe first and second text sequence, respectively,\nin the input of BERT. To predict the position of the start of the text span,\nthe same additional fully connected layer will transform\nthe BERT representation of any token from the passage of position $i$\ninto a scalar score $s_i$. Such scores of all the passage tokens\nare further transformed by the softmax operation\ninto a probability distribution,\nso that each token position $i$ in the passage is assigned\na probability $p_i$ of being the start of the text span. Predicting the end of the text span\nis the same as above, except that\nparameters in its additional fully connected layer\nare independent from those for predicting the start. When predicting the end,\nany passage token of position $i$\nis transformed by the same fully connected layer\ninto a scalar score $e_i$. :numref:`fig_bert-qa`\ndepicts fine-tuning BERT for question answering."
    },
    {
      "chunk_id": "503a460d1f03_1",
      "chapter": "finetuning-bert",
      "heading": "Question Answering",
      "text": "When predicting the end,\nany passage token of position $i$\nis transformed by the same fully connected layer\ninto a scalar score $e_i$. :numref:`fig_bert-qa`\ndepicts fine-tuning BERT for question answering. For question answering,\nthe supervised learning's training objective is as straightforward as\nmaximizing the log-likelihoods of the ground-truth start and end positions. When predicting the span,\nwe can compute the score $s_i + e_j$ for a valid span\nfrom position $i$ to position $j$ ($i \\leq j$),\nand output the span with the highest score."
    },
    {
      "chunk_id": "32b2c4235ba2_0",
      "chapter": "finetuning-bert",
      "heading": "Summary",
      "text": "* BERT requires minimal architecture changes (extra fully connected layers) for sequence-level and token-level natural language processing applications, such as single text classification (e.g., sentiment analysis and testing linguistic acceptability), text pair classification or regression (e.g., natural language inference and semantic textual similarity), text tagging (e.g., part-of-speech tagging), and question answering.\n* During supervised learning of a downstream application, parameters of the extra layers are learned from scratch while all the parameters in the pretrained BERT model are fine-tuned."
    },
    {
      "chunk_id": "61e77a4d956f_0",
      "chapter": "finetuning-bert",
      "heading": "Exercises",
      "text": "1. Let's design a search engine algorithm for news articles. When the system receives an query (e.g., \"oil industry during the coronavirus outbreak\"), it should return a ranked list of news articles that are most relevant to the query. Suppose that we have a huge pool of news articles and a large number of queries. To simplify the problem, suppose that the most relevant article has been labeled for each query. How can we apply negative sampling (see :numref:`subsec_negative-sampling`) and BERT in the algorithm design?\n1. How can we leverage BERT in training language models?\n1. Can we leverage BERT in machine translation?\n\n[Discussions](https://discuss.d2l.ai/t/396)"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Natural Language Processing: Applications\n:label:`chap_nlp_app`\n\nWe have seen how to represent tokens in text sequences and train their representations in :numref:`chap_nlp_pretrain`. Such pretrained text representations can be fed to various models for different downstream natural language processing tasks. In fact,\nearlier chapters have already discussed some natural language processing applications\n*without pretraining*,\njust for explaining deep learning architectures. For instance, in :numref:`chap_rnn`,\nwe have relied on RNNs to design language models to generate novella-like text. In :numref:`chap_modern_rnn` and :numref:`chap_attention-and-transformers`,\nwe have also designed models based on RNNs and attention mechanisms for machine translation. However, this book does not intend to cover all such applications in a comprehensive manner. Instead,\nour focus is on *how to apply (deep) representation learning of languages to addressing natural language processing problems*. Given pretrained text representations,\nthis chapter will explore two \npopular and representative\ndownstream natural language processing tasks:\nsentiment analysis and natural language inference,\nwhich analyze single text and relationships of text pairs, respectively. ![Pretrained text representations can be fed to various deep learning architectures for different downstream natural language processing applications. This chapter focuses on how to design models for different downstream natural language processing applications.](../img/nlp-map-app.svg)\n:label:`fig_nlp-map-app`\n\nAs depicted in :numref:`fig_nlp-map-app`,\nthis chapter focuses on describing the basic ideas of designing natural language processing models using different types of deep learning architectures, such as MLPs, CNNs, RNNs, and attention. Though it is possible to combine any pretrained text representations with any architecture for either application in :numref:`fig_nlp-map-app`,\nwe select a few representative combinations."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "Though it is possible to combine any pretrained text representations with any architecture for either application in :numref:`fig_nlp-map-app`,\nwe select a few representative combinations. Specifically, we will explore popular architectures based on RNNs and CNNs for sentiment analysis. For natural language inference, we choose attention and MLPs to demonstrate how to analyze text pairs. In the end, we introduce how to fine-tune a pretrained BERT model\nfor a wide range of natural language processing applications,\nsuch as on a sequence level (single text classification and text pair classification)\nand a token level (text tagging and question answering). As a concrete empirical case,\nwe will fine-tune BERT for natural language inference. As we have introduced in :numref:`sec_bert`,\nBERT requires minimal architecture changes\nfor a wide range of natural language processing applications. However, this benefit comes at the cost of fine-tuning\na huge number of BERT parameters for the downstream applications. When space or time is limited,\nthose crafted models based on MLPs, CNNs, RNNs, and attention\nare more feasible. In the following, we start by the sentiment analysis application\nand illustrate the model design based on RNNs and CNNs, respectively. ```toc\n:maxdepth: 2\n\nsentiment-analysis-and-dataset\nsentiment-analysis-rnn\nsentiment-analysis-cnn\nnatural-language-inference-and-dataset\nnatural-language-inference-attention\nfinetuning-bert\nnatural-language-inference-bert\n```"
    },
    {
      "chunk_id": "1e7d8bcbf209_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "natural-language-inference-and-dataset",
      "text": "# Natural Language Inference and the Dataset\n:label:`sec_natural-language-inference-and-dataset`\n\nIn :numref:`sec_sentiment`, we discussed the problem of sentiment analysis.\nThis task aims to classify a single text sequence into predefined categories,\nsuch as a set of sentiment polarities.\nHowever, when there is a need to decide whether one sentence can be inferred form another, \nor eliminate redundancy by identifying sentences that are semantically equivalent,\nknowing how to classify one text sequence is insufficient.\nInstead, we need to be able to reason over pairs of text sequences."
    },
    {
      "chunk_id": "94fccdf7c00a_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "Natural Language Inference",
      "text": "*Natural language inference* studies whether a *hypothesis*\ncan be inferred from a *premise*, where both are a text sequence.\nIn other words, natural language inference determines the logical relationship between a pair of text sequences.\nSuch relationships usually fall into three types:\n\n* *Entailment*: the hypothesis can be inferred from the premise.\n* *Contradiction*: the negation of the hypothesis can be inferred from the premise.\n* *Neutral*: all the other cases.\n\nNatural language inference is also known as the recognizing textual entailment task.\nFor example, the following pair will be labeled as *entailment* because \"showing affection\" in the hypothesis can be inferred from \"hugging one another\" in the premise.\n\n> Premise: Two women are hugging each other.\n\n> Hypothesis: Two women are showing affection.\n\nThe following is an example of *contradiction* as \"running the coding example\" indicates \"not sleeping\" rather than \"sleeping\".\n\n> Premise: A man is running the coding example from Dive into Deep Learning.\n\n> Hypothesis: The man is sleeping.\n\nThe third example shows a *neutrality* relationship because neither \"famous\" nor \"not famous\" can be inferred from the fact that \"are performing for us\". \n\n> Premise: The musicians are performing for us.\n\n> Hypothesis: The musicians are famous.\n\nNatural language inference has been a central topic for understanding natural language.\nIt enjoys wide applications ranging from\ninformation retrieval to open-domain question answering.\nTo study this problem, we will begin by investigating a popular natural language inference benchmark dataset."
    },
    {
      "chunk_id": "693b46235b63_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "The Stanford Natural Language Inference (SNLI) Dataset",
      "text": "[**Stanford Natural Language Inference (SNLI) Corpus**] is a collection of over 500000 labeled English sentence pairs :cite:`Bowman.Angeli.Potts.ea.2015`.\nWe download and store the extracted SNLI dataset in the path `../data/snli_1.0`.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, np, npx\nimport os\nimport re\n\nnpx.set_np()\n\n#@save\nd2l.DATA_HUB['SNLI'] = (\n    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n\ndata_dir = d2l.download_extract('SNLI')\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nimport os\nimport re\n\n#@save\nd2l.DATA_HUB['SNLI'] = (\n    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n\ndata_dir = d2l.download_extract('SNLI')\n```"
    },
    {
      "chunk_id": "3784f12eba4e_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "[**Reading the Dataset**]",
      "text": "The original SNLI dataset contains much richer information than what we really need in our experiments. Thus, we define a function `read_snli` to only extract part of the dataset, then return lists of premises, hypotheses, and their labels. ```{.python .input}\n#@tab all\n#@save\ndef read_snli(data_dir, is_train):\n    \"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\n    def extract_text(s):\n        # Remove information that will not be used by us\n        s = re.sub('\\\\(', '', s) \n        s = re.sub('\\\\)', '', s)\n        # Substitute two or more consecutive whitespace with space\n        s = re.sub('\\\\s{2,}', ' ', s)\n        return s.strip()\n    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n                             if is_train else 'snli_1.0_test.txt')\n    with open(file_name, 'r') as f:\n        rows = [row.split('\\t') for row in f.readlines()[1:]]\n    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n    return premises, hypotheses, labels\n```\n\nNow let's [**print the first 3 pairs**] of premise and hypothesis, as well as their labels (\"0\", \"1\", and \"2\" correspond to \"entailment\", \"contradiction\", and \"neutral\", respectively ). ```{.python .input}\n#@tab all\ntrain_data = read_snli(data_dir, is_train=True)\nfor x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n    print('premise:', x0)\n    print('hypothesis:', x1)\n    print('label:', y)\n```\n\nThe training set has about 550000 pairs,\nand the testing set has about 10000 pairs. The following shows that \nthe three [**labels \"entailment\", \"contradiction\", and \"neutral\" are balanced**] in \nboth the training set and the testing set."
    },
    {
      "chunk_id": "3784f12eba4e_1",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "[**Reading the Dataset**]",
      "text": "The following shows that \nthe three [**labels \"entailment\", \"contradiction\", and \"neutral\" are balanced**] in \nboth the training set and the testing set. ```{.python .input}\n#@tab all\ntest_data = read_snli(data_dir, is_train=False)\nfor data in [train_data, test_data]:\n    print([[row for row in data[2]].count(i) for i in range(3)])\n```"
    },
    {
      "chunk_id": "cb3a81b10534_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "[**Defining a Class for Loading the Dataset**]",
      "text": "Below we define a class for loading the SNLI dataset by inheriting from the `Dataset` class in Gluon. The argument `num_steps` in the class constructor specifies the length of a text sequence so that each minibatch of sequences will have the same shape. In other words,\ntokens after the first `num_steps` ones in longer sequence are trimmed, while special tokens \u201c&lt;pad&gt;\u201d will be appended to shorter sequences until their length becomes `num_steps`. By implementing the `__getitem__` function, we can arbitrarily access the premise, hypothesis, and label with the index `idx`."
    },
    {
      "chunk_id": "cb3a81b10534_1",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "[**Defining a Class for Loading the Dataset**]",
      "text": "By implementing the `__getitem__` function, we can arbitrarily access the premise, hypothesis, and label with the index `idx`. ```{.python .input}\n#@tab mxnet\n#@save\nclass SNLIDataset(gluon.data.Dataset):\n    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n    def __init__(self, dataset, num_steps, vocab=None):\n        self.num_steps = num_steps\n        all_premise_tokens = d2l.tokenize(dataset[0])\n        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n        if vocab is None:\n            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n                                   min_freq=5, reserved_tokens=['<pad>'])\n        else:\n            self.vocab = vocab\n        self.premises = self._pad(all_premise_tokens)\n        self.hypotheses = self._pad(all_hypothesis_tokens)\n        self.labels = np.array(dataset[2])\n        print('read ' + str(len(self.premises)) + ' examples')\n\n    def _pad(self, lines):\n        return np.array([d2l.truncate_pad(\n            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n                         for line in lines])\n\n    def __getitem__(self, idx):\n        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n\n    def __len__(self):\n        return len(self.premises)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass SNLIDataset(torch.utils.data.Dataset):\n    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n    def __init__(self, dataset, num_steps, vocab=None):\n        self.num_steps = num_steps\n        all_premise_tokens = d2l.tokenize(dataset[0])\n        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n        if vocab is None:\n            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n                                   min_freq=5, reserved_tokens=['<pad>'])\n        else:\n            self.vocab = vocab\n        self.premises = self._pad(all_premise_tokens)\n        self.hypotheses = self._pad(all_hypothesis_tokens)\n        self.labels = torch.tensor(dataset[2])\n        print('read ' + str(len(self.premises)) + ' examples')\n\n    def _pad(self, lines):\n        return torch.tensor([d2l.truncate_pad(\n            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n                         for line in lines])\n\n    def __getitem__(self, idx):\n        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n\n    def __len__(self):\n        return len(self.premises)\n```"
    },
    {
      "chunk_id": "718d1e167982_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "[**Putting It All Together**]",
      "text": "Now we can invoke the `read_snli` function and the `SNLIDataset` class to download the SNLI dataset and return `DataLoader` instances for both training and testing sets, together with the vocabulary of the training set. It is noteworthy that we must use the vocabulary constructed from the training set\nas that of the testing set. As a result, any new token from the testing set will be unknown to the model trained on the training set."
    },
    {
      "chunk_id": "718d1e167982_1",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "[**Putting It All Together**]",
      "text": "It is noteworthy that we must use the vocabulary constructed from the training set\nas that of the testing set. As a result, any new token from the testing set will be unknown to the model trained on the training set. ```{.python .input}\n#@tab mxnet\n#@save\ndef load_data_snli(batch_size, num_steps=50):\n    \"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\n    num_workers = d2l.get_dataloader_workers()\n    data_dir = d2l.download_extract('SNLI')\n    train_data = read_snli(data_dir, True)\n    test_data = read_snli(data_dir, False)\n    train_set = SNLIDataset(train_data, num_steps)\n    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,\n                                       num_workers=num_workers)\n    test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,\n                                      num_workers=num_workers)\n    return train_iter, test_iter, train_set.vocab\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef load_data_snli(batch_size, num_steps=50):\n    \"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\n    num_workers = d2l.get_dataloader_workers()\n    data_dir = d2l.download_extract('SNLI')\n    train_data = read_snli(data_dir, True)\n    test_data = read_snli(data_dir, False)\n    train_set = SNLIDataset(train_data, num_steps)\n    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n                                             shuffle=True,\n                                             num_workers=num_workers)\n    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n                                            shuffle=False,\n                                            num_workers=num_workers)\n    return train_iter, test_iter, train_set.vocab\n```\n\nHere we set the batch size to 128 and sequence length to 50,\nand invoke the `load_data_snli` function to get the data iterators and vocabulary."
    },
    {
      "chunk_id": "718d1e167982_2",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "[**Putting It All Together**]",
      "text": "Then we print the vocabulary size. ```{.python .input}\n#@tab all\ntrain_iter, test_iter, vocab = load_data_snli(128, 50)\nlen(vocab)\n```\n\nNow we print the shape of the first minibatch. Contrary to sentiment analysis,\nwe have two inputs `X[0]` and `X[1]` representing pairs of premises and hypotheses. ```{.python .input}\n#@tab all\nfor X, Y in train_iter:\n    print(X[0].shape)\n    print(X[1].shape)\n    print(Y.shape)\n    break\n```"
    },
    {
      "chunk_id": "ef9c14dedf79_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "Summary",
      "text": "* Natural language inference studies whether a hypothesis can be inferred from a premise, where both are a text sequence.\n* In natural language inference, relationships between premises and hypotheses include entailment, contradiction, and neutral.\n* Stanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of natural language inference."
    },
    {
      "chunk_id": "78d3b027cf31_0",
      "chapter": "natural-language-inference-and-dataset",
      "heading": "Exercises",
      "text": "1. Machine translation has long been evaluated based on superficial $n$-gram matching between an output translation and a ground-truth translation. Can you design a measure for evaluating machine translation results by using natural language inference?\n1. How can we change hyperparameters to reduce the vocabulary size?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/394)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1388)\n:end_tab:"
    },
    {
      "chunk_id": "c42eac865392_0",
      "chapter": "natural-language-inference-attention",
      "heading": "natural-language-inference-attention",
      "text": "# Natural Language Inference: Using Attention\n:label:`sec_natural-language-inference-attention`\n\nWe introduced the natural language inference task and the SNLI dataset in :numref:`sec_natural-language-inference-and-dataset`. In view of many models that are based on complex and deep architectures, :citet:`Parikh.Tackstrom.Das.ea.2016` proposed to address natural language inference with attention mechanisms and called it a \"decomposable attention model\".\nThis results in a model without recurrent or convolutional layers, achieving the best result at the time on the SNLI dataset with much fewer parameters.\nIn this section, we will describe and implement this attention-based method (with MLPs) for natural language inference, as depicted in :numref:`fig_nlp-map-nli-attention`.\n\n![This section feeds pretrained GloVe to an architecture based on attention and MLPs for natural language inference.](../img/nlp-map-nli-attention.svg)\n:label:`fig_nlp-map-nli-attention`"
    },
    {
      "chunk_id": "b8ea3eb6ac3e_0",
      "chapter": "natural-language-inference-attention",
      "heading": "The Model",
      "text": "Simpler than preserving the order of tokens in premises and hypotheses,\nwe can just align tokens in one text sequence to every token in the other, and vice versa,\nthen compare and aggregate such information to predict the logical relationships\nbetween premises and hypotheses.\nSimilar to alignment of tokens between source and target sentences in machine translation,\nthe alignment of tokens between premises and hypotheses\ncan be neatly accomplished by attention mechanisms.\n\n![Natural language inference using attention mechanisms.](../img/nli-attention.svg)\n:label:`fig_nli_attention`\n\n:numref:`fig_nli_attention` depicts the natural language inference method using attention mechanisms.\nAt a high level, it consists of three jointly trained steps: attending, comparing, and aggregating.\nWe will illustrate them step by step in the following.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, init, np, npx\nfrom mxnet.gluon import nn\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```"
    },
    {
      "chunk_id": "f62935a362d3_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Attending",
      "text": "The first step is to align tokens in one text sequence to each token in the other sequence. Suppose that the premise is \"i do need sleep\" and the hypothesis is \"i am tired\". Due to semantical similarity,\nwe may wish to align \"i\" in the hypothesis with \"i\" in the premise,\nand align \"tired\" in the hypothesis with \"sleep\" in the premise. Likewise, we may wish to align \"i\" in the premise with \"i\" in the hypothesis,\nand align \"need\" and \"sleep\" in the premise with \"tired\" in the hypothesis. Note that such alignment is *soft* using weighted average,\nwhere ideally large weights are associated with the tokens to be aligned. For ease of demonstration, :numref:`fig_nli_attention` shows such alignment in a *hard* way. Now we describe the soft alignment using attention mechanisms in more detail. Denote by $\\mathbf{A} = (\\mathbf{a}_1, \\ldots, \\mathbf{a}_m)$\nand $\\mathbf{B} = (\\mathbf{b}_1, \\ldots, \\mathbf{b}_n)$ the premise and hypothesis,\nwhose number of tokens are $m$ and $n$, respectively,\nwhere $\\mathbf{a}_i, \\mathbf{b}_j \\in \\mathbb{R}^{d}$ ($i = 1, \\ldots, m, j = 1, \\ldots, n$) is a $d$-dimensional word vector. For soft alignment, we compute the attention weights $e_{ij} \\in \\mathbb{R}$ as\n\n$$e_{ij} = f(\\mathbf{a}_i)^\\top f(\\mathbf{b}_j),$$\n:eqlabel:`eq_nli_e`\n\nwhere the function $f$ is an MLP defined in the following `mlp` function. The output dimension of $f$ is specified by the `num_hiddens` argument of `mlp`."
    },
    {
      "chunk_id": "f62935a362d3_1",
      "chapter": "natural-language-inference-attention",
      "heading": "Attending",
      "text": "The output dimension of $f$ is specified by the `num_hiddens` argument of `mlp`. ```{.python .input}\n#@tab mxnet\ndef mlp(num_hiddens, flatten):\n    net = nn.Sequential()\n    net.add(nn.Dropout(0.2))\n    net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))\n    net.add(nn.Dropout(0.2))\n    net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))\n    return net\n```\n\n```{.python .input}\n#@tab pytorch\ndef mlp(num_inputs, num_hiddens, flatten):\n    net = []\n    net.append(nn.Dropout(0.2))\n    net.append(nn.Linear(num_inputs, num_hiddens))\n    net.append(nn.ReLU())\n    if flatten:\n        net.append(nn.Flatten(start_dim=1))\n    net.append(nn.Dropout(0.2))\n    net.append(nn.Linear(num_hiddens, num_hiddens))\n    net.append(nn.ReLU())\n    if flatten:\n        net.append(nn.Flatten(start_dim=1))\n    return nn.Sequential(*net)\n```\n\nIt should be highlighted that, in :eqref:`eq_nli_e`\n$f$ takes inputs $\\mathbf{a}_i$ and $\\mathbf{b}_j$ separately rather than takes a pair of them together as input. This *decomposition* trick leads to only $m + n$ applications (linear complexity) of $f$ rather than $mn$ applications\n(quadratic complexity). Normalizing the attention weights in :eqref:`eq_nli_e`,\nwe compute the weighted average of all the token vectors in the hypothesis\nto obtain representation of the hypothesis that is softly aligned with the token indexed by $i$ in the premise:\n\n$$\n\\boldsymbol{\\beta}_i = \\sum_{j=1}^{n}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{n} \\exp(e_{ik})} \\mathbf{b}_j. $$\n\nLikewise, we compute soft alignment of premise tokens for each token indexed by $j$ in the hypothesis:\n\n$$\n\\boldsymbol{\\alpha}_j = \\sum_{i=1}^{m}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{m} \\exp(e_{kj})} \\mathbf{a}_i. $$\n\nBelow we define the `Attend` class to compute the soft alignment of hypotheses (`beta`) with input premises `A` and soft alignment of premises (`alpha`) with input hypotheses `B`."
    },
    {
      "chunk_id": "f62935a362d3_2",
      "chapter": "natural-language-inference-attention",
      "heading": "Attending",
      "text": "$$\n\nBelow we define the `Attend` class to compute the soft alignment of hypotheses (`beta`) with input premises `A` and soft alignment of premises (`alpha`) with input hypotheses `B`. ```{.python .input}\n#@tab mxnet\nclass Attend(nn.Block):\n    def __init__(self, num_hiddens, **kwargs):\n        super(Attend, self).__init__(**kwargs)\n        self.f = mlp(num_hiddens=num_hiddens, flatten=False)\n\n    def forward(self, A, B):\n        # Shape of `A`/`B`: (b`atch_size`, no. of tokens in sequence A/B,\n        # `embed_size`)\n        # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n        # `num_hiddens`)\n        f_A = self.f(A)\n        f_B = self.f(B)\n        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n        # no. of tokens in sequence B)\n        e = npx.batch_dot(f_A, f_B, transpose_b=True)\n        # Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n        # `embed_size`), where sequence B is softly aligned with each token\n        # (axis 1 of `beta`) in sequence A\n        beta = npx.batch_dot(npx.softmax(e), B)\n        # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n        # `embed_size`), where sequence A is softly aligned with each token\n        # (axis 1 of `alpha`) in sequence B\n        alpha = npx.batch_dot(npx.softmax(e.transpose(0, 2, 1)), A)\n        return beta, alpha\n```\n\n```{.python .input}\n#@tab pytorch\nclass Attend(nn.Module):\n    def __init__(self, num_inputs, num_hiddens, **kwargs):\n        super(Attend, self).__init__(**kwargs)\n        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n\n    def forward(self, A, B):\n        # Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n        # `embed_size`)\n        # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n        # `num_hiddens`)\n        f_A = self.f(A)\n        f_B = self.f(B)\n        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n        # no."
    },
    {
      "chunk_id": "f62935a362d3_3",
      "chapter": "natural-language-inference-attention",
      "heading": "Attending",
      "text": "of tokens in sequence A/B,\n        # `num_hiddens`)\n        f_A = self.f(A)\n        f_B = self.f(B)\n        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n        # no. of tokens in sequence B)\n        e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n        # Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n        # `embed_size`), where sequence B is softly aligned with each token\n        # (axis 1 of `beta`) in sequence A\n        beta = torch.bmm(F.softmax(e, dim=-1), B)\n        # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n        # `embed_size`), where sequence A is softly aligned with each token\n        # (axis 1 of `alpha`) in sequence B\n        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n        return beta, alpha\n```"
    },
    {
      "chunk_id": "f56404dbe8d5_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Comparing",
      "text": "In the next step, we compare a token in one sequence with the other sequence that is softly aligned with that token. Note that in soft alignment, all the tokens from one sequence, though with probably different attention weights, will be compared with a token in the other sequence. For easy of demonstration, :numref:`fig_nli_attention` pairs tokens with aligned tokens in a *hard* way. For example, suppose that the attending step determines that \"need\" and \"sleep\" in the premise are both aligned with \"tired\" in the hypothesis, the pair \"tired--need sleep\" will be compared. In the comparing step, we feed the concatenation (operator $[\\cdot, \\cdot]$) of tokens from one sequence and aligned tokens from the other sequence into a function $g$ (an MLP):\n\n$$\\mathbf{v}_{A,i} = g([\\mathbf{a}_i, \\boldsymbol{\\beta}_i]), i = 1, \\ldots, m\\\\ \\mathbf{v}_{B,j} = g([\\mathbf{b}_j, \\boldsymbol{\\alpha}_j]), j = 1, \\ldots, n.$$\n\n:eqlabel:`eq_nli_v_ab`\n\n\nIn :eqref:`eq_nli_v_ab`, $\\mathbf{v}_{A,i}$ is the comparison between token $i$ in the premise and all the hypothesis tokens that are softly aligned with token $i$;\nwhile $\\mathbf{v}_{B,j}$ is the comparison between token $j$ in the hypothesis and all the premise tokens that are softly aligned with token $j$. The following `Compare` class defines such as comparing step."
    },
    {
      "chunk_id": "f56404dbe8d5_1",
      "chapter": "natural-language-inference-attention",
      "heading": "Comparing",
      "text": "The following `Compare` class defines such as comparing step. ```{.python .input}\n#@tab mxnet\nclass Compare(nn.Block):\n    def __init__(self, num_hiddens, **kwargs):\n        super(Compare, self).__init__(**kwargs)\n        self.g = mlp(num_hiddens=num_hiddens, flatten=False)\n\n    def forward(self, A, B, beta, alpha):\n        V_A = self.g(np.concatenate([A, beta], axis=2))\n        V_B = self.g(np.concatenate([B, alpha], axis=2))\n        return V_A, V_B\n```\n\n```{.python .input}\n#@tab pytorch\nclass Compare(nn.Module):\n    def __init__(self, num_inputs, num_hiddens, **kwargs):\n        super(Compare, self).__init__(**kwargs)\n        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n\n    def forward(self, A, B, beta, alpha):\n        V_A = self.g(torch.cat([A, beta], dim=2))\n        V_B = self.g(torch.cat([B, alpha], dim=2))\n        return V_A, V_B\n```"
    },
    {
      "chunk_id": "86ec4b60c26c_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Aggregating",
      "text": "With two sets of comparison vectors $\\mathbf{v}_{A,i}$ ($i = 1, \\ldots, m$) and $\\mathbf{v}_{B,j}$ ($j = 1, \\ldots, n$) on hand,\nin the last step we will aggregate such information to infer the logical relationship.\nWe begin by summing up both sets:\n\n$$\n\\mathbf{v}_A = \\sum_{i=1}^{m} \\mathbf{v}_{A,i}, \\quad \\mathbf{v}_B = \\sum_{j=1}^{n}\\mathbf{v}_{B,j}.\n$$\n\nNext we feed the concatenation of both summarization results into function $h$ (an MLP) to obtain the classification result of the logical relationship:\n\n$$\n\\hat{\\mathbf{y}} = h([\\mathbf{v}_A, \\mathbf{v}_B]).\n$$\n\nThe aggregation step is defined in the following `Aggregate` class.\n\n```{.python .input}\n#@tab mxnet\nclass Aggregate(nn.Block):\n    def __init__(self, num_hiddens, num_outputs, **kwargs):\n        super(Aggregate, self).__init__(**kwargs)\n        self.h = mlp(num_hiddens=num_hiddens, flatten=True)\n        self.h.add(nn.Dense(num_outputs))\n\n    def forward(self, V_A, V_B):\n        # Sum up both sets of comparison vectors\n        V_A = V_A.sum(axis=1)\n        V_B = V_B.sum(axis=1)\n        # Feed the concatenation of both summarization results into an MLP\n        Y_hat = self.h(np.concatenate([V_A, V_B], axis=1))\n        return Y_hat\n```\n\n```{.python .input}\n#@tab pytorch\nclass Aggregate(nn.Module):\n    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n        super(Aggregate, self).__init__(**kwargs)\n        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n        self.linear = nn.Linear(num_hiddens, num_outputs)\n\n    def forward(self, V_A, V_B):\n        # Sum up both sets of comparison vectors\n        V_A = V_A.sum(dim=1)\n        V_B = V_B.sum(dim=1)\n        # Feed the concatenation of both summarization results into an MLP\n        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n        return Y_hat\n```"
    },
    {
      "chunk_id": "89c167eba08c_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Putting It All Together",
      "text": "By putting the attending, comparing, and aggregating steps together,\nwe define the decomposable attention model to jointly train these three steps.\n\n```{.python .input}\n#@tab mxnet\nclass DecomposableAttention(nn.Block):\n    def __init__(self, vocab, embed_size, num_hiddens, **kwargs):\n        super(DecomposableAttention, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(len(vocab), embed_size)\n        self.attend = Attend(num_hiddens)\n        self.compare = Compare(num_hiddens)\n        # There are 3 possible outputs: entailment, contradiction, and neutral\n        self.aggregate = Aggregate(num_hiddens, 3)\n\n    def forward(self, X):\n        premises, hypotheses = X\n        A = self.embedding(premises)\n        B = self.embedding(hypotheses)\n        beta, alpha = self.attend(A, B)\n        V_A, V_B = self.compare(A, B, beta, alpha)\n        Y_hat = self.aggregate(V_A, V_B)\n        return Y_hat\n```\n\n```{.python .input}\n#@tab pytorch\nclass DecomposableAttention(nn.Module):\n    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\n                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n        super(DecomposableAttention, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(len(vocab), embed_size)\n        self.attend = Attend(num_inputs_attend, num_hiddens)\n        self.compare = Compare(num_inputs_compare, num_hiddens)\n        # There are 3 possible outputs: entailment, contradiction, and neutral\n        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n\n    def forward(self, X):\n        premises, hypotheses = X\n        A = self.embedding(premises)\n        B = self.embedding(hypotheses)\n        beta, alpha = self.attend(A, B)\n        V_A, V_B = self.compare(A, B, beta, alpha)\n        Y_hat = self.aggregate(V_A, V_B)\n        return Y_hat\n```"
    },
    {
      "chunk_id": "dd9780a1343d_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Training and Evaluating the Model",
      "text": "Now we will train and evaluate the defined decomposable attention model on the SNLI dataset.\nWe begin by reading the dataset."
    },
    {
      "chunk_id": "8474125840fa_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Reading the dataset",
      "text": "We download and read the SNLI dataset using the function defined in :numref:`sec_natural-language-inference-and-dataset`. The batch size and sequence length are set to $256$ and $50$, respectively.\n\n```{.python .input}\n#@tab all\nbatch_size, num_steps = 256, 50\ntrain_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)\n```"
    },
    {
      "chunk_id": "792f274baece_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Creating the Model",
      "text": "We use the pretrained 100-dimensional GloVe embedding to represent the input tokens.\nThus, we predefine the dimension of vectors $\\mathbf{a}_i$ and $\\mathbf{b}_j$ in :eqref:`eq_nli_e` as 100.\nThe output dimension of functions $f$ in :eqref:`eq_nli_e` and $g$ in :eqref:`eq_nli_v_ab` is set to 200.\nThen we create a model instance, initialize its parameters,\nand load the GloVe embedding to initialize vectors of input tokens.\n\n```{.python .input}\n#@tab mxnet\nembed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\nnet = DecomposableAttention(vocab, embed_size, num_hiddens)\nnet.initialize(init.Xavier(), ctx=devices)\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\nembeds = glove_embedding[vocab.idx_to_token]\nnet.embedding.weight.set_data(embeds)\n```\n\n```{.python .input}\n#@tab pytorch\nembed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\nnet = DecomposableAttention(vocab, embed_size, num_hiddens)\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\nembeds = glove_embedding[vocab.idx_to_token]\nnet.embedding.weight.data.copy_(embeds);\n```"
    },
    {
      "chunk_id": "dd9780a1343d_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Training and Evaluating the Model",
      "text": "In contrast to the `split_batch` function in :numref:`sec_multi_gpu` that takes single inputs such as text sequences (or images),\nwe define a `split_batch_multi_inputs` function to take multiple inputs such as premises and hypotheses in minibatches.\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef split_batch_multi_inputs(X, y, devices):\n    \"\"\"Split multi-input `X` and `y` into multiple devices.\"\"\"\n    X = list(zip(*[gluon.utils.split_and_load(\n        feature, devices, even_split=False) for feature in X]))\n    return (X, gluon.utils.split_and_load(y, devices, even_split=False))\n```\n\nNow we can train and evaluate the model on the SNLI dataset.\n\n```{.python .input}\n#@tab mxnet\nlr, num_epochs = 0.001, 4\ntrainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,\n               split_batch_multi_inputs)\n```\n\n```{.python .input}\n#@tab pytorch\nlr, num_epochs = 0.001, 4\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction=\"none\")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```"
    },
    {
      "chunk_id": "bf0c1d6beed9_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Using the Model",
      "text": "Finally, define the prediction function to output the logical relationship between a pair of premise and hypothesis.\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef predict_snli(net, vocab, premise, hypothesis):\n    \"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\n    premise = np.array(vocab[premise], ctx=d2l.try_gpu())\n    hypothesis = np.array(vocab[hypothesis], ctx=d2l.try_gpu())\n    label = np.argmax(net([premise.reshape((1, -1)),\n                           hypothesis.reshape((1, -1))]), axis=1)\n    return 'entailment' if label == 0 else 'contradiction' if label == 1 \\\n            else 'neutral'\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef predict_snli(net, vocab, premise, hypothesis):\n    \"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\n    net.eval()\n    premise = torch.tensor(vocab[premise], device=d2l.try_gpu())\n    hypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())\n    label = torch.argmax(net([premise.reshape((1, -1)),\n                           hypothesis.reshape((1, -1))]), dim=1)\n    return 'entailment' if label == 0 else 'contradiction' if label == 1 \\\n            else 'neutral'\n```\n\nWe can use the trained model to obtain the natural language inference result for a sample pair of sentences.\n\n```{.python .input}\n#@tab all\npredict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])\n```"
    },
    {
      "chunk_id": "217d826e436e_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Summary",
      "text": "* The decomposable attention model consists of three steps for predicting the logical relationships between premises and hypotheses: attending, comparing, and aggregating.\n* With attention mechanisms, we can align tokens in one text sequence to every token in the other, and vice versa. Such alignment is soft using weighted average, where ideally large weights are associated with the tokens to be aligned.\n* The decomposition trick leads to a more desirable linear complexity than quadratic complexity when computing attention weights.\n* We can use pretrained word vectors as the input representation for downstream natural language processing task such as natural language inference."
    },
    {
      "chunk_id": "d925c79cae08_0",
      "chapter": "natural-language-inference-attention",
      "heading": "Exercises",
      "text": "1. Train the model with other combinations of hyperparameters. Can you get better accuracy on the test set?\n1. What are major drawbacks of the decomposable attention model for natural language inference?\n1. Suppose that we want to get the level of semantical similarity (e.g., a continuous value between 0 and 1) for any pair of sentences. How shall we collect and label the dataset? Can you design a model with attention mechanisms?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/395)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1530)\n:end_tab:"
    },
    {
      "chunk_id": "45ed232a39bf_0",
      "chapter": "natural-language-inference-bert",
      "heading": "natural-language-inference-bert",
      "text": "# Natural Language Inference: Fine-Tuning BERT\n:label:`sec_natural-language-inference-bert`\n\nIn earlier sections of this chapter,\nwe have designed an attention-based architecture\n(in :numref:`sec_natural-language-inference-attention`)\nfor the natural language inference task\non the SNLI dataset (as described in :numref:`sec_natural-language-inference-and-dataset`).\nNow we revisit this task by fine-tuning BERT.\nAs discussed in :numref:`sec_finetuning-bert`,\nnatural language inference is a sequence-level text pair classification problem,\nand fine-tuning BERT only requires an additional MLP-based architecture,\nas illustrated in :numref:`fig_nlp-map-nli-bert`.\n\n![This section feeds pretrained BERT to an MLP-based architecture for natural language inference.](../img/nlp-map-nli-bert.svg)\n:label:`fig_nlp-map-nli-bert`\n\nIn this section,\nwe will download a pretrained small version of BERT,\nthen fine-tune it\nfor natural language inference on the SNLI dataset.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nimport json\nimport multiprocessing\nfrom mxnet import gluon, np, npx\nfrom mxnet.gluon import nn\nimport os\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport json\nimport multiprocessing\nimport torch\nfrom torch import nn\nimport os\n```"
    },
    {
      "chunk_id": "cbd74bbc36a1_0",
      "chapter": "natural-language-inference-bert",
      "heading": "[**Loading Pretrained BERT**]",
      "text": "We have explained how to pretrain BERT on the WikiText-2 dataset in\n:numref:`sec_bert-dataset` and :numref:`sec_bert-pretraining`\n(note that the original BERT model is pretrained on much bigger corpora). As discussed in :numref:`sec_bert-pretraining`,\nthe original BERT model has hundreds of millions of parameters. In the following,\nwe provide two versions of pretrained BERT:\n\"bert.base\" is about as big as the original BERT base model that requires a lot of computational resources to fine-tune,\nwhile \"bert.small\" is a small version to facilitate demonstration. ```{.python .input}\n#@tab mxnet\nd2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.zip',\n                             '7b3820b35da691042e5d34c0971ac3edbd80d3f4')\nd2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.zip',\n                              'a4e718a47137ccd1809c9107ab4f5edd317bae2c')\n```\n\n```{.python .input}\n#@tab pytorch\nd2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n                             '225d66f04cae318b841a13d32af3acc165f253ac')\nd2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n                              'c72329e68a732bef0452e4b96a1c341c8910f81f')\n```\n\nEither pretrained BERT model contains a \"vocab.json\" file that defines the vocabulary set\nand a \"pretrained.params\" file of the pretrained parameters. We implement the following `load_pretrained_model` function to [**load pretrained BERT parameters**]."
    },
    {
      "chunk_id": "cbd74bbc36a1_1",
      "chapter": "natural-language-inference-bert",
      "heading": "[**Loading Pretrained BERT**]",
      "text": "We implement the following `load_pretrained_model` function to [**load pretrained BERT parameters**]. ```{.python .input}\n#@tab mxnet\ndef load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n                          num_heads, num_blks, dropout, max_len, devices):\n    data_dir = d2l.download_extract(pretrained_model)\n    # Define an empty vocabulary to load the predefined vocabulary\n    vocab = d2l.Vocab()\n    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n        vocab.idx_to_token)}\n    bert = d2l.BERTModel(len(vocab), num_hiddens, ffn_num_hiddens, num_heads, \n                         num_blks, dropout, max_len)\n    # Load pretrained BERT parameters\n    bert.load_parameters(os.path.join(data_dir, 'pretrained.params'),\n                         ctx=devices)\n    return bert, vocab\n```\n\n```{.python .input}\n#@tab pytorch\ndef load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n                          num_heads, num_blks, dropout, max_len, devices):\n    data_dir = d2l.download_extract(pretrained_model)\n    # Define an empty vocabulary to load the predefined vocabulary\n    vocab = d2l.Vocab()\n    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n        vocab.idx_to_token)}\n    bert = d2l.BERTModel(\n        len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=4,\n        num_blks=2, dropout=0.2, max_len=max_len)\n    # Load pretrained BERT parameters\n    bert.load_state_dict(torch.load(os.path.join(data_dir,\n                                                 'pretrained.params')))\n    return bert, vocab\n```\n\nTo facilitate demonstration on most of machines,\nwe will load and fine-tune the small version (\"bert.small\") of the pretrained BERT in this section. In the exercise, we will show how to fine-tune the much larger \"bert.base\" to significantly improve the testing accuracy."
    },
    {
      "chunk_id": "cbd74bbc36a1_2",
      "chapter": "natural-language-inference-bert",
      "heading": "[**Loading Pretrained BERT**]",
      "text": "In the exercise, we will show how to fine-tune the much larger \"bert.base\" to significantly improve the testing accuracy. ```{.python .input}\n#@tab all\ndevices = d2l.try_all_gpus()\nbert, vocab = load_pretrained_model(\n    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n    num_blks=2, dropout=0.1, max_len=512, devices=devices)\n```"
    },
    {
      "chunk_id": "0c19843c8def_0",
      "chapter": "natural-language-inference-bert",
      "heading": "[**The Dataset for Fine-Tuning BERT**]",
      "text": "For the downstream task natural language inference on the SNLI dataset,\nwe define a customized dataset class `SNLIBERTDataset`. In each example,\nthe premise and hypothesis form a pair of text sequence\nand is packed into one BERT input sequence as depicted in :numref:`fig_bert-two-seqs`. Recall :numref:`subsec_bert_input_rep` that segment IDs\nare used to distinguish the premise and the hypothesis in a BERT input sequence. With the predefined maximum length of a BERT input sequence (`max_len`),\nthe last token of the longer of the input text pair keeps getting removed until\n`max_len` is met. To accelerate generation of the SNLI dataset\nfor fine-tuning BERT,\nwe use 4 worker processes to generate training or testing examples in parallel."
    },
    {
      "chunk_id": "0c19843c8def_1",
      "chapter": "natural-language-inference-bert",
      "heading": "[**The Dataset for Fine-Tuning BERT**]",
      "text": "To accelerate generation of the SNLI dataset\nfor fine-tuning BERT,\nwe use 4 worker processes to generate training or testing examples in parallel. ```{.python .input}\n#@tab mxnet\nclass SNLIBERTDataset(gluon.data.Dataset):\n    def __init__(self, dataset, max_len, vocab=None):\n        all_premise_hypothesis_tokens = [[\n            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n            *[d2l.tokenize([s.lower() for s in sentences])\n              for sentences in dataset[:2]])]\n        \n        self.labels = np.array(dataset[2])\n        self.vocab = vocab\n        self.max_len = max_len\n        (self.all_token_ids, self.all_segments,\n         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n        print('read ' + str(len(self.all_token_ids)) + ' examples')\n\n    def _preprocess(self, all_premise_hypothesis_tokens):\n        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n        all_token_ids = [\n            token_ids for token_ids, segments, valid_len in out]\n        all_segments = [segments for token_ids, segments, valid_len in out]\n        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n        return (np.array(all_token_ids, dtype='int32'),\n                np.array(all_segments, dtype='int32'), \n                np.array(valid_lens))\n\n    def _mp_worker(self, premise_hypothesis_tokens):\n        p_tokens, h_tokens = premise_hypothesis_tokens\n        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n                             * (self.max_len - len(tokens))\n        segments = segments + [0] * (self.max_len - len(segments))\n        valid_len = len(tokens)\n        return token_ids, segments, valid_len\n\n    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n        # input\n        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n            if len(p_tokens) > len(h_tokens):\n                p_tokens.pop()\n            else:\n                h_tokens.pop()\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx]), self.labels[idx]\n\n    def __len__(self):\n        return len(self.all_token_ids)\n```\n\n```{.python .input}\n#@tab pytorch\nclass SNLIBERTDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, max_len, vocab=None):\n        all_premise_hypothesis_tokens = [[\n            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n            *[d2l.tokenize([s.lower() for s in sentences])\n              for sentences in dataset[:2]])]\n        \n        self.labels = torch.tensor(dataset[2])\n        self.vocab = vocab\n        self.max_len = max_len\n        (self.all_token_ids, self.all_segments,\n         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n        print('read ' + str(len(self.all_token_ids)) + ' examples')\n\n    def _preprocess(self, all_premise_hypothesis_tokens):\n        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n        all_token_ids = [\n            token_ids for token_ids, segments, valid_len in out]\n        all_segments = [segments for token_ids, segments, valid_len in out]\n        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n        return (torch.tensor(all_token_ids, dtype=torch.long),\n                torch.tensor(all_segments, dtype=torch.long), \n                torch.tensor(valid_lens))\n\n    def _mp_worker(self, premise_hypothesis_tokens):\n        p_tokens, h_tokens = premise_hypothesis_tokens\n        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n                             * (self.max_len - len(tokens))\n        segments = segments + [0] * (self.max_len - len(segments))\n        valid_len = len(tokens)\n        return token_ids, segments, valid_len\n\n    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n        # input\n        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n            if len(p_tokens) > len(h_tokens):\n                p_tokens.pop()\n            else:\n                h_tokens.pop()\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx]), self.labels[idx]\n\n    def __len__(self):\n        return len(self.all_token_ids)\n```\n\nAfter downloading the SNLI dataset,\nwe [**generate training and testing examples**]\nby instantiating the `SNLIBERTDataset` class."
    },
    {
      "chunk_id": "0c19843c8def_2",
      "chapter": "natural-language-inference-bert",
      "heading": "[**The Dataset for Fine-Tuning BERT**]",
      "text": "Such examples will be read in minibatches during training and testing\nof natural language inference. ```{.python .input}\n#@tab mxnet\n# Reduce `batch_size` if there is an out of memory error. In the original BERT\n# model, `max_len` = 512\nbatch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\ndata_dir = d2l.download_extract('SNLI')\ntrain_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\ntest_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\ntrain_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,\n                                   num_workers=num_workers)\ntest_iter = gluon.data.DataLoader(test_set, batch_size,\n                                  num_workers=num_workers)\n```\n\n```{.python .input}\n#@tab pytorch\n# Reduce `batch_size` if there is an out of memory error. In the original BERT\n# model, `max_len` = 512\nbatch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\ndata_dir = d2l.download_extract('SNLI')\ntrain_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\ntest_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n                                   num_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(test_set, batch_size,\n                                  num_workers=num_workers)\n```"
    },
    {
      "chunk_id": "206a491918eb_0",
      "chapter": "natural-language-inference-bert",
      "heading": "Fine-Tuning BERT",
      "text": "As :numref:`fig_bert-two-seqs` indicates,\nfine-tuning BERT for natural language inference\nrequires only an extra MLP consisting of two fully connected layers\n(see `self.hidden` and `self.output` in the following `BERTClassifier` class). [**This MLP transforms the\nBERT representation of the special \u201c&lt;cls&gt;\u201d token**],\nwhich encodes the information of both the premise and the hypothesis,\n(**into three outputs of natural language inference**):\nentailment, contradiction, and neutral. ```{.python .input}\n#@tab mxnet\nclass BERTClassifier(nn.Block):\n    def __init__(self, bert):\n        super(BERTClassifier, self).__init__()\n        self.encoder = bert.encoder\n        self.hidden = bert.hidden\n        self.output = nn.Dense(3)\n\n    def forward(self, inputs):\n        tokens_X, segments_X, valid_lens_x = inputs\n        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n        return self.output(self.hidden(encoded_X[:, 0, :]))\n```\n\n```{.python .input}\n#@tab pytorch\nclass BERTClassifier(nn.Module):\n    def __init__(self, bert):\n        super(BERTClassifier, self).__init__()\n        self.encoder = bert.encoder\n        self.hidden = bert.hidden\n        self.output = nn.LazyLinear(3)\n\n    def forward(self, inputs):\n        tokens_X, segments_X, valid_lens_x = inputs\n        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n        return self.output(self.hidden(encoded_X[:, 0, :]))\n```\n\nIn the following,\nthe pretrained BERT model `bert` is fed into the `BERTClassifier` instance `net` for\nthe downstream application. In common implementations of BERT fine-tuning,\nonly the parameters of the output layer of the additional MLP (`net.output`) will be learned from scratch. All the parameters of the pretrained BERT encoder (`net.encoder`) and the hidden layer of the additional MLP (`net.hidden`) will be fine-tuned."
    },
    {
      "chunk_id": "206a491918eb_1",
      "chapter": "natural-language-inference-bert",
      "heading": "Fine-Tuning BERT",
      "text": "All the parameters of the pretrained BERT encoder (`net.encoder`) and the hidden layer of the additional MLP (`net.hidden`) will be fine-tuned. ```{.python .input}\n#@tab mxnet\nnet = BERTClassifier(bert)\nnet.output.initialize(ctx=devices)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = BERTClassifier(bert)\n```\n\nRecall that\nin :numref:`sec_bert`\nboth the `MaskLM` class and the `NextSentencePred` class\nhave parameters in their employed MLPs. These parameters are part of those in the pretrained BERT model\n`bert`, and thus part of parameters in `net`. However, such parameters are only for computing\nthe masked language modeling loss\nand the next sentence prediction loss\nduring pretraining. These two loss functions are irrelevant to fine-tuning downstream applications,\nthus the parameters of the employed MLPs in \n`MaskLM` and `NextSentencePred` are not updated (staled) when BERT is fine-tuned. To allow parameters with stale gradients,\nthe flag `ignore_stale_grad=True` is set in the `step` function of `d2l.train_batch_ch13`. We use this function to train and evaluate the model `net` using the training set\n(`train_iter`) and the testing set (`test_iter`) of SNLI. Due to the limited computational resources, [**the training**] and testing accuracy\ncan be further improved: we leave its discussions in the exercises. ```{.python .input}\n#@tab mxnet\nlr, num_epochs = 1e-4, 5\ntrainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices,\n               d2l.split_batch_multi_inputs)\n```\n\n```{.python .input}\n#@tab pytorch\nlr, num_epochs = 1e-4, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction='none')\nnet(next(iter(train_iter))[0])\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```"
    },
    {
      "chunk_id": "114610f6c142_0",
      "chapter": "natural-language-inference-bert",
      "heading": "Summary",
      "text": "* We can fine-tune the pretrained BERT model for downstream applications, such as natural language inference on the SNLI dataset.\n* During fine-tuning, the BERT model becomes part of the model for the downstream application. Parameters that are only related to pretraining loss will not be updated during fine-tuning."
    },
    {
      "chunk_id": "19d95ceff8b0_0",
      "chapter": "natural-language-inference-bert",
      "heading": "Exercises",
      "text": "1. Fine-tune a much larger pretrained BERT model that is about as big as the original BERT base model if your computational resource allows. Set arguments in the `load_pretrained_model` function as: replacing 'bert.small' with 'bert.base', increasing values of `num_hiddens=256`, `ffn_num_hiddens=512`, `num_heads=4`, and `num_blks=2` to 768, 3072, 12, and 12, respectively. By increasing fine-tuning epochs (and possibly tuning other hyperparameters), can you get a testing accuracy higher than 0.86?\n1. How to truncate a pair of sequences according to their ratio of length? Compare this pair truncation method and the one used in the `SNLIBERTDataset` class. What are their pros and cons?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/397)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1526)\n:end_tab:"
    },
    {
      "chunk_id": "863a798dba03_0",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "sentiment-analysis-and-dataset",
      "text": "# Sentiment Analysis and the Dataset\n:label:`sec_sentiment`\n\n\nWith the proliferation of online social media\nand review platforms,\na plethora of\nopinionated data\nhas been logged,\nbearing great potential for\nsupporting decision making processes.\n*Sentiment analysis*\nstudies people's sentiments\nin their produced text,\nsuch as product reviews,\nblog comments,\nand\nforum discussions.\nIt enjoys wide applications\nto fields as diverse as \npolitics (e.g., analysis of public sentiments towards policies),\nfinance (e.g., analysis of sentiments of the market),\nand \nmarketing (e.g., product research and brand management).\n\nSince sentiments\ncan be categorized\nas discrete polarities or scales (e.g., positive and negative),\nwe can consider \nsentiment analysis \nas a text classification task,\nwhich transforms a varying-length text sequence\ninto a fixed-length text category.\nIn this chapter,\nwe will use Stanford's [large movie review dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/)\nfor sentiment analysis. \nIt consists of a training set and a testing set, \neither containing 25000 movie reviews downloaded from IMDb.\nIn both datasets, \nthere are equal number of \n\"positive\" and \"negative\" labels,\nindicating different sentiment polarities.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nimport os\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nimport os\n```"
    },
    {
      "chunk_id": "2a2424cf80d3_0",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "Reading the Dataset",
      "text": "First, download and extract this IMDb review dataset\nin the path `../data/aclImdb`.\n\n```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['aclImdb'] = (d2l.DATA_URL + 'aclImdb_v1.tar.gz', \n                          '01ada507287d82875905620988597833ad4e0903')\n\ndata_dir = d2l.download_extract('aclImdb', 'aclImdb')\n```\n\nNext, read the training and test datasets. Each example is a review and its label: 1 for \"positive\" and 0 for \"negative\".\n\n```{.python .input}\n#@tab all\n#@save\ndef read_imdb(data_dir, is_train):\n    \"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\n    data, labels = [], []\n    for label in ('pos', 'neg'):\n        folder_name = os.path.join(data_dir, 'train' if is_train else 'test',\n                                   label)\n        for file in os.listdir(folder_name):\n            with open(os.path.join(folder_name, file), 'rb') as f:\n                review = f.read().decode('utf-8').replace('\\n', '')\n                data.append(review)\n                labels.append(1 if label == 'pos' else 0)\n    return data, labels\n\ntrain_data = read_imdb(data_dir, is_train=True)\nprint('# trainings:', len(train_data[0]))\nfor x, y in zip(train_data[0][:3], train_data[1][:3]):\n    print('label:', y, 'review:', x[:60])\n```"
    },
    {
      "chunk_id": "8bf45608ee02_0",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "Preprocessing the Dataset",
      "text": "Treating each word as a token\nand filtering out words that appear less than 5 times,\nwe create a vocabulary out of the training dataset.\n\n```{.python .input}\n#@tab all\ntrain_tokens = d2l.tokenize(train_data[0], token='word')\nvocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])\n```\n\nAfter tokenization,\nlet's plot the histogram of\nreview lengths in tokens.\n\n```{.python .input}\n#@tab all\nd2l.set_figsize()\nd2l.plt.xlabel('# tokens per review')\nd2l.plt.ylabel('count')\nd2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));\n```\n\nAs we expected,\nthe reviews have varying lengths.\nTo process\na minibatch of such reviews at each time,\nwe set the length of each review to 500 with truncation and padding,\nwhich is similar to \nthe preprocessing step \nfor the machine translation dataset\nin :numref:`sec_machine_translation`.\n\n```{.python .input}\n#@tab all\nnum_steps = 500  # sequence length\ntrain_features = d2l.tensor([d2l.truncate_pad(\n    vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\nprint(train_features.shape)\n```"
    },
    {
      "chunk_id": "ff3f01132b88_0",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "Creating Data Iterators",
      "text": "Now we can create data iterators.\nAt each iteration, a minibatch of examples are returned.\n\n```{.python .input}\n#@tab mxnet\ntrain_iter = d2l.load_array((train_features, train_data[1]), 64)\n\nfor X, y in train_iter:\n    print('X:', X.shape, ', y:', y.shape)\n    break\nprint('# batches:', len(train_iter))\n```\n\n```{.python .input}\n#@tab pytorch\ntrain_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), 64)\n\nfor X, y in train_iter:\n    print('X:', X.shape, ', y:', y.shape)\n    break\nprint('# batches:', len(train_iter))\n```"
    },
    {
      "chunk_id": "30d8eadd6d25_0",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "Putting It All Together",
      "text": "Last, we wrap up the above steps into the `load_data_imdb` function. It returns training and test data iterators and the vocabulary of the IMDb review dataset."
    },
    {
      "chunk_id": "30d8eadd6d25_1",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "Putting It All Together",
      "text": "Last, we wrap up the above steps into the `load_data_imdb` function. It returns training and test data iterators and the vocabulary of the IMDb review dataset. ```{.python .input}\n#@tab mxnet\n#@save\ndef load_data_imdb(batch_size, num_steps=500):\n    \"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\n    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n    train_data = read_imdb(data_dir, True)\n    test_data = read_imdb(data_dir, False)\n    train_tokens = d2l.tokenize(train_data[0], token='word')\n    test_tokens = d2l.tokenize(test_data[0], token='word')\n    vocab = d2l.Vocab(train_tokens, min_freq=5)\n    train_features = np.array([d2l.truncate_pad(\n        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n    test_features = np.array([d2l.truncate_pad(\n        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n    train_iter = d2l.load_array((train_features, train_data[1]), batch_size)\n    test_iter = d2l.load_array((test_features, test_data[1]), batch_size,\n                               is_train=False)\n    return train_iter, test_iter, vocab\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef load_data_imdb(batch_size, num_steps=500):\n    \"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\n    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n    train_data = read_imdb(data_dir, True)\n    test_data = read_imdb(data_dir, False)\n    train_tokens = d2l.tokenize(train_data[0], token='word')\n    test_tokens = d2l.tokenize(test_data[0], token='word')\n    vocab = d2l.Vocab(train_tokens, min_freq=5)\n    train_features = torch.tensor([d2l.truncate_pad(\n        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n    test_features = torch.tensor([d2l.truncate_pad(\n        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),\n                                batch_size)\n    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),\n                               batch_size,\n                               is_train=False)\n    return train_iter, test_iter, vocab\n```"
    },
    {
      "chunk_id": "3e72b1cda09b_0",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "Summary",
      "text": "* Sentiment analysis studies people's sentiments in their produced text, which is considered as a text classification problem that transforms a varying-length text sequence\ninto a fixed-length text category.\n* After preprocessing, we can load Stanford's large movie review dataset (IMDb review dataset) into data iterators with a vocabulary."
    },
    {
      "chunk_id": "4bee830c0eda_0",
      "chapter": "sentiment-analysis-and-dataset",
      "heading": "Exercises",
      "text": "1. What hyperparameters in this section can we modify to accelerate training sentiment analysis models?\n1. Can you implement a function to load the dataset of [Amazon reviews](https://snap.stanford.edu/data/web-Amazon.html) into data iterators and labels for sentiment analysis?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/391)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1387)\n:end_tab:"
    },
    {
      "chunk_id": "37779132397d_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "sentiment-analysis-cnn",
      "text": "# Sentiment Analysis: Using Convolutional Neural Networks\n:label:`sec_sentiment_cnn` \n\n\nIn :numref:`chap_cnn`,\nwe investigated mechanisms\nfor processing\ntwo-dimensional image data\nwith two-dimensional CNNs,\nwhich were applied to\nlocal features such as adjacent pixels.\nThough originally\ndesigned for computer vision,\nCNNs are also widely used\nfor natural language processing.\nSimply put,\njust think of any text sequence\nas a one-dimensional image.\nIn this way,\none-dimensional CNNs\ncan process local features\nsuch as $n$-grams in text.\n\nIn this section,\nwe will use the *textCNN* model\nto demonstrate\nhow to design a CNN architecture\nfor representing single text :cite:`Kim.2014`.\nCompared with\n:numref:`fig_nlp-map-sa-rnn`\nthat uses an RNN architecture with GloVe pretraining\nfor sentiment analysis,\nthe only difference in :numref:`fig_nlp-map-sa-cnn`\nlies in\nthe choice of the architecture.\n\n\n![This section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.](../img/nlp-map-sa-cnn.svg)\n:label:`fig_nlp-map-sa-cnn`\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n\nbatch_size = 64\ntrain_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n\nbatch_size = 64\ntrain_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)\n```"
    },
    {
      "chunk_id": "ac6543dddd81_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "One-Dimensional Convolutions",
      "text": "Before introducing the model,\nlet's see how a one-dimensional convolution works. Bear in mind that it is just a special case\nof a two-dimensional convolution\nbased on the cross-correlation operation. ![One-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times1+1\\times2=2$.](../img/conv1d.svg)\n:label:`fig_conv1d`\n\nAs shown in :numref:`fig_conv1d`,\nin the one-dimensional case,\nthe convolution window\nslides from left to right\nacross the input tensor. During sliding,\nthe input subtensor (e.g., $0$ and $1$ in :numref:`fig_conv1d`) contained in the convolution window\nat a certain position\nand the kernel tensor (e.g., $1$ and $2$ in :numref:`fig_conv1d`) are multiplied elementwise. The sum of these multiplications\ngives the single scalar value (e.g., $0\\times1+1\\times2=2$ in :numref:`fig_conv1d`)\nat the corresponding position of the output tensor. We implement one-dimensional cross-correlation in the following `corr1d` function. Given an input tensor `X`\nand a kernel tensor `K`,\nit returns the output tensor `Y`. ```{.python .input}\n#@tab all\ndef corr1d(X, K):\n    w = K.shape[0]\n    Y = d2l.zeros((X.shape[0] - w + 1))\n    for i in range(Y.shape[0]):\n        Y[i] = (X[i: i + w] * K).sum()\n    return Y\n```\n\nWe can construct the input tensor `X` and the kernel tensor `K` from :numref:`fig_conv1d` to validate the output of the above one-dimensional cross-correlation implementation. ```{.python .input}\n#@tab all\nX, K = d2l.tensor([0, 1, 2, 3, 4, 5, 6]), d2l.tensor([1, 2])\ncorr1d(X, K)\n```\n\nFor any\none-dimensional input with multiple channels,\nthe convolution kernel\nneeds to have the same number of input channels. Then for each channel,\nperform a cross-correlation operation on the one-dimensional tensor of the input and the one-dimensional tensor of the convolution kernel,\nsumming the results over all the channels\nto produce the one-dimensional output tensor."
    },
    {
      "chunk_id": "ac6543dddd81_1",
      "chapter": "sentiment-analysis-cnn",
      "heading": "One-Dimensional Convolutions",
      "text": "Then for each channel,\nperform a cross-correlation operation on the one-dimensional tensor of the input and the one-dimensional tensor of the convolution kernel,\nsumming the results over all the channels\nto produce the one-dimensional output tensor. :numref:`fig_conv1d_channel` shows a one-dimensional cross-correlation operation with 3 input channels. ![One-dimensional cross-correlation operation with 3 input channels. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times1+1\\times2+1\\times3+2\\times4+2\\times(-1)+3\\times(-3)=2$.](../img/conv1d-channel.svg)\n:label:`fig_conv1d_channel`\n\n\nWe can implement the one-dimensional cross-correlation operation for multiple input channels\nand validate the results in :numref:`fig_conv1d_channel`. ```{.python .input}\n#@tab all\ndef corr1d_multi_in(X, K):\n    # First, iterate through the 0th dimension (channel dimension) of `X` and\n    # `K`. Then, add them together\n    return sum(corr1d(x, k) for x, k in zip(X, K))\n\nX = d2l.tensor([[0, 1, 2, 3, 4, 5, 6],\n              [1, 2, 3, 4, 5, 6, 7],\n              [2, 3, 4, 5, 6, 7, 8]])\nK = d2l.tensor([[1, 2], [3, 4], [-1, -3]])\ncorr1d_multi_in(X, K)\n```\n\nNote that\nmulti-input-channel one-dimensional cross-correlations\nare equivalent\nto\nsingle-input-channel\ntwo-dimensional cross-correlations. To illustrate,\nan equivalent form of\nthe multi-input-channel one-dimensional cross-correlation\nin :numref:`fig_conv1d_channel`\nis\nthe\nsingle-input-channel\ntwo-dimensional cross-correlation\nin :numref:`fig_conv1d_2d`,\nwhere the height of the convolution kernel\nhas to be the same as that of the input tensor. ![Two-dimensional cross-correlation operation with a single input channel."
    },
    {
      "chunk_id": "ac6543dddd81_2",
      "chapter": "sentiment-analysis-cnn",
      "heading": "One-Dimensional Convolutions",
      "text": "![Two-dimensional cross-correlation operation with a single input channel. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $2\\times(-1)+3\\times(-3)+1\\times3+2\\times4+0\\times1+1\\times2=2$.](../img/conv1d-2d.svg)\n:label:`fig_conv1d_2d`\n\nBoth the outputs in :numref:`fig_conv1d` and :numref:`fig_conv1d_channel` have only one channel. Same as two-dimensional convolutions with multiple output channels described in :numref:`subsec_multi-output-channels`,\nwe can also specify multiple output channels\nfor one-dimensional convolutions."
    },
    {
      "chunk_id": "4710ab10dde3_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Max-Over-Time Pooling",
      "text": "Similarly, we can use pooling\nto extract the highest value\nfrom sequence representations\nas the most important feature\nacross time steps.\nThe *max-over-time pooling* used in textCNN\nworks like\nthe one-dimensional global max-pooling\n:cite:`Collobert.Weston.Bottou.ea.2011`.\nFor a multi-channel input\nwhere each channel stores values\nat different time steps,\nthe output at each channel\nis the maximum value\nfor that channel.\nNote that\nthe max-over-time pooling\nallows different numbers of time steps\nat different channels."
    },
    {
      "chunk_id": "c28bf963cd16_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "The textCNN Model",
      "text": "Using the one-dimensional convolution\nand max-over-time pooling,\nthe textCNN model\ntakes individual pretrained token representations\nas input,\nthen obtains and transforms sequence representations\nfor the downstream application.\n\nFor a single text sequence\nwith $n$ tokens represented by\n$d$-dimensional vectors,\nthe width, height, and number of channels\nof the input tensor\nare $n$, $1$, and $d$, respectively.\nThe textCNN model transforms the input\ninto the output as follows:\n\n1. Define multiple one-dimensional convolution kernels and perform convolution operations separately on the inputs. Convolution kernels with different widths may capture local features among different numbers of adjacent tokens.\n1. Perform max-over-time pooling on all the output channels, and then concatenate all the scalar pooling outputs as a vector.\n1. Transform the concatenated vector into the output categories using the fully connected layer. Dropout can be used for reducing overfitting.\n\n![The model architecture of textCNN.](../img/textcnn.svg)\n:label:`fig_conv1d_textcnn`\n\n:numref:`fig_conv1d_textcnn`\nillustrates the model architecture of textCNN\nwith a concrete example.\nThe input is a sentence with 11 tokens,\nwhere\neach token is represented by a 6-dimensional vectors.\nSo we have a 6-channel input with width 11.\nDefine\ntwo one-dimensional convolution kernels\nof widths 2 and 4,\nwith 4 and 5 output channels, respectively.\nThey produce\n4 output channels with width $11-2+1=10$\nand 5 output channels with width $11-4+1=8$.\nDespite different widths of these 9 channels,\nthe max-over-time pooling\ngives a concatenated 9-dimensional vector,\nwhich is finally transformed\ninto a 2-dimensional output vector\nfor binary sentiment predictions."
    },
    {
      "chunk_id": "0e66d74a08cc_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Defining the Model",
      "text": "We implement the textCNN model in the following class. Compared with the bidirectional RNN model in\n:numref:`sec_sentiment_rnn`,\nbesides\nreplacing recurrent layers with convolutional layers,\nwe also use two embedding layers:\none with trainable weights and the other\nwith fixed weights. ```{.python .input}\n#@tab mxnet\nclass TextCNN(nn.Block):\n    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n                 **kwargs):\n        super(TextCNN, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        # The embedding layer not to be trained\n        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n        self.dropout = nn.Dropout(0.5)\n        self.decoder = nn.Dense(2)\n        # The max-over-time pooling layer has no parameters, so this instance\n        # can be shared\n        self.pool = nn.GlobalMaxPool1D()\n        # Create multiple one-dimensional convolutional layers\n        self.convs = nn.Sequential()\n        for c, k in zip(num_channels, kernel_sizes):\n            self.convs.add(nn.Conv1D(c, k, activation='relu'))\n\n    def forward(self, inputs):\n        # Concatenate two embedding layer outputs with shape (batch size, no. # of tokens, token vector dimension) along vectors\n        embeddings = np.concatenate((\n            self.embedding(inputs), self.constant_embedding(inputs)), axis=2)\n        # Per the input format of one-dimensional convolutional layers,\n        # rearrange the tensor so that the second dimension stores channels\n        embeddings = embeddings.transpose(0, 2, 1)\n        # For each one-dimensional convolutional layer, after max-over-time\n        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n        # obtained."
    },
    {
      "chunk_id": "0e66d74a08cc_1",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Defining the Model",
      "text": "of channels, 1) is\n        # obtained. Remove the last dimension and concatenate along channels\n        encoding = np.concatenate([\n            np.squeeze(self.pool(conv(embeddings)), axis=-1)\n            for conv in self.convs], axis=1)\n        outputs = self.decoder(self.dropout(encoding))\n        return outputs\n```\n\n```{.python .input}\n#@tab pytorch\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n                 **kwargs):\n        super(TextCNN, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        # The embedding layer not to be trained\n        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n        self.dropout = nn.Dropout(0.5)\n        self.decoder = nn.Linear(sum(num_channels), 2)\n        # The max-over-time pooling layer has no parameters, so this instance\n        # can be shared\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.relu = nn.ReLU()\n        # Create multiple one-dimensional convolutional layers\n        self.convs = nn.ModuleList()\n        for c, k in zip(num_channels, kernel_sizes):\n            self.convs.append(nn.Conv1d(2 * embed_size, c, k))\n\n    def forward(self, inputs):\n        # Concatenate two embedding layer outputs with shape (batch size, no. # of tokens, token vector dimension) along vectors\n        embeddings = torch.cat((\n            self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n        # Per the input format of one-dimensional convolutional layers,\n        # rearrange the tensor so that the second dimension stores channels\n        embeddings = embeddings.permute(0, 2, 1)\n        # For each one-dimensional convolutional layer, after max-over-time\n        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n        # obtained."
    },
    {
      "chunk_id": "0e66d74a08cc_2",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Defining the Model",
      "text": "of channels, 1) is\n        # obtained. Remove the last dimension and concatenate along channels\n        encoding = torch.cat([\n            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n            for conv in self.convs], dim=1)\n        outputs = self.decoder(self.dropout(encoding))\n        return outputs\n```\n\nLet's create a textCNN instance. It has 3 convolutional layers with kernel widths of 3, 4, and 5, all with 100 output channels. ```{.python .input}\n#@tab mxnet\nembed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\ndevices = d2l.try_all_gpus()\nnet = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\nnet.initialize(init.Xavier(), ctx=devices)\n```\n\n```{.python .input}\n#@tab pytorch\nembed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\ndevices = d2l.try_all_gpus()\nnet = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\n\ndef init_weights(module):\n    if type(module) in (nn.Linear, nn.Conv1d):\n        nn.init.xavier_uniform_(module.weight)\n\nnet.apply(init_weights);\n```"
    },
    {
      "chunk_id": "50dbb3e4b502_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Loading Pretrained Word Vectors",
      "text": "Same as :numref:`sec_sentiment_rnn`,\nwe load pretrained 100-dimensional GloVe embeddings\nas the initialized token representations.\nThese token representations (embedding weights)\nwill be trained in `embedding`\nand fixed in `constant_embedding`.\n\n```{.python .input}\n#@tab mxnet\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\nembeds = glove_embedding[vocab.idx_to_token]\nnet.embedding.weight.set_data(embeds)\nnet.constant_embedding.weight.set_data(embeds)\nnet.constant_embedding.collect_params().setattr('grad_req', 'null')\n```\n\n```{.python .input}\n#@tab pytorch\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\nembeds = glove_embedding[vocab.idx_to_token]\nnet.embedding.weight.data.copy_(embeds)\nnet.constant_embedding.weight.data.copy_(embeds)\nnet.constant_embedding.weight.requires_grad = False\n```"
    },
    {
      "chunk_id": "a5b3560a5ff6_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Training and Evaluating the Model",
      "text": "Now we can train the textCNN model for sentiment analysis.\n\n```{.python .input}\n#@tab mxnet\nlr, num_epochs = 0.001, 5\ntrainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```\n\n```{.python .input}\n#@tab pytorch\nlr, num_epochs = 0.001, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction=\"none\")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```\n\nBelow we use the trained model to predict the sentiment for two simple sentences.\n\n```{.python .input}\n#@tab all\nd2l.predict_sentiment(net, vocab, 'this movie is so great')\n```\n\n```{.python .input}\n#@tab all\nd2l.predict_sentiment(net, vocab, 'this movie is so bad')\n```"
    },
    {
      "chunk_id": "a8e219b95b9b_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Summary",
      "text": "* One-dimensional CNNs can process local features such as $n$-grams in text.\n* Multi-input-channel one-dimensional cross-correlations are equivalent to single-input-channel two-dimensional cross-correlations.\n* The max-over-time pooling allows different numbers of time steps at different channels.\n* The textCNN model transforms individual token representations into downstream application outputs using one-dimensional convolutional layers and max-over-time pooling layers."
    },
    {
      "chunk_id": "0f6372012618_0",
      "chapter": "sentiment-analysis-cnn",
      "heading": "Exercises",
      "text": "1. Tune hyperparameters and compare the two architectures for sentiment analysis in :numref:`sec_sentiment_rnn` and in this section, such as in classification accuracy and computational efficiency.\n1. Can you further improve the classification accuracy of the model by using the methods introduced in the exercises of :numref:`sec_sentiment_rnn`?\n1. Add positional encoding in the input representations. Does it improve the classification accuracy?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/393)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1425)\n:end_tab:"
    },
    {
      "chunk_id": "679b0643ad63_0",
      "chapter": "sentiment-analysis-rnn",
      "heading": "sentiment-analysis-rnn",
      "text": "# Sentiment Analysis: Using Recurrent Neural Networks\n:label:`sec_sentiment_rnn` \n\n\nLike word similarity and analogy tasks,\nwe can also apply pretrained word vectors\nto sentiment analysis.\nSince the IMDb review dataset\nin :numref:`sec_sentiment`\nis not very big,\nusing text representations\nthat were pretrained\non large-scale corpora\nmay reduce overfitting of the model.\nAs a specific example\nillustrated in :numref:`fig_nlp-map-sa-rnn`,\nwe will represent each token\nusing the pretrained GloVe model,\nand feed these token representations\ninto a multilayer bidirectional RNN\nto obtain the text sequence representation,\nwhich will\nbe transformed into \nsentiment analysis outputs :cite:`Maas.Daly.Pham.ea.2011`.\nFor the same downstream application,\nwe will consider a different architectural\nchoice later.\n\n![This section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.](../img/nlp-map-sa-rnn.svg)\n:label:`fig_nlp-map-sa-rnn`\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, init, np, npx\nfrom mxnet.gluon import nn, rnn\nnpx.set_np()\n\nbatch_size = 64\ntrain_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n\nbatch_size = 64\ntrain_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)\n```"
    },
    {
      "chunk_id": "216dffebbfa5_0",
      "chapter": "sentiment-analysis-rnn",
      "heading": "Representing Single Text with RNNs",
      "text": "In text classifications tasks,\nsuch as sentiment analysis,\na varying-length text sequence \nwill be transformed into fixed-length categories. In the following `BiRNN` class,\nwhile each token of a text sequence\ngets its individual\npretrained GloVe\nrepresentation via the embedding layer\n(`self.embedding`),\nthe entire sequence\nis encoded by a bidirectional RNN (`self.encoder`). More concretely,\nthe hidden states (at the last layer)\nof the bidirectional LSTM\nat both the initial and final time steps\nare concatenated \nas the representation of the text sequence. This single text representation\nis then transformed into output categories\nby a fully connected layer (`self.decoder`)\nwith two outputs (\"positive\" and \"negative\"). ```{.python .input}\n#@tab mxnet\nclass BiRNN(nn.Block):\n    def __init__(self, vocab_size, embed_size, num_hiddens,\n                 num_layers, **kwargs):\n        super(BiRNN, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        # Set `bidirectional` to True to get a bidirectional RNN\n        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n                                bidirectional=True, input_size=embed_size)\n        self.decoder = nn.Dense(2)\n\n    def forward(self, inputs):\n        # The shape of `inputs` is (batch size, no. of time steps). Because\n        # LSTM requires its input's first dimension to be the temporal\n        # dimension, the input is transposed before obtaining token\n        # representations. The output shape is (no. of time steps, batch size,\n        # word vector dimension)\n        embeddings = self.embedding(inputs.T)\n        # Returns hidden states of the last hidden layer at different time\n        # steps. The shape of `outputs` is (no. of time steps, batch size,\n        # 2 * no. of hidden units)\n        outputs = self.encoder(embeddings)\n        # Concatenate the hidden states at the initial and final time steps as\n        # the input of the fully connected layer. Its shape is (batch size,\n        # 4 * no."
    },
    {
      "chunk_id": "216dffebbfa5_1",
      "chapter": "sentiment-analysis-rnn",
      "heading": "Representing Single Text with RNNs",
      "text": "of hidden units)\n        outputs = self.encoder(embeddings)\n        # Concatenate the hidden states at the initial and final time steps as\n        # the input of the fully connected layer. Its shape is (batch size,\n        # 4 * no. of hidden units)\n        encoding = np.concatenate((outputs[0], outputs[-1]), axis=1)\n        outs = self.decoder(encoding)\n        return outs\n```\n\n```{.python .input}\n#@tab pytorch\nclass BiRNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, num_hiddens,\n                 num_layers, **kwargs):\n        super(BiRNN, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        # Set `bidirectional` to True to get a bidirectional RNN\n        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n                                bidirectional=True)\n        self.decoder = nn.Linear(4 * num_hiddens, 2)\n\n    def forward(self, inputs):\n        # The shape of `inputs` is (batch size, no. of time steps). Because\n        # LSTM requires its input's first dimension to be the temporal\n        # dimension, the input is transposed before obtaining token\n        # representations. The output shape is (no. of time steps, batch size,\n        # word vector dimension)\n        embeddings = self.embedding(inputs.T)\n        self.encoder.flatten_parameters()\n        # Returns hidden states of the last hidden layer at different time\n        # steps. The shape of `outputs` is (no. of time steps, batch size,\n        # 2 * no. of hidden units)\n        outputs, _ = self.encoder(embeddings)\n        # Concatenate the hidden states at the initial and final time steps as\n        # the input of the fully connected layer. Its shape is (batch size,\n        # 4 * no. of hidden units)\n        encoding = torch.cat((outputs[0], outputs[-1]), dim=1) \n        outs = self.decoder(encoding)\n        return outs\n```\n\nLet's construct a bidirectional RNN with two hidden layers to represent single text for sentiment analysis."
    },
    {
      "chunk_id": "216dffebbfa5_2",
      "chapter": "sentiment-analysis-rnn",
      "heading": "Representing Single Text with RNNs",
      "text": "of hidden units)\n        encoding = torch.cat((outputs[0], outputs[-1]), dim=1) \n        outs = self.decoder(encoding)\n        return outs\n```\n\nLet's construct a bidirectional RNN with two hidden layers to represent single text for sentiment analysis. ```{.python .input}\n#@tab all\nembed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus()\nnet = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)\n```\n\n```{.python .input}\n#@tab mxnet\nnet.initialize(init.Xavier(), ctx=devices)\n```\n\n```{.python .input}\n#@tab pytorch\ndef init_weights(module):\n    if type(module) == nn.Linear:\n        nn.init.xavier_uniform_(module.weight)\n    if type(module) == nn.LSTM:\n        for param in module._flat_weights_names:\n            if \"weight\" in param:\n                nn.init.xavier_uniform_(module._parameters[param])\nnet.apply(init_weights);\n```"
    },
    {
      "chunk_id": "fa557ee879ac_0",
      "chapter": "sentiment-analysis-rnn",
      "heading": "Loading Pretrained Word Vectors",
      "text": "Below we load the pretrained 100-dimensional (needs to be consistent with `embed_size`) GloVe embeddings for tokens in the vocabulary.\n\n```{.python .input}\n#@tab all\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n```\n\nPrint the shape of the vectors\nfor all the tokens in the vocabulary.\n\n```{.python .input}\n#@tab all\nembeds = glove_embedding[vocab.idx_to_token]\nembeds.shape\n```\n\nWe use these pretrained\nword vectors\nto represent tokens in the reviews\nand will not update\nthese vectors during training.\n\n```{.python .input}\n#@tab mxnet\nnet.embedding.weight.set_data(embeds)\nnet.embedding.collect_params().setattr('grad_req', 'null')\n```\n\n```{.python .input}\n#@tab pytorch\nnet.embedding.weight.data.copy_(embeds)\nnet.embedding.weight.requires_grad = False\n```"
    },
    {
      "chunk_id": "1e2fd82edfc3_0",
      "chapter": "sentiment-analysis-rnn",
      "heading": "Training and Evaluating the Model",
      "text": "Now we can train the bidirectional RNN for sentiment analysis.\n\n```{.python .input}\n#@tab mxnet\nlr, num_epochs = 0.01, 5\ntrainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```\n\n```{.python .input}\n#@tab pytorch\nlr, num_epochs = 0.01, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction=\"none\")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```\n\nWe define the following function to predict the sentiment of a text sequence using the trained model `net`.\n\n```{.python .input}\n#@tab mxnet\n#@save\ndef predict_sentiment(net, vocab, sequence):\n    \"\"\"Predict the sentiment of a text sequence.\"\"\"\n    sequence = np.array(vocab[sequence.split()], ctx=d2l.try_gpu())\n    label = np.argmax(net(sequence.reshape(1, -1)), axis=1)\n    return 'positive' if label == 1 else 'negative'\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef predict_sentiment(net, vocab, sequence):\n    \"\"\"Predict the sentiment of a text sequence.\"\"\"\n    sequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())\n    label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)\n    return 'positive' if label == 1 else 'negative'\n```\n\nFinally, let's use the trained model to predict the sentiment for two simple sentences.\n\n```{.python .input}\n#@tab all\npredict_sentiment(net, vocab, 'this movie is so great')\n```\n\n```{.python .input}\n#@tab all\npredict_sentiment(net, vocab, 'this movie is so bad')\n```"
    },
    {
      "chunk_id": "28a03790ad6c_0",
      "chapter": "sentiment-analysis-rnn",
      "heading": "Summary",
      "text": "* Pretrained word vectors can represent individual tokens in a text sequence.\n* Bidirectional RNNs can represent a text sequence, such as via the concatenation of its hidden states at the initial and final time steps. This single text representation can be transformed into categories using a fully connected layer."
    },
    {
      "chunk_id": "f7fc54cb5249_0",
      "chapter": "sentiment-analysis-rnn",
      "heading": "Exercises",
      "text": "1. Increase the number of epochs. Can you improve the training and testing accuracies? How about tuning other hyperparameters?\n1. Use larger pretrained word vectors, such as 300-dimensional GloVe embeddings. Does it improve classification accuracy?\n1. Can we improve the classification accuracy by using the spaCy tokenization? You need to install spaCy (`pip install spacy`) and install the English package (`python -m spacy download en`). In the code, first, import spaCy (`import spacy`). Then, load the spaCy English package (`spacy_en = spacy.load('en')`). Finally, define the function `def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]` and replace the original `tokenizer` function. Note the different forms of phrase tokens in GloVe and spaCy. For example, the phrase token \"new york\" takes the form of \"new-york\" in GloVe and the form of \"new york\" after the spaCy tokenization.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/392)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1424)\n:end_tab:"
    },
    {
      "chunk_id": "7f20e0a2edb3_0",
      "chapter": "approx-training",
      "heading": "approx-training",
      "text": "# Approximate Training\n:label:`sec_approx_train`\n\nRecall our discussions in :numref:`sec_word2vec`.\nThe main idea of the skip-gram model is\nusing softmax operations to calculate\nthe conditional probability of\ngenerating a context word $w_o$\nbased on the given center word $w_c$\nin :eqref:`eq_skip-gram-softmax`,\nwhose corresponding logarithmic loss is given by\nthe opposite of :eqref:`eq_skip-gram-log`.\n\n\n\nDue to the nature of the softmax operation,\nsince a context word may be anyone in the\ndictionary $\\mathcal{V}$,\nthe opposite of :eqref:`eq_skip-gram-log`\ncontains the summation\nof items as many as the entire size of the vocabulary.\nConsequently,\nthe gradient calculation\nfor the skip-gram model\nin :eqref:`eq_skip-gram-grad`\nand that\nfor the continuous bag-of-words model\nin :eqref:`eq_cbow-gradient`\nboth contain\nthe summation.\nUnfortunately,\nthe computational cost\nfor such gradients\nthat sum over\na large dictionary\n(often with\nhundreds of thousands or millions of words)\nis huge!\n\nIn order to reduce the aforementioned computational complexity, this section will introduce two approximate training methods:\n*negative sampling* and *hierarchical softmax*.\nDue to the similarity\nbetween the skip-gram model and\nthe continuous bag of words model,\nwe will just take the skip-gram model as an example\nto describe these two approximate training methods."
    },
    {
      "chunk_id": "bcc2a87dc7d7_0",
      "chapter": "approx-training",
      "heading": "Negative Sampling",
      "text": ":label:`subsec_negative-sampling`\n\n\nNegative sampling modifies the original objective function. Given the context window of a center word $w_c$,\nthe fact that any (context) word $w_o$\ncomes from this context window\nis considered as an event with the probability\nmodeled by\n\n\n$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c),$$\n\nwhere $\\sigma$ uses the definition of the sigmoid activation function:\n\n$$\\sigma(x) = \\frac{1}{1+\\exp(-x)}.$$\n:eqlabel:`eq_sigma-f`\n\nLet's begin by\nmaximizing the joint probability of\nall such events in text sequences\nto train word embeddings. Specifically,\ngiven a text sequence of length $T$,\ndenote by $w^{(t)}$ the word at time step $t$\nand let the context window size be $m$,\nconsider maximizing the joint probability\n\n\n$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).$$\n:eqlabel:`eq-negative-sample-pos`\n\n\nHowever,\n:eqref:`eq-negative-sample-pos`\nonly considers those events\nthat involve positive examples. As a result,\nthe joint probability in\n:eqref:`eq-negative-sample-pos`\nis maximized to 1\nonly if all the word vectors are equal to infinity. Of course,\nsuch results are meaningless. To make the objective function\nmore meaningful,\n*negative sampling*\nadds negative examples sampled\nfrom a predefined distribution. Denote by $S$\nthe event that\na context word $w_o$ comes from\nthe context window of a center word $w_c$. For this event involving $w_o$,\nfrom a predefined distribution $P(w)$\nsample $K$ *noise words*\nthat are not from this context window. Denote by $N_k$\nthe event that\na noise word $w_k$ ($k=1, \\ldots, K$)\ndoes not come from\nthe context window of $w_c$. Assume that\nthese events involving\nboth the positive example and negative examples\n$S, N_1, \\ldots, N_K$ are mutually independent."
    },
    {
      "chunk_id": "bcc2a87dc7d7_1",
      "chapter": "approx-training",
      "heading": "Negative Sampling",
      "text": "Denote by $N_k$\nthe event that\na noise word $w_k$ ($k=1, \\ldots, K$)\ndoes not come from\nthe context window of $w_c$. Assume that\nthese events involving\nboth the positive example and negative examples\n$S, N_1, \\ldots, N_K$ are mutually independent. Negative sampling\nrewrites the joint probability (involving only positive examples)\nin :eqref:`eq-negative-sample-pos`\nas\n\n$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n\nwhere the conditional probability is approximated through\nevents $S, N_1, \\ldots, N_K$:\n\n$$ P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).$$\n:eqlabel:`eq-negative-sample-conditional-prob`\n\nDenote by\n$i_t$ and $h_k$\nthe indices of\na word $w^{(t)}$ at time step $t$\nof a text sequence\nand a noise word $w_k$,\nrespectively. The logarithmic loss with respect to the conditional probabilities in :eqref:`eq-negative-sample-conditional-prob` is\n\n$$\n\\begin{aligned}\n-\\log P(w^{(t+j)} \\mid w^{(t)})\n=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)\\\\\n=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right). \\end{aligned}\n$$\n\n\nWe can see that\nnow the computational cost for gradients\nat each training step\nhas nothing to do with the dictionary size,\nbut linearly depends on $K$. When setting the hyperparameter $K$\nto a smaller value,\nthe computational cost for gradients\nat each training step with negative sampling\nis smaller."
    },
    {
      "chunk_id": "5fb3a3f0a2af_0",
      "chapter": "approx-training",
      "heading": "Hierarchical Softmax",
      "text": "As an alternative approximate training method,\n*hierarchical softmax*\nuses the binary tree,\na data structure\nillustrated in :numref:`fig_hi_softmax`,\nwhere each leaf node\nof the tree represents\na word in dictionary $\\mathcal{V}$. ![Hierarchical softmax for approximate training, where each leaf node of the tree represents a word in the dictionary.](../img/hi-softmax.svg)\n:label:`fig_hi_softmax`\n\nDenote by $L(w)$\nthe number of nodes (including both ends)\non the path\nfrom the root node to the leaf node representing word $w$\nin the binary tree. Let $n(w,j)$ be the $j^\\textrm{th}$ node on this path,\nwith its context word vector being\n$\\mathbf{u}_{n(w, j)}$. For example,\n$L(w_3) = 4$ in  :numref:`fig_hi_softmax`. Hierarchical softmax approximates the conditional probability in :eqref:`eq_skip-gram-softmax` as\n\n\n$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\textrm{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right),$$\n\nwhere function $\\sigma$\nis defined in :eqref:`eq_sigma-f`,\nand $\\textrm{leftChild}(n)$ is the left child node of node $n$: if $x$ is true, $[\\![x]\\!] = 1$; otherwise $[\\![x]\\!] = -1$. To illustrate,\nlet's calculate\nthe conditional probability\nof generating word $w_3$\ngiven word $w_c$ in :numref:`fig_hi_softmax`."
    },
    {
      "chunk_id": "5fb3a3f0a2af_1",
      "chapter": "approx-training",
      "heading": "Hierarchical Softmax",
      "text": "To illustrate,\nlet's calculate\nthe conditional probability\nof generating word $w_3$\ngiven word $w_c$ in :numref:`fig_hi_softmax`. This requires dot products\nbetween the word vector\n$\\mathbf{v}_c$ of $w_c$\nand\nnon-leaf node vectors\non the path (the path in bold in :numref:`fig_hi_softmax`) from the root to $w_3$,\nwhich is traversed left, right, then left:\n\n\n$$P(w_3 \\mid w_c) = \\sigma(\\mathbf{u}_{n(w_3, 1)}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{n(w_3, 2)}^\\top \\mathbf{v}_c) \\cdot \\sigma(\\mathbf{u}_{n(w_3, 3)}^\\top \\mathbf{v}_c).$$\n\nSince $\\sigma(x)+\\sigma(-x) = 1$,\nit holds that\nthe conditional probabilities of\ngenerating all the words in\ndictionary $\\mathcal{V}$\nbased on any word $w_c$\nsum up to one:\n\n$$\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.$$\n:eqlabel:`eq_hi-softmax-sum-one`\n\nFortunately, since $L(w_o)-1$ is on the order of $\\mathcal{O}(\\textrm{log}_2|\\mathcal{V}|)$ due to the binary tree structure,\nwhen the dictionary size $\\mathcal{V}$ is huge,\nthe computational cost for  each training step using hierarchical softmax\nis significantly reduced compared with that\nwithout approximate training."
    },
    {
      "chunk_id": "2241f550fd3c_0",
      "chapter": "approx-training",
      "heading": "Summary",
      "text": "* Negative sampling constructs the loss function by considering mutually independent events that involve both positive and negative examples. The computational cost for training is linearly dependent on the number of noise words at each step.\n* Hierarchical softmax constructs the loss function using  the path from the root node to the leaf node in the binary tree. The computational cost for training is dependent on the logarithm of the dictionary size at each step."
    },
    {
      "chunk_id": "437df089a786_0",
      "chapter": "approx-training",
      "heading": "Exercises",
      "text": "1. How can we sample noise words in negative sampling?\n1. Verify that :eqref:`eq_hi-softmax-sum-one` holds.\n1. How to train the continuous bag of words model using negative sampling and hierarchical softmax, respectively?\n\n[Discussions](https://discuss.d2l.ai/t/382)"
    },
    {
      "chunk_id": "e5a9ad7e216f_0",
      "chapter": "bert-dataset",
      "heading": "bert-dataset",
      "text": "# The Dataset for Pretraining BERT\n:label:`sec_bert-dataset`\n\nTo pretrain the BERT model as implemented in :numref:`sec_bert`,\nwe need to generate the dataset in the ideal format to facilitate\nthe two pretraining tasks:\nmasked language modeling and next sentence prediction. On the one hand,\nthe original BERT model is pretrained on the concatenation of\ntwo huge corpora BookCorpus and English Wikipedia (see :numref:`subsec_bert_pretraining_tasks`),\nmaking it hard to run for most readers of this book. On the other hand,\nthe off-the-shelf pretrained BERT model\nmay not fit for applications from specific domains like medicine. Thus, it is getting popular to pretrain BERT on a customized dataset. To facilitate the demonstration of BERT pretraining,\nwe use a smaller corpus WikiText-2 :cite:`Merity.Xiong.Bradbury.ea.2016`. Comparing with the PTB dataset used for pretraining word2vec in :numref:`sec_word2vec_data`,\nWikiText-2 (i) retains the original punctuation, making it suitable for next sentence prediction; (ii) retains the original case and numbers; (iii) is over twice larger. ```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, np, npx\nimport os\nimport random\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport os\nimport random\nimport torch\n```\n\nIn [**the WikiText-2 dataset**],\neach line represents a paragraph where\nspace is inserted between any punctuation and its preceding token. Paragraphs with at least two sentences are retained. To split sentences, we only use the period as the delimiter for simplicity. We leave discussions of more complex sentence splitting techniques in the exercises\nat the end of this section."
    },
    {
      "chunk_id": "e5a9ad7e216f_1",
      "chapter": "bert-dataset",
      "heading": "bert-dataset",
      "text": "Paragraphs with at least two sentences are retained. To split sentences, we only use the period as the delimiter for simplicity. We leave discussions of more complex sentence splitting techniques in the exercises\nat the end of this section. ```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['wikitext-2'] = (\n    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n\n#@save\ndef _read_wiki(data_dir):\n    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n    with open(file_name, 'r') as f:\n        lines = f.readlines()\n    # Uppercase letters are converted to lowercase ones\n    paragraphs = [line.strip().lower().split(' . ')\n                  for line in lines if len(line.split(' . ')) >= 2]\n    random.shuffle(paragraphs)\n    return paragraphs\n```"
    },
    {
      "chunk_id": "0d708490d638_0",
      "chapter": "bert-dataset",
      "heading": "Defining Helper Functions for Pretraining Tasks",
      "text": "In the following,\nwe begin by implementing helper functions for the two BERT pretraining tasks:\nnext sentence prediction and masked language modeling.\nThese helper functions will be invoked later\nwhen transforming the raw text corpus\ninto the dataset of the ideal format to pretrain BERT."
    },
    {
      "chunk_id": "df6b77fa303f_0",
      "chapter": "bert-dataset",
      "heading": "[**Generating the Next Sentence Prediction Task**]",
      "text": "According to descriptions of :numref:`subsec_nsp`,\nthe `_get_next_sentence` function generates a training example\nfor the binary classification task.\n\n```{.python .input}\n#@tab all\n#@save\ndef _get_next_sentence(sentence, next_sentence, paragraphs):\n    if random.random() < 0.5:\n        is_next = True\n    else:\n        # `paragraphs` is a list of lists of lists\n        next_sentence = random.choice(random.choice(paragraphs))\n        is_next = False\n    return sentence, next_sentence, is_next\n```\n\nThe following function generates training examples for next sentence prediction\nfrom the input `paragraph` by invoking the `_get_next_sentence` function.\nHere `paragraph` is a list of sentences, where each sentence is a list of tokens.\nThe argument `max_len` specifies the maximum length of a BERT input sequence during pretraining.\n\n```{.python .input}\n#@tab all\n#@save\ndef _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n    nsp_data_from_paragraph = []\n    for i in range(len(paragraph) - 1):\n        tokens_a, tokens_b, is_next = _get_next_sentence(\n            paragraph[i], paragraph[i + 1], paragraphs)\n        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n            continue\n        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n        nsp_data_from_paragraph.append((tokens, segments, is_next))\n    return nsp_data_from_paragraph\n```"
    },
    {
      "chunk_id": "767f57701c36_0",
      "chapter": "bert-dataset",
      "heading": "[**Generating the Masked Language Modeling Task**]",
      "text": ":label:`subsec_prepare_mlm_data`\n\nIn order to generate training examples\nfor the masked language modeling task\nfrom a BERT input sequence,\nwe define the following `_replace_mlm_tokens` function. In its inputs, `tokens` is a list of tokens representing a BERT input sequence,\n`candidate_pred_positions` is a list of token indices of the BERT input sequence\nexcluding those of special tokens (special tokens are not predicted in the masked language modeling task),\nand `num_mlm_preds` indicates the number of predictions (recall 15% random tokens to predict). Following the definition of the masked language modeling task in :numref:`subsec_mlm`,\nat each prediction position, the input may be replaced by\na special \u201c&lt;mask&gt;\u201d token or a random token, or remain unchanged. In the end, the function returns the input tokens after possible replacement,\nthe token indices where predictions take place and labels for these predictions."
    },
    {
      "chunk_id": "767f57701c36_1",
      "chapter": "bert-dataset",
      "heading": "[**Generating the Masked Language Modeling Task**]",
      "text": "In the end, the function returns the input tokens after possible replacement,\nthe token indices where predictions take place and labels for these predictions. ```{.python .input}\n#@tab all\n#@save\ndef _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n                        vocab):\n    # For the input of a masked language model, make a new copy of tokens and\n    # replace some of them by '<mask>' or random tokens\n    mlm_input_tokens = [token for token in tokens]\n    pred_positions_and_labels = []\n    # Shuffle for getting 15% random tokens for prediction in the masked\n    # language modeling task\n    random.shuffle(candidate_pred_positions)\n    for mlm_pred_position in candidate_pred_positions:\n        if len(pred_positions_and_labels) >= num_mlm_preds:\n            break\n        masked_token = None\n        # 80% of the time: replace the word with the '<mask>' token\n        if random.random() < 0.8:\n            masked_token = '<mask>'\n        else:\n            # 10% of the time: keep the word unchanged\n            if random.random() < 0.5:\n                masked_token = tokens[mlm_pred_position]\n            # 10% of the time: replace the word with a random word\n            else:\n                masked_token = random.choice(vocab.idx_to_token)\n        mlm_input_tokens[mlm_pred_position] = masked_token\n        pred_positions_and_labels.append(\n            (mlm_pred_position, tokens[mlm_pred_position]))\n    return mlm_input_tokens, pred_positions_and_labels\n```\n\nBy invoking the aforementioned `_replace_mlm_tokens` function,\nthe following function takes a BERT input sequence (`tokens`)\nas an input and returns indices of the input tokens\n(after possible token replacement as described in :numref:`subsec_mlm`),\nthe token indices where predictions take place,\nand label indices for these predictions."
    },
    {
      "chunk_id": "767f57701c36_2",
      "chapter": "bert-dataset",
      "heading": "[**Generating the Masked Language Modeling Task**]",
      "text": "```{.python .input}\n#@tab all\n#@save\ndef _get_mlm_data_from_tokens(tokens, vocab):\n    candidate_pred_positions = []\n    # `tokens` is a list of strings\n    for i, token in enumerate(tokens):\n        # Special tokens are not predicted in the masked language modeling\n        # task\n        if token in ['<cls>', '<sep>']:\n            continue\n        candidate_pred_positions.append(i)\n    # 15% of random tokens are predicted in the masked language modeling task\n    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n    pred_positions_and_labels = sorted(pred_positions_and_labels,\n                                       key=lambda x: x[0])\n    pred_positions = [v[0] for v in pred_positions_and_labels]\n    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n```"
    },
    {
      "chunk_id": "9ddf491e952a_0",
      "chapter": "bert-dataset",
      "heading": "Transforming Text into the Pretraining Dataset",
      "text": "Now we are almost ready to customize a `Dataset` class for pretraining BERT. Before that, \nwe still need to define a helper function `_pad_bert_inputs`\nto [**append the special \u201c&lt;pad&gt;\u201d tokens to the inputs.**]\nIts argument `examples` contain the outputs from the helper functions `_get_nsp_data_from_paragraph` and `_get_mlm_data_from_tokens` for the two pretraining tasks."
    },
    {
      "chunk_id": "9ddf491e952a_1",
      "chapter": "bert-dataset",
      "heading": "Transforming Text into the Pretraining Dataset",
      "text": "```{.python .input}\n#@tab mxnet\n#@save\ndef _pad_bert_inputs(examples, max_len, vocab):\n    max_num_mlm_preds = round(max_len * 0.15)\n    all_token_ids, all_segments, valid_lens,  = [], [], []\n    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n    nsp_labels = []\n    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n         is_next) in examples:\n        all_token_ids.append(np.array(token_ids + [vocab['<pad>']] * (\n            max_len - len(token_ids)), dtype='int32'))\n        all_segments.append(np.array(segments + [0] * (\n            max_len - len(segments)), dtype='int32'))\n        # `valid_lens` excludes count of '<pad>' tokens\n        valid_lens.append(np.array(len(token_ids), dtype='float32'))\n        all_pred_positions.append(np.array(pred_positions + [0] * (\n            max_num_mlm_preds - len(pred_positions)), dtype='int32'))\n        # Predictions of padded tokens will be filtered out in the loss via\n        # multiplication of 0 weights\n        all_mlm_weights.append(\n            np.array([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n                max_num_mlm_preds - len(pred_positions)), dtype='float32'))\n        all_mlm_labels.append(np.array(mlm_pred_label_ids + [0] * (\n            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype='int32'))\n        nsp_labels.append(np.array(is_next))\n    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n            all_mlm_weights, all_mlm_labels, nsp_labels)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef _pad_bert_inputs(examples, max_len, vocab):\n    max_num_mlm_preds = round(max_len * 0.15)\n    all_token_ids, all_segments, valid_lens,  = [], [], []\n    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n    nsp_labels = []\n    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n         is_next) in examples:\n        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n            max_len - len(token_ids)), dtype=torch.long))\n        all_segments.append(torch.tensor(segments + [0] * (\n            max_len - len(segments)), dtype=torch.long))\n        # `valid_lens` excludes count of '<pad>' tokens\n        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n        # Predictions of padded tokens will be filtered out in the loss via\n        # multiplication of 0 weights\n        all_mlm_weights.append(\n            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n                max_num_mlm_preds - len(pred_positions)),\n                dtype=torch.float32))\n        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n            all_mlm_weights, all_mlm_labels, nsp_labels)\n```\n\nPutting the helper functions for\ngenerating training examples of the two pretraining tasks,\nand the helper function for padding inputs together,\nwe customize the following `_WikiTextDataset` class as [**the WikiText-2 dataset for pretraining BERT**]."
    },
    {
      "chunk_id": "9ddf491e952a_2",
      "chapter": "bert-dataset",
      "heading": "Transforming Text into the Pretraining Dataset",
      "text": "By implementing the `__getitem__ `function,\nwe can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples \ngenerated from a pair of sentences from the WikiText-2 corpus. The original BERT model uses WordPiece embeddings whose vocabulary size is 30000 :cite:`Wu.Schuster.Chen.ea.2016`. The tokenization method of WordPiece is a slight modification of\nthe original byte pair encoding algorithm in :numref:`subsec_Byte_Pair_Encoding`. For simplicity, we use the `d2l.tokenize` function for tokenization. Infrequent tokens that appear less than five times are filtered out."
    },
    {
      "chunk_id": "9ddf491e952a_3",
      "chapter": "bert-dataset",
      "heading": "Transforming Text into the Pretraining Dataset",
      "text": "For simplicity, we use the `d2l.tokenize` function for tokenization. Infrequent tokens that appear less than five times are filtered out. ```{.python .input}\n#@tab mxnet\n#@save\nclass _WikiTextDataset(gluon.data.Dataset):\n    def __init__(self, paragraphs, max_len):\n        # Input `paragraphs[i]` is a list of sentence strings representing a\n        # paragraph; while output `paragraphs[i]` is a list of sentences\n        # representing a paragraph, where each sentence is a list of tokens\n        paragraphs = [d2l.tokenize(\n            paragraph, token='word') for paragraph in paragraphs]\n        sentences = [sentence for paragraph in paragraphs\n                     for sentence in paragraph]\n        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n            '<pad>', '<mask>', '<cls>', '<sep>'])\n        # Get data for the next sentence prediction task\n        examples = []\n        for paragraph in paragraphs:\n            examples.extend(_get_nsp_data_from_paragraph(\n                paragraph, paragraphs, self.vocab, max_len))\n        # Get data for the masked language model task\n        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n                      + (segments, is_next))\n                     for tokens, segments, is_next in examples]\n        # Pad inputs\n        (self.all_token_ids, self.all_segments, self.valid_lens,\n         self.all_pred_positions, self.all_mlm_weights,\n         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n            examples, max_len, self.vocab)\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx], self.all_pred_positions[idx],\n                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n                self.nsp_labels[idx])\n\n    def __len__(self):\n        return len(self.all_token_ids)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass _WikiTextDataset(torch.utils.data.Dataset):\n    def __init__(self, paragraphs, max_len):\n        # Input `paragraphs[i]` is a list of sentence strings representing a\n        # paragraph; while output `paragraphs[i]` is a list of sentences\n        # representing a paragraph, where each sentence is a list of tokens\n        paragraphs = [d2l.tokenize(\n            paragraph, token='word') for paragraph in paragraphs]\n        sentences = [sentence for paragraph in paragraphs\n                     for sentence in paragraph]\n        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n            '<pad>', '<mask>', '<cls>', '<sep>'])\n        # Get data for the next sentence prediction task\n        examples = []\n        for paragraph in paragraphs:\n            examples.extend(_get_nsp_data_from_paragraph(\n                paragraph, paragraphs, self.vocab, max_len))\n        # Get data for the masked language model task\n        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n                      + (segments, is_next))\n                     for tokens, segments, is_next in examples]\n        # Pad inputs\n        (self.all_token_ids, self.all_segments, self.valid_lens,\n         self.all_pred_positions, self.all_mlm_weights,\n         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n            examples, max_len, self.vocab)\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx], self.all_pred_positions[idx],\n                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n                self.nsp_labels[idx])\n\n    def __len__(self):\n        return len(self.all_token_ids)\n```\n\nBy using the `_read_wiki` function and the `_WikiTextDataset` class,\nwe define the following `load_data_wiki` to [**download and WikiText-2 dataset\nand generate pretraining examples**] from it."
    },
    {
      "chunk_id": "9ddf491e952a_4",
      "chapter": "bert-dataset",
      "heading": "Transforming Text into the Pretraining Dataset",
      "text": "```{.python .input}\n#@tab mxnet\n#@save\ndef load_data_wiki(batch_size, max_len):\n    \"\"\"Load the WikiText-2 dataset.\"\"\"\n    num_workers = d2l.get_dataloader_workers()\n    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n    paragraphs = _read_wiki(data_dir)\n    train_set = _WikiTextDataset(paragraphs, max_len)\n    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,\n                                       num_workers=num_workers)\n    return train_iter, train_set.vocab\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef load_data_wiki(batch_size, max_len):\n    \"\"\"Load the WikiText-2 dataset.\"\"\"\n    num_workers = d2l.get_dataloader_workers()\n    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n    paragraphs = _read_wiki(data_dir)\n    train_set = _WikiTextDataset(paragraphs, max_len)\n    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n                                        shuffle=True, num_workers=num_workers)\n    return train_iter, train_set.vocab\n```\n\nSetting the batch size to 512 and the maximum length of a BERT input sequence to be 64,\nwe [**print out the shapes of a minibatch of BERT pretraining examples**]. Note that in each BERT input sequence,\n$10$ ($64 \\times 0.15$) positions are predicted for the masked language modeling task. ```{.python .input}\n#@tab all\nbatch_size, max_len = 512, 64\ntrain_iter, vocab = load_data_wiki(batch_size, max_len)\n\nfor (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n     mlm_Y, nsp_y) in train_iter:\n    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n          nsp_y.shape)\n    break\n```\n\nIn the end, let's take a look at the vocabulary size. Even after filtering out infrequent tokens,\nit is still over twice larger than that of the PTB dataset. ```{.python .input}\n#@tab all\nlen(vocab)\n```"
    },
    {
      "chunk_id": "62e4d5c297d0_0",
      "chapter": "bert-dataset",
      "heading": "Summary",
      "text": "* Comparing with the PTB dataset, the WikiText-2 dateset retains the original punctuation, case and numbers, and is over twice larger.\n* We can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples generated from a pair of sentences from the WikiText-2 corpus."
    },
    {
      "chunk_id": "f7806c643140_0",
      "chapter": "bert-dataset",
      "heading": "Exercises",
      "text": "1. For simplicity, the period is used as the only delimiter for splitting sentences. Try other sentence splitting techniques, such as the spaCy and NLTK. Take NLTK as an example. You need to install NLTK first: `pip install nltk`. In the code, first `import nltk`. Then, download the Punkt sentence tokenizer: `nltk.download('punkt')`. To split sentences such as `sentences = 'This is great ! Why not ?'`, invoking `nltk.tokenize.sent_tokenize(sentences)` will return a list of two sentence strings: `['This is great !', 'Why not ?']`.\n1. What is the vocabulary size if we do not filter out any infrequent token?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/389)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1496)\n:end_tab:"
    },
    {
      "chunk_id": "368b1ba8a134_0",
      "chapter": "bert-pretraining",
      "heading": "bert-pretraining",
      "text": "# Pretraining BERT\n:label:`sec_bert-pretraining`\n\nWith the BERT model implemented in :numref:`sec_bert`\nand the pretraining examples generated from the WikiText-2 dataset in :numref:`sec_bert-dataset`, we will pretrain BERT on the WikiText-2 dataset in this section.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\nTo start, we load the WikiText-2 dataset as minibatches\nof pretraining examples for masked language modeling and next sentence prediction.\nThe batch size is 512 and the maximum length of a BERT input sequence is 64.\nNote that in the original BERT model, the maximum length is 512.\n\n```{.python .input}\n#@tab all\nbatch_size, max_len = 512, 64\ntrain_iter, vocab = d2l.load_data_wiki(batch_size, max_len)\n```"
    },
    {
      "chunk_id": "f62486f71551_0",
      "chapter": "bert-pretraining",
      "heading": "Pretraining BERT",
      "text": "The original BERT has two versions of different model sizes :cite:`Devlin.Chang.Lee.ea.2018`. The base model ($\\textrm{BERT}_{\\textrm{BASE}}$) uses 12 layers (Transformer encoder blocks)\nwith 768 hidden units (hidden size) and 12 self-attention heads. The large model ($\\textrm{BERT}_{\\textrm{LARGE}}$) uses 24 layers\nwith 1024 hidden units and 16 self-attention heads. Notably, the former has 110 million parameters while the latter has 340 million parameters. For demonstration with ease,\nwe define [**a small BERT, using 2 layers, 128 hidden units, and 2 self-attention heads**]. ```{.python .input}\n#@tab mxnet\nnet = d2l.BERTModel(len(vocab), num_hiddens=128, ffn_num_hiddens=256,\n                    num_heads=2, num_blks=2, dropout=0.2)\ndevices = d2l.try_all_gpus()\nnet.initialize(init.Xavier(), ctx=devices)\nloss = gluon.loss.SoftmaxCELoss()\n```\n\n```{.python .input}\n#@tab pytorch\nnet = d2l.BERTModel(len(vocab), num_hiddens=128, \n                    ffn_num_hiddens=256, num_heads=2, num_blks=2, dropout=0.2)\ndevices = d2l.try_all_gpus()\nloss = nn.CrossEntropyLoss()\n```\n\nBefore defining the training loop,\nwe define a helper function `_get_batch_loss_bert`. Given the shard of training examples,\nthis function [**computes the loss for both the masked language modeling and next sentence prediction tasks**]. Note that the final loss of BERT pretraining\nis just the sum of both the masked language modeling loss\nand the next sentence prediction loss."
    },
    {
      "chunk_id": "f62486f71551_1",
      "chapter": "bert-pretraining",
      "heading": "Pretraining BERT",
      "text": "Note that the final loss of BERT pretraining\nis just the sum of both the masked language modeling loss\nand the next sentence prediction loss. ```{.python .input}\n#@tab mxnet\n#@save\ndef _get_batch_loss_bert(net, loss, vocab_size, tokens_X_shards,\n                         segments_X_shards, valid_lens_x_shards,\n                         pred_positions_X_shards, mlm_weights_X_shards,\n                         mlm_Y_shards, nsp_y_shards):\n    mlm_ls, nsp_ls, ls = [], [], []\n    for (tokens_X_shard, segments_X_shard, valid_lens_x_shard,\n         pred_positions_X_shard, mlm_weights_X_shard, mlm_Y_shard,\n         nsp_y_shard) in zip(\n        tokens_X_shards, segments_X_shards, valid_lens_x_shards,\n        pred_positions_X_shards, mlm_weights_X_shards, mlm_Y_shards,\n        nsp_y_shards):\n        # Forward pass\n        _, mlm_Y_hat, nsp_Y_hat = net(\n            tokens_X_shard, segments_X_shard, valid_lens_x_shard.reshape(-1),\n            pred_positions_X_shard)\n        # Compute masked language model loss\n        mlm_l = loss(\n            mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y_shard.reshape(-1),\n            mlm_weights_X_shard.reshape((-1, 1)))\n        mlm_l = mlm_l.sum() / (mlm_weights_X_shard.sum() + 1e-8)\n        # Compute next sentence prediction loss\n        nsp_l = loss(nsp_Y_hat, nsp_y_shard)\n        nsp_l = nsp_l.mean()\n        mlm_ls.append(mlm_l)\n        nsp_ls.append(nsp_l)\n        ls.append(mlm_l + nsp_l)\n        npx.waitall()\n    return mlm_ls, nsp_ls, ls\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n                         segments_X, valid_lens_x,\n                         pred_positions_X, mlm_weights_X,\n                         mlm_Y, nsp_y):\n    # Forward pass\n    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n                                  valid_lens_x.reshape(-1),\n                                  pred_positions_X)\n    # Compute masked language model loss\n    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n    mlm_weights_X.reshape(-1, 1)\n    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n    # Compute next sentence prediction loss\n    nsp_l = loss(nsp_Y_hat, nsp_y)\n    l = mlm_l + nsp_l\n    return mlm_l, nsp_l, l\n```\n\nInvoking the two aforementioned helper functions,\nthe following `train_bert` function\ndefines the procedure to [**pretrain BERT (`net`) on the WikiText-2 (`train_iter`) dataset**]."
    },
    {
      "chunk_id": "f62486f71551_2",
      "chapter": "bert-pretraining",
      "heading": "Pretraining BERT",
      "text": "Training BERT can take very long. Instead of specifying the number of epochs for training\nas in the `train_ch13` function (see :numref:`sec_image_augmentation`),\nthe input `num_steps` of the following function\nspecifies the number of iteration steps for training. ```{.python .input}\n#@tab mxnet\ndef train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n    trainer = gluon.Trainer(net.collect_params(), 'adam',\n                            {'learning_rate': 0.01})\n    step, timer = 0, d2l.Timer()\n    animator = d2l.Animator(xlabel='step', ylabel='loss',\n                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n    # Sum of masked language modeling losses, sum of next sentence prediction\n    # losses, no."
    },
    {
      "chunk_id": "f62486f71551_3",
      "chapter": "bert-pretraining",
      "heading": "Pretraining BERT",
      "text": "of sentence pairs, count\n    metric = d2l.Accumulator(4)\n    num_steps_reached = False\n    while step < num_steps and not num_steps_reached:\n        for batch in train_iter:\n            (tokens_X_shards, segments_X_shards, valid_lens_x_shards,\n             pred_positions_X_shards, mlm_weights_X_shards,\n             mlm_Y_shards, nsp_y_shards) = [gluon.utils.split_and_load(\n                elem, devices, even_split=False) for elem in batch]\n            timer.start()\n            with autograd.record():\n                mlm_ls, nsp_ls, ls = _get_batch_loss_bert(\n                    net, loss, vocab_size, tokens_X_shards, segments_X_shards,\n                    valid_lens_x_shards, pred_positions_X_shards,\n                    mlm_weights_X_shards, mlm_Y_shards, nsp_y_shards)\n            for l in ls:\n                l.backward()\n            trainer.step(1)\n            mlm_l_mean = sum([float(l) for l in mlm_ls]) / len(mlm_ls)\n            nsp_l_mean = sum([float(l) for l in nsp_ls]) / len(nsp_ls)\n            metric.add(mlm_l_mean, nsp_l_mean, batch[0].shape[0], 1)\n            timer.stop()\n            animator.add(step + 1,\n                         (metric[0] / metric[3], metric[1] / metric[3]))\n            step += 1\n            if step == num_steps:\n                num_steps_reached = True\n                break\n\n    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n          f'NSP loss {metric[1] / metric[3]:.3f}')\n    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n          f'{str(devices)}')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n    net(*next(iter(train_iter))[:4])\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n    step, timer = 0, d2l.Timer()\n    animator = d2l.Animator(xlabel='step', ylabel='loss',\n                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n    # Sum of masked language modeling losses, sum of next sentence prediction\n    # losses, no."
    },
    {
      "chunk_id": "f62486f71551_4",
      "chapter": "bert-pretraining",
      "heading": "Pretraining BERT",
      "text": "of sentence pairs, count\n    metric = d2l.Accumulator(4)\n    num_steps_reached = False\n    while step < num_steps and not num_steps_reached:\n        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n            tokens_X = tokens_X.to(devices[0])\n            segments_X = segments_X.to(devices[0])\n            valid_lens_x = valid_lens_x.to(devices[0])\n            pred_positions_X = pred_positions_X.to(devices[0])\n            mlm_weights_X = mlm_weights_X.to(devices[0])\n            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n            trainer.zero_grad()\n            timer.start()\n            mlm_l, nsp_l, l = _get_batch_loss_bert(\n                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n            l.backward()\n            trainer.step()\n            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n            timer.stop()\n            animator.add(step + 1,\n                         (metric[0] / metric[3], metric[1] / metric[3]))\n            step += 1\n            if step == num_steps:\n                num_steps_reached = True\n                break\n\n    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n          f'NSP loss {metric[1] / metric[3]:.3f}')\n    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n          f'{str(devices)}')\n```\n\nWe can plot both the masked language modeling loss and the next sentence prediction loss\nduring BERT pretraining. ```{.python .input}\n#@tab all\ntrain_bert(train_iter, net, loss, len(vocab), devices, 50)\n```"
    },
    {
      "chunk_id": "b5c8694edf02_0",
      "chapter": "bert-pretraining",
      "heading": "[**Representing Text with BERT**]",
      "text": "After pretraining BERT,\nwe can use it to represent single text, text pairs, or any token in them. The following function returns the BERT (`net`) representations for all tokens\nin `tokens_a` and `tokens_b`. ```{.python .input}\n#@tab mxnet\ndef get_bert_encoding(net, tokens_a, tokens_b=None):\n    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n    token_ids = np.expand_dims(np.array(vocab[tokens], ctx=devices[0]),\n                               axis=0)\n    segments = np.expand_dims(np.array(segments, ctx=devices[0]), axis=0)\n    valid_len = np.expand_dims(np.array(len(tokens), ctx=devices[0]), axis=0)\n    encoded_X, _, _ = net(token_ids, segments, valid_len)\n    return encoded_X\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_bert_encoding(net, tokens_a, tokens_b=None):\n    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n    encoded_X, _, _ = net(token_ids, segments, valid_len)\n    return encoded_X\n```\n\n[**Consider the sentence \"a crane is flying\".**]\nRecall the input representation of BERT as discussed in :numref:`subsec_bert_input_rep`. After inserting special tokens \u201c&lt;cls&gt;\u201d (used for classification)\nand \u201c&lt;sep&gt;\u201d (used for separation),\nthe BERT input sequence has a length of six. Since zero is the index of the \u201c&lt;cls&gt;\u201d token,\n`encoded_text[:, 0, :]` is the BERT representation of the entire input sentence. To evaluate the polysemy token \"crane\",\nwe also print out the first three elements of the BERT representation of the token."
    },
    {
      "chunk_id": "b5c8694edf02_1",
      "chapter": "bert-pretraining",
      "heading": "[**Representing Text with BERT**]",
      "text": "Since zero is the index of the \u201c&lt;cls&gt;\u201d token,\n`encoded_text[:, 0, :]` is the BERT representation of the entire input sentence. To evaluate the polysemy token \"crane\",\nwe also print out the first three elements of the BERT representation of the token. ```{.python .input}\n#@tab all\ntokens_a = ['a', 'crane', 'is', 'flying']\nencoded_text = get_bert_encoding(net, tokens_a)\n# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\nencoded_text_cls = encoded_text[:, 0, :]\nencoded_text_crane = encoded_text[:, 2, :]\nencoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]\n```\n\n[**Now consider a sentence pair\n\"a crane driver came\" and \"he just left\".**]\nSimilarly, `encoded_pair[:, 0, :]` is the encoded result of the entire sentence pair from the pretrained BERT. Note that the first three elements of the polysemy token \"crane\" are different from those when the context is different. This supports that BERT representations are context-sensitive. ```{.python .input}\n#@tab all\ntokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\nencoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n# Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',\n# 'left', '<sep>'\nencoded_pair_cls = encoded_pair[:, 0, :]\nencoded_pair_crane = encoded_pair[:, 2, :]\nencoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]\n```\n\nIn :numref:`chap_nlp_app`, we will fine-tune a pretrained BERT model\nfor downstream natural language processing applications."
    },
    {
      "chunk_id": "50ce57f8f9ae_0",
      "chapter": "bert-pretraining",
      "heading": "Summary",
      "text": "* The original BERT has two versions, where the base model has 110 million parameters and the large model has 340 million parameters.\n* After pretraining BERT, we can use it to represent single text, text pairs, or any token in them.\n* In the experiment, the same token has different BERT representation when their contexts are different. This supports that BERT representations are context-sensitive."
    },
    {
      "chunk_id": "b26a1b4acaa7_0",
      "chapter": "bert-pretraining",
      "heading": "Exercises",
      "text": "1. In the experiment, we can see that the masked language modeling loss is significantly higher than the next sentence prediction loss. Why?\n2. Set the maximum length of a BERT input sequence to be 512 (same as the original BERT model). Use the configurations of the original BERT model such as $\\textrm{BERT}_{\\textrm{LARGE}}$. Do you encounter any error when running this section? Why?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/390)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1497)\n:end_tab:"
    },
    {
      "chunk_id": "701a72e6973b_0",
      "chapter": "bert",
      "heading": "bert",
      "text": "# Bidirectional Encoder Representations from Transformers (BERT)\n:label:`sec_bert`\n\nWe have introduced several word embedding models for natural language understanding.\nAfter pretraining, the output can be thought of as a matrix\nwhere each row is a vector that represents a word of a predefined vocabulary.\nIn fact, these word embedding models are all *context-independent*.\nLet's begin by illustrating this property."
    },
    {
      "chunk_id": "c83b24ff3972_0",
      "chapter": "bert",
      "heading": "From Context-Independent to Context-Sensitive",
      "text": "Recall the experiments in :numref:`sec_word2vec_pretraining` and :numref:`sec_synonyms`. For instance, word2vec and GloVe both assign the same pretrained vector to the same word regardless of the context of the word (if any). Formally, a context-independent representation of any token $x$\nis a function $f(x)$ that only takes $x$ as its input. Given the abundance of polysemy and complex semantics in natural languages,\ncontext-independent representations have obvious limitations. For instance, the word \"crane\" in contexts\n\"a crane is flying\" and \"a crane driver came\" has completely different meanings;\nthus, the same word may be assigned different representations depending on contexts. This motivates the development of *context-sensitive* word representations,\nwhere representations of words depend on their contexts. Hence, a context-sensitive representation of token $x$ is a function $f(x, c(x))$\ndepending on both $x$ and its context $c(x)$. Popular context-sensitive representations\ninclude TagLM (language-model-augmented sequence tagger) :cite:`Peters.Ammar.Bhagavatula.ea.2017`,\nCoVe (Context Vectors) :cite:`McCann.Bradbury.Xiong.ea.2017`,\nand ELMo (Embeddings from Language Models) :cite:`Peters.Neumann.Iyyer.ea.2018`. For example, by taking the entire sequence as input,\nELMo is a function that assigns a representation to each word from the input sequence. Specifically, ELMo combines all the intermediate layer representations from pretrained bidirectional LSTM as the output representation. Then the ELMo representation will be added to a downstream task's existing supervised model\nas additional features, such as by concatenating ELMo representation and the original representation (e.g., GloVe) of tokens in the existing model. On the one hand,\nall the weights in the pretrained bidirectional LSTM model are frozen after ELMo representations are added. On the other hand,\nthe existing supervised model is specifically customized for a given task."
    },
    {
      "chunk_id": "c83b24ff3972_1",
      "chapter": "bert",
      "heading": "From Context-Independent to Context-Sensitive",
      "text": "On the one hand,\nall the weights in the pretrained bidirectional LSTM model are frozen after ELMo representations are added. On the other hand,\nthe existing supervised model is specifically customized for a given task. Leveraging different best models for different tasks at that time,\nadding ELMo improved the state of the art across six natural language processing tasks:\nsentiment analysis, natural language inference,\nsemantic role labeling, coreference resolution,\nnamed entity recognition, and question answering."
    },
    {
      "chunk_id": "f24bfe4d2644_0",
      "chapter": "bert",
      "heading": "From Task-Specific to Task-Agnostic",
      "text": "Although ELMo has significantly improved solutions to a diverse set of natural language processing tasks,\neach solution still hinges on a *task-specific* architecture.\nHowever, it is practically non-trivial to craft a specific architecture for every natural language processing task.\nThe GPT (Generative Pre-Training) model represents an effort in designing\na general *task-agnostic* model for context-sensitive representations :cite:`Radford.Narasimhan.Salimans.ea.2018`.\nBuilt on a Transformer decoder,\nGPT pretrains a language model that will be used to represent text sequences.\nWhen applying GPT to a downstream task,\nthe output of the language model will be fed into an added linear output layer\nto predict the label of the task.\nIn sharp contrast to ELMo that freezes parameters of the pretrained model,\nGPT fine-tunes *all* the parameters in the pretrained Transformer decoder\nduring supervised learning of the downstream task.\nGPT was evaluated on twelve tasks of natural language inference,\nquestion answering, sentence similarity, and classification,\nand improved the state of the art in nine of them with minimal changes\nto the model architecture.\n\nHowever, due to the autoregressive nature of language models,\nGPT only looks forward (left-to-right).\nIn contexts \"i went to the bank to deposit cash\" and \"i went to the bank to sit down\",\nas \"bank\" is sensitive to the context to its left,\nGPT will return the same representation for \"bank\",\nthough it has different meanings."
    },
    {
      "chunk_id": "b8b0a7c9def6_0",
      "chapter": "bert",
      "heading": "BERT: Combining the Best of Both Worlds",
      "text": "As we have seen,\nELMo encodes context bidirectionally but uses task-specific architectures;\nwhile GPT is task-agnostic but encodes context left-to-right. Combining the best of both worlds,\nBERT (Bidirectional Encoder Representations from Transformers)\nencodes context bidirectionally and requires minimal architecture changes\nfor a wide range of natural language processing tasks :cite:`Devlin.Chang.Lee.ea.2018`. Using a pretrained Transformer encoder,\nBERT is able to represent any token based on its bidirectional context. During supervised learning of downstream tasks,\nBERT is similar to GPT in two aspects. First, BERT representations will be fed into an added output layer,\nwith minimal changes to the model architecture depending on nature of tasks,\nsuch as predicting for every token vs. predicting for the entire sequence. Second,\nall the parameters of the pretrained Transformer encoder are fine-tuned,\nwhile the additional output layer will be trained from scratch. :numref:`fig_elmo-gpt-bert` depicts the differences among ELMo, GPT, and BERT. ![A comparison of ELMo, GPT, and BERT.](../img/elmo-gpt-bert.svg)\n:label:`fig_elmo-gpt-bert`\n\n\nBERT further improved the state of the art on eleven natural language processing tasks\nunder broad categories of (i) single text classification (e.g., sentiment analysis), (ii) text pair classification (e.g., natural language inference),\n(iii) question answering, (iv) text tagging (e.g., named entity recognition). All proposed in 2018,\nfrom context-sensitive ELMo to task-agnostic GPT and BERT,\nconceptually simple yet empirically powerful pretraining of deep representations for natural languages have revolutionized solutions to various natural language processing tasks. In the rest of this chapter,\nwe will dive into the pretraining of BERT. When natural language processing applications are explained in :numref:`chap_nlp_app`,\nwe will illustrate fine-tuning of BERT for downstream applications."
    },
    {
      "chunk_id": "b8b0a7c9def6_1",
      "chapter": "bert",
      "heading": "BERT: Combining the Best of Both Worlds",
      "text": "In the rest of this chapter,\nwe will dive into the pretraining of BERT. When natural language processing applications are explained in :numref:`chap_nlp_app`,\nwe will illustrate fine-tuning of BERT for downstream applications. ```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, np, npx\nfrom mxnet.gluon import nn\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```"
    },
    {
      "chunk_id": "4cb67b7ef279_0",
      "chapter": "bert",
      "heading": "[**Input Representation**]",
      "text": ":label:`subsec_bert_input_rep`\n\nIn natural language processing,\nsome tasks (e.g., sentiment analysis) take single text as input,\nwhile in some other tasks (e.g., natural language inference),\nthe input is a pair of text sequences. The BERT input sequence unambiguously represents both single text and text pairs. In the former,\nthe BERT input sequence is the concatenation of\nthe special classification token \u201c&lt;cls&gt;\u201d,\ntokens of a text sequence,\nand the special separation token \u201c&lt;sep&gt;\u201d. In the latter,\nthe BERT input sequence is the concatenation of\n\u201c&lt;cls&gt;\u201d, tokens of the first text sequence,\n\u201c&lt;sep&gt;\u201d, tokens of the second text sequence, and \u201c&lt;sep&gt;\u201d. We will consistently distinguish the terminology \"BERT input sequence\"\nfrom other types of \"sequences\". For instance, one *BERT input sequence* may include either one *text sequence* or two *text sequences*. To distinguish text pairs,\nthe learned segment embeddings $\\mathbf{e}_A$ and $\\mathbf{e}_B$\nare added to the token embeddings of the first sequence and the second sequence, respectively. For single text inputs, only $\\mathbf{e}_A$ is used. The following `get_tokens_and_segments` takes either one sentence or two sentences\nas input, then returns tokens of the BERT input sequence\nand their corresponding segment IDs. ```{.python .input}\n#@tab all\n#@save\ndef get_tokens_and_segments(tokens_a, tokens_b=None):\n    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n    tokens = ['<cls>'] + tokens_a + ['<sep>']\n    # 0 and 1 are marking segment A and B, respectively\n    segments = [0] * (len(tokens_a) + 2)\n    if tokens_b is not None:\n        tokens += tokens_b + ['<sep>']\n        segments += [1] * (len(tokens_b) + 1)\n    return tokens, segments\n```\n\nBERT chooses the Transformer encoder as its bidirectional architecture. Common in the Transformer encoder,\npositional embeddings are added at every position of the BERT input sequence. However, different from the original Transformer encoder,\nBERT uses *learnable* positional embeddings."
    },
    {
      "chunk_id": "4cb67b7ef279_1",
      "chapter": "bert",
      "heading": "[**Input Representation**]",
      "text": "Common in the Transformer encoder,\npositional embeddings are added at every position of the BERT input sequence. However, different from the original Transformer encoder,\nBERT uses *learnable* positional embeddings. To sum up, :numref:`fig_bert-input` shows that\nthe embeddings of the BERT input sequence are the sum\nof the token embeddings, segment embeddings, and positional embeddings. ![The embeddings of the BERT input sequence are the sum\nof the token embeddings, segment embeddings, and positional embeddings.](../img/bert-input.svg)\n:label:`fig_bert-input`\n\nThe following [**`BERTEncoder` class**] is similar to the `TransformerEncoder` class\nas implemented in :numref:`sec_transformer`. Different from `TransformerEncoder`, `BERTEncoder` uses\nsegment embeddings and learnable positional embeddings."
    },
    {
      "chunk_id": "4cb67b7ef279_2",
      "chapter": "bert",
      "heading": "[**Input Representation**]",
      "text": "Different from `TransformerEncoder`, `BERTEncoder` uses\nsegment embeddings and learnable positional embeddings. ```{.python .input}\n#@tab mxnet\n#@save\nclass BERTEncoder(nn.Block):\n    \"\"\"BERT encoder.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                 num_blks, dropout, max_len=1000, **kwargs):\n        super(BERTEncoder, self).__init__(**kwargs)\n        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.segment_embedding = nn.Embedding(2, num_hiddens)\n        self.blks = nn.Sequential()\n        for _ in range(num_blks):\n            self.blks.add(d2l.TransformerEncoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n        # In BERT, positional embeddings are learnable, thus we create a\n        # parameter of positional embeddings that are long enough\n        self.pos_embedding = self.params.get('pos_embedding',\n                                             shape=(1, max_len, num_hiddens))\n\n    def forward(self, tokens, segments, valid_lens):\n        # Shape of `X` remains unchanged in the following code snippet:\n        # (batch size, max sequence length, `num_hiddens`)\n        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n        X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[1], :]\n        for blk in self.blks:\n            X = blk(X, valid_lens)\n        return X\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass BERTEncoder(nn.Module):\n    \"\"\"BERT encoder.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                 num_blks, dropout, max_len=1000, **kwargs):\n        super(BERTEncoder, self).__init__(**kwargs)\n        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.segment_embedding = nn.Embedding(2, num_hiddens)\n        self.blks = nn.Sequential()\n        for i in range(num_blks):\n            self.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n        # In BERT, positional embeddings are learnable, thus we create a\n        # parameter of positional embeddings that are long enough\n        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n                                                      num_hiddens))\n\n    def forward(self, tokens, segments, valid_lens):\n        # Shape of `X` remains unchanged in the following code snippet:\n        # (batch size, max sequence length, `num_hiddens`)\n        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n        X = X + self.pos_embedding[:, :X.shape[1], :]\n        for blk in self.blks:\n            X = blk(X, valid_lens)\n        return X\n```\n\nSuppose that the vocabulary size is 10000."
    },
    {
      "chunk_id": "4cb67b7ef279_3",
      "chapter": "bert",
      "heading": "[**Input Representation**]",
      "text": "To demonstrate forward [**inference of `BERTEncoder`**],\nlet's create an instance of it and initialize its parameters. ```{.python .input}\n#@tab mxnet\nvocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\nnum_blks, dropout = 2, 0.2\nencoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                      num_blks, dropout)\nencoder.initialize()\n```\n\n```{.python .input}\n#@tab pytorch\nvocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\nffn_num_input, num_blks, dropout = 768, 2, 0.2\nencoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                      num_blks, dropout)\n```\n\nWe define `tokens` to be 2 BERT input sequences of length 8,\nwhere each token is an index of the vocabulary. The forward inference of `BERTEncoder` with the input `tokens`\nreturns the encoded result where each token is represented by a vector\nwhose length is predefined by the hyperparameter `num_hiddens`. This hyperparameter is usually referred to as the *hidden size*\n(number of hidden units) of the Transformer encoder. ```{.python .input}\n#@tab mxnet\ntokens = np.random.randint(0, vocab_size, (2, 8))\nsegments = np.array([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\nencoded_X = encoder(tokens, segments, None)\nencoded_X.shape\n```\n\n```{.python .input}\n#@tab pytorch\ntokens = torch.randint(0, vocab_size, (2, 8))\nsegments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\nencoded_X = encoder(tokens, segments, None)\nencoded_X.shape\n```"
    },
    {
      "chunk_id": "4ff5a6c65730_0",
      "chapter": "bert",
      "heading": "Pretraining Tasks",
      "text": ":label:`subsec_bert_pretraining_tasks`\n\nThe forward inference of `BERTEncoder` gives the BERT representation\nof each token of the input text and the inserted\nspecial tokens \u201c&lt;cls&gt;\u201d and \u201c&lt;seq&gt;\u201d.\nNext, we will use these representations to compute the loss function\nfor pretraining BERT.\nThe pretraining is composed of the following two tasks:\nmasked language modeling and next sentence prediction."
    },
    {
      "chunk_id": "f34eed86585b_0",
      "chapter": "bert",
      "heading": "[**Masked Language Modeling**]",
      "text": ":label:`subsec_mlm`\n\nAs illustrated in :numref:`sec_language-model`,\na language model predicts a token using the context on its left. To encode context bidirectionally for representing each token,\nBERT randomly masks tokens and uses tokens from the bidirectional context to\npredict the masked tokens in a self-supervised fashion. This task is referred to as a *masked language model*. In this pretraining task,\n15% of tokens will be selected at random as the masked tokens for prediction. To predict a masked token without cheating by using the label,\none straightforward approach is to always replace it with a special \u201c&lt;mask&gt;\u201d token in the BERT input sequence. However, the artificial special token \u201c&lt;mask&gt;\u201d will never appear\nin fine-tuning. To avoid such a mismatch between pretraining and fine-tuning,\nif a token is masked for prediction (e.g., \"great\" is selected to be masked and predicted in \"this movie is great\"),\nin the input it will be replaced with:\n\n* a special \u201c&lt;mask&gt;\u201d token for 80% of the time (e.g., \"this movie is great\" becomes \"this movie is &lt;mask&gt;\");\n* a random token for 10% of the time (e.g., \"this movie is great\" becomes \"this movie is drink\");\n* the unchanged label token for 10% of the time (e.g., \"this movie is great\" becomes \"this movie is great\"). Note that for 10% of 15% time a random token is inserted. This occasional noise encourages BERT to be less biased towards the masked token (especially when the label token remains unchanged) in its bidirectional context encoding. We implement the following `MaskLM` class to predict masked tokens\nin the masked language model task of BERT pretraining. The prediction uses a one-hidden-layer MLP (`self.mlp`). In forward inference, it takes two inputs:\nthe encoded result of `BERTEncoder` and the token positions for prediction. The output is the prediction results at these positions."
    },
    {
      "chunk_id": "f34eed86585b_1",
      "chapter": "bert",
      "heading": "[**Masked Language Modeling**]",
      "text": "The prediction uses a one-hidden-layer MLP (`self.mlp`). In forward inference, it takes two inputs:\nthe encoded result of `BERTEncoder` and the token positions for prediction. The output is the prediction results at these positions. ```{.python .input}\n#@tab mxnet\n#@save\nclass MaskLM(nn.Block):\n    \"\"\"The masked language model task of BERT.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, **kwargs):\n        super(MaskLM, self).__init__(**kwargs)\n        self.mlp = nn.Sequential()\n        self.mlp.add(\n            nn.Dense(num_hiddens, flatten=False, activation='relu'))\n        self.mlp.add(nn.LayerNorm())\n        self.mlp.add(nn.Dense(vocab_size, flatten=False))\n\n    def forward(self, X, pred_positions):\n        num_pred_positions = pred_positions.shape[1]\n        pred_positions = pred_positions.reshape(-1)\n        batch_size = X.shape[0]\n        batch_idx = np.arange(0, batch_size)\n        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n        # `batch_idx` is `np.array([0, 0, 0, 1, 1, 1])`\n        batch_idx = np.repeat(batch_idx, num_pred_positions)\n        masked_X = X[batch_idx, pred_positions]\n        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n        mlm_Y_hat = self.mlp(masked_X)\n        return mlm_Y_hat\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass MaskLM(nn.Module):\n    \"\"\"The masked language model task of BERT.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, **kwargs):\n        super(MaskLM, self).__init__(**kwargs)\n        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),\n                                 nn.ReLU(),\n                                 nn.LayerNorm(num_hiddens),\n                                 nn.LazyLinear(vocab_size))\n\n    def forward(self, X, pred_positions):\n        num_pred_positions = pred_positions.shape[1]\n        pred_positions = pred_positions.reshape(-1)\n        batch_size = X.shape[0]\n        batch_idx = torch.arange(0, batch_size)\n        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n        masked_X = X[batch_idx, pred_positions]\n        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n        mlm_Y_hat = self.mlp(masked_X)\n        return mlm_Y_hat\n```\n\nTo demonstrate [**the forward inference of `MaskLM`**],\nwe create its instance `mlm` and initialize it."
    },
    {
      "chunk_id": "f34eed86585b_2",
      "chapter": "bert",
      "heading": "[**Masked Language Modeling**]",
      "text": "Recall that `encoded_X` from the forward inference of `BERTEncoder`\nrepresents 2 BERT input sequences. We define `mlm_positions` as the 3 indices to predict in either BERT input sequence of `encoded_X`. The forward inference of `mlm` returns prediction results `mlm_Y_hat`\nat all the masked positions `mlm_positions` of `encoded_X`. For each prediction, the size of the result is equal to the vocabulary size. ```{.python .input}\n#@tab mxnet\nmlm = MaskLM(vocab_size, num_hiddens)\nmlm.initialize()\nmlm_positions = np.array([[1, 5, 2], [6, 1, 5]])\nmlm_Y_hat = mlm(encoded_X, mlm_positions)\nmlm_Y_hat.shape\n```\n\n```{.python .input}\n#@tab pytorch\nmlm = MaskLM(vocab_size, num_hiddens)\nmlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\nmlm_Y_hat = mlm(encoded_X, mlm_positions)\nmlm_Y_hat.shape\n```\n\nWith the ground truth labels `mlm_Y` of the predicted tokens `mlm_Y_hat` under masks,\nwe can calculate the cross-entropy loss of the masked language model task in BERT pretraining. ```{.python .input}\n#@tab mxnet\nmlm_Y = np.array([[7, 8, 9], [10, 20, 30]])\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\nmlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\nmlm_l.shape\n```\n\n```{.python .input}\n#@tab pytorch\nmlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\nloss = nn.CrossEntropyLoss(reduction='none')\nmlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\nmlm_l.shape\n```"
    },
    {
      "chunk_id": "a3b17633c704_0",
      "chapter": "bert",
      "heading": "[**Next Sentence Prediction**]",
      "text": ":label:`subsec_nsp`\n\nAlthough masked language modeling is able to encode bidirectional context\nfor representing words, it does not explicitly model the logical relationship\nbetween text pairs. To help understand the relationship between two text sequences,\nBERT considers a binary classification task, *next sentence prediction*, in its pretraining. When generating sentence pairs for pretraining,\nfor half of the time they are indeed consecutive sentences with the label \"True\";\nwhile for the other half of the time the second sentence is randomly sampled from the corpus with the label \"False\". The following `NextSentencePred` class uses a one-hidden-layer MLP\nto predict whether the second sentence is the next sentence of the first\nin the BERT input sequence. Due to self-attention in the Transformer encoder,\nthe BERT representation of the special token \u201c&lt;cls&gt;\u201d\nencodes both the two sentences from the input. Hence, the output layer (`self.output`) of the MLP classifier takes `X` as input,\nwhere `X` is the output of the MLP hidden layer whose input is the encoded \u201c&lt;cls&gt;\u201d token. ```{.python .input}\n#@tab mxnet\n#@save\nclass NextSentencePred(nn.Block):\n    \"\"\"The next sentence prediction task of BERT.\"\"\"\n    def __init__(self, **kwargs):\n        super(NextSentencePred, self).__init__(**kwargs)\n        self.output = nn.Dense(2)\n\n    def forward(self, X):\n        # `X` shape: (batch size, `num_hiddens`)\n        return self.output(X)\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass NextSentencePred(nn.Module):\n    \"\"\"The next sentence prediction task of BERT.\"\"\"\n    def __init__(self, **kwargs):\n        super(NextSentencePred, self).__init__(**kwargs)\n        self.output = nn.LazyLinear(2)\n\n    def forward(self, X):\n        # `X` shape: (batch size, `num_hiddens`)\n        return self.output(X)\n```\n\nWe can see that [**the forward inference of an `NextSentencePred`**] instance\nreturns binary predictions for each BERT input sequence."
    },
    {
      "chunk_id": "a3b17633c704_1",
      "chapter": "bert",
      "heading": "[**Next Sentence Prediction**]",
      "text": "```{.python .input}\n#@tab mxnet\nnsp = NextSentencePred()\nnsp.initialize()\nnsp_Y_hat = nsp(encoded_X)\nnsp_Y_hat.shape\n```\n\n```{.python .input}\n#@tab pytorch\n# PyTorch by default will not flatten the tensor as seen in mxnet where, if\n# flatten=True, all but the first axis of input data are collapsed together\nencoded_X = torch.flatten(encoded_X, start_dim=1)\n# input_shape for NSP: (batch size, `num_hiddens`)\nnsp = NextSentencePred()\nnsp_Y_hat = nsp(encoded_X)\nnsp_Y_hat.shape\n```\n\nThe cross-entropy loss of the 2 binary classifications can also be computed. ```{.python .input}\n#@tab mxnet\nnsp_y = np.array([0, 1])\nnsp_l = loss(nsp_Y_hat, nsp_y)\nnsp_l.shape\n```\n\n```{.python .input}\n#@tab pytorch\nnsp_y = torch.tensor([0, 1])\nnsp_l = loss(nsp_Y_hat, nsp_y)\nnsp_l.shape\n```\n\nIt is noteworthy that all the labels in both the aforementioned pretraining tasks\ncan be trivially obtained from the pretraining corpus without manual labeling effort. The original BERT has been pretrained on the concatenation of BookCorpus :cite:`Zhu.Kiros.Zemel.ea.2015`\nand English Wikipedia. These two text corpora are huge:\nthey have 800 million words and 2.5 billion words, respectively."
    },
    {
      "chunk_id": "6f1ec7444fa1_0",
      "chapter": "bert",
      "heading": "[**Putting It All Together**]",
      "text": "When pretraining BERT, the final loss function is a linear combination of\nboth the loss functions for masked language modeling and next sentence prediction. Now we can define the `BERTModel` class by instantiating the three classes\n`BERTEncoder`, `MaskLM`, and `NextSentencePred`. The forward inference returns the encoded BERT representations `encoded_X`,\npredictions of masked language modeling `mlm_Y_hat`,\nand next sentence predictions `nsp_Y_hat`. ```{.python .input}\n#@tab mxnet\n#@save\nclass BERTModel(nn.Block):\n    \"\"\"The BERT model.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                 num_blks, dropout, max_len=1000):\n        super(BERTModel, self).__init__()\n        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n                                   num_heads, num_blks, dropout, max_len)\n        self.hidden = nn.Dense(num_hiddens, activation='tanh')\n        self.mlm = MaskLM(vocab_size, num_hiddens)\n        self.nsp = NextSentencePred()\n\n    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n        encoded_X = self.encoder(tokens, segments, valid_lens)\n        if pred_positions is not None:\n            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n        else:\n            mlm_Y_hat = None\n        # The hidden layer of the MLP classifier for next sentence prediction."
    },
    {
      "chunk_id": "6f1ec7444fa1_1",
      "chapter": "bert",
      "heading": "[**Putting It All Together**]",
      "text": "# 0 is the index of the '<cls>' token\n        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n        return encoded_X, mlm_Y_hat, nsp_Y_hat\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nclass BERTModel(nn.Module):\n    \"\"\"The BERT model.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, \n                 num_heads, num_blks, dropout, max_len=1000):\n        super(BERTModel, self).__init__()\n        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n                                   num_heads, num_blks, dropout,\n                                   max_len=max_len)\n        self.hidden = nn.Sequential(nn.LazyLinear(num_hiddens),\n                                    nn.Tanh())\n        self.mlm = MaskLM(vocab_size, num_hiddens)\n        self.nsp = NextSentencePred()\n\n    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n        encoded_X = self.encoder(tokens, segments, valid_lens)\n        if pred_positions is not None:\n            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n        else:\n            mlm_Y_hat = None\n        # The hidden layer of the MLP classifier for next sentence prediction. # 0 is the index of the '<cls>' token\n        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n        return encoded_X, mlm_Y_hat, nsp_Y_hat\n```"
    },
    {
      "chunk_id": "5349fb46e47d_0",
      "chapter": "bert",
      "heading": "Summary",
      "text": "* Word embedding models such as word2vec and GloVe are context-independent. They assign the same pretrained vector to the same word regardless of the context of the word (if any). It is hard for them to handle well polysemy or complex semantics in natural languages.\n* For context-sensitive word representations such as ELMo and GPT, representations of words depend on their contexts.\n* ELMo encodes context bidirectionally but uses task-specific architectures (however, it is practically non-trivial to craft a specific architecture for every natural language processing task); while GPT is task-agnostic but encodes context left-to-right.\n* BERT combines the best of both worlds: it encodes context bidirectionally and requires minimal architecture changes for a wide range of natural language processing tasks.\n* The embeddings of the BERT input sequence are the sum of the token embeddings, segment embeddings, and positional embeddings.\n* Pretraining BERT is composed of two tasks: masked language modeling and next sentence prediction. The former is able to encode bidirectional context for representing words, while the latter explicitly models the logical relationship between text pairs."
    },
    {
      "chunk_id": "7f12befb69f7_0",
      "chapter": "bert",
      "heading": "Exercises",
      "text": "1. All other things being equal, will a masked language model require more or fewer pretraining steps to converge than a left-to-right language model? Why?\n1. In the original implementation of BERT, the positionwise feed-forward network in `BERTEncoder` (via `d2l.TransformerEncoderBlock`) and the fully connected layer in `MaskLM` both use the Gaussian error linear unit (GELU) :cite:`Hendrycks.Gimpel.2016` as the activation function. Research into the difference between GELU and ReLU.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/388)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1490)\n:end_tab:"
    },
    {
      "chunk_id": "5f5f58518d24_0",
      "chapter": "glove",
      "heading": "glove",
      "text": "# Word Embedding with Global Vectors (GloVe)\n:label:`sec_glove`\n\n\nWord-word co-occurrences\nwithin context windows\nmay carry rich semantic information.\nFor example,\nin a large corpus\nword \"solid\" is\nmore likely to co-occur\nwith \"ice\" than \"steam\",\nbut word \"gas\"\nprobably co-occurs with \"steam\"\nmore frequently than \"ice\".\nBesides,\nglobal corpus statistics\nof such co-occurrences\ncan be precomputed:\nthis can lead to more efficient training.\nTo leverage statistical\ninformation in the entire corpus\nfor word embedding,\nlet's first revisit\nthe skip-gram model in :numref:`subsec_skip-gram`,\nbut interpreting it\nusing global corpus statistics\nsuch as co-occurrence counts."
    },
    {
      "chunk_id": "53c178b751d2_0",
      "chapter": "glove",
      "heading": "Skip-Gram with Global Corpus Statistics",
      "text": ":label:`subsec_skipgram-global`\n\nDenoting by $q_{ij}$\nthe conditional probability\n$P(w_j\\mid w_i)$\nof word $w_j$ given word $w_i$\nin the skip-gram model,\nwe have\n\n$$q_{ij}=\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{ \\sum_{k \\in \\mathcal{V}} \\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)},$$\n\nwhere\nfor any index $i$\nvectors $\\mathbf{v}_i$ and $\\mathbf{u}_i$\nrepresent word $w_i$\nas the center word and context word,\nrespectively, and $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$\nis the index set of the vocabulary. Consider word $w_i$\nthat may occur multiple times\nin the corpus. In the entire corpus,\nall the context words\nwherever $w_i$ is taken as their center word\nform a *multiset* $\\mathcal{C}_i$\nof word indices\nthat *allows for multiple instances of the same element*. For any element,\nits number of instances is called its *multiplicity*. To illustrate with an example,\nsuppose that word $w_i$ occurs twice in the corpus\nand indices of the context words\nthat take $w_i$ as their center word\nin the two context windows\nare\n$k, j, m, k$ and $k, l, k, j$. Thus, multiset $\\mathcal{C}_i = \\{j, j, k, k, k, k, l, m\\}$, where\nmultiplicities of elements $j, k, l, m$\nare 2, 4, 1, 1, respectively. Now let's denote the multiplicity of element $j$ in\nmultiset $\\mathcal{C}_i$ as $x_{ij}$. This is the global co-occurrence count\nof word $w_j$ (as the context word)\nand word $w_i$ (as the center word)\nin the same context window\nin the entire corpus. Using such global corpus statistics,\nthe loss function of the skip-gram model\nis equivalent to\n\n$$-\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij} \\log\\,q_{ij}.$$\n:eqlabel:`eq_skipgram-x_ij`\n\nWe further denote by\n$x_i$\nthe number of all the context words\nin the context windows\nwhere $w_i$ occurs as their center word,\nwhich is equivalent to $|\\mathcal{C}_i|$."
    },
    {
      "chunk_id": "53c178b751d2_1",
      "chapter": "glove",
      "heading": "Skip-Gram with Global Corpus Statistics",
      "text": "Letting $p_{ij}$\nbe the conditional probability\n$x_{ij}/x_i$ for generating\ncontext word $w_j$ given center word $w_i$,\n:eqref:`eq_skipgram-x_ij`\ncan be rewritten as\n\n$$-\\sum_{i\\in\\mathcal{V}} x_i \\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}.$$\n:eqlabel:`eq_skipgram-p_ij`\n\nIn :eqref:`eq_skipgram-p_ij`, $-\\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}$ calculates\nthe cross-entropy\nof\nthe conditional distribution $p_{ij}$\nof global corpus statistics\nand\nthe\nconditional distribution $q_{ij}$\nof model predictions. This loss\nis also weighted by $x_i$ as explained above. Minimizing the loss function in\n:eqref:`eq_skipgram-p_ij`\nwill allow\nthe predicted conditional distribution\nto get close to\nthe conditional distribution\nfrom the global corpus statistics. Though being commonly used\nfor measuring the distance\nbetween probability distributions,\nthe cross-entropy loss function may not be a good choice here. On the one hand, as we mentioned in :numref:`sec_approx_train`,\nthe cost of properly normalizing $q_{ij}$\nresults in the sum over the entire vocabulary,\nwhich can be computationally expensive. On the other hand,\na large number of rare\nevents from a large corpus\nare often modeled by the cross-entropy loss\nto be assigned with\ntoo much weight."
    },
    {
      "chunk_id": "953cdde550b0_0",
      "chapter": "glove",
      "heading": "The GloVe Model",
      "text": "In view of this,\nthe *GloVe* model makes three changes\nto the skip-gram model based on squared loss :cite:`Pennington.Socher.Manning.2014`:\n\n1. Use variables $p'_{ij}=x_{ij}$ and $q'_{ij}=\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)$\nthat are not probability distributions\nand take the logarithm of both, so the squared loss term is $\\left(\\log\\,p'_{ij} - \\log\\,q'_{ij}\\right)^2 = \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i - \\log\\,x_{ij}\\right)^2$. 2. Add two scalar model parameters for each word $w_i$: the center word bias $b_i$ and the context word bias $c_i$. 3. Replace the weight of each loss term with the weight function $h(x_{ij})$, where $h(x)$ is increasing in the interval of $[0, 1]$. Putting all things together, training GloVe is to minimize the following loss function:\n\n$$\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}} h(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log\\,x_{ij}\\right)^2.$$\n:eqlabel:`eq_glove-loss`\n\nFor the weight function, a suggested choice is:\n$h(x) = (x/c) ^\\alpha$ (e.g $\\alpha = 0.75$) if $x < c$ (e.g., $c = 100$); otherwise $h(x) = 1$. In this case,\nbecause $h(0)=0$,\nthe squared loss term for any $x_{ij}=0$ can be omitted\nfor computational efficiency. For example,\nwhen using minibatch stochastic gradient descent for training,\nat each iteration\nwe randomly sample a minibatch of *non-zero* $x_{ij}$\nto calculate gradients\nand update the model parameters. Note that these non-zero $x_{ij}$ are precomputed\nglobal corpus statistics;\nthus, the model is called GloVe\nfor *Global Vectors*. It should be emphasized that\nif word $w_i$ appears in the context window of\nword $w_j$, then *vice versa*. Therefore, $x_{ij}=x_{ji}$. Unlike word2vec\nthat fits the asymmetric conditional probability\n$p_{ij}$,\nGloVe fits the symmetric $\\log \\, x_{ij}$. Therefore, the center word vector and\nthe context word vector of any word are mathematically equivalent in the GloVe model."
    },
    {
      "chunk_id": "953cdde550b0_1",
      "chapter": "glove",
      "heading": "The GloVe Model",
      "text": "Unlike word2vec\nthat fits the asymmetric conditional probability\n$p_{ij}$,\nGloVe fits the symmetric $\\log \\, x_{ij}$. Therefore, the center word vector and\nthe context word vector of any word are mathematically equivalent in the GloVe model. However in practice, owing to different initialization values,\nthe same word may still get different values\nin these two vectors after training:\nGloVe sums them up as the output vector."
    },
    {
      "chunk_id": "fbfc622ffc19_0",
      "chapter": "glove",
      "heading": "Interpreting GloVe from the Ratio of Co-occurrence Probabilities",
      "text": "We can also interpret the GloVe model from another perspective. Using the same notation in\n:numref:`subsec_skipgram-global`,\nlet $p_{ij} \\stackrel{\\textrm{def}}{=} P(w_j \\mid w_i)$ be the conditional probability of generating the context word $w_j$ given $w_i$ as the center word in the corpus. :numref:`tab_glove`\nlists several co-occurrence probabilities\ngiven words \"ice\" and \"steam\"\nand their ratios based on  statistics from a large corpus. :Word-word co-occurrence probabilities and their ratios from a large corpus (adapted from Table 1 in :citet:`Pennington.Socher.Manning.2014`)\n:label:`tab_glove`\n\n|$w_k$=|solid|gas|water|fashion|\n|:--|:-|:-|:-|:-|\n|$p_1=P(w_k\\mid \\textrm{ice})$|0.00019|0.000066|0.003|0.000017|\n|$p_2=P(w_k\\mid\\textrm{steam})$|0.000022|0.00078|0.0022|0.000018|\n|$p_1/p_2$|8.9|0.085|1.36|0.96|\n\n\n\nWe can observe the following from :numref:`tab_glove`:\n\n* For a word $w_k$ that is related to \"ice\" but unrelated to \"steam\", such as $w_k=\\textrm{solid}$, we expect a larger ratio of co-occurence probabilities, such as 8.9. * For a word $w_k$ that is related to \"steam\" but unrelated to \"ice\", such as $w_k=\\textrm{gas}$, we expect a smaller ratio of co-occurence probabilities, such as 0.085. * For a word $w_k$ that is related to both \"ice\" and \"steam\", such as $w_k=\\textrm{water}$, we expect a ratio of co-occurence probabilities that is close to 1, such as 1.36. * For a word $w_k$ that is unrelated to both \"ice\" and \"steam\", such as $w_k=\\textrm{fashion}$, we expect a ratio of co-occurence probabilities that is close to 1, such as 0.96. It can be seen that the ratio\nof co-occurrence probabilities\ncan intuitively express\nthe relationship between words. Thus, we can design a function\nof three word vectors\nto fit this ratio."
    },
    {
      "chunk_id": "fbfc622ffc19_1",
      "chapter": "glove",
      "heading": "Interpreting GloVe from the Ratio of Co-occurrence Probabilities",
      "text": "It can be seen that the ratio\nof co-occurrence probabilities\ncan intuitively express\nthe relationship between words. Thus, we can design a function\nof three word vectors\nto fit this ratio. For the ratio of co-occurrence probabilities\n${p_{ij}}/{p_{ik}}$\nwith $w_i$ being the center word\nand $w_j$ and $w_k$ being the context words,\nwe want to fit this ratio\nusing some function $f$:\n\n$$f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) \\approx \\frac{p_{ij}}{p_{ik}}.$$\n:eqlabel:`eq_glove-f`\n\nAmong many possible designs for $f$,\nwe only pick a reasonable choice in the following. Since the ratio of co-occurrence probabilities\nis a scalar,\nwe require that\n$f$ be a scalar function, such as\n$f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = f\\left((\\mathbf{u}_j - \\mathbf{u}_k)^\\top {\\mathbf{v}}_i\\right)$. Switching word indices\n$j$ and $k$ in :eqref:`eq_glove-f`,\nit must hold that\n$f(x)f(-x)=1$,\nso one possibility is $f(x)=\\exp(x)$,\ni.e.,\n\n$$f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = \\frac{\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right)}{\\exp\\left(\\mathbf{u}_k^\\top {\\mathbf{v}}_i\\right)} \\approx \\frac{p_{ij}}{p_{ik}}.$$\n\nNow let's pick\n$\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right) \\approx \\alpha p_{ij}$,\nwhere $\\alpha$ is a constant. Since $p_{ij}=x_{ij}/x_i$, after taking the logarithm on both sides we get $\\mathbf{u}_j^\\top {\\mathbf{v}}_i \\approx \\log\\,\\alpha + \\log\\,x_{ij} - \\log\\,x_i$. We may use additional bias terms to fit $- \\log\\, \\alpha + \\log\\, x_i$, such as the center word bias $b_i$ and the context word bias $c_j$:\n\n$$\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j \\approx \\log\\, x_{ij}.$$\n:eqlabel:`eq_glove-square`\n\nMeasuring the squared error of\n:eqref:`eq_glove-square` with weights,\nthe GloVe loss function in\n:eqref:`eq_glove-loss` is obtained."
    },
    {
      "chunk_id": "22fa132d86e3_0",
      "chapter": "glove",
      "heading": "Summary",
      "text": "* The skip-gram model can be interpreted using global corpus statistics such as word-word co-occurrence counts.\n* The cross-entropy loss may not be a good choice for measuring the difference of two probability distributions, especially for a large corpus. GloVe uses squared loss to fit precomputed global corpus statistics.\n* The center word vector and the context word vector are mathematically equivalent for any word in GloVe.\n* GloVe can be interpreted from the ratio of word-word co-occurrence probabilities."
    },
    {
      "chunk_id": "cbf077d93081_0",
      "chapter": "glove",
      "heading": "Exercises",
      "text": "1. If words $w_i$ and $w_j$ co-occur in the same context window, how can we use their   distance in the text sequence to redesign the method for  calculating the conditional probability $p_{ij}$? Hint: see Section 4.2 of the GloVe paper :cite:`Pennington.Socher.Manning.2014`.\n1. For any word, are its center word bias  and context word bias mathematically equivalent in GloVe? Why?\n\n\n[Discussions](https://discuss.d2l.ai/t/385)"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Natural Language Processing: Pretraining\n:label:`chap_nlp_pretrain`\n\n\nHumans need to communicate. Out of this basic need of the human condition, a vast amount of written text has been generated on an everyday basis. Given rich text in social media, chat apps, emails, product reviews, news articles,  research papers, and books, it becomes vital to enable computers to understand them to offer assistance or make decisions based on human languages. *Natural language processing* studies interactions between computers and humans using natural languages. In practice, it is very common to use natural language processing techniques to process and analyze text (human natural language) data, such as language models in :numref:`sec_language-model` and machine translation models in :numref:`sec_machine_translation`. To understand text, we can begin by learning\nits representations. Leveraging the existing text sequences\nfrom large corpora,\n*self-supervised learning*\nhas been extensively\nused to pretrain text representations,\nsuch as by predicting some hidden part of the text\nusing some other part of their surrounding text. In this way,\nmodels learn through supervision\nfrom *massive* text data\nwithout *expensive* labeling efforts! As we will see in this chapter,\nwhen treating each word or subword as an individual token,\nthe representation of each token can be pretrained\nusing word2vec, GloVe, or subword embedding models\non large corpora. After pretraining, representation of each token can be a vector,\nhowever, it remains the same no matter what the context is. For instance, the vector representation of \"bank\" is the same\nin both\n\"go to the bank to deposit some money\"\nand\n\"go to the bank to sit down\". Thus, many more recent pretraining models adapt representation of the same token\nto different contexts. Among them is BERT, a much deeper self-supervised model based on the Transformer encoder. In this chapter, we will focus on how to pretrain such representations for text,\nas highlighted in :numref:`fig_nlp-map-pretrain`."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "Among them is BERT, a much deeper self-supervised model based on the Transformer encoder. In this chapter, we will focus on how to pretrain such representations for text,\nas highlighted in :numref:`fig_nlp-map-pretrain`. ![Pretrained text representations can be fed to various deep learning architectures for different downstream natural language processing applications. This chapter focuses on the upstream text representation pretraining.](../img/nlp-map-pretrain.svg)\n:label:`fig_nlp-map-pretrain`\n\n\nFor sight of the big picture,\n:numref:`fig_nlp-map-pretrain` shows that\nthe pretrained text representations can be fed to\na variety of deep learning architectures for different downstream natural language processing applications. We will cover them in :numref:`chap_nlp_app`. ```toc\n:maxdepth: 2\n\nword2vec\napprox-training\nword-embedding-dataset\nword2vec-pretraining\nglove\nsubword-embedding\nsimilarity-analogy\nbert\nbert-dataset\nbert-pretraining\n\n```"
    },
    {
      "chunk_id": "56310530b141_0",
      "chapter": "similarity-analogy",
      "heading": "similarity-analogy",
      "text": "# Word Similarity and Analogy\n:label:`sec_synonyms`\n\nIn :numref:`sec_word2vec_pretraining`, \nwe trained a word2vec model on a small dataset, \nand applied it\nto find semantically similar words \nfor an input word.\nIn practice,\nword vectors that are pretrained\non large corpora can be\napplied to downstream\nnatural language processing tasks,\nwhich will be covered later\nin :numref:`chap_nlp_app`.\nTo demonstrate \nsemantics of pretrained word vectors\nfrom large corpora in a straightforward way,\nlet's apply them\nin the word similarity and analogy tasks.\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nimport os\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nimport os\n```"
    },
    {
      "chunk_id": "2b85eaf66df8_0",
      "chapter": "similarity-analogy",
      "heading": "Loading Pretrained Word Vectors",
      "text": "Below lists pretrained GloVe embeddings of dimension 50, 100, and 300,\nwhich can be downloaded from the [GloVe website](https://nlp.stanford.edu/projects/glove/). The pretrained fastText embeddings are available in multiple languages. Here we consider one English version (300-dimensional \"wiki.en\") that can be downloaded from the\n[fastText website](https://fasttext.cc/). ```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n\n#@save\nd2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n\n#@save\nd2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n\n#@save\nd2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n                           'c1816da3821ae9f43899be655002f6c723e91b88')\n```\n\nTo load these pretrained GloVe and fastText embeddings, we define the following `TokenEmbedding` class."
    },
    {
      "chunk_id": "2b85eaf66df8_1",
      "chapter": "similarity-analogy",
      "heading": "Loading Pretrained Word Vectors",
      "text": "```{.python .input}\n#@tab all\n#@save\nclass TokenEmbedding:\n    \"\"\"Token Embedding.\"\"\"\n    def __init__(self, embedding_name):\n        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n            embedding_name)\n        self.unknown_idx = 0\n        self.token_to_idx = {token: idx for idx, token in\n                             enumerate(self.idx_to_token)}\n\n    def _load_embedding(self, embedding_name):\n        idx_to_token, idx_to_vec = ['<unk>'], []\n        data_dir = d2l.download_extract(embedding_name)\n        # GloVe website: https://nlp.stanford.edu/projects/glove/\n        # fastText website: https://fasttext.cc/\n        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n            for line in f:\n                elems = line.rstrip().split(' ')\n                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n                # Skip header information, such as the top row in fastText\n                if len(elems) > 1:\n                    idx_to_token.append(token)\n                    idx_to_vec.append(elems)\n        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n        return idx_to_token, d2l.tensor(idx_to_vec)\n\n    def __getitem__(self, tokens):\n        indices = [self.token_to_idx.get(token, self.unknown_idx)\n                   for token in tokens]\n        vecs = self.idx_to_vec[d2l.tensor(indices)]\n        return vecs\n\n    def __len__(self):\n        return len(self.idx_to_token)\n```\n\nBelow we load the\n50-dimensional GloVe embeddings\n(pretrained on a Wikipedia subset). When creating the `TokenEmbedding` instance,\nthe specified embedding file has to be downloaded if it\nwas not yet. ```{.python .input}\n#@tab all\nglove_6b50d = TokenEmbedding('glove.6b.50d')\n```\n\nOutput the vocabulary size. The vocabulary contains 400000 words (tokens) and a special unknown token. ```{.python .input}\n#@tab all\nlen(glove_6b50d)\n```\n\nWe can get the index of a word in the vocabulary, and vice versa."
    },
    {
      "chunk_id": "2b85eaf66df8_2",
      "chapter": "similarity-analogy",
      "heading": "Loading Pretrained Word Vectors",
      "text": "The vocabulary contains 400000 words (tokens) and a special unknown token. ```{.python .input}\n#@tab all\nlen(glove_6b50d)\n```\n\nWe can get the index of a word in the vocabulary, and vice versa. ```{.python .input}\n#@tab all\nglove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]\n```"
    },
    {
      "chunk_id": "50fa060f1211_0",
      "chapter": "similarity-analogy",
      "heading": "Applying Pretrained Word Vectors",
      "text": "Using the loaded GloVe vectors,\nwe will demonstrate their semantics\nby applying them\nin the following word similarity and analogy tasks."
    },
    {
      "chunk_id": "e7ba7acdf744_0",
      "chapter": "similarity-analogy",
      "heading": "Word Similarity",
      "text": "Similar to :numref:`subsec_apply-word-embed`,\nin order to find semantically similar words\nfor an input word\nbased on cosine similarities between\nword vectors,\nwe implement the following `knn`\n($k$-nearest neighbors) function.\n\n```{.python .input}\n#@tab mxnet\ndef knn(W, x, k):\n    # Add 1e-9 for numerical stability\n    cos = np.dot(W, x.reshape(-1,)) / (\n        np.sqrt(np.sum(W * W, axis=1) + 1e-9) * np.sqrt((x * x).sum()))\n    topk = npx.topk(cos, k=k, ret_typ='indices')\n    return topk, [cos[int(i)] for i in topk]\n```\n\n```{.python .input}\n#@tab pytorch\ndef knn(W, x, k):\n    # Add 1e-9 for numerical stability\n    cos = torch.mv(W, x.reshape(-1,)) / (\n        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n        torch.sqrt((x * x).sum()))\n    _, topk = torch.topk(cos, k=k)\n    return topk, [cos[int(i)] for i in topk]\n```\n\nThen, we \nsearch for similar words\nusing the pretrained word vectors \nfrom the `TokenEmbedding` instance `embed`.\n\n```{.python .input}\n#@tab all\ndef get_similar_tokens(query_token, k, embed):\n    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n    for i, c in zip(topk[1:], cos[1:]):  # Exclude the input word\n        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')\n```\n\nThe vocabulary of the pretrained word vectors\nin `glove_6b50d` contains 400000 words and a special unknown token. \nExcluding the input word and unknown token,\namong this vocabulary\nlet's find \nthree most semantically similar words\nto word \"chip\".\n\n```{.python .input}\n#@tab all\nget_similar_tokens('chip', 3, glove_6b50d)\n```\n\nBelow outputs similar words\nto \"baby\" and \"beautiful\".\n\n```{.python .input}\n#@tab all\nget_similar_tokens('baby', 3, glove_6b50d)\n```\n\n```{.python .input}\n#@tab all\nget_similar_tokens('beautiful', 3, glove_6b50d)\n```"
    },
    {
      "chunk_id": "59a2b4267347_0",
      "chapter": "similarity-analogy",
      "heading": "Word Analogy",
      "text": "Besides finding similar words,\nwe can also apply word vectors\nto word analogy tasks.\nFor example,\n\u201cman\u201d:\u201cwoman\u201d::\u201cson\u201d:\u201cdaughter\u201d\nis the form of a word analogy:\n\u201cman\u201d is to \u201cwoman\u201d as \u201cson\u201d is to \u201cdaughter\u201d.\nSpecifically,\nthe word analogy completion task\ncan be defined as:\nfor a word analogy \n$a : b :: c : d$, given the first three words $a$, $b$ and $c$, find $d$. \nDenote the vector of word $w$ by $\\textrm{vec}(w)$. \nTo complete the analogy,\nwe will find the word \nwhose vector is most similar\nto the result of $\\textrm{vec}(c)+\\textrm{vec}(b)-\\textrm{vec}(a)$.\n\n```{.python .input}\n#@tab all\ndef get_analogy(token_a, token_b, token_c, embed):\n    vecs = embed[[token_a, token_b, token_c]]\n    x = vecs[1] - vecs[0] + vecs[2]\n    topk, cos = knn(embed.idx_to_vec, x, 1)\n    return embed.idx_to_token[int(topk[0])]  # Remove unknown words\n```\n\nLet's verify the \"male-female\" analogy using the loaded word vectors.\n\n```{.python .input}\n#@tab all\nget_analogy('man', 'woman', 'son', glove_6b50d)\n```\n\nBelow completes a\n\u201ccapital-country\u201d analogy: \n\u201cbeijing\u201d:\u201cchina\u201d::\u201ctokyo\u201d:\u201cjapan\u201d.\nThis demonstrates \nsemantics in the pretrained word vectors.\n\n```{.python .input}\n#@tab all\nget_analogy('beijing', 'china', 'tokyo', glove_6b50d)\n```\n\nFor the\n\u201cadjective-superlative adjective\u201d analogy\nsuch as \n\u201cbad\u201d:\u201cworst\u201d::\u201cbig\u201d:\u201cbiggest\u201d,\nwe can see that the pretrained word vectors\nmay capture the syntactic information.\n\n```{.python .input}\n#@tab all\nget_analogy('bad', 'worst', 'big', glove_6b50d)\n```\n\nTo show the captured notion\nof past tense in the pretrained word vectors,\nwe can test the syntax using the\n\"present tense-past tense\" analogy: \u201cdo\u201d:\u201cdid\u201d::\u201cgo\u201d:\u201cwent\u201d.\n\n```{.python .input}\n#@tab all\nget_analogy('do', 'did', 'go', glove_6b50d)\n```"
    },
    {
      "chunk_id": "605896649629_0",
      "chapter": "similarity-analogy",
      "heading": "Summary",
      "text": "* In practice, word vectors that are pretrained on large corpora can be applied to downstream natural language processing tasks.\n* Pretrained word vectors can be applied to the word similarity and analogy tasks."
    },
    {
      "chunk_id": "3f3113fad766_0",
      "chapter": "similarity-analogy",
      "heading": "Exercises",
      "text": "1. Test the fastText results using `TokenEmbedding('wiki.en')`.\n1. When the vocabulary is extremely large, how can we find similar words or complete a word analogy faster?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/387)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1336)\n:end_tab:"
    },
    {
      "chunk_id": "101b1b310c4c_0",
      "chapter": "subword-embedding",
      "heading": "subword-embedding",
      "text": "# Subword Embedding\n:label:`sec_fasttext`\n\nIn English,\nwords such as\n\"helps\", \"helped\", and \"helping\" are \ninflected forms of the same word \"help\".\nThe relationship \nbetween \"dog\" and \"dogs\"\nis the same as \nthat between \"cat\" and \"cats\",\nand \nthe relationship \nbetween \"boy\" and \"boyfriend\"\nis the same as \nthat between \"girl\" and \"girlfriend\".\nIn other languages\nsuch as French and Spanish,\nmany verbs have over 40 inflected forms,\nwhile in Finnish,\na noun may have up to 15 cases.\nIn linguistics,\nmorphology studies word formation and word relationships.\nHowever,\nthe internal structure of words\nwas neither explored in word2vec\nnor in GloVe."
    },
    {
      "chunk_id": "fe8dacdf8ce2_0",
      "chapter": "subword-embedding",
      "heading": "The fastText Model",
      "text": "Recall how words are represented in word2vec.\nIn both the skip-gram model\nand the continuous bag-of-words model,\ndifferent inflected forms of the same word\nare directly represented by different vectors\nwithout shared parameters.\nTo use morphological information,\nthe *fastText* model\nproposed a *subword embedding* approach,\nwhere a subword is a character $n$-gram :cite:`Bojanowski.Grave.Joulin.ea.2017`.\nInstead of learning word-level vector representations,\nfastText can be considered as\nthe subword-level skip-gram,\nwhere each *center word* is represented by the sum of \nits subword vectors.\n\nLet's illustrate how to obtain \nsubwords for each center word in fastText\nusing the word \"where\".\nFirst, add special characters \u201c&lt;\u201d and \u201c&gt;\u201d \nat the beginning and end of the word to distinguish prefixes and suffixes from other subwords. \nThen, extract character $n$-grams from the word.\nFor example, when $n=3$,\nwe obtain all subwords of length 3: \"&lt;wh\", \"whe\", \"her\", \"ere\", \"re&gt;\", and the special subword \"&lt;where&gt;\".\n\n\nIn fastText, for any word $w$,\ndenote by $\\mathcal{G}_w$\nthe union of all its subwords of length between 3 and 6\nand its special subword.\nThe vocabulary \nis the union of the subwords of all words.\nLetting $\\mathbf{z}_g$\nbe the vector of subword $g$ in the dictionary,\nthe vector $\\mathbf{v}_w$ for \nword $w$ as a center word\nin the skip-gram model\nis the sum of its subword vectors:\n\n$$\\mathbf{v}_w = \\sum_{g\\in\\mathcal{G}_w} \\mathbf{z}_g.$$\n\nThe rest of fastText is the same as the skip-gram model. Compared with the skip-gram model, \nthe vocabulary in fastText is larger,\nresulting in more model parameters. \nBesides, \nto calculate the representation of a word,\nall its subword vectors\nhave to be summed,\nleading to higher computational complexity.\nHowever,\nthanks to shared parameters from subwords among words with similar structures,\nrare words and even out-of-vocabulary words\nmay obtain better vector representations in fastText."
    },
    {
      "chunk_id": "3243d286c8f2_0",
      "chapter": "subword-embedding",
      "heading": "Byte Pair Encoding",
      "text": ":label:`subsec_Byte_Pair_Encoding`\n\nIn fastText, all the extracted subwords have to be of the specified lengths, such as $3$ to $6$, thus the vocabulary size cannot be predefined. To allow for variable-length subwords in a fixed-size vocabulary,\nwe can apply a compression algorithm\ncalled *byte pair encoding* (BPE) to extract subwords :cite:`Sennrich.Haddow.Birch.2015`. Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols within a word,\nsuch as consecutive characters of arbitrary length. Starting from symbols of length 1,\nbyte pair encoding iteratively merges the most frequent pair of consecutive symbols to produce new longer symbols. Note that for efficiency, pairs crossing word boundaries are not considered. In the end, we can use such symbols as subwords to segment words. Byte pair encoding and its variants has been used for input representations in popular natural language processing pretraining models such as GPT-2 :cite:`Radford.Wu.Child.ea.2019` and RoBERTa :cite:`Liu.Ott.Goyal.ea.2019`. In the following, we will illustrate how byte pair encoding works. First, we initialize the vocabulary of symbols as all the English lowercase characters, a special end-of-word symbol `'_'`, and a special unknown symbol `'[UNK]'`. ```{.python .input}\n#@tab all\nimport collections\n\nsymbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n           '_', '[UNK]']\n```\n\nSince we do not consider symbol pairs that cross boundaries of words,\nwe only need a dictionary `raw_token_freqs` that maps words to their frequencies (number of occurrences)\nin a dataset. Note that the special symbol `'_'` is appended to each word so that\nwe can easily recover a word sequence (e.g., \"a taller man\")\nfrom a sequence of output symbols ( e.g., \"a_ tall er_ man\")."
    },
    {
      "chunk_id": "3243d286c8f2_1",
      "chapter": "subword-embedding",
      "heading": "Byte Pair Encoding",
      "text": "Note that the special symbol `'_'` is appended to each word so that\nwe can easily recover a word sequence (e.g., \"a taller man\")\nfrom a sequence of output symbols ( e.g., \"a_ tall er_ man\"). Since we start the merging process from a vocabulary of only single characters and special symbols, space is inserted between every pair of consecutive characters within each word (keys of the dictionary `token_freqs`). In other words, space is the delimiter between symbols within a word. ```{.python .input}\n#@tab all\nraw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\ntoken_freqs = {}\nfor token, freq in raw_token_freqs.items():\n    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\ntoken_freqs\n```\n\nWe define the following `get_max_freq_pair` function that\nreturns the most frequent pair of consecutive symbols within a word,\nwhere words come from keys of the input dictionary `token_freqs`. ```{.python .input}\n#@tab all\ndef get_max_freq_pair(token_freqs):\n    pairs = collections.defaultdict(int)\n    for token, freq in token_freqs.items():\n        symbols = token.split()\n        for i in range(len(symbols) - 1):\n            # Key of `pairs` is a tuple of two consecutive symbols\n            pairs[symbols[i], symbols[i + 1]] += freq\n    return max(pairs, key=pairs.get)  # Key of `pairs` with the max value\n```\n\nAs a greedy approach based on frequency of consecutive symbols,\nbyte pair encoding will use the following `merge_symbols` function to merge the most frequent pair of consecutive symbols to produce new symbols."
    },
    {
      "chunk_id": "3243d286c8f2_2",
      "chapter": "subword-embedding",
      "heading": "Byte Pair Encoding",
      "text": "```{.python .input}\n#@tab all\ndef merge_symbols(max_freq_pair, token_freqs, symbols):\n    symbols.append(''.join(max_freq_pair))\n    new_token_freqs = dict()\n    for token, freq in token_freqs.items():\n        new_token = token.replace(' '.join(max_freq_pair),\n                                  ''.join(max_freq_pair))\n        new_token_freqs[new_token] = token_freqs[token]\n    return new_token_freqs\n```\n\nNow we iteratively perform the byte pair encoding algorithm over the keys of the dictionary `token_freqs`. In the first iteration, the most frequent pair of consecutive symbols are `'t'` and `'a'`, thus byte pair encoding merges them to produce a new symbol `'ta'`. In the second iteration, byte pair encoding continues to merge `'ta'` and `'l'` to result in another new symbol `'tal'`. ```{.python .input}\n#@tab all\nnum_merges = 10\nfor i in range(num_merges):\n    max_freq_pair = get_max_freq_pair(token_freqs)\n    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n    print(f'merge #{i + 1}:', max_freq_pair)\n```\n\nAfter 10 iterations of byte pair encoding, we can see that list `symbols` now contains 10 more symbols that are iteratively merged from other symbols. ```{.python .input}\n#@tab all\nprint(symbols)\n```\n\nFor the same dataset specified in the keys of the dictionary `raw_token_freqs`,\neach word in the dataset is now segmented by subwords \"fast_\", \"fast\", \"er_\", \"tall_\", and \"tall\"\nas a result of the byte pair encoding algorithm. For instance, words \"faster_\" and \"taller_\" are segmented as \"fast er_\" and \"tall er_\", respectively. ```{.python .input}\n#@tab all\nprint(list(token_freqs.keys()))\n```\n\nNote that the result of byte pair encoding depends on the dataset being used. We can also use the subwords learned from one dataset\nto segment words of another dataset. As a greedy approach, the following `segment_BPE` function tries to break words into the longest possible subwords from the input argument `symbols`."
    },
    {
      "chunk_id": "3243d286c8f2_3",
      "chapter": "subword-embedding",
      "heading": "Byte Pair Encoding",
      "text": "We can also use the subwords learned from one dataset\nto segment words of another dataset. As a greedy approach, the following `segment_BPE` function tries to break words into the longest possible subwords from the input argument `symbols`. ```{.python .input}\n#@tab all\ndef segment_BPE(tokens, symbols):\n    outputs = []\n    for token in tokens:\n        start, end = 0, len(token)\n        cur_output = []\n        # Segment token with the longest possible subwords from symbols\n        while start < len(token) and start < end:\n            if token[start: end] in symbols:\n                cur_output.append(token[start: end])\n                start = end\n                end = len(token)\n            else:\n                end -= 1\n        if start < len(token):\n            cur_output.append('[UNK]')\n        outputs.append(' '.join(cur_output))\n    return outputs\n```\n\nIn the following, we use the subwords in list `symbols`, which is learned from the aforementioned dataset,\nto segment `tokens` that represent another dataset. ```{.python .input}\n#@tab all\ntokens = ['tallest_', 'fatter_']\nprint(segment_BPE(tokens, symbols))\n```"
    },
    {
      "chunk_id": "7834a33d7aad_0",
      "chapter": "subword-embedding",
      "heading": "Summary",
      "text": "* The fastText model proposes a subword embedding approach. Based on the skip-gram model in word2vec, it represents a center word as the sum of its subword vectors.\n* Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols within a word. As a greedy approach, byte pair encoding iteratively merges the most frequent pair of consecutive symbols.\n* Subword embedding may improve the quality of representations of rare words and out-of-dictionary words."
    },
    {
      "chunk_id": "2ca548bba14c_0",
      "chapter": "subword-embedding",
      "heading": "Exercises",
      "text": "1. As an example, there are about $3\\times 10^8$ possible  $6$-grams in English. What is the issue when there are too many subwords? How to address the issue? Hint: refer to the end of Section 3.2 of the fastText paper :cite:`Bojanowski.Grave.Joulin.ea.2017`.\n1. How to design a subword embedding model based on the continuous bag-of-words model?\n1. To get a vocabulary of size $m$, how many merging operations are needed when the initial symbol vocabulary size is $n$?\n1. How to extend the idea of byte pair encoding to extract phrases?\n\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/386)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/4587)\n:end_tab:"
    },
    {
      "chunk_id": "18776638112d_0",
      "chapter": "word-embedding-dataset",
      "heading": "word-embedding-dataset",
      "text": "# The Dataset for Pretraining Word Embeddings\n:label:`sec_word2vec_data`\n\nNow that we know the technical details of \nthe word2vec models and approximate training methods,\nlet's walk through their implementations. \nSpecifically,\nwe will take the skip-gram model in :numref:`sec_word2vec`\nand negative sampling in :numref:`sec_approx_train`\nas an example.\nIn this section,\nwe begin with the dataset\nfor pretraining the word embedding model:\nthe original format of the data\nwill be transformed\ninto minibatches\nthat can be iterated over during training.\n\n```{.python .input}\n#@tab mxnet\nimport collections\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import gluon, np\nimport os\nimport random\n```\n\n```{.python .input}\n#@tab pytorch\nimport collections\nfrom d2l import torch as d2l\nimport math\nimport torch\nimport os\nimport random\n```"
    },
    {
      "chunk_id": "5800458b606f_0",
      "chapter": "word-embedding-dataset",
      "heading": "Reading the Dataset",
      "text": "The dataset that we use here\nis [Penn Tree Bank (PTB)]( https://catalog.ldc.upenn.edu/LDC99T42). \nThis corpus is sampled\nfrom Wall Street Journal articles,\nsplit into training, validation, and test sets.\nIn the original format,\neach line of the text file\nrepresents a sentence of words that are separated by spaces.\nHere we treat each word as a token.\n\n```{.python .input}\n#@tab all\n#@save\nd2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n                       '319d85e578af0cdc590547f26231e4e31cdf1e42')\n\n#@save\ndef read_ptb():\n    \"\"\"Load the PTB dataset into a list of text lines.\"\"\"\n    data_dir = d2l.download_extract('ptb')\n    # Read the training set\n    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n        raw_text = f.read()\n    return [line.split() for line in raw_text.split('\\n')]\n\nsentences = read_ptb()\nf'# sentences: {len(sentences)}'\n```\n\nAfter reading the training set,\nwe build a vocabulary for the corpus,\nwhere any word that appears \nless than 10 times is replaced by \nthe \"&lt;unk&gt;\" token.\nNote that the original dataset\nalso contains \"&lt;unk&gt;\" tokens that represent rare (unknown) words.\n\n```{.python .input}\n#@tab all\nvocab = d2l.Vocab(sentences, min_freq=10)\nf'vocab size: {len(vocab)}'\n```"
    },
    {
      "chunk_id": "5bddfb86d9e6_0",
      "chapter": "word-embedding-dataset",
      "heading": "Subsampling",
      "text": "Text data\ntypically have high-frequency words\nsuch as \"the\", \"a\", and \"in\":\nthey may even occur billions of times in\nvery large corpora. However,\nthese words often co-occur\nwith many different words in\ncontext windows, providing little useful signals. For instance,\nconsider the word \"chip\" in a context window:\nintuitively\nits co-occurrence with a low-frequency word \"intel\"\nis more useful in training\nthan \nthe co-occurrence with a high-frequency word \"a\". Moreover, training with vast amounts of (high-frequency) words\nis slow. Thus, when training word embedding models, \nhigh-frequency words can be *subsampled* :cite:`Mikolov.Sutskever.Chen.ea.2013`. Specifically, \neach indexed word $w_i$ \nin the dataset will be discarded with probability\n\n\n$$ P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right),$$\n\nwhere $f(w_i)$ is the ratio of \nthe number of words $w_i$\nto the total number of words in the dataset, \nand the constant $t$ is a hyperparameter\n($10^{-4}$ in the experiment). We can see that only when\nthe relative frequency\n$f(w_i) > t$  can the (high-frequency) word $w_i$ be discarded, \nand the higher the relative frequency of the word, \nthe greater the probability of being discarded. ```{.python .input}\n#@tab all\n#@save\ndef subsample(sentences, vocab):\n    \"\"\"Subsample high-frequency words.\"\"\"\n    # Exclude unknown tokens ('<unk>')\n    sentences = [[token for token in line if vocab[token] != vocab.unk]\n                 for line in sentences]\n    counter = collections.Counter([\n        token for line in sentences for token in line])\n    num_tokens = sum(counter.values())\n\n    # Return True if `token` is kept during subsampling\n    def keep(token):\n        return(random.uniform(0, 1) <\n               math.sqrt(1e-4 / counter[token] * num_tokens))\n\n    return ([[token for token in line if keep(token)] for line in sentences],\n            counter)\n\nsubsampled, counter = subsample(sentences, vocab)\n```\n\nThe following code snippet \nplots the histogram of\nthe number of tokens per sentence\nbefore and after subsampling."
    },
    {
      "chunk_id": "5bddfb86d9e6_1",
      "chapter": "word-embedding-dataset",
      "heading": "Subsampling",
      "text": "As expected, \nsubsampling significantly shortens sentences\nby dropping high-frequency words,\nwhich will lead to training speedup. ```{.python .input}\n#@tab all\nd2l.show_list_len_pair_hist(['origin', 'subsampled'], '# tokens per sentence',\n                            'count', sentences, subsampled);\n```\n\nFor individual tokens, the sampling rate of the high-frequency word \"the\" is less than 1/20. ```{.python .input}\n#@tab all\ndef compare_counts(token):\n    return (f'# of \"{token}\": '\n            f'before={sum([l.count(token) for l in sentences])}, '\n            f'after={sum([l.count(token) for l in subsampled])}')\n\ncompare_counts('the')\n```\n\nIn contrast, \nlow-frequency words \"join\" are completely kept. ```{.python .input}\n#@tab all\ncompare_counts('join')\n```\n\nAfter subsampling, we map tokens to their indices for the corpus. ```{.python .input}\n#@tab all\ncorpus = [vocab[line] for line in subsampled]\ncorpus[:3]\n```"
    },
    {
      "chunk_id": "0ee2afed9870_0",
      "chapter": "word-embedding-dataset",
      "heading": "Extracting Center Words and Context Words",
      "text": "The following `get_centers_and_contexts`\nfunction extracts all the \ncenter words and their context words\nfrom `corpus`.\nIt uniformly samples an integer between 1 and `max_window_size`\nat random as the context window size.\nFor any center word,\nthose words \nwhose distance from it\ndoes not exceed the sampled\ncontext window size\nare its context words.\n\n```{.python .input}\n#@tab all\n#@save\ndef get_centers_and_contexts(corpus, max_window_size):\n    \"\"\"Return center words and context words in skip-gram.\"\"\"\n    centers, contexts = [], []\n    for line in corpus:\n        # To form a \"center word--context word\" pair, each sentence needs to\n        # have at least 2 words\n        if len(line) < 2:\n            continue\n        centers += line\n        for i in range(len(line)):  # Context window centered at `i`\n            window_size = random.randint(1, max_window_size)\n            indices = list(range(max(0, i - window_size),\n                                 min(len(line), i + 1 + window_size)))\n            # Exclude the center word from the context words\n            indices.remove(i)\n            contexts.append([line[idx] for idx in indices])\n    return centers, contexts\n```\n\nNext, we create an artificial dataset containing two sentences of 7 and 3 words, respectively. \nLet the maximum context window size be 2 \nand print all the center words and their context words.\n\n```{.python .input}\n#@tab all\ntiny_dataset = [list(range(7)), list(range(7, 10))]\nprint('dataset', tiny_dataset)\nfor center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n    print('center', center, 'has contexts', context)\n```\n\nWhen training on the PTB dataset,\nwe set the maximum context window size to 5. \nThe following extracts all the center words and their context words in the dataset.\n\n```{.python .input}\n#@tab all\nall_centers, all_contexts = get_centers_and_contexts(corpus, 5)\nf'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'\n```"
    },
    {
      "chunk_id": "442c20d8c4b9_0",
      "chapter": "word-embedding-dataset",
      "heading": "Negative Sampling",
      "text": "We use negative sampling for approximate training. To sample noise words according to \na predefined distribution,\nwe define the following `RandomGenerator` class,\nwhere the (possibly unnormalized) sampling distribution is passed\nvia the argument `sampling_weights`. ```{.python .input}\n#@tab all\n#@save\nclass RandomGenerator:\n    \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n    def __init__(self, sampling_weights):\n        # Exclude \n        self.population = list(range(1, len(sampling_weights) + 1))\n        self.sampling_weights = sampling_weights\n        self.candidates = []\n        self.i = 0\n\n    def draw(self):\n        if self.i == len(self.candidates):\n            # Cache `k` random sampling results\n            self.candidates = random.choices(\n                self.population, self.sampling_weights, k=10000)\n            self.i = 0\n        self.i += 1\n        return self.candidates[self.i - 1]\n```\n\nFor example, \nwe can draw 10 random variables $X$\namong indices 1, 2, and 3\nwith sampling probabilities $P(X=1)=2/9, P(X=2)=3/9$, and $P(X=3)=4/9$ as follows. ```{.python .input}\n#@tab mxnet\ngenerator = RandomGenerator([2, 3, 4])\n[generator.draw() for _ in range(10)]\n```\n\nFor a pair of center word and context word, \nwe randomly sample `K` (5 in the experiment) noise words. According to the suggestions in the word2vec paper,\nthe sampling probability $P(w)$ of \na noise word $w$\nis \nset to its relative frequency \nin the dictionary\nraised to \nthe power of 0.75 :cite:`Mikolov.Sutskever.Chen.ea.2013`. ```{.python .input}\n#@tab all\n#@save\ndef get_negatives(all_contexts, vocab, counter, K):\n    \"\"\"Return noise words in negative sampling.\"\"\"\n    # Sampling weights for words with indices 1, 2, ..."
    },
    {
      "chunk_id": "442c20d8c4b9_1",
      "chapter": "word-embedding-dataset",
      "heading": "Negative Sampling",
      "text": "```{.python .input}\n#@tab all\n#@save\ndef get_negatives(all_contexts, vocab, counter, K):\n    \"\"\"Return noise words in negative sampling.\"\"\"\n    # Sampling weights for words with indices 1, 2, ... (index 0 is the\n    # excluded unknown token) in the vocabulary\n    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n                        for i in range(1, len(vocab))]\n    all_negatives, generator = [], RandomGenerator(sampling_weights)\n    for contexts in all_contexts:\n        negatives = []\n        while len(negatives) < len(contexts) * K:\n            neg = generator.draw()\n            # Noise words cannot be context words\n            if neg not in contexts:\n                negatives.append(neg)\n        all_negatives.append(negatives)\n    return all_negatives\n\nall_negatives = get_negatives(all_contexts, vocab, counter, 5)\n```"
    },
    {
      "chunk_id": "b0a5d3bc0ed5_0",
      "chapter": "word-embedding-dataset",
      "heading": "Loading Training Examples in Minibatches",
      "text": ":label:`subsec_word2vec-minibatch-loading`\n\nAfter\nall the center words\ntogether with their\ncontext words and sampled noise words are extracted,\nthey will be transformed into \nminibatches of examples\nthat can be iteratively loaded\nduring training. In a minibatch,\nthe $i^\\textrm{th}$ example includes a center word\nand its $n_i$ context words and $m_i$ noise words. Due to varying context window sizes,\n$n_i+m_i$ varies for different $i$. Thus,\nfor each example\nwe concatenate its context words and noise words in \nthe `contexts_negatives` variable,\nand pad zeros until the concatenation length\nreaches $\\max_i n_i+m_i$ (`max_len`). To exclude paddings\nin the calculation of the loss,\nwe define a mask variable `masks`. There is a one-to-one correspondence\nbetween elements in `masks` and elements in `contexts_negatives`,\nwhere zeros (otherwise ones) in `masks` correspond to paddings in `contexts_negatives`. To distinguish between positive and negative examples,\nwe separate context words from noise words in  `contexts_negatives` via a `labels` variable. Similar to `masks`,\nthere is also a one-to-one correspondence\nbetween elements in `labels` and elements in `contexts_negatives`,\nwhere ones (otherwise zeros) in `labels` correspond to context words (positive examples) in `contexts_negatives`. The above idea is implemented in the following `batchify` function. Its input `data` is a list with length\nequal to the batch size,\nwhere each element is an example\nconsisting of\nthe center word `center`, its context words `context`, and its noise words `negative`. This function returns \na minibatch that can be loaded for calculations \nduring training,\nsuch as including the mask variable."
    },
    {
      "chunk_id": "b0a5d3bc0ed5_1",
      "chapter": "word-embedding-dataset",
      "heading": "Loading Training Examples in Minibatches",
      "text": "This function returns \na minibatch that can be loaded for calculations \nduring training,\nsuch as including the mask variable. ```{.python .input}\n#@tab all\n#@save\ndef batchify(data):\n    \"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\n    max_len = max(len(c) + len(n) for _, c, n in data)\n    centers, contexts_negatives, masks, labels = [], [], [], []\n    for center, context, negative in data:\n        cur_len = len(context) + len(negative)\n        centers += [center]\n        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n    return (d2l.reshape(d2l.tensor(centers), (-1, 1)), d2l.tensor(\n        contexts_negatives), d2l.tensor(masks), d2l.tensor(labels))\n```\n\nLet's test this function using a minibatch of two examples. ```{.python .input}\n#@tab all\nx_1 = (1, [2, 2], [3, 3, 3, 3])\nx_2 = (1, [2, 2, 2], [3, 3])\nbatch = batchify((x_1, x_2))\n\nnames = ['centers', 'contexts_negatives', 'masks', 'labels']\nfor name, data in zip(names, batch):\n    print(name, '=', data)\n```"
    },
    {
      "chunk_id": "c024f801b71a_0",
      "chapter": "word-embedding-dataset",
      "heading": "Putting It All Together",
      "text": "Last, we define the `load_data_ptb` function that reads the PTB dataset and returns the data iterator and the vocabulary."
    },
    {
      "chunk_id": "c024f801b71a_1",
      "chapter": "word-embedding-dataset",
      "heading": "Putting It All Together",
      "text": "Last, we define the `load_data_ptb` function that reads the PTB dataset and returns the data iterator and the vocabulary. ```{.python .input}\n#@tab mxnet\n#@save\ndef load_data_ptb(batch_size, max_window_size, num_noise_words):\n    \"\"\"Download the PTB dataset and then load it into memory.\"\"\"\n    sentences = read_ptb()\n    vocab = d2l.Vocab(sentences, min_freq=10)\n    subsampled, counter = subsample(sentences, vocab)\n    corpus = [vocab[line] for line in subsampled]\n    all_centers, all_contexts = get_centers_and_contexts(\n        corpus, max_window_size)\n    all_negatives = get_negatives(\n        all_contexts, vocab, counter, num_noise_words)\n    dataset = gluon.data.ArrayDataset(\n        all_centers, all_contexts, all_negatives)\n    data_iter = gluon.data.DataLoader(\n        dataset, batch_size, shuffle=True,batchify_fn=batchify,\n        num_workers=d2l.get_dataloader_workers())\n    return data_iter, vocab\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef load_data_ptb(batch_size, max_window_size, num_noise_words):\n    \"\"\"Download the PTB dataset and then load it into memory.\"\"\"\n    num_workers = d2l.get_dataloader_workers()\n    sentences = read_ptb()\n    vocab = d2l.Vocab(sentences, min_freq=10)\n    subsampled, counter = subsample(sentences, vocab)\n    corpus = [vocab[line] for line in subsampled]\n    all_centers, all_contexts = get_centers_and_contexts(\n        corpus, max_window_size)\n    all_negatives = get_negatives(\n        all_contexts, vocab, counter, num_noise_words)\n\n    class PTBDataset(torch.utils.data.Dataset):\n        def __init__(self, centers, contexts, negatives):\n            assert len(centers) == len(contexts) == len(negatives)\n            self.centers = centers\n            self.contexts = contexts\n            self.negatives = negatives\n\n        def __getitem__(self, index):\n            return (self.centers[index], self.contexts[index],\n                    self.negatives[index])\n\n        def __len__(self):\n            return len(self.centers)\n\n    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n\n    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,\n                                      collate_fn=batchify,\n                                      num_workers=num_workers)\n    return data_iter, vocab\n```\n\nLet's print the first minibatch of the data iterator."
    },
    {
      "chunk_id": "c024f801b71a_2",
      "chapter": "word-embedding-dataset",
      "heading": "Putting It All Together",
      "text": "```{.python .input}\n#@tab all\ndata_iter, vocab = load_data_ptb(512, 5, 5)\nfor batch in data_iter:\n    for name, data in zip(names, batch):\n        print(name, 'shape:', data.shape)\n    break\n```"
    },
    {
      "chunk_id": "39d56f369f99_0",
      "chapter": "word-embedding-dataset",
      "heading": "Summary",
      "text": "* High-frequency words may not be so useful in training. We can subsample them for speedup in training.\n* For computational efficiency, we load examples in minibatches. We can define other variables to distinguish paddings from non-paddings, and positive examples from negative ones."
    },
    {
      "chunk_id": "394e4d191dfe_0",
      "chapter": "word-embedding-dataset",
      "heading": "Exercises",
      "text": "1. How does the running time of code in this section changes if not using subsampling?\n1. The `RandomGenerator` class caches `k` random sampling results. Set `k` to other values and see how it affects the data loading speed.\n1. What other hyperparameters in the code of this section may affect the data loading speed?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/383)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1330)\n:end_tab:"
    },
    {
      "chunk_id": "9d7380b94c84_0",
      "chapter": "word2vec-pretraining",
      "heading": "word2vec-pretraining",
      "text": "# Pretraining word2vec\n:label:`sec_word2vec_pretraining`\n\n\nWe go on to implement the skip-gram\nmodel defined in\n:numref:`sec_word2vec`.\nThen\nwe will pretrain word2vec using negative sampling\non the PTB dataset.\nFirst of all,\nlet's obtain the data iterator\nand the vocabulary for this dataset\nby calling the `d2l.load_data_ptb`\nfunction, which was described in :numref:`sec_word2vec_data`\n\n```{.python .input}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import autograd, gluon, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n\nbatch_size, max_window_size, num_noise_words = 512, 5, 5\ndata_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,\n                                     num_noise_words)\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport math\nimport torch\nfrom torch import nn\n\nbatch_size, max_window_size, num_noise_words = 512, 5, 5\ndata_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,\n                                     num_noise_words)\n```"
    },
    {
      "chunk_id": "1a7df4f8440c_0",
      "chapter": "word2vec-pretraining",
      "heading": "The Skip-Gram Model",
      "text": "We implement the skip-gram model\nby using embedding layers and batch matrix multiplications.\nFirst, let's review\nhow embedding layers work."
    },
    {
      "chunk_id": "514e37ee3b6c_0",
      "chapter": "word2vec-pretraining",
      "heading": "Embedding Layer",
      "text": "As described in :numref:`sec_seq2seq`,\nan embedding layer\nmaps a token's index to its feature vector.\nThe weight of this layer\nis a matrix whose number of rows equals to\nthe dictionary size (`input_dim`) and\nnumber of columns equals to\nthe vector dimension for each token (`output_dim`).\nAfter a word embedding model is trained,\nthis weight is what we need.\n\n```{.python .input}\n#@tab mxnet\nembed = nn.Embedding(input_dim=20, output_dim=4)\nembed.initialize()\nembed.weight\n```\n\n```{.python .input}\n#@tab pytorch\nembed = nn.Embedding(num_embeddings=20, embedding_dim=4)\nprint(f'Parameter embedding_weight ({embed.weight.shape}, '\n      f'dtype={embed.weight.dtype})')\n```\n\nThe input of an embedding layer is the\nindex of a token (word).\nFor any token index $i$,\nits vector representation\ncan be obtained from\nthe $i^\\textrm{th}$ row of the weight matrix\nin the embedding layer.\nSince the vector dimension (`output_dim`)\nwas set to 4,\nthe embedding layer\nreturns vectors with shape (2, 3, 4)\nfor a minibatch of token indices with shape\n(2, 3).\n\n```{.python .input}\n#@tab all\nx = d2l.tensor([[1, 2, 3], [4, 5, 6]])\nembed(x)\n```"
    },
    {
      "chunk_id": "aefd0f7e01b7_0",
      "chapter": "word2vec-pretraining",
      "heading": "Defining the Forward Propagation",
      "text": "In the forward propagation,\nthe input of the skip-gram model\nincludes\nthe center word indices `center`\nof shape (batch size, 1)\nand\nthe concatenated context and noise word indices `contexts_and_negatives`\nof shape (batch size, `max_len`),\nwhere `max_len`\nis defined\nin :numref:`subsec_word2vec-minibatch-loading`.\nThese two variables are first transformed from the\ntoken indices into vectors via the embedding layer,\nthen their batch matrix multiplication\n(described in :numref:`subsec_batch_dot`)\nreturns\nan output of shape (batch size, 1, `max_len`).\nEach element in the output is the dot product of\na center word vector and a context or noise word vector.\n\n```{.python .input}\n#@tab mxnet\ndef skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n    v = embed_v(center)\n    u = embed_u(contexts_and_negatives)\n    pred = npx.batch_dot(v, u.swapaxes(1, 2))\n    return pred\n```\n\n```{.python .input}\n#@tab pytorch\ndef skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n    v = embed_v(center)\n    u = embed_u(contexts_and_negatives)\n    pred = torch.bmm(v, u.permute(0, 2, 1))\n    return pred\n```\n\nLet's print the output shape of this `skip_gram` function for some example inputs.\n\n```{.python .input}\n#@tab mxnet\nskip_gram(np.ones((2, 1)), np.ones((2, 4)), embed, embed).shape\n```\n\n```{.python .input}\n#@tab pytorch\nskip_gram(torch.ones((2, 1), dtype=torch.long),\n          torch.ones((2, 4), dtype=torch.long), embed, embed).shape\n```"
    },
    {
      "chunk_id": "5561b490afaa_0",
      "chapter": "word2vec-pretraining",
      "heading": "Training",
      "text": "Before training the skip-gram model with negative sampling,\nlet's first define its loss function."
    },
    {
      "chunk_id": "89aca26d791d_0",
      "chapter": "word2vec-pretraining",
      "heading": "Binary Cross-Entropy Loss",
      "text": "According to the definition of the loss function\nfor negative sampling in :numref:`subsec_negative-sampling`, \nwe will use \nthe binary cross-entropy loss.\n\n```{.python .input}\n#@tab mxnet\nloss = gluon.loss.SigmoidBCELoss()\n```\n\n```{.python .input}\n#@tab pytorch\nclass SigmoidBCELoss(nn.Module):\n    # Binary cross-entropy loss with masking\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, inputs, target, mask=None):\n        out = nn.functional.binary_cross_entropy_with_logits(\n            inputs, target, weight=mask, reduction=\"none\")\n        return out.mean(dim=1)\n\nloss = SigmoidBCELoss()\n```\n\nRecall our descriptions\nof the mask variable\nand the label variable in\n:numref:`subsec_word2vec-minibatch-loading`.\nThe following\ncalculates the \nbinary cross-entropy loss\nfor the given variables.\n\n```{.python .input}\n#@tab all\npred = d2l.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)\nlabel = d2l.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\nmask = d2l.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\nloss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)\n```\n\nBelow shows\nhow the above results are calculated\n(in a less efficient way)\nusing the\nsigmoid activation function\nin the binary cross-entropy loss.\nWe can consider \nthe two outputs as\ntwo normalized losses\nthat are averaged over non-masked predictions.\n\n```{.python .input}\n#@tab all\ndef sigmd(x):\n    return -math.log(1 / (1 + math.exp(-x)))\n\nprint(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\nprint(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')\n```"
    },
    {
      "chunk_id": "356a0c41355a_0",
      "chapter": "word2vec-pretraining",
      "heading": "Initializing Model Parameters",
      "text": "We define two embedding layers\nfor all the words in the vocabulary\nwhen they are used as center words\nand context words, respectively.\nThe word vector dimension\n`embed_size` is set to 100.\n\n```{.python .input}\n#@tab mxnet\nembed_size = 100\nnet = nn.Sequential()\nnet.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),\n        nn.Embedding(input_dim=len(vocab), output_dim=embed_size))\n```\n\n```{.python .input}\n#@tab pytorch\nembed_size = 100\nnet = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),\n                                 embedding_dim=embed_size),\n                    nn.Embedding(num_embeddings=len(vocab),\n                                 embedding_dim=embed_size))\n```"
    },
    {
      "chunk_id": "7820bca390ed_0",
      "chapter": "word2vec-pretraining",
      "heading": "Defining the Training Loop",
      "text": "The training loop is defined below. Because of the existence of padding, the calculation of the loss function is slightly different compared to the previous training functions. ```{.python .input}\n#@tab mxnet\ndef train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n    net.initialize(ctx=device, force_reinit=True)\n    trainer = gluon.Trainer(net.collect_params(), 'adam',\n                            {'learning_rate': lr})\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs])\n    # Sum of normalized losses, no. of normalized losses\n    metric = d2l.Accumulator(2)\n    for epoch in range(num_epochs):\n        timer, num_batches = d2l.Timer(), len(data_iter)\n        for i, batch in enumerate(data_iter):\n            center, context_negative, mask, label = [\n                data.as_in_ctx(device) for data in batch]\n            with autograd.record():\n                pred = skip_gram(center, context_negative, net[0], net[1])\n                l = (loss(pred.reshape(label.shape), label, mask) *\n                     mask.shape[1] / mask.sum(axis=1))\n            l.backward()\n            trainer.step(batch_size)\n            metric.add(l.sum(), l.size)\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[1],))\n    print(f'loss {metric[0] / metric[1]:.3f}, '\n          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')\n```\n\n```{.python .input}\n#@tab pytorch\ndef train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n    def init_weights(module):\n        if type(module) == nn.Embedding:\n            nn.init.xavier_uniform_(module.weight)\n    net.apply(init_weights)\n    net = net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs])\n    # Sum of normalized losses, no."
    },
    {
      "chunk_id": "7820bca390ed_1",
      "chapter": "word2vec-pretraining",
      "heading": "Defining the Training Loop",
      "text": "of normalized losses\n    metric = d2l.Accumulator(2)\n    for epoch in range(num_epochs):\n        timer, num_batches = d2l.Timer(), len(data_iter)\n        for i, batch in enumerate(data_iter):\n            optimizer.zero_grad()\n            center, context_negative, mask, label = [\n                data.to(device) for data in batch]\n\n            pred = skip_gram(center, context_negative, net[0], net[1])\n            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n                     / mask.sum(axis=1) * mask.shape[1])\n            l.sum().backward()\n            optimizer.step()\n            metric.add(l.sum(), l.numel())\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) / num_batches,\n                             (metric[0] / metric[1],))\n    print(f'loss {metric[0] / metric[1]:.3f}, '\n          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')\n```\n\nNow we can train a skip-gram model using negative sampling. ```{.python .input}\n#@tab all\nlr, num_epochs = 0.002, 5\ntrain(net, data_iter, lr, num_epochs)\n```"
    },
    {
      "chunk_id": "c79ceb89f269_0",
      "chapter": "word2vec-pretraining",
      "heading": "Applying Word Embeddings",
      "text": ":label:`subsec_apply-word-embed`\n\n\nAfter training the word2vec model,\nwe can use the cosine similarity\nof word vectors from the trained model\nto \nfind words from the dictionary\nthat are most semantically similar\nto an input word.\n\n```{.python .input}\n#@tab mxnet\ndef get_similar_tokens(query_token, k, embed):\n    W = embed.weight.data()\n    x = W[vocab[query_token]]\n    # Compute the cosine similarity. Add 1e-9 for numerical stability\n    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\n    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n    for i in topk[1:]:  # Remove the input words\n        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')\n\nget_similar_tokens('chip', 3, net[0])\n```\n\n```{.python .input}\n#@tab pytorch\ndef get_similar_tokens(query_token, k, embed):\n    W = embed.weight.data\n    x = W[vocab[query_token]]\n    # Compute the cosine similarity. Add 1e-9 for numerical stability\n    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\n                                      torch.sum(x * x) + 1e-9)\n    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n    for i in topk[1:]:  # Remove the input words\n        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')\n\nget_similar_tokens('chip', 3, net[0])\n```"
    },
    {
      "chunk_id": "bbf7638314f5_0",
      "chapter": "word2vec-pretraining",
      "heading": "Summary",
      "text": "* We can train a skip-gram model with negative sampling using embedding layers and the binary cross-entropy loss.\n* Applications of word embeddings include finding semantically similar words for a given word based on the cosine similarity of word vectors."
    },
    {
      "chunk_id": "2574778fcc30_0",
      "chapter": "word2vec-pretraining",
      "heading": "Exercises",
      "text": "1. Using the trained model, find semantically similar words for other input words. Can you improve the results by tuning hyperparameters?\n1. When a training corpus is huge, we often sample context words and noise words for the center words in the current minibatch *when updating model parameters*. In other words, the same center word may have different context words or noise words in different training epochs. What are the benefits of this method? Try to implement this training method.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/384)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1335)\n:end_tab:"
    },
    {
      "chunk_id": "ce5f1d555208_0",
      "chapter": "word2vec",
      "heading": "word2vec",
      "text": "# Word Embedding (word2vec)\n:label:`sec_word2vec`\n\n\nNatural language is a complex system used to express meanings.\nIn this system, words are the basic unit of the meaning.\nAs the name implies,\n*word vectors* are vectors used to represent words,\nand can also be considered as feature vectors or representations of words.\nThe technique of mapping words to real vectors\nis called *word embedding*.\nIn recent years,\nword embedding has gradually become\nthe basic knowledge of natural language processing."
    },
    {
      "chunk_id": "58137376bc04_0",
      "chapter": "word2vec",
      "heading": "One-Hot Vectors Are a Bad Choice",
      "text": "We used one-hot vectors to represent words (characters are words) in :numref:`sec_rnn-scratch`.\nSuppose that the number of different words in the dictionary (the dictionary size) is $N$,\nand each word corresponds to\na different integer (index) from $0$ to $N-1$.\nTo obtain the one-hot vector representation\nfor any word with index $i$,\nwe create a length-$N$ vector with all 0s\nand set the element at position $i$ to 1.\nIn this way, each word is represented as a vector of length $N$, and it\ncan be used directly by neural networks.\n\n\nAlthough one-hot word vectors are easy to construct,\nthey are usually not a good choice.\nA main reason is that one-hot word vectors cannot accurately express the similarity between different words, such as the *cosine similarity* that we often use.\nFor vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their cosine similarity is the cosine of the angle between them:\n\n\n$$\\frac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\in [-1, 1].$$\n\n\nSince the cosine similarity between one-hot vectors of any two different words is 0,\none-hot vectors cannot encode similarities among words."
    },
    {
      "chunk_id": "af4e125b27c8_0",
      "chapter": "word2vec",
      "heading": "Self-Supervised word2vec",
      "text": "The [word2vec](https://code.google.com/archive/p/word2vec/) tool was proposed to address the above issue.\nIt maps each word to a fixed-length vector, and  these vectors can better express the similarity and analogy relationship among different words.\nThe word2vec tool contains two models, namely *skip-gram* :cite:`Mikolov.Sutskever.Chen.ea.2013`  and *continuous bag of words* (CBOW) :cite:`Mikolov.Chen.Corrado.ea.2013`.\nFor semantically meaningful representations,\ntheir training relies on\nconditional probabilities\nthat can be viewed as predicting\nsome words using some of their surrounding words\nin corpora.\nSince supervision comes from the data without labels,\nboth skip-gram and continuous bag of words\nare self-supervised models.\n\nIn the following, we will introduce these two models and their training methods."
    },
    {
      "chunk_id": "0d7dbe60a949_0",
      "chapter": "word2vec",
      "heading": "The Skip-Gram Model",
      "text": ":label:`subsec_skip-gram`\n\nThe *skip-gram* model assumes that a word can be used to generate its surrounding words in a text sequence. Take the text sequence \"the\", \"man\", \"loves\", \"his\", \"son\" as an example. Let's choose \"loves\" as the *center word* and set the context window size to 2. As shown in :numref:`fig_skip_gram`,\ngiven the center word \"loves\",\nthe skip-gram model considers\nthe conditional probability for generating the *context words*: \"the\", \"man\", \"his\", and \"son\",\nwhich are no more than 2 words away from the center word:\n\n$$P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n\nAssume that\nthe context words are independently generated\ngiven the center word (i.e., conditional independence). In this case, the above conditional probability\ncan be rewritten as\n\n$$P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n\n![The skip-gram model considers the conditional probability of generating the surrounding context words given a center word.](../img/skip-gram.svg)\n:label:`fig_skip_gram`\n\nIn the skip-gram model, each word\nhas two $d$-dimensional-vector representations\nfor calculating conditional probabilities. More concretely,\nfor any word with index $i$ in the dictionary,\ndenote by $\\mathbf{v}_i\\in\\mathbb{R}^d$\nand $\\mathbf{u}_i\\in\\mathbb{R}^d$\nits two vectors\nwhen used as a *center* word and a *context* word, respectively. The conditional probability of generating any\ncontext word $w_o$ (with index $o$ in the dictionary) given the center word $w_c$ (with index $c$ in the dictionary) can be modeled by\na softmax operation on vector dot products:\n\n\n$$P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},$$\n:eqlabel:`eq_skip-gram-softmax`\n\nwhere the vocabulary index set $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$."
    },
    {
      "chunk_id": "0d7dbe60a949_1",
      "chapter": "word2vec",
      "heading": "The Skip-Gram Model",
      "text": "Given a text sequence of length $T$, where the word at time step $t$ is denoted as $w^{(t)}$. Assume that\ncontext words are independently generated\ngiven any center word. For context window size $m$,\nthe likelihood function of the skip-gram model\nis the probability of generating all context words\ngiven any center word:\n\n\n$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n\nwhere any time step that is less than $1$ or greater than $T$ can be omitted."
    },
    {
      "chunk_id": "1906db33e113_0",
      "chapter": "word2vec",
      "heading": "Training",
      "text": "The skip-gram model parameters are the center word vector and context word vector for each word in the vocabulary. In training, we learn the model parameters by maximizing the likelihood function (i.e., maximum likelihood estimation). This is equivalent to minimizing the following loss function:\n\n$$ - \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\textrm{log}\\, P(w^{(t+j)} \\mid w^{(t)}).$$\n\nWhen using stochastic gradient descent to minimize the loss,\nin each iteration\nwe can\nrandomly sample a shorter subsequence to calculate the (stochastic) gradient for this subsequence to update the model parameters. To calculate this (stochastic) gradient,\nwe need to obtain\nthe gradients of\nthe log conditional probability with respect to the center word vector and the context word vector. In general, according to :eqref:`eq_skip-gram-softmax`\nthe log conditional probability\ninvolving any pair of the center word $w_c$ and\nthe context word $w_o$ is\n\n\n$$\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$\n:eqlabel:`eq_skip-gram-log`\n\nThrough differentiation, we can obtain its gradient\nwith respect to the center word vector $\\mathbf{v}_c$ as\n\n$$\\begin{aligned}\\frac{\\partial \\textrm{log}\\, P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c}&= \\mathbf{u}_o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)\\mathbf{u}_j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\right) \\mathbf{u}_j\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j.\\end{aligned}$$\n:eqlabel:`eq_skip-gram-grad`\n\n\nNote that the calculation in :eqref:`eq_skip-gram-grad` requires the conditional probabilities of all words in the dictionary with $w_c$ as the center word. The gradients for the other word vectors can be obtained in the same way."
    },
    {
      "chunk_id": "1906db33e113_1",
      "chapter": "word2vec",
      "heading": "Training",
      "text": "The gradients for the other word vectors can be obtained in the same way. After training, for any word with index $i$ in the dictionary, we obtain both word vectors\n$\\mathbf{v}_i$ (as the center word) and $\\mathbf{u}_i$ (as the context word). In natural language processing applications, the center word vectors of the skip-gram model are typically\nused as the word representations."
    },
    {
      "chunk_id": "1ed3fe661891_0",
      "chapter": "word2vec",
      "heading": "The Continuous Bag of Words (CBOW) Model",
      "text": "The *continuous bag of words* (CBOW) model is similar to the skip-gram model. The major difference\nfrom the skip-gram model is that\nthe continuous bag of words model\nassumes that a center word is generated\nbased on its surrounding context words in the text sequence. For example,\nin the same text sequence \"the\", \"man\", \"loves\", \"his\", and \"son\", with \"loves\" as the center word and the context window size being 2,\nthe continuous bag of words model\nconsiders\nthe conditional probability of generating the center word \"loves\" based on the context words \"the\", \"man\", \"his\" and \"son\" (as shown in :numref:`fig_cbow`), which is\n\n$$P(\\textrm{\"loves\"}\\mid\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}).$$\n\n![The continuous bag of words model considers the conditional probability of generating the center word given its surrounding context words.](../img/cbow.svg)\n:label:`fig_cbow`\n\n\nSince there are multiple context words\nin the continuous bag of words model,\nthese context word vectors are averaged\nin the calculation of the conditional probability. Specifically,\nfor any word with index $i$ in the dictionary,\ndenote by $\\mathbf{v}_i\\in\\mathbb{R}^d$\nand $\\mathbf{u}_i\\in\\mathbb{R}^d$\nits two vectors\nwhen used as a *context* word and a *center* word\n(meanings are switched in the skip-gram model), respectively. The conditional probability of generating any\ncenter word $w_c$ (with index $c$ in the dictionary) given its surrounding context words $w_{o_1}, \\ldots, w_{o_{2m}}$ (with index $o_1, \\ldots, o_{2m}$ in the dictionary) can be modeled by\n\n\n\n$$P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\exp\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\exp\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}.$$\n:eqlabel:`fig_cbow-full`\n\n\nFor brevity, let $\\mathcal{W}_o= \\{w_{o_1}, \\ldots, w_{o_{2m}}\\}$ and $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}} \\right)/(2m)$."
    },
    {
      "chunk_id": "1ed3fe661891_1",
      "chapter": "word2vec",
      "heading": "The Continuous Bag of Words (CBOW) Model",
      "text": "Then :eqref:`fig_cbow-full` can be simplified as\n\n$$P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}.$$\n\nGiven a text sequence of length $T$, where the word at time step $t$ is denoted as $w^{(t)}$. For context window size $m$,\nthe likelihood function of the continuous bag of words model\nis the probability of generating all center words\ngiven their context words:\n\n\n$$ \\prod_{t=1}^{T}  P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$"
    },
    {
      "chunk_id": "1906db33e113_0",
      "chapter": "word2vec",
      "heading": "Training",
      "text": "Training continuous bag of words models\nis almost the same as\ntraining skip-gram models.\nThe maximum likelihood estimation of the\ncontinuous bag of words model is equivalent to minimizing the following loss function:\n\n\n\n$$  -\\sum_{t=1}^T  \\textrm{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n\nNotice that\n\n$$\\log\\,P(w_c \\mid \\mathcal{W}_o) = \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).$$\n\nThrough differentiation, we can obtain its gradient\nwith respect to any context word vector $\\mathbf{v}_{o_i}$($i = 1, \\ldots, 2m$)\nas\n\n\n$$\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} = \\frac{1}{2m} \\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^\\top \\bar{\\mathbf{v}}_o)\\mathbf{u}_j}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o)} \\right) = \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right).$$\n:eqlabel:`eq_cbow-gradient`\n\n\nThe gradients for the other word vectors can be obtained in the same way.\nUnlike the skip-gram model,\nthe continuous bag of words model\ntypically\nuses context word vectors as the word representations."
    },
    {
      "chunk_id": "2fb9826a5faf_0",
      "chapter": "word2vec",
      "heading": "Summary",
      "text": "* Word vectors are vectors used to represent words, and can also be considered as feature vectors or representations of words. The technique of mapping words to real vectors is called word embedding.\n* The word2vec tool contains both the skip-gram  and continuous bag of words models.\n* The skip-gram model assumes that a word can be used to generate its surrounding words in a text sequence; while the continuous bag of words model assumes that a center word is generated based on its surrounding context words."
    },
    {
      "chunk_id": "54341026b04d_0",
      "chapter": "word2vec",
      "heading": "Exercises",
      "text": "1. What is the computational complexity for calculating each gradient? What could be the issue if the dictionary size is huge?\n1. Some fixed phrases in English consist of multiple words, such as \"new york\". How to train their word vectors? Hint: see Section 4 in the word2vec paper :cite:`Mikolov.Sutskever.Chen.ea.2013`.\n1. Let's reflect on the word2vec design by taking the skip-gram model as an example. What is the relationship between the dot product of two word vectors in the skip-gram model and the cosine similarity? For a pair of words with similar semantics, why may the cosine similarity of their word vectors (trained by the skip-gram model) be high?\n\n[Discussions](https://discuss.d2l.ai/t/381)"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Notation\n:label:`chap_notation`\n\nThroughout this book, we adhere \nto the following notational conventions.\nNote that some of these symbols are placeholders,\nwhile others refer to specific objects.\nAs a general rule of thumb, \nthe indefinite article \"a\" often indicates\nthat the symbol is a placeholder\nand that similarly formatted symbols\ncan denote other objects of the same type.\nFor example, \"$x$: a scalar\" means \nthat lowercased letters generally\nrepresent scalar values,\nbut \"$\\mathbb{Z}$: the set of integers\"\nrefers specifically to the symbol $\\mathbb{Z}$."
    },
    {
      "chunk_id": "9da8f0ed3021_0",
      "chapter": "index",
      "heading": "Numerical Objects",
      "text": "* $x$: a scalar\n* $\\mathbf{x}$: a vector\n* $\\mathbf{X}$: a matrix\n* $\\mathsf{X}$: a general tensor\n* $\\mathbf{I}$: the identity matrix (of some given dimension), i.e., a square matrix with $1$ on all diagonal entries and $0$ on all off-diagonals\n* $x_i$, $[\\mathbf{x}]_i$: the $i^\\textrm{th}$ element of vector $\\mathbf{x}$\n* $x_{ij}$, $x_{i,j}$,$[\\mathbf{X}]_{ij}$, $[\\mathbf{X}]_{i,j}$: the element of matrix $\\mathbf{X}$ at row $i$ and column $j$."
    },
    {
      "chunk_id": "a66faa1f0f8f_0",
      "chapter": "index",
      "heading": "Set Theory",
      "text": "* $\\mathcal{X}$: a set\n* $\\mathbb{Z}$: the set of integers\n* $\\mathbb{Z}^+$: the set of positive integers\n* $\\mathbb{R}$: the set of real numbers\n* $\\mathbb{R}^n$: the set of $n$-dimensional vectors of real numbers\n* $\\mathbb{R}^{a\\times b}$: The set of matrices of real numbers with $a$ rows and $b$ columns\n* $|\\mathcal{X}|$: cardinality (number of elements) of set $\\mathcal{X}$\n* $\\mathcal{A}\\cup\\mathcal{B}$: union of sets $\\mathcal{A}$ and $\\mathcal{B}$\n* $\\mathcal{A}\\cap\\mathcal{B}$: intersection of sets $\\mathcal{A}$ and $\\mathcal{B}$\n* $\\mathcal{A}\\setminus\\mathcal{B}$: set subtraction of $\\mathcal{B}$ from $\\mathcal{A}$ (contains only those elements of $\\mathcal{A}$ that do not belong to $\\mathcal{B}$)"
    },
    {
      "chunk_id": "447ca01a0449_0",
      "chapter": "index",
      "heading": "Functions and Operators",
      "text": "* $f(\\cdot)$: a function\n* $\\log(\\cdot)$: the natural logarithm (base $e$)\n* $\\log_2(\\cdot)$: logarithm to base $2$\n* $\\exp(\\cdot)$: the exponential function\n* $\\mathbf{1}(\\cdot)$: the indicator function; evaluates to $1$ if the boolean argument is true, and $0$ otherwise\n* $\\mathbf{1}_{\\mathcal{X}}(z)$: the set-membership indicator function; evaluates to $1$ if the element $z$ belongs to the set $\\mathcal{X}$ and $0$ otherwise\n* $\\mathbf{(\\cdot)}^\\top$: transpose of a vector or a matrix\n* $\\mathbf{X}^{-1}$: inverse of matrix $\\mathbf{X}$\n* $\\odot$: Hadamard (elementwise) product\n* $[\\cdot, \\cdot]$: concatenation\n* $\\|\\cdot\\|_p$: $\\ell_p$ norm\n* $\\|\\cdot\\|$: $\\ell_2$ norm\n* $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$: inner (dot) product of vectors $\\mathbf{x}$ and $\\mathbf{y}$\n* $\\sum$: summation over a collection of elements\n* $\\prod$: product over a collection of elements\n* $\\stackrel{\\textrm{def}}{=}$: an equality asserted as a definition of the symbol on the left-hand side"
    },
    {
      "chunk_id": "0f63bebead02_0",
      "chapter": "index",
      "heading": "Calculus",
      "text": "* $\\frac{dy}{dx}$: derivative of $y$ with respect to $x$\n* $\\frac{\\partial y}{\\partial x}$: partial derivative of $y$ with respect to $x$\n* $\\nabla_{\\mathbf{x}} y$: gradient of $y$ with respect to $\\mathbf{x}$\n* $\\int_a^b f(x) \\;dx$: definite integral of $f$ from $a$ to $b$ with respect to $x$\n* $\\int f(x) \\;dx$: indefinite integral of $f$ with respect to $x$"
    },
    {
      "chunk_id": "73117253f539_0",
      "chapter": "index",
      "heading": "Probability and Information Theory",
      "text": "* $X$: a random variable\n* $P$: a probability distribution\n* $X \\sim P$: the random variable $X$ follows distribution $P$\n* $P(X=x)$: the probability assigned to the event where random variable $X$ takes value $x$\n* $P(X \\mid Y)$: the conditional probability distribution of $X$ given $Y$\n* $p(\\cdot)$: a probability density function (PDF) associated with distribution $P$\n* ${E}[X]$: expectation of a random variable $X$\n* $X \\perp Y$: random variables $X$ and $Y$ are independent\n* $X \\perp Y \\mid Z$: random variables  $X$  and  $Y$ are conditionally independent given $Z$\n* $\\sigma_X$: standard deviation of random variable $X$\n* $\\textrm{Var}(X)$: variance of random variable $X$, equal to $\\sigma^2_X$\n* $\\textrm{Cov}(X, Y)$: covariance of random variables $X$ and $Y$\n* $\\rho(X, Y)$: the Pearson correlation coefficient between $X$ and $Y$, equals $\\frac{\\textrm{Cov}(X, Y)}{\\sigma_X \\sigma_Y}$\n* $H(X)$: entropy of random variable $X$\n* $D_{\\textrm{KL}}(P\\|Q)$: the KL-divergence (or relative entropy) from distribution $Q$ to distribution $P$\n\n\n\n[Discussions](https://discuss.d2l.ai/t/25)"
    },
    {
      "chunk_id": "742950a21cae_0",
      "chapter": "adadelta",
      "heading": "adadelta",
      "text": "# Adadelta\n:label:`sec_adadelta`\n\nAdadelta is yet another variant of AdaGrad (:numref:`sec_adagrad`). The main difference lies in the fact that it decreases the amount by which the learning rate is adaptive to coordinates. Moreover, traditionally it referred to as not having a learning rate since it uses the amount of change itself as calibration for future change. The algorithm was proposed in :citet:`Zeiler.2012`. It is fairly straightforward, given the discussion of previous algorithms so far."
    },
    {
      "chunk_id": "453a4c6a23a7_0",
      "chapter": "adadelta",
      "heading": "The Algorithm",
      "text": "In a nutshell, Adadelta uses two state variables, $\\mathbf{s}_t$ to store a leaky average of the second moment of the gradient and $\\Delta\\mathbf{x}_t$ to store a leaky average of the second moment of the change of parameters in the model itself. Note that we use the original notation and naming of the authors for compatibility with other publications and implementations (there is no other real reason why one should use different Greek variables to indicate a parameter serving the same purpose in momentum, Adagrad, RMSProp, and Adadelta).\n\nHere are the technical details of Adadelta. Given the parameter du jour is $\\rho$, we obtain the following leaky updates similarly to :numref:`sec_rmsprop`:\n\n$$\\begin{aligned}\n    \\mathbf{s}_t & = \\rho \\mathbf{s}_{t-1} + (1 - \\rho) \\mathbf{g}_t^2.\n\\end{aligned}$$\n\nThe difference to :numref:`sec_rmsprop` is that we perform updates with the rescaled gradient $\\mathbf{g}_t'$, i.e.,\n\n$$\\begin{aligned}\n    \\mathbf{x}_t  & = \\mathbf{x}_{t-1} - \\mathbf{g}_t'. \\\\\n\\end{aligned}$$\n\nSo what is the rescaled gradient $\\mathbf{g}_t'$? We can calculate it as follows:\n\n$$\\begin{aligned}\n    \\mathbf{g}_t' & = \\frac{\\sqrt{\\Delta\\mathbf{x}_{t-1} + \\epsilon}}{\\sqrt{{\\mathbf{s}_t + \\epsilon}}} \\odot \\mathbf{g}_t, \\\\\n\\end{aligned}$$\n\nwhere $\\Delta \\mathbf{x}_{t-1}$ is the leaky average of the squared rescaled gradients $\\mathbf{g}_t'$. We initialize $\\Delta \\mathbf{x}_{0}$ to be $0$ and update it at each step with $\\mathbf{g}_t'$, i.e.,\n\n$$\\begin{aligned}\n    \\Delta \\mathbf{x}_t & = \\rho \\Delta\\mathbf{x}_{t-1} + (1 - \\rho) {\\mathbf{g}_t'}^2,\n\\end{aligned}$$\n\nand $\\epsilon$ (a small value such as $10^{-5}$) is added to maintain numerical stability."
    },
    {
      "chunk_id": "2ae2b9c8441d_0",
      "chapter": "adadelta",
      "heading": "Implementation",
      "text": "Adadelta needs to maintain two state variables for each variable, $\\mathbf{s}_t$ and $\\Delta\\mathbf{x}_t$. This yields the following implementation."
    },
    {
      "chunk_id": "2ae2b9c8441d_1",
      "chapter": "adadelta",
      "heading": "Implementation",
      "text": "Adadelta needs to maintain two state variables for each variable, $\\mathbf{s}_t$ and $\\Delta\\mathbf{x}_t$. This yields the following implementation. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n\ndef init_adadelta_states(feature_dim):\n    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    delta_w, delta_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    return ((s_w, delta_w), (s_b, delta_b))\n\ndef adadelta(params, states, hyperparams):\n    rho, eps = hyperparams['rho'], 1e-5\n    for p, (s, delta) in zip(params, states):\n        # In-place updates via [:]\n        s[:] = rho * s + (1 - rho) * np.square(p.grad)\n        g = (np.sqrt(delta + eps) / np.sqrt(s + eps)) * p.grad\n        p[:] -= g\n        delta[:] = rho * delta + (1 - rho) * g * g\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n\ndef init_adadelta_states(feature_dim):\n    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    delta_w, delta_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    return ((s_w, delta_w), (s_b, delta_b))\n\ndef adadelta(params, states, hyperparams):\n    rho, eps = hyperparams['rho'], 1e-5\n    for p, (s, delta) in zip(params, states):\n        with torch.no_grad():\n            # In-place updates via [:]\n            s[:] = rho * s + (1 - rho) * torch.square(p.grad)\n            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad\n            p[:] -= g\n            delta[:] = rho * delta + (1 - rho) * g * g\n        p.grad.data.zero_()\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n\ndef init_adadelta_states(feature_dim):\n    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))\n    s_b = tf.Variable(d2l.zeros(1))\n    delta_w = tf.Variable(d2l.zeros((feature_dim, 1)))\n    delta_b = tf.Variable(d2l.zeros(1))\n    return ((s_w, delta_w), (s_b, delta_b))\n\ndef adadelta(params, grads, states, hyperparams):\n    rho, eps = hyperparams['rho'], 1e-5\n    for p, (s, delta), grad in zip(params, states, grads):\n        s[:].assign(rho * s + (1 - rho) * tf.math.square(grad))\n        g = (tf.math.sqrt(delta + eps) / tf.math.sqrt(s + eps)) * grad\n        p[:].assign(p - g)\n        delta[:].assign(rho * delta + (1 - rho) * g * g)\n```\n\nChoosing $\\rho = 0.9$ amounts to a half-life time of 10 for each parameter update."
    },
    {
      "chunk_id": "2ae2b9c8441d_2",
      "chapter": "adadelta",
      "heading": "Implementation",
      "text": "This tends to work quite well. We get the following behavior. ```{.python .input}\n#@tab all\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(adadelta, init_adadelta_states(feature_dim),\n               {'rho': 0.9}, data_iter, feature_dim);\n```\n\nFor a concise implementation we simply use the Adadelta algorithm from high-level APIs. This yields the following one-liner for a much more compact invocation. ```{.python .input}\n#@tab mxnet\nd2l.train_concise_ch11('adadelta', {'rho': 0.9}, data_iter)\n```\n\n```{.python .input}\n#@tab pytorch\ntrainer = torch.optim.Adadelta\nd2l.train_concise_ch11(trainer, {'rho': 0.9}, data_iter)\n```\n\n```{.python .input}\n#@tab tensorflow\n# adadelta is not converging at default learning rate\n# but it is converging at lr = 5.0\ntrainer = tf.keras.optimizers.Adadelta\nd2l.train_concise_ch11(trainer, {'learning_rate':5.0, 'rho': 0.9}, data_iter)\n```"
    },
    {
      "chunk_id": "9191a14684ce_0",
      "chapter": "adadelta",
      "heading": "Summary",
      "text": "* Adadelta has no learning rate parameter. Instead, it uses the rate of change in the parameters itself to adapt the learning rate.\n* Adadelta requires two state variables to store the second moments of gradient and the change in parameters.\n* Adadelta uses leaky averages to keep a running estimate of the appropriate statistics."
    },
    {
      "chunk_id": "f5ef441ff010_0",
      "chapter": "adadelta",
      "heading": "Exercises",
      "text": "1. Adjust the value of $\\rho$. What happens?\n1. Show how to implement the algorithm without the use of $\\mathbf{g}_t'$. Why might this be a good idea?\n1. Is Adadelta really learning rate free? Could you find optimization problems that break Adadelta?\n1. Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/357)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1076)\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1077)\n:end_tab:"
    },
    {
      "chunk_id": "866d6de033c1_0",
      "chapter": "adagrad",
      "heading": "adagrad",
      "text": "# Adagrad\n:label:`sec_adagrad`\n\nLet's begin by considering learning problems with features that occur infrequently."
    },
    {
      "chunk_id": "91ec81906240_0",
      "chapter": "adagrad",
      "heading": "Sparse Features and Learning Rates",
      "text": "Imagine that we are training a language model. To get good accuracy we typically want to decrease the learning rate as we keep on training, usually at a rate of $\\mathcal{O}(t^{-\\frac{1}{2}})$ or slower. Now consider a model training on sparse features, i.e., features that occur only infrequently. This is common for natural language, e.g., it is a lot less likely that we will see the word *preconditioning* than *learning*. However, it is also common in other areas such as computational advertising and personalized collaborative filtering. After all, there are many things that are of interest only for a small number of people. Parameters associated with infrequent features only receive meaningful updates whenever these features occur. Given a decreasing learning rate we might end up in a situation where the parameters for common features converge rather quickly to their optimal values, whereas for infrequent features we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate either decreases too slowly for frequent features or too quickly for infrequent ones. A possible hack to redress this issue would be to count the number of times we see a particular feature and to use this as a clock for adjusting learning rates. That is, rather than choosing a learning rate of the form $\\eta = \\frac{\\eta_0}{\\sqrt{t + c}}$ we could use $\\eta_i = \\frac{\\eta_0}{\\sqrt{s(i, t) + c}}$. Here $s(i, t)$ counts the number of nonzeros for feature $i$ that we have observed up to time $t$. This is actually quite easy to implement at no meaningful overhead. However, it fails whenever we do not quite have sparsity but rather just data where the gradients are often very small and only rarely large. After all, it is unclear where one would draw the line between something that qualifies as an observed feature or not."
    },
    {
      "chunk_id": "91ec81906240_1",
      "chapter": "adagrad",
      "heading": "Sparse Features and Learning Rates",
      "text": "After all, it is unclear where one would draw the line between something that qualifies as an observed feature or not. Adagrad by :citet:`Duchi.Hazan.Singer.2011` addresses this by replacing the rather crude counter $s(i, t)$ by an aggregate of the squares of previously observed gradients. In particular, it uses $s(i, t+1) = s(i, t) + \\left(\\partial_i f(\\mathbf{x})\\right)^2$ as a means to adjust the learning rate. This has two benefits: first, we no longer need to decide just when a gradient is large enough. Second, it scales automatically with the magnitude of the gradients. Coordinates that routinely correspond to large gradients are scaled down significantly, whereas others with small gradients receive a much more gentle treatment. In practice this leads to a very effective optimization procedure for computational advertising and related problems. But this hides some of the additional benefits inherent in Adagrad that are best understood in the context of preconditioning."
    },
    {
      "chunk_id": "e7530f6b73e1_0",
      "chapter": "adagrad",
      "heading": "Preconditioning",
      "text": "Convex optimization problems are good for analyzing the characteristics of algorithms. After all, for most nonconvex problems it is difficult to derive meaningful theoretical guarantees, but *intuition* and *insight* often carry over. Let's look at the problem of minimizing $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} + \\mathbf{c}^\\top \\mathbf{x} + b$. As we saw in :numref:`sec_momentum`, it is possible to rewrite this problem in terms of its eigendecomposition $\\mathbf{Q} = \\mathbf{U}^\\top \\boldsymbol{\\Lambda} \\mathbf{U}$ to arrive at a much simplified problem where each coordinate can be solved individually:\n\n$$f(\\mathbf{x}) = \\bar{f}(\\bar{\\mathbf{x}}) = \\frac{1}{2} \\bar{\\mathbf{x}}^\\top \\boldsymbol{\\Lambda} \\bar{\\mathbf{x}} + \\bar{\\mathbf{c}}^\\top \\bar{\\mathbf{x}} + b.$$\n\nHere we used $\\bar{\\mathbf{x}} = \\mathbf{U} \\mathbf{x}$ and consequently $\\bar{\\mathbf{c}} = \\mathbf{U} \\mathbf{c}$. The modified problem has as its minimizer $\\bar{\\mathbf{x}} = -\\boldsymbol{\\Lambda}^{-1} \\bar{\\mathbf{c}}$ and minimum value $-\\frac{1}{2} \\bar{\\mathbf{c}}^\\top \\boldsymbol{\\Lambda}^{-1} \\bar{\\mathbf{c}} + b$. This is much easier to compute since $\\boldsymbol{\\Lambda}$ is a diagonal matrix containing the eigenvalues of $\\mathbf{Q}$. If we perturb $\\mathbf{c}$ slightly we would hope to find only slight changes in the minimizer of $f$. Unfortunately this is not the case. While slight changes in $\\mathbf{c}$ lead to equally slight changes in $\\bar{\\mathbf{c}}$, this is not the case for the minimizer of $f$ (and of $\\bar{f}$ respectively). Whenever the eigenvalues $\\boldsymbol{\\Lambda}_i$ are large we will see only small changes in $\\bar{x}_i$ and in the minimum of $\\bar{f}$. Conversely, for small $\\boldsymbol{\\Lambda}_i$ changes in $\\bar{x}_i$ can be dramatic. The ratio between the largest and the smallest eigenvalue is called the condition number of an optimization problem."
    },
    {
      "chunk_id": "e7530f6b73e1_1",
      "chapter": "adagrad",
      "heading": "Preconditioning",
      "text": "Conversely, for small $\\boldsymbol{\\Lambda}_i$ changes in $\\bar{x}_i$ can be dramatic. The ratio between the largest and the smallest eigenvalue is called the condition number of an optimization problem. $$\\kappa = \\frac{\\boldsymbol{\\Lambda}_1}{\\boldsymbol{\\Lambda}_d}.$$\n\nIf the condition number $\\kappa$ is large, it is difficult to solve the optimization problem accurately. We need to ensure that we are careful in getting a large dynamic range of values right. Our analysis leads to an obvious, albeit somewhat naive question: couldn't we simply \"fix\" the problem by distorting the space such that all eigenvalues are $1$. In theory this is quite easy: we only need the eigenvalues and eigenvectors of $\\mathbf{Q}$ to rescale the problem from $\\mathbf{x}$ to one in $\\mathbf{z} \\stackrel{\\textrm{def}}{=} \\boldsymbol{\\Lambda}^{\\frac{1}{2}} \\mathbf{U} \\mathbf{x}$. In the new coordinate system $\\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x}$ could be simplified to $\\|\\mathbf{z}\\|^2$. Alas, this is a rather impractical suggestion. Computing eigenvalues and eigenvectors is in general *much more* expensive than solving the actual  problem. While computing eigenvalues exactly might be expensive, guessing them and computing them even somewhat approximately may already be a lot better than not doing anything at all. In particular, we could use the diagonal entries of $\\mathbf{Q}$ and rescale it accordingly. This is *much* cheaper than computing eigenvalues. $$\\tilde{\\mathbf{Q}} = \\textrm{diag}^{-\\frac{1}{2}}(\\mathbf{Q}) \\mathbf{Q} \\textrm{diag}^{-\\frac{1}{2}}(\\mathbf{Q}).$$\n\nIn this case we have $\\tilde{\\mathbf{Q}}_{ij} = \\mathbf{Q}_{ij} / \\sqrt{\\mathbf{Q}_{ii} \\mathbf{Q}_{jj}}$ and specifically $\\tilde{\\mathbf{Q}}_{ii} = 1$ for all $i$. In most cases this simplifies the condition number considerably. For instance, the cases we discussed previously, this would entirely eliminate the problem at hand since the problem is axis aligned."
    },
    {
      "chunk_id": "e7530f6b73e1_2",
      "chapter": "adagrad",
      "heading": "Preconditioning",
      "text": "In most cases this simplifies the condition number considerably. For instance, the cases we discussed previously, this would entirely eliminate the problem at hand since the problem is axis aligned. Unfortunately we face yet another problem: in deep learning we typically do not even have access to the second derivative of the objective function: for $\\mathbf{x} \\in \\mathbb{R}^d$ the second derivative even on a minibatch may require $\\mathcal{O}(d^2)$ space and work to compute, thus making it practically infeasible. The ingenious idea of Adagrad is to use a proxy for that elusive diagonal of the Hessian that is both relatively cheap to compute and effective---the magnitude of the gradient itself. In order to see why this works, let's look at $\\bar{f}(\\bar{\\mathbf{x}})$. We have that\n\n$$\\partial_{\\bar{\\mathbf{x}}} \\bar{f}(\\bar{\\mathbf{x}}) = \\boldsymbol{\\Lambda} \\bar{\\mathbf{x}} + \\bar{\\mathbf{c}} = \\boldsymbol{\\Lambda} \\left(\\bar{\\mathbf{x}} - \\bar{\\mathbf{x}}_0\\right),$$\n\nwhere $\\bar{\\mathbf{x}}_0$ is the minimizer of $\\bar{f}$. Hence the magnitude of the gradient depends both on $\\boldsymbol{\\Lambda}$ and the distance from optimality. If $\\bar{\\mathbf{x}} - \\bar{\\mathbf{x}}_0$ did not change, this would be all that is needed. After all, in this case the magnitude of the gradient $\\partial_{\\bar{\\mathbf{x}}} \\bar{f}(\\bar{\\mathbf{x}})$ suffices. Since AdaGrad is a stochastic gradient descent algorithm, we will see gradients with nonzero variance even at optimality. As a result we can safely use the variance of the gradients as a cheap proxy for the scale of the Hessian. A thorough analysis is beyond the scope of this section (it would be several pages). We refer the reader to :cite:`Duchi.Hazan.Singer.2011` for details."
    },
    {
      "chunk_id": "9c68768a087a_0",
      "chapter": "adagrad",
      "heading": "The Algorithm",
      "text": "Let's formalize the discussion from above. We use the variable $\\mathbf{s}_t$ to accumulate past gradient variance as follows. $$\\begin{aligned}\n    \\mathbf{g}_t & = \\partial_{\\mathbf{w}} l(y_t, f(\\mathbf{x}_t, \\mathbf{w})), \\\\\n    \\mathbf{s}_t & = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2, \\\\\n    \\mathbf{w}_t & = \\mathbf{w}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\cdot \\mathbf{g}_t. \\end{aligned}$$\n\nHere the operation are applied coordinate wise. That is, $\\mathbf{v}^2$ has entries $v_i^2$. Likewise $\\frac{1}{\\sqrt{v}}$ has entries $\\frac{1}{\\sqrt{v_i}}$ and $\\mathbf{u} \\cdot \\mathbf{v}$ has entries $u_i v_i$. As before $\\eta$ is the learning rate and $\\epsilon$ is an additive constant that ensures that we do not divide by $0$. Last, we initialize $\\mathbf{s}_0 = \\mathbf{0}$. Just like in the case of momentum we need to keep track of an auxiliary variable, in this case to allow for an individual learning rate per coordinate. This does not increase the cost of Adagrad significantly relative to SGD, simply since the main cost is typically to compute $l(y_t, f(\\mathbf{x}_t, \\mathbf{w}))$ and its derivative. Note that accumulating squared gradients in $\\mathbf{s}_t$ means that $\\mathbf{s}_t$ grows essentially at linear rate (somewhat slower than linearly in practice, since the gradients initially diminish). This leads to an $\\mathcal{O}(t^{-\\frac{1}{2}})$ learning rate, albeit adjusted on a per coordinate basis. For convex problems this is perfectly adequate. In deep learning, though, we might want to decrease the learning rate rather more slowly. This led to a number of Adagrad variants that we will discuss in the subsequent chapters. For now let's see how it behaves in a quadratic convex problem. We use the same problem as before:\n\n$$f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$\n\nWe are going to implement Adagrad using the same learning rate previously, i.e., $\\eta = 0.4$. As we can see, the iterative trajectory of the independent variable is smoother."
    },
    {
      "chunk_id": "9c68768a087a_1",
      "chapter": "adagrad",
      "heading": "The Algorithm",
      "text": "We use the same problem as before:\n\n$$f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$\n\nWe are going to implement Adagrad using the same learning rate previously, i.e., $\\eta = 0.4$. As we can see, the iterative trajectory of the independent variable is smoother. However, due to the cumulative effect of $\\boldsymbol{s}_t$, the learning rate continuously decays, so the independent variable does not move as much during later stages of iteration. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport math\nimport torch\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport math\nimport tensorflow as tf\n```\n\n```{.python .input}\n#@tab all\ndef adagrad_2d(x1, x2, s1, s2):\n    eps = 1e-6\n    g1, g2 = 0.2 * x1, 4 * x2\n    s1 += g1 ** 2\n    s2 += g2 ** 2\n    x1 -= eta / math.sqrt(s1 + eps) * g1\n    x2 -= eta / math.sqrt(s2 + eps) * g2\n    return x1, x2, s1, s2\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\neta = 0.4\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))\n```\n\nAs we increase the learning rate to $2$ we see much better behavior. This already indicates that the decrease in learning rate might be rather aggressive, even in the noise-free case and we need to ensure that parameters converge appropriately. ```{.python .input}\n#@tab all\neta = 2\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))\n```"
    },
    {
      "chunk_id": "37586de79c06_0",
      "chapter": "adagrad",
      "heading": "Implementation from Scratch",
      "text": "Just like the momentum method, Adagrad needs to maintain a state variable of the same shape as the parameters.\n\n```{.python .input}\n#@tab mxnet\ndef init_adagrad_states(feature_dim):\n    s_w = d2l.zeros((feature_dim, 1))\n    s_b = d2l.zeros(1)\n    return (s_w, s_b)\n\ndef adagrad(params, states, hyperparams):\n    eps = 1e-6\n    for p, s in zip(params, states):\n        s[:] += np.square(p.grad)\n        p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)\n```\n\n```{.python .input}\n#@tab pytorch\ndef init_adagrad_states(feature_dim):\n    s_w = d2l.zeros((feature_dim, 1))\n    s_b = d2l.zeros(1)\n    return (s_w, s_b)\n\ndef adagrad(params, states, hyperparams):\n    eps = 1e-6\n    for p, s in zip(params, states):\n        with torch.no_grad():\n            s[:] += torch.square(p.grad)\n            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)\n        p.grad.data.zero_()\n```\n\n```{.python .input}\n#@tab tensorflow\ndef init_adagrad_states(feature_dim):\n    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))\n    s_b = tf.Variable(d2l.zeros(1))\n    return (s_w, s_b)\n\ndef adagrad(params, grads, states, hyperparams):\n    eps = 1e-6\n    for p, s, g in zip(params, states, grads):\n        s[:].assign(s + tf.math.square(g))\n        p[:].assign(p - hyperparams['lr'] * g / tf.math.sqrt(s + eps))\n```\n\nCompared to the experiment in :numref:`sec_minibatch_sgd` we use a\nlarger learning rate to train the model.\n\n```{.python .input}\n#@tab all\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(adagrad, init_adagrad_states(feature_dim),\n               {'lr': 0.1}, data_iter, feature_dim);\n```"
    },
    {
      "chunk_id": "d2576695663e_0",
      "chapter": "adagrad",
      "heading": "Concise Implementation",
      "text": "Using the `Trainer` instance of the algorithm `adagrad`, we can invoke the Adagrad algorithm in Gluon.\n\n```{.python .input}\n#@tab mxnet\nd2l.train_concise_ch11('adagrad', {'learning_rate': 0.1}, data_iter)\n```\n\n```{.python .input}\n#@tab pytorch\ntrainer = torch.optim.Adagrad\nd2l.train_concise_ch11(trainer, {'lr': 0.1}, data_iter)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrainer = tf.keras.optimizers.Adagrad\nd2l.train_concise_ch11(trainer, {'learning_rate' : 0.1}, data_iter)\n```"
    },
    {
      "chunk_id": "90d1b14e0866_0",
      "chapter": "adagrad",
      "heading": "Summary",
      "text": "* Adagrad decreases the learning rate dynamically on a per-coordinate basis.\n* It uses the magnitude of the gradient as a means of adjusting how quickly progress is achieved - coordinates with large gradients are compensated with a smaller learning rate.\n* Computing the exact second derivative is typically infeasible in deep learning problems due to memory and computational constraints. The gradient can be a useful proxy.\n* If the optimization problem has a rather uneven structure Adagrad can help mitigate the distortion.\n* Adagrad is particularly effective for sparse features where the learning rate needs to decrease more slowly for infrequently occurring terms.\n* On deep learning problems Adagrad can sometimes be too aggressive in reducing learning rates. We will discuss strategies for mitigating this in the context of :numref:`sec_adam`."
    },
    {
      "chunk_id": "236e6245581a_0",
      "chapter": "adagrad",
      "heading": "Exercises",
      "text": "1. Prove that for an orthogonal matrix $\\mathbf{U}$ and a vector $\\mathbf{c}$ the following holds: $\\|\\mathbf{c} - \\mathbf{\\delta}\\|_2 = \\|\\mathbf{U} \\mathbf{c} - \\mathbf{U} \\mathbf{\\delta}\\|_2$. Why does this mean that the magnitude of perturbations does not change after an orthogonal change of variables?\n1. Try out Adagrad for $f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2$ and also for the objective function was rotated by 45 degrees, i.e., $f(\\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2$. Does it behave differently?\n1. Prove [Gerschgorin's circle theorem](https://en.wikipedia.org/wiki/Gershgorin_circle_theorem) which states that eigenvalues $\\lambda_i$ of a matrix $\\mathbf{M}$ satisfy $|\\lambda_i - \\mathbf{M}_{jj}| \\leq \\sum_{k \\neq j} |\\mathbf{M}_{jk}|$ for at least one choice of $j$.\n1. What does Gerschgorin's theorem tell us about the eigenvalues of the diagonally preconditioned matrix $\\textrm{diag}^{-\\frac{1}{2}}(\\mathbf{M}) \\mathbf{M} \\textrm{diag}^{-\\frac{1}{2}}(\\mathbf{M})$?\n1. Try out Adagrad for a proper deep network, such as :numref:`sec_lenet` when applied to Fashion-MNIST.\n1. How would you need to modify Adagrad to achieve a less aggressive decay in learning rate?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/355)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1072)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1073)\n:end_tab:"
    },
    {
      "chunk_id": "c87fbf86d320_0",
      "chapter": "adam",
      "heading": "adam",
      "text": "# Adam\n:label:`sec_adam`\n\nIn the discussions leading up to this section we encountered a number of techniques for efficient optimization. Let's recap them in detail here:\n\n* We saw that :numref:`sec_sgd` is more effective than Gradient Descent when solving optimization problems, e.g., due to its inherent resilience to redundant data. \n* We saw that :numref:`sec_minibatch_sgd` affords significant additional efficiency arising from vectorization, using larger sets of observations in one minibatch. This is the key to efficient multi-machine, multi-GPU and overall parallel processing. \n* :numref:`sec_momentum` added a mechanism for aggregating a history of past gradients to accelerate convergence.\n* :numref:`sec_adagrad` used per-coordinate scaling to allow for a computationally efficient preconditioner. \n* :numref:`sec_rmsprop` decoupled per-coordinate scaling from a learning rate adjustment. \n\nAdam :cite:`Kingma.Ba.2014` combines all these techniques into one efficient learning algorithm. As expected, this is an algorithm that has become rather popular as one of the more robust and effective optimization algorithms to use in deep learning. It is not without issues, though. In particular, :cite:`Reddi.Kale.Kumar.2019` show that there are situations where Adam can diverge due to poor variance control. In a follow-up work :citet:`Zaheer.Reddi.Sachan.ea.2018` proposed a hotfix to Adam, called Yogi which addresses these issues. More on this later. For now let's review the Adam algorithm."
    },
    {
      "chunk_id": "c7522a1bc7c4_0",
      "chapter": "adam",
      "heading": "The Algorithm",
      "text": "One of the key components of Adam is that it uses exponential weighted moving averages (also known as leaky averaging) to obtain an estimate of both the momentum and also the second moment of the gradient. That is, it uses the state variables\n\n$$\\begin{aligned}\n    \\mathbf{v}_t & \\leftarrow \\beta_1 \\mathbf{v}_{t-1} + (1 - \\beta_1) \\mathbf{g}_t, \\\\\n    \\mathbf{s}_t & \\leftarrow \\beta_2 \\mathbf{s}_{t-1} + (1 - \\beta_2) \\mathbf{g}_t^2. \\end{aligned}$$\n\nHere $\\beta_1$ and $\\beta_2$ are nonnegative weighting parameters. Common choices for them are $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. That is, the variance estimate moves *much more slowly* than the momentum term. Note that if we initialize $\\mathbf{v}_0 = \\mathbf{s}_0 = 0$ we have a significant amount of bias initially towards smaller values. This can be addressed by using the fact that $\\sum_{i=0}^{t-1} \\beta^i = \\frac{1 - \\beta^t}{1 - \\beta}$ to re-normalize terms. Correspondingly the normalized state variables are given by \n\n$$\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_1^t} \\textrm{ and } \\hat{\\mathbf{s}}_t = \\frac{\\mathbf{s}_t}{1 - \\beta_2^t}.$$\n\nArmed with the proper estimates we can now write out the update equations. First, we rescale the gradient in a manner very much akin to that of RMSProp to obtain\n\n$$\\mathbf{g}_t' = \\frac{\\eta \\hat{\\mathbf{v}}_t}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}.$$\n\nUnlike RMSProp our update uses the momentum $\\hat{\\mathbf{v}}_t$ rather than the gradient itself. Moreover, there is a slight cosmetic difference as the rescaling happens using $\\frac{1}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}$ instead of $\\frac{1}{\\sqrt{\\hat{\\mathbf{s}}_t + \\epsilon}}$. The former works arguably slightly better in practice, hence the deviation from RMSProp. Typically we pick $\\epsilon = 10^{-6}$ for a good trade-off between numerical stability and fidelity. Now we have all the pieces in place to compute updates."
    },
    {
      "chunk_id": "c7522a1bc7c4_1",
      "chapter": "adam",
      "heading": "The Algorithm",
      "text": "The former works arguably slightly better in practice, hence the deviation from RMSProp. Typically we pick $\\epsilon = 10^{-6}$ for a good trade-off between numerical stability and fidelity. Now we have all the pieces in place to compute updates. This is slightly anticlimactic and we have a simple update of the form\n\n$$\\mathbf{x}_t \\leftarrow \\mathbf{x}_{t-1} - \\mathbf{g}_t'.$$\n\nReviewing the design of Adam its inspiration is clear. Momentum and scale are clearly visible in the state variables. Their rather peculiar definition forces us to debias terms (this could be fixed by a slightly different initialization and update condition). Second, the combination of both terms is pretty straightforward, given RMSProp. Last, the explicit learning rate $\\eta$ allows us to control the step length to address issues of convergence."
    },
    {
      "chunk_id": "830463bc5033_0",
      "chapter": "adam",
      "heading": "Implementation",
      "text": "Implementing Adam from scratch is not very daunting. For convenience we store the time step counter $t$ in the `hyperparams` dictionary. Beyond that all is straightforward."
    },
    {
      "chunk_id": "830463bc5033_1",
      "chapter": "adam",
      "heading": "Implementation",
      "text": "Implementing Adam from scratch is not very daunting. For convenience we store the time step counter $t$ in the `hyperparams` dictionary. Beyond that all is straightforward. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n\ndef init_adam_states(feature_dim):\n    v_w, v_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    return ((v_w, s_w), (v_b, s_b))\n\ndef adam(params, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-6\n    for p, (v, s) in zip(params, states):\n        v[:] = beta1 * v + (1 - beta1) * p.grad\n        s[:] = beta2 * s + (1 - beta2) * np.square(p.grad)\n        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n        p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps)\n    hyperparams['t'] += 1\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n\ndef init_adam_states(feature_dim):\n    v_w, v_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    s_w, s_b = d2l.zeros((feature_dim, 1)), d2l.zeros(1)\n    return ((v_w, s_w), (v_b, s_b))\n\ndef adam(params, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-6\n    for p, (v, s) in zip(params, states):\n        with torch.no_grad():\n            v[:] = beta1 * v + (1 - beta1) * p.grad\n            s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)\n            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)\n                                                       + eps)\n        p.grad.data.zero_()\n    hyperparams['t'] += 1\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n\ndef init_adam_states(feature_dim):\n    v_w = tf.Variable(d2l.zeros((feature_dim, 1)))\n    v_b = tf.Variable(d2l.zeros(1))\n    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))\n    s_b = tf.Variable(d2l.zeros(1))\n    return ((v_w, s_w), (v_b, s_b))\n\ndef adam(params, grads, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-6\n    for p, (v, s), grad in zip(params, states, grads):\n        v[:].assign(beta1 * v  + (1 - beta1) * grad)\n        s[:].assign(beta2 * s + (1 - beta2) * tf.math.square(grad))\n        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n        p[:].assign(p - hyperparams['lr'] * v_bias_corr  \n                    / tf.math.sqrt(s_bias_corr) + eps)\n```\n\nWe are ready to use Adam to train the model."
    },
    {
      "chunk_id": "830463bc5033_2",
      "chapter": "adam",
      "heading": "Implementation",
      "text": "We use a learning rate of $\\eta = 0.01$. ```{.python .input}\n#@tab all\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(adam, init_adam_states(feature_dim),\n               {'lr': 0.01, 't': 1}, data_iter, feature_dim);\n```\n\nA more concise implementation is straightforward since `adam` is one of the algorithms provided as part of the Gluon `trainer` optimization library. Hence we only need to pass configuration parameters for an implementation in Gluon. ```{.python .input}\n#@tab mxnet\nd2l.train_concise_ch11('adam', {'learning_rate': 0.01}, data_iter)\n```\n\n```{.python .input}\n#@tab pytorch\ntrainer = torch.optim.Adam\nd2l.train_concise_ch11(trainer, {'lr': 0.01}, data_iter)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrainer = tf.keras.optimizers.Adam\nd2l.train_concise_ch11(trainer, {'learning_rate': 0.01}, data_iter)\n```"
    },
    {
      "chunk_id": "0167c71358f5_0",
      "chapter": "adam",
      "heading": "Yogi",
      "text": "One of the problems of Adam is that it can fail to converge even in convex settings when the second moment estimate in $\\mathbf{s}_t$ blows up. As a fix :citet:`Zaheer.Reddi.Sachan.ea.2018` proposed a refined update (and initialization) for $\\mathbf{s}_t$. To understand what's going on, let's rewrite the Adam update as follows:\n\n$$\\mathbf{s}_t \\leftarrow \\mathbf{s}_{t-1} + (1 - \\beta_2) \\left(\\mathbf{g}_t^2 - \\mathbf{s}_{t-1}\\right).$$\n\nWhenever $\\mathbf{g}_t^2$ has high variance or updates are sparse, $\\mathbf{s}_t$ might forget past values too quickly. A possible fix for this is to replace $\\mathbf{g}_t^2 - \\mathbf{s}_{t-1}$ by $\\mathbf{g}_t^2 \\odot \\mathop{\\textrm{sgn}}(\\mathbf{g}_t^2 - \\mathbf{s}_{t-1})$. Now the magnitude of the update no longer depends on the amount of deviation. This yields the Yogi updates\n\n$$\\mathbf{s}_t \\leftarrow \\mathbf{s}_{t-1} + (1 - \\beta_2) \\mathbf{g}_t^2 \\odot \\mathop{\\textrm{sgn}}(\\mathbf{g}_t^2 - \\mathbf{s}_{t-1}).$$\n\nThe authors furthermore advise to initialize the momentum on a larger initial batch rather than just initial pointwise estimate. We omit the details since they are not material to the discussion and since even without this convergence remains pretty good."
    },
    {
      "chunk_id": "0167c71358f5_1",
      "chapter": "adam",
      "heading": "Yogi",
      "text": "We omit the details since they are not material to the discussion and since even without this convergence remains pretty good. ```{.python .input}\n#@tab mxnet\ndef yogi(params, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-3\n    for p, (v, s) in zip(params, states):\n        v[:] = beta1 * v + (1 - beta1) * p.grad\n        s[:] = s + (1 - beta2) * np.sign(\n            np.square(p.grad) - s) * np.square(p.grad)\n        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n        p[:] -= hyperparams['lr'] * v_bias_corr / (np.sqrt(s_bias_corr) + eps)\n    hyperparams['t'] += 1\n\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(yogi, init_adam_states(feature_dim),\n               {'lr': 0.01, 't': 1}, data_iter, feature_dim);\n```\n\n```{.python .input}\n#@tab pytorch\ndef yogi(params, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-3\n    for p, (v, s) in zip(params, states):\n        with torch.no_grad():\n            v[:] = beta1 * v + (1 - beta1) * p.grad\n            s[:] = s + (1 - beta2) * torch.sign(\n                torch.square(p.grad) - s) * torch.square(p.grad)\n            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)\n                                                       + eps)\n        p.grad.data.zero_()\n    hyperparams['t'] += 1\n\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(yogi, init_adam_states(feature_dim),\n               {'lr': 0.01, 't': 1}, data_iter, feature_dim);\n```\n\n```{.python .input}\n#@tab tensorflow\ndef yogi(params, grads, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-6\n    for p, (v, s), grad in zip(params, states, grads):\n        v[:].assign(beta1 * v  + (1 - beta1) * grad)\n        s[:].assign(s + (1 - beta2) * tf.math.sign(\n                   tf.math.square(grad) - s) * tf.math.square(grad))\n        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n        p[:].assign(p - hyperparams['lr'] * v_bias_corr  \n                    / tf.math.sqrt(s_bias_corr) + eps)\n    hyperparams['t'] += 1\n\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(yogi, init_adam_states(feature_dim),\n               {'lr': 0.01, 't': 1}, data_iter, feature_dim);\n```"
    },
    {
      "chunk_id": "d3dcd2d35f06_0",
      "chapter": "adam",
      "heading": "Summary",
      "text": "* Adam combines features of many optimization algorithms into a fairly robust update rule. \n* Created on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic gradient.\n* Adam uses bias correction to adjust for a slow startup when estimating momentum and a second moment. \n* For gradients with significant variance we may encounter issues with convergence. They can be amended by using larger minibatches or by switching to an improved estimate for $\\mathbf{s}_t$. Yogi offers such an alternative."
    },
    {
      "chunk_id": "82ba1ace3f11_0",
      "chapter": "adam",
      "heading": "Exercises",
      "text": "1. Adjust the learning rate and observe and analyze the experimental results.\n1. Can you rewrite momentum and second moment updates such that it does not require bias correction?\n1. Why do you need to reduce the learning rate $\\eta$ as we converge?\n1. Try to construct a case for which Adam diverges and Yogi converges?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/358)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1078)\n:end_tab:\n\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1079)\n:end_tab:"
    },
    {
      "chunk_id": "b462e2f0b2bf_0",
      "chapter": "convexity",
      "heading": "convexity",
      "text": "# Convexity\n:label:`sec_convexity`\n\nConvexity plays a vital role in the design of optimization algorithms. \nThis is largely due to the fact that it is much easier to analyze and test algorithms in such a context. \nIn other words,\nif the algorithm performs poorly even in the convex setting,\ntypically we should not hope to see great results otherwise. \nFurthermore, even though the optimization problems in deep learning are generally nonconvex, they often exhibit some properties of convex ones near local minima. This can lead to exciting new optimization variants such as :cite:`Izmailov.Podoprikhin.Garipov.ea.2018`.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mpl_toolkits import mplot3d\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nimport torch\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nimport tensorflow as tf\n```"
    },
    {
      "chunk_id": "6e50f153bfee_0",
      "chapter": "convexity",
      "heading": "Definitions",
      "text": "Before convex analysis,\nwe need to define *convex sets* and *convex functions*.\nThey lead to mathematical tools that are commonly applied to machine learning."
    },
    {
      "chunk_id": "e9aaca1478b7_0",
      "chapter": "convexity",
      "heading": "Convex Sets",
      "text": "Sets are the basis of convexity. Simply put, a set $\\mathcal{X}$ in a vector space is *convex* if for any $a, b \\in \\mathcal{X}$ the line segment connecting $a$ and $b$ is also in $\\mathcal{X}$. In mathematical terms this means that for all $\\lambda \\in [0, 1]$ we have\n\n$$\\lambda  a + (1-\\lambda)  b \\in \\mathcal{X} \\textrm{ whenever } a, b \\in \\mathcal{X}.$$\n\nThis sounds a bit abstract. Consider :numref:`fig_pacman`. The first set is not convex since there exist line segments that are not contained in it. The other two sets suffer no such problem. ![The first set is nonconvex and the other two are convex.](../img/pacman.svg)\n:label:`fig_pacman`\n\nDefinitions on their own are not particularly useful unless you can do something with them. In this case we can look at intersections as shown in :numref:`fig_convex_intersect`. Assume that $\\mathcal{X}$ and $\\mathcal{Y}$ are convex sets. Then $\\mathcal{X} \\cap \\mathcal{Y}$ is also convex. To see this, consider any $a, b \\in \\mathcal{X} \\cap \\mathcal{Y}$. Since $\\mathcal{X}$ and $\\mathcal{Y}$ are convex, the line segments connecting $a$ and $b$ are contained in both $\\mathcal{X}$ and $\\mathcal{Y}$. Given that, they also need to be contained in $\\mathcal{X} \\cap \\mathcal{Y}$, thus proving our theorem. ![The intersection between two convex sets is convex.](../img/convex-intersect.svg)\n:label:`fig_convex_intersect`\n\nWe can strengthen this result with little effort: given convex sets $\\mathcal{X}_i$, their intersection $\\cap_{i} \\mathcal{X}_i$ is convex. To see that the converse is not true, consider two disjoint sets $\\mathcal{X} \\cap \\mathcal{Y} = \\emptyset$. Now pick $a \\in \\mathcal{X}$ and $b \\in \\mathcal{Y}$. The line segment in :numref:`fig_nonconvex` connecting $a$ and $b$ needs to contain some part that is neither in $\\mathcal{X}$ nor in $\\mathcal{Y}$, since we assumed that $\\mathcal{X} \\cap \\mathcal{Y} = \\emptyset$. Hence the line segment is not in $\\mathcal{X} \\cup \\mathcal{Y}$ either, thus proving that in general unions of convex sets need not be convex."
    },
    {
      "chunk_id": "e9aaca1478b7_1",
      "chapter": "convexity",
      "heading": "Convex Sets",
      "text": "Hence the line segment is not in $\\mathcal{X} \\cup \\mathcal{Y}$ either, thus proving that in general unions of convex sets need not be convex. ![The union of two convex sets need not be convex.](../img/nonconvex.svg)\n:label:`fig_nonconvex`\n\nTypically the problems in deep learning are defined on convex sets. For instance, $\\mathbb{R}^d$,\nthe set of $d$-dimensional vectors of real numbers,\nis a convex set (after all, the line between any two points in $\\mathbb{R}^d$ remains in $\\mathbb{R}^d$). In some cases we work with variables of bounded length, such as balls of radius $r$ as defined by $\\{\\mathbf{x} | \\mathbf{x} \\in \\mathbb{R}^d \\textrm{ and } \\|\\mathbf{x}\\| \\leq r\\}$."
    },
    {
      "chunk_id": "712501f8bb8f_0",
      "chapter": "convexity",
      "heading": "Convex Functions",
      "text": "Now that we have convex sets we can introduce *convex functions* $f$.\nGiven a convex set $\\mathcal{X}$, a function $f: \\mathcal{X} \\to \\mathbb{R}$ is *convex* if for all $x, x' \\in \\mathcal{X}$ and for all $\\lambda \\in [0, 1]$ we have\n\n$$\\lambda f(x) + (1-\\lambda) f(x') \\geq f(\\lambda x + (1-\\lambda) x').$$\n\nTo illustrate this let's plot a few functions and check which ones satisfy the requirement.\nBelow we define a few functions, both convex and nonconvex.\n\n```{.python .input}\n#@tab all\nf = lambda x: 0.5 * x**2  # Convex\ng = lambda x: d2l.cos(np.pi * x)  # Nonconvex\nh = lambda x: d2l.exp(0.5 * x)  # Convex\n\nx, segment = d2l.arange(-2, 2, 0.01), d2l.tensor([-1.5, 1])\nd2l.use_svg_display()\n_, axes = d2l.plt.subplots(1, 3, figsize=(9, 3))\nfor ax, func in zip(axes, [f, g, h]):\n    d2l.plot([x, segment], [func(x), func(segment)], axes=ax)\n```\n\nAs expected, the cosine function is *nonconvex*, whereas the parabola and the exponential function are. Note that the requirement that $\\mathcal{X}$ is a convex set is necessary for the condition to make sense. Otherwise the outcome of $f(\\lambda x + (1-\\lambda) x')$ might not be well defined."
    },
    {
      "chunk_id": "2438c9193612_0",
      "chapter": "convexity",
      "heading": "Jensen's Inequality",
      "text": "Given a convex function $f$,\none of the most useful mathematical tools\nis *Jensen's inequality*.\nIt amounts to a generalization of the definition of convexity:\n\n$$\\sum_i \\alpha_i f(x_i)  \\geq f\\left(\\sum_i \\alpha_i x_i\\right)    \\textrm{ and }    E_X[f(X)]  \\geq f\\left(E_X[X]\\right),$$\n:eqlabel:`eq_jensens-inequality`\n\nwhere $\\alpha_i$ are nonnegative real numbers such that $\\sum_i \\alpha_i = 1$ and $X$ is a random variable.\nIn other words, the expectation of a convex function is no less than the convex function of an expectation, where the latter is usually a simpler expression. \nTo prove the first inequality we repeatedly apply the definition of convexity to one term in the sum at a time.\n\n\nOne of the common applications of Jensen's inequality is\nto bound a more complicated expression by a simpler one.\nFor example,\nits application can be\nwith regard to the log-likelihood of partially observed random variables. That is, we use\n\n$$E_{Y \\sim P(Y)}[-\\log P(X \\mid Y)] \\geq -\\log P(X),$$\n\nsince $\\int P(Y) P(X \\mid Y) dY = P(X)$.\nThis can be used in variational methods. Here $Y$ is typically the unobserved random variable, $P(Y)$ is the best guess of how it might be distributed, and $P(X)$ is the distribution with $Y$ integrated out. For instance, in clustering $Y$ might be the cluster labels and $P(X \\mid Y)$ is the generative model when applying cluster labels."
    },
    {
      "chunk_id": "b843a505c00b_0",
      "chapter": "convexity",
      "heading": "Properties",
      "text": "Convex functions have many useful properties. We describe a few commonly-used ones below."
    },
    {
      "chunk_id": "7bb0c05fe057_0",
      "chapter": "convexity",
      "heading": "Local Minima Are Global Minima",
      "text": "First and foremost, the local minima of convex functions are also the global minima. \nWe can prove it by contradiction as follows.\n\nConsider a convex function $f$ defined on a convex set $\\mathcal{X}$.\nSuppose that $x^{\\ast} \\in \\mathcal{X}$ is a local minimum:\nthere exists a small positive value $p$ so that for $x \\in \\mathcal{X}$ that satisfies $0 < |x - x^{\\ast}| \\leq p$ we have $f(x^{\\ast}) < f(x)$.\n\nAssume that the local minimum $x^{\\ast}$\nis not the global minimum of $f$:\nthere exists $x' \\in \\mathcal{X}$ for which $f(x') < f(x^{\\ast})$. \nThere also exists \n$\\lambda \\in [0, 1)$ such as $\\lambda = 1 - \\frac{p}{|x^{\\ast} - x'|}$\nso that\n$0 < |\\lambda x^{\\ast} + (1-\\lambda) x' - x^{\\ast}| \\leq p$. \n\nHowever,\naccording to the definition of convex functions, we have\n\n$$\\begin{aligned}\n    f(\\lambda x^{\\ast} + (1-\\lambda) x') &\\leq \\lambda f(x^{\\ast}) + (1-\\lambda) f(x') \\\\\n    &< \\lambda f(x^{\\ast}) + (1-\\lambda) f(x^{\\ast}) \\\\\n    &= f(x^{\\ast}),\n\\end{aligned}$$\n\nwhich contradicts with our statement that $x^{\\ast}$ is a local minimum.\nTherefore, there does not exist $x' \\in \\mathcal{X}$ for which $f(x') < f(x^{\\ast})$. The local minimum $x^{\\ast}$ is also the global minimum.\n\nFor instance, the convex function $f(x) = (x-1)^2$ has a local minimum at $x=1$, which is also the global minimum.\n\n```{.python .input}\n#@tab all\nf = lambda x: (x - 1) ** 2\nd2l.set_figsize()\nd2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')\n```\n\nThe fact that the local minima for convex functions are also the global minima is very convenient. \nIt means that if we minimize functions we cannot \"get stuck\". \nNote, though, that this does not mean that there cannot be more than one global minimum or that there might even exist one. For instance, the function $f(x) = \\mathrm{max}(|x|-1, 0)$ attains its minimum value over the interval $[-1, 1]$. Conversely, the function $f(x) = \\exp(x)$ does not attain a minimum value on $\\mathbb{R}$: for $x \\to -\\infty$ it asymptotes to $0$, but there is no $x$ for which $f(x) = 0$."
    },
    {
      "chunk_id": "730e26eca8f0_0",
      "chapter": "convexity",
      "heading": "Below Sets of Convex Functions Are Convex",
      "text": "We can conveniently \ndefine convex sets \nvia *below sets* of convex functions.\nConcretely,\ngiven a convex function $f$ defined on a convex set $\\mathcal{X}$,\nany below set\n\n$$\\mathcal{S}_b \\stackrel{\\textrm{def}}{=} \\{x | x \\in \\mathcal{X} \\textrm{ and } f(x) \\leq b\\}$$\n\nis convex. \n\nLet's prove this quickly. Recall that for any $x, x' \\in \\mathcal{S}_b$ we need to show that $\\lambda x + (1-\\lambda) x' \\in \\mathcal{S}_b$ as long as $\\lambda \\in [0, 1]$. \nSince $f(x) \\leq b$ and $f(x') \\leq b$,\nby the definition of convexity we have \n\n$$f(\\lambda x + (1-\\lambda) x') \\leq \\lambda f(x) + (1-\\lambda) f(x') \\leq b.$$"
    },
    {
      "chunk_id": "4c9b0be5e016_0",
      "chapter": "convexity",
      "heading": "Convexity and Second Derivatives",
      "text": "Whenever the second derivative of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ exists it is very easy to check whether $f$ is convex. All we need to do is check whether the Hessian of $f$ is positive semidefinite: $\\nabla^2f \\succeq 0$, i.e., \ndenoting the Hessian matrix $\\nabla^2f$ by $\\mathbf{H}$,\n$\\mathbf{x}^\\top \\mathbf{H} \\mathbf{x} \\geq 0$\nfor all $\\mathbf{x} \\in \\mathbb{R}^n$. For instance, the function $f(\\mathbf{x}) = \\frac{1}{2} \\|\\mathbf{x}\\|^2$ is convex since $\\nabla^2 f = \\mathbf{1}$, i.e., its Hessian is an identity matrix. Formally, a twice-differentiable one-dimensional function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is convex\nif and only if its second derivative $f'' \\geq 0$. For any twice-differentiable multidimensional function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$,\nit is convex if and only if its Hessian $\\nabla^2f \\succeq 0$. First, we need to prove the one-dimensional case. To see that \nconvexity of $f$ implies \n$f'' \\geq 0$  we use the fact that\n\n$$\\frac{1}{2} f(x + \\epsilon) + \\frac{1}{2} f(x - \\epsilon) \\geq f\\left(\\frac{x + \\epsilon}{2} + \\frac{x - \\epsilon}{2}\\right) = f(x).$$\n\nSince the second derivative is given by the limit over finite differences it follows that\n\n$$f''(x) = \\lim_{\\epsilon \\to 0} \\frac{f(x+\\epsilon) + f(x - \\epsilon) - 2f(x)}{\\epsilon^2} \\geq 0.$$\n\nTo see that \n$f'' \\geq 0$ implies that $f$ is convex\nwe use the fact that $f'' \\geq 0$ implies that $f'$ is a monotonically nondecreasing function. Let $a < x < b$ be three points in $\\mathbb{R}$,\nwhere $x = (1-\\lambda)a + \\lambda b$ and $\\lambda \\in (0, 1)$. According to the mean value theorem,\nthere exist $\\alpha \\in [a, x]$ and $\\beta \\in [x, b]$\nsuch that\n\n$$f'(\\alpha) = \\frac{f(x) - f(a)}{x-a} \\textrm{ and } f'(\\beta) = \\frac{f(b) - f(x)}{b-x}.$$\n\n\nBy monotonicity $f'(\\beta) \\geq f'(\\alpha)$, hence\n\n$$\\frac{x-a}{b-a}f(b) + \\frac{b-x}{b-a}f(a) \\geq f(x).$$\n\nSince $x = (1-\\lambda)a + \\lambda b$,\nwe have\n\n$$\\lambda f(b) + (1-\\lambda)f(a) \\geq f((1-\\lambda)a + \\lambda b),$$\n\nthus proving convexity."
    },
    {
      "chunk_id": "4c9b0be5e016_1",
      "chapter": "convexity",
      "heading": "Convexity and Second Derivatives",
      "text": "Second, we need a lemma before \nproving the multidimensional case:\n$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\nis convex if and only if for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$\n\n$$g(z) \\stackrel{\\textrm{def}}{=} f(z \\mathbf{x} + (1-z)  \\mathbf{y}) \\textrm{ where } z \\in [0,1]$$ \n\nis convex. To prove that convexity of $f$ implies that $g$ is convex,\nwe can show that for all $a, b, \\lambda \\in [0, 1]$ (thus\n$0 \\leq \\lambda a + (1-\\lambda) b \\leq 1$)\n\n$$\\begin{aligned} &g(\\lambda a + (1-\\lambda) b)\\\\\n=&f\\left(\\left(\\lambda a + (1-\\lambda) b\\right)\\mathbf{x} + \\left(1-\\lambda a - (1-\\lambda) b\\right)\\mathbf{y} \\right)\\\\\n=&f\\left(\\lambda \\left(a \\mathbf{x} + (1-a)  \\mathbf{y}\\right)  + (1-\\lambda) \\left(b \\mathbf{x} + (1-b)  \\mathbf{y}\\right) \\right)\\\\\n\\leq& \\lambda f\\left(a \\mathbf{x} + (1-a)  \\mathbf{y}\\right)  + (1-\\lambda) f\\left(b \\mathbf{x} + (1-b)  \\mathbf{y}\\right) \\\\\n=& \\lambda g(a) + (1-\\lambda) g(b). \\end{aligned}$$\n\nTo prove the converse,\nwe can show that for \nall $\\lambda \\in [0, 1]$ \n\n$$\\begin{aligned} &f(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y})\\\\\n=&g(\\lambda \\cdot 1 + (1-\\lambda) \\cdot 0)\\\\\n\\leq& \\lambda g(1)  + (1-\\lambda) g(0) \\\\\n=& \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}). \\end{aligned}$$\n\n\nFinally,\nusing the lemma above and the result of the one-dimensional case,\nthe multidimensional case\ncan be proven as follows. A multidimensional function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is convex\nif and only if for all $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ $g(z) \\stackrel{\\textrm{def}}{=} f(z \\mathbf{x} + (1-z)  \\mathbf{y})$, where $z \\in [0,1]$,\nis convex. According to the one-dimensional case,\nthis holds if and only if\n$g'' = (\\mathbf{x} - \\mathbf{y})^\\top \\mathbf{H}(\\mathbf{x} - \\mathbf{y}) \\geq 0$ ($\\mathbf{H} \\stackrel{\\textrm{def}}{=} \\nabla^2f$)\nfor all $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$,\nwhich is equivalent to $\\mathbf{H} \\succeq 0$\nper the definition of positive semidefinite matrices."
    },
    {
      "chunk_id": "c10c4ab4d5bb_0",
      "chapter": "convexity",
      "heading": "Constraints",
      "text": "One of the nice properties of convex optimization is that it allows us to handle constraints efficiently. That is, it allows us to solve *constrained optimization* problems of the form:\n\n$$\\begin{aligned} \\mathop{\\textrm{minimize~}}_{\\mathbf{x}} & f(\\mathbf{x}) \\\\\n    \\textrm{ subject to } & c_i(\\mathbf{x}) \\leq 0 \\textrm{ for all } i \\in \\{1, \\ldots, n\\},\n\\end{aligned}$$\n\nwhere $f$ is the objective and the functions $c_i$ are constraint functions. To see what this does consider the case where $c_1(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 - 1$. In this case the parameters $\\mathbf{x}$ are constrained to the unit ball. If a second constraint is $c_2(\\mathbf{x}) = \\mathbf{v}^\\top \\mathbf{x} + b$, then this corresponds to all $\\mathbf{x}$ lying on a half-space. Satisfying both constraints simultaneously amounts to selecting a slice of a ball."
    },
    {
      "chunk_id": "aebd2fafae6b_0",
      "chapter": "convexity",
      "heading": "Lagrangian",
      "text": "In general, solving a constrained optimization problem is difficult. One way of addressing it stems from physics with a rather simple intuition. Imagine a ball inside a box. The ball will roll to the place that is lowest and the forces of gravity will be balanced out with the forces that the sides of the box can impose on the ball. In short, the gradient of the objective function (i.e., gravity) will be offset by the gradient of the constraint function (the ball need to remain inside the box by virtue of the walls \"pushing back\"). \nNote that some constraints may not be active:\nthe walls that are not touched by the ball\nwill not be able to exert any force on the ball.\n\n\nSkipping over the derivation of the *Lagrangian* $L$,\nthe above reasoning\ncan be expressed via the following saddle point optimization problem:\n\n$$L(\\mathbf{x}, \\alpha_1, \\ldots, \\alpha_n) = f(\\mathbf{x}) + \\sum_{i=1}^n \\alpha_i c_i(\\mathbf{x}) \\textrm{ where } \\alpha_i \\geq 0.$$\n\nHere the variables $\\alpha_i$ ($i=1,\\ldots,n$) are the so-called *Lagrange multipliers* that ensure that constraints are properly enforced. They are chosen just large enough to ensure that $c_i(\\mathbf{x}) \\leq 0$ for all $i$. For instance, for any $\\mathbf{x}$ where $c_i(\\mathbf{x}) < 0$ naturally, we'd end up picking $\\alpha_i = 0$. Moreover, this is a saddle point optimization problem where one wants to *maximize* $L$ with respect to all $\\alpha_i$ and simultaneously *minimize* it with respect to $\\mathbf{x}$. There is a rich body of literature explaining how to arrive at the function $L(\\mathbf{x}, \\alpha_1, \\ldots, \\alpha_n)$. For our purposes it is sufficient to know that the saddle point of $L$ is where the original constrained optimization problem is solved optimally."
    },
    {
      "chunk_id": "722de0cd06a2_0",
      "chapter": "convexity",
      "heading": "Penalties",
      "text": "One way of satisfying constrained optimization problems at least *approximately* is to adapt the Lagrangian $L$. \nRather than satisfying $c_i(\\mathbf{x}) \\leq 0$ we simply add $\\alpha_i c_i(\\mathbf{x})$ to the objective function $f(x)$. This ensures that the constraints will not be violated too badly.\n\nIn fact, we have been using this trick all along. Consider weight decay in :numref:`sec_weight_decay`. In it we add $\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2$ to the objective function to ensure that $\\mathbf{w}$ does not grow too large. From the constrained optimization point of view we can see that this will ensure that $\\|\\mathbf{w}\\|^2 - r^2 \\leq 0$ for some radius $r$. Adjusting the value of $\\lambda$ allows us to vary the size of $\\mathbf{w}$.\n\nIn general, adding penalties is a good way of ensuring approximate constraint satisfaction. In practice this turns out to be much more robust than exact satisfaction. Furthermore, for nonconvex problems many of the properties that make the exact approach so appealing in the convex case (e.g., optimality) no longer hold."
    },
    {
      "chunk_id": "e537f17a804e_0",
      "chapter": "convexity",
      "heading": "Projections",
      "text": "An alternative strategy for satisfying constraints is projections. Again, we encountered them before, e.g., when dealing with gradient clipping in :numref:`sec_rnn-scratch`. There we ensured that a gradient has length bounded by $\\theta$ via\n\n$$\\mathbf{g} \\leftarrow \\mathbf{g} \\cdot \\mathrm{min}(1, \\theta/\\|\\mathbf{g}\\|).$$\n\nThis turns out to be a *projection* of $\\mathbf{g}$ onto the ball of radius $\\theta$. More generally, a projection on a convex set $\\mathcal{X}$ is defined as\n\n$$\\textrm{Proj}_\\mathcal{X}(\\mathbf{x}) = \\mathop{\\mathrm{argmin}}_{\\mathbf{x}' \\in \\mathcal{X}} \\|\\mathbf{x} - \\mathbf{x}'\\|,$$\n\nwhich is the closest point in $\\mathcal{X}$ to $\\mathbf{x}$. \n\n![Convex Projections.](../img/projections.svg)\n:label:`fig_projections`\n\nThe mathematical definition of projections may sound a bit abstract. :numref:`fig_projections` explains it somewhat more clearly. In it we have two convex sets, a circle and a diamond. \nPoints inside both sets (yellow) remain unchanged during projections. \nPoints outside both sets (black) are projected to \nthe points inside the sets (red) that are closet to the original points (black).\nWhile for $\\ell_2$ balls this leaves the direction unchanged, this need not be the case in general, as can be seen in the case of the diamond.\n\n\nOne of the uses for convex projections is to compute sparse weight vectors. In this case we project weight vectors onto an $\\ell_1$ ball,\nwhich is a generalized version of the diamond case in :numref:`fig_projections`."
    },
    {
      "chunk_id": "ddb8e0a0f213_0",
      "chapter": "convexity",
      "heading": "Summary",
      "text": "In the context of deep learning the main purpose of convex functions is to motivate optimization algorithms and help us understand them in detail. In the following we will see how gradient descent and stochastic gradient descent can be derived accordingly.\n\n\n* Intersections of convex sets are convex. Unions are not.\n* The expectation of a convex function is no less than the convex function of an expectation (Jensen's inequality).\n* A twice-differentiable function is convex if and only if its Hessian (a matrix of second derivatives) is positive semidefinite.\n* Convex constraints can be added via the Lagrangian. In practice we may simply add them with a penalty to the objective function.\n* Projections map to points in the convex set closest to the original points."
    },
    {
      "chunk_id": "8f3ce751844d_0",
      "chapter": "convexity",
      "heading": "Exercises",
      "text": "1. Assume that we want to verify convexity of a set by drawing all lines between points within the set and checking whether the lines are contained.\n    1. Prove that it is sufficient to check only the points on the boundary.\n    1. Prove that it is sufficient to check only the vertices of the set.\n1. Denote by $\\mathcal{B}_p[r] \\stackrel{\\textrm{def}}{=} \\{\\mathbf{x} | \\mathbf{x} \\in \\mathbb{R}^d \\textrm{ and } \\|\\mathbf{x}\\|_p \\leq r\\}$ the ball of radius $r$ using the $p$-norm. Prove that $\\mathcal{B}_p[r]$ is convex for all $p \\geq 1$.\n1. Given convex functions $f$ and $g$, show that $\\mathrm{max}(f, g)$ is convex, too. Prove that $\\mathrm{min}(f, g)$ is not convex.\n1. Prove that the normalization of the softmax function is convex. More specifically prove the convexity of\n    $f(x) = \\log \\sum_i \\exp(x_i)$.\n1. Prove that linear subspaces, i.e., $\\mathcal{X} = \\{\\mathbf{x} | \\mathbf{W} \\mathbf{x} = \\mathbf{b}\\}$, are convex sets.\n1. Prove that in the case of linear subspaces with $\\mathbf{b} = \\mathbf{0}$ the projection $\\textrm{Proj}_\\mathcal{X}$ can be written as $\\mathbf{M} \\mathbf{x}$ for some matrix $\\mathbf{M}$.\n1. Show that for  twice-differentiable convex functions $f$ we can write $f(x + \\epsilon) = f(x) + \\epsilon f'(x) + \\frac{1}{2} \\epsilon^2 f''(x + \\xi)$ for some $\\xi \\in [0, \\epsilon]$.\n1. Given a convex set $\\mathcal{X}$ and two vectors $\\mathbf{x}$ and $\\mathbf{y}$, prove that projections never increase distances, i.e., $\\|\\mathbf{x} - \\mathbf{y}\\| \\geq \\|\\textrm{Proj}_\\mathcal{X}(\\mathbf{x}) - \\textrm{Proj}_\\mathcal{X}(\\mathbf{y})\\|$.\n\n\n[Discussions](https://discuss.d2l.ai/t/350)"
    },
    {
      "chunk_id": "7ab424661116_0",
      "chapter": "gd",
      "heading": "gd",
      "text": "# Gradient Descent\n:label:`sec_gd`\n\nIn this section we are going to introduce the basic concepts underlying *gradient descent*.\nAlthough it is rarely used directly in deep learning, an understanding of gradient descent is key to understanding stochastic gradient descent algorithms.\nFor instance, the optimization problem might diverge due to an overly large learning rate. This phenomenon can already be seen in gradient descent. Likewise, preconditioning is a common technique in gradient descent and carries over to more advanced algorithms.\nLet's start with a simple special case."
    },
    {
      "chunk_id": "552f25248666_0",
      "chapter": "gd",
      "heading": "One-Dimensional Gradient Descent",
      "text": "Gradient descent in one dimension is an excellent example to explain why the gradient descent algorithm may reduce the value of the objective function. Consider some continuously differentiable real-valued function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. Using a Taylor expansion we obtain\n\n$$f(x + \\epsilon) = f(x) + \\epsilon f'(x) + \\mathcal{O}(\\epsilon^2).$$\n:eqlabel:`gd-taylor`\n\nThat is, in first-order approximation $f(x+\\epsilon)$ is given by the function value $f(x)$ and the first derivative $f'(x)$ at $x$. It is not unreasonable to assume that for small $\\epsilon$ moving in the direction of the negative gradient will decrease $f$. To keep things simple we pick a fixed step size $\\eta > 0$ and choose $\\epsilon = -\\eta f'(x)$. Plugging this into the Taylor expansion above we get\n\n$$f(x - \\eta f'(x)) = f(x) - \\eta f'^2(x) + \\mathcal{O}(\\eta^2 f'^2(x)).$$\n:eqlabel:`gd-taylor-2`\n\nIf the derivative $f'(x) \\neq 0$ does not vanish we make progress since $\\eta f'^2(x)>0$. Moreover, we can always choose $\\eta$ small enough for the higher-order terms to become irrelevant. Hence we arrive at\n\n$$f(x - \\eta f'(x)) \\lessapprox f(x).$$\n\nThis means that, if we use\n\n$$x \\leftarrow x - \\eta f'(x)$$\n\nto iterate $x$, the value of function $f(x)$ might decline. Therefore, in gradient descent we first choose an initial value $x$ and a constant $\\eta > 0$ and then use them to continuously iterate $x$ until the stop condition is reached, for example, when the magnitude of the gradient $|f'(x)|$ is small enough or the number of iterations has reached a certain value. For simplicity we choose the objective function $f(x)=x^2$ to illustrate how to implement gradient descent. Although we know that $x=0$ is the solution to minimize $f(x)$, we still use this simple function to observe how $x$ changes."
    },
    {
      "chunk_id": "552f25248666_1",
      "chapter": "gd",
      "heading": "One-Dimensional Gradient Descent",
      "text": "For simplicity we choose the objective function $f(x)=x^2$ to illustrate how to implement gradient descent. Although we know that $x=0$ is the solution to minimize $f(x)$, we still use this simple function to observe how $x$ changes. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport numpy as np\nimport torch\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport numpy as np\nimport tensorflow as tf\n```\n\n```{.python .input}\n#@tab all\ndef f(x):  # Objective function\n    return x ** 2\n\ndef f_grad(x):  # Gradient (derivative) of the objective function\n    return 2 * x\n```\n\nNext, we use $x=10$ as the initial value and assume $\\eta=0.2$. Using gradient descent to iterate $x$ for 10 times we can see that, eventually, the value of $x$ approaches the optimal solution. ```{.python .input}\n#@tab all\ndef gd(eta, f_grad):\n    x = 10.0\n    results = [x]\n    for i in range(10):\n        x -= eta * f_grad(x)\n        results.append(float(x))\n    print(f'epoch 10, x: {x:f}')\n    return results\n\nresults = gd(0.2, f_grad)\n```\n\nThe progress of optimizing over $x$ can be plotted as follows. ```{.python .input}\n#@tab all\ndef show_trace(results, f):\n    n = max(abs(min(results)), abs(max(results)))\n    f_line = d2l.arange(-n, n, 0.01)\n    d2l.set_figsize()\n    d2l.plot([f_line, results], [[f(x) for x in f_line], [\n        f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n\nshow_trace(results, f)\n```"
    },
    {
      "chunk_id": "e27975da33fd_0",
      "chapter": "gd",
      "heading": "Learning Rate",
      "text": ":label:`subsec_gd-learningrate`\n\nThe learning rate $\\eta$ can be set by the algorithm designer. If we use a learning rate that is too small, it will cause $x$ to update very slowly, requiring more iterations to get a better solution. To show what happens in such a case, consider the progress in the same optimization problem for $\\eta = 0.05$. As we can see, even after 10 steps we are still very far from the optimal solution.\n\n```{.python .input}\n#@tab all\nshow_trace(gd(0.05, f_grad), f)\n```\n\nConversely, if we use an excessively high learning rate, $\\left|\\eta f'(x)\\right|$ might be too large for the first-order Taylor expansion formula. That is, the term $\\mathcal{O}(\\eta^2 f'^2(x))$ in :eqref:`gd-taylor-2` might become significant. In this case, we cannot guarantee that the iteration of $x$ will be able to lower the value of $f(x)$. For example, when we set the learning rate to $\\eta=1.1$, $x$ overshoots the optimal solution $x=0$ and gradually diverges.\n\n```{.python .input}\n#@tab all\nshow_trace(gd(1.1, f_grad), f)\n```"
    },
    {
      "chunk_id": "78410af718b6_0",
      "chapter": "gd",
      "heading": "Local Minima",
      "text": "To illustrate what happens for nonconvex functions consider the case of $f(x) = x \\cdot \\cos(cx)$ for some constant $c$. This function has infinitely many local minima. Depending on our choice of the learning rate and depending on how well conditioned the problem is, we may end up with one of many solutions. The example below illustrates how an (unrealistically) high learning rate will lead to a poor local minimum.\n\n```{.python .input}\n#@tab all\nc = d2l.tensor(0.15 * np.pi)\n\ndef f(x):  # Objective function\n    return x * d2l.cos(c * x)\n\ndef f_grad(x):  # Gradient of the objective function\n    return d2l.cos(c * x) - c * x * d2l.sin(c * x)\n\nshow_trace(gd(2, f_grad), f)\n```"
    },
    {
      "chunk_id": "06f50d0480cc_0",
      "chapter": "gd",
      "heading": "Multivariate Gradient Descent",
      "text": "Now that we have a better intuition of the univariate case, let's consider the situation where $\\mathbf{x} = [x_1, x_2, \\ldots, x_d]^\\top$. That is, the objective function $f: \\mathbb{R}^d \\to \\mathbb{R}$ maps vectors into scalars. Correspondingly its gradient is multivariate, too. It is a vector consisting of $d$ partial derivatives:\n\n$$\\nabla f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\\bigg]^\\top.$$\n\nEach partial derivative element $\\partial f(\\mathbf{x})/\\partial x_i$ in the gradient indicates the rate of change of $f$ at $\\mathbf{x}$ with respect to the input $x_i$. As before in the univariate case we can use the corresponding Taylor approximation for multivariate functions to get some idea of what we should do. In particular, we have that\n\n$$f(\\mathbf{x} + \\boldsymbol{\\epsilon}) = f(\\mathbf{x}) + \\mathbf{\\boldsymbol{\\epsilon}}^\\top \\nabla f(\\mathbf{x}) + \\mathcal{O}(\\|\\boldsymbol{\\epsilon}\\|^2).$$\n:eqlabel:`gd-multi-taylor`\n\nIn other words, up to second-order terms in $\\boldsymbol{\\epsilon}$ the direction of steepest descent is given by the negative gradient $-\\nabla f(\\mathbf{x})$. Choosing a suitable learning rate $\\eta > 0$ yields the prototypical gradient descent algorithm:\n\n$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x}).$$\n\nTo see how the algorithm behaves in practice let's construct an objective function $f(\\mathbf{x})=x_1^2+2x_2^2$ with a two-dimensional vector $\\mathbf{x} = [x_1, x_2]^\\top$ as input and a scalar as output. The gradient is given by $\\nabla f(\\mathbf{x}) = [2x_1, 4x_2]^\\top$. We will observe the trajectory of $\\mathbf{x}$ by gradient descent from the initial position $[-5, -2]$. To begin with, we need two more helper functions. The first uses an update function and applies it 20 times to the initial value. The second helper visualizes the trajectory of $\\mathbf{x}$."
    },
    {
      "chunk_id": "06f50d0480cc_1",
      "chapter": "gd",
      "heading": "Multivariate Gradient Descent",
      "text": "To begin with, we need two more helper functions. The first uses an update function and applies it 20 times to the initial value. The second helper visualizes the trajectory of $\\mathbf{x}$. ```{.python .input}\n#@tab all\ndef train_2d(trainer, steps=20, f_grad=None):  #@save\n    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n    # `s1` and `s2` are internal state variables that will be used in Momentum, adagrad, RMSProp\n    x1, x2, s1, s2 = -5, -2, 0, 0\n    results = [(x1, x2)]\n    for i in range(steps):\n        if f_grad:\n            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n        else:\n            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n        results.append((x1, x2))\n    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n    return results\n```\n\n```{.python .input}\n#@tab mxnet\ndef show_trace_2d(f, results):  #@save\n    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n    d2l.set_figsize()\n    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n    x1, x2 = d2l.meshgrid(d2l.arange(-55, 1, 1),\n                          d2l.arange(-30, 1, 1))\n    x1, x2 = x1.asnumpy()*0.1, x2.asnumpy()*0.1\n    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n    d2l.plt.xlabel('x1')\n    d2l.plt.ylabel('x2')\n```\n\n```{.python .input}\n#@tab tensorflow\ndef show_trace_2d(f, results):  #@save\n    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n    d2l.set_figsize()\n    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n    x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),\n                          d2l.arange(-3.0, 1.0, 0.1))\n    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n    d2l.plt.xlabel('x1')\n    d2l.plt.ylabel('x2')\n```\n\n```{.python .input}\n#@tab pytorch\ndef show_trace_2d(f, results):  #@save\n    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n    d2l.set_figsize()\n    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n    x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),\n                          d2l.arange(-3.0, 1.0, 0.1), indexing='ij')\n    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n    d2l.plt.xlabel('x1')\n    d2l.plt.ylabel('x2')\n```\n\nNext, we observe the trajectory of the optimization variable $\\mathbf{x}$ for learning rate $\\eta = 0.1$."
    },
    {
      "chunk_id": "06f50d0480cc_2",
      "chapter": "gd",
      "heading": "Multivariate Gradient Descent",
      "text": "We can see that after 20 steps the value of $\\mathbf{x}$ approaches its minimum at $[0, 0]$. Progress is fairly well-behaved albeit rather slow. ```{.python .input}\n#@tab all\ndef f_2d(x1, x2):  # Objective function\n    return x1 ** 2 + 2 * x2 ** 2\n\ndef f_2d_grad(x1, x2):  # Gradient of the objective function\n    return (2 * x1, 4 * x2)\n\ndef gd_2d(x1, x2, s1, s2, f_grad):\n    g1, g2 = f_grad(x1, x2)\n    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n\neta = 0.1\nshow_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))\n```"
    },
    {
      "chunk_id": "015966b1b966_0",
      "chapter": "gd",
      "heading": "Adaptive Methods",
      "text": "As we could see in :numref:`subsec_gd-learningrate`, getting the learning rate $\\eta$ \"just right\" is tricky. If we pick it too small, we make little progress. If we pick it too large, the solution oscillates and in the worst case it might even diverge. What if we could determine $\\eta$ automatically or get rid of having to select a learning rate at all?\nSecond-order methods that look not only at the value and gradient of the objective function\nbut also at its *curvature* can help in this case. While these methods cannot be applied to deep learning directly due to the computational cost, they provide useful intuition into how to design advanced optimization algorithms that mimic many of the desirable properties of the algorithms outlined below."
    },
    {
      "chunk_id": "0f39c8ea0bf4_0",
      "chapter": "gd",
      "heading": "Newton's Method",
      "text": "Reviewing the Taylor expansion of some function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ there is no need to stop after the first term. In fact, we can write it as\n\n$$f(\\mathbf{x} + \\boldsymbol{\\epsilon}) = f(\\mathbf{x}) + \\boldsymbol{\\epsilon}^\\top \\nabla f(\\mathbf{x}) + \\frac{1}{2} \\boldsymbol{\\epsilon}^\\top \\nabla^2 f(\\mathbf{x}) \\boldsymbol{\\epsilon} + \\mathcal{O}(\\|\\boldsymbol{\\epsilon}\\|^3).$$\n:eqlabel:`gd-hot-taylor`\n\nTo avoid cumbersome notation we define $\\mathbf{H} \\stackrel{\\textrm{def}}{=} \\nabla^2 f(\\mathbf{x})$ to be the Hessian of $f$, which is a $d \\times d$ matrix. For small $d$ and simple problems $\\mathbf{H}$ is easy to compute. For deep neural networks, on the other hand, $\\mathbf{H}$ may be prohibitively large, due to the cost of storing $\\mathcal{O}(d^2)$ entries. Furthermore it may be too expensive to compute via backpropagation. For now let's ignore such considerations and look at what algorithm we would get. After all, the minimum of $f$ satisfies $\\nabla f = 0$. Following calculus rules in :numref:`subsec_calculus-grad`,\nby taking derivatives of :eqref:`gd-hot-taylor` with regard to $\\boldsymbol{\\epsilon}$ and ignoring higher-order terms we arrive at\n\n$$\\nabla f(\\mathbf{x}) + \\mathbf{H} \\boldsymbol{\\epsilon} = 0 \\textrm{ and hence }\n\\boldsymbol{\\epsilon} = -\\mathbf{H}^{-1} \\nabla f(\\mathbf{x}).$$\n\nThat is, we need to invert the Hessian $\\mathbf{H}$ as part of the optimization problem. As a simple example, for $f(x) = \\frac{1}{2} x^2$ we have $\\nabla f(x) = x$ and $\\mathbf{H} = 1$. Hence for any $x$ we obtain $\\epsilon = -x$. In other words, a *single* step is sufficient to converge perfectly without the need for any adjustment! Alas, we got a bit lucky here: the Taylor expansion was exact since $f(x+\\epsilon)= \\frac{1}{2} x^2 + \\epsilon x + \\frac{1}{2} \\epsilon^2$. Let's see what happens in other problems. Given a convex hyperbolic cosine function $f(x) = \\cosh(cx)$ for some constant $c$, we can see that\nthe global minimum at $x=0$ is reached\nafter a few iterations."
    },
    {
      "chunk_id": "0f39c8ea0bf4_1",
      "chapter": "gd",
      "heading": "Newton's Method",
      "text": "Let's see what happens in other problems. Given a convex hyperbolic cosine function $f(x) = \\cosh(cx)$ for some constant $c$, we can see that\nthe global minimum at $x=0$ is reached\nafter a few iterations. ```{.python .input}\n#@tab all\nc = d2l.tensor(0.5)\n\ndef f(x):  # Objective function\n    return d2l.cosh(c * x)\n\ndef f_grad(x):  # Gradient of the objective function\n    return c * d2l.sinh(c * x)\n\ndef f_hess(x):  # Hessian of the objective function\n    return c**2 * d2l.cosh(c * x)\n\ndef newton(eta=1):\n    x = 10.0\n    results = [x]\n    for i in range(10):\n        x -= eta * f_grad(x) / f_hess(x)\n        results.append(float(x))\n    print('epoch 10, x:', x)\n    return results\n\nshow_trace(newton(), f)\n```\n\nNow let's consider a *nonconvex* function, such as $f(x) = x \\cos(c x)$ for some constant $c$. After all, note that in Newton's method we end up dividing by the Hessian. This means that if the second derivative is *negative* we may walk into the direction of *increasing* the value of $f$. That is a fatal flaw of the algorithm. Let's see what happens in practice. ```{.python .input}\n#@tab all\nc = d2l.tensor(0.15 * np.pi)\n\ndef f(x):  # Objective function\n    return x * d2l.cos(c * x)\n\ndef f_grad(x):  # Gradient of the objective function\n    return d2l.cos(c * x) - c * x * d2l.sin(c * x)\n\ndef f_hess(x):  # Hessian of the objective function\n    return - 2 * c * d2l.sin(c * x) - x * c**2 * d2l.cos(c * x)\n\nshow_trace(newton(), f)\n```\n\nThis went spectacularly wrong. How can we fix it? One way would be to \"fix\" the Hessian by taking its absolute value instead. Another strategy is to bring back the learning rate. This seems to defeat the purpose, but not quite. Having second-order information allows us to be cautious whenever the curvature is large and to take longer steps whenever the objective function is flatter. Let's see how this works with a slightly smaller learning rate, say $\\eta = 0.5$. As we can see, we have quite an efficient algorithm. ```{.python .input}\n#@tab all\nshow_trace(newton(0.5), f)\n```"
    },
    {
      "chunk_id": "a42661345f7d_0",
      "chapter": "gd",
      "heading": "Convergence Analysis",
      "text": "We only analyze the convergence rate of Newton's method for some convex and three times differentiable objective function $f$, where the second derivative is nonzero, i.e., $f'' > 0$. The multivariate proof is a straightforward extension of the one-dimensional argument below and omitted since it does not help us much in terms of intuition. Denote by $x^{(k)}$ the value of $x$ at the $k^\\textrm{th}$ iteration and let $e^{(k)} \\stackrel{\\textrm{def}}{=} x^{(k)} - x^*$ be the distance from optimality at the $k^\\textrm{th}$ iteration. By Taylor  expansion we have that the condition $f'(x^*) = 0$ can be written as\n\n$$0 = f'(x^{(k)} - e^{(k)}) = f'(x^{(k)}) - e^{(k)} f''(x^{(k)}) + \\frac{1}{2} (e^{(k)})^2 f'''(\\xi^{(k)}),$$\n\nwhich holds for some $\\xi^{(k)} \\in [x^{(k)} - e^{(k)}, x^{(k)}]$. Dividing the above expansion by $f''(x^{(k)})$ yields\n\n$$e^{(k)} - \\frac{f'(x^{(k)})}{f''(x^{(k)})} = \\frac{1}{2} (e^{(k)})^2 \\frac{f'''(\\xi^{(k)})}{f''(x^{(k)})}.$$\n\nRecall that we have the update $x^{(k+1)} = x^{(k)} - f'(x^{(k)}) / f''(x^{(k)})$. Plugging in this update equation and taking the absolute value of both sides, we have\n\n$$\\left|e^{(k+1)}\\right| = \\frac{1}{2}(e^{(k)})^2 \\frac{\\left|f'''(\\xi^{(k)})\\right|}{f''(x^{(k)})}.$$\n\nConsequently, whenever we are in a region of bounded $\\left|f'''(\\xi^{(k)})\\right| / (2f''(x^{(k)})) \\leq c$, we have a quadratically decreasing error\n\n$$\\left|e^{(k+1)}\\right| \\leq c (e^{(k)})^2.$$\n\n\nAs an aside, optimization researchers call this *linear* convergence, whereas a condition such as $\\left|e^{(k+1)}\\right| \\leq \\alpha \\left|e^{(k)}\\right|$ would be called a *constant* rate of convergence. Note that this analysis comes with a number of caveats. First, we do not really have much of a guarantee when we will reach the region of rapid convergence. Instead, we only know that once we reach it, convergence will be very quick. Second, this analysis requires that $f$ is well-behaved up to higher-order derivatives."
    },
    {
      "chunk_id": "a42661345f7d_1",
      "chapter": "gd",
      "heading": "Convergence Analysis",
      "text": "Instead, we only know that once we reach it, convergence will be very quick. Second, this analysis requires that $f$ is well-behaved up to higher-order derivatives. It comes down to ensuring that $f$ does not have any \"surprising\" properties in terms of how it might change its values."
    },
    {
      "chunk_id": "46f80b81f69e_0",
      "chapter": "gd",
      "heading": "Preconditioning",
      "text": "Quite unsurprisingly computing and storing the full Hessian is very expensive. It is thus desirable to find alternatives. One way to improve matters is *preconditioning*. It avoids computing the Hessian in its entirety but only computes the *diagonal* entries. This leads to update algorithms of the form\n\n$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\textrm{diag}(\\mathbf{H})^{-1} \\nabla f(\\mathbf{x}).$$\n\n\nWhile this is not quite as good as the full Newton's method, it is still much better than not using it.\nTo see why this might be a good idea consider a situation where one variable denotes height in millimeters and the other one denotes height in kilometers. Assuming that for both the natural scale is in meters, we have a terrible mismatch in parametrizations. Fortunately, using preconditioning removes this. Effectively preconditioning with gradient descent amounts to selecting a different learning rate for each variable (coordinate of vector $\\mathbf{x}$).\nAs we will see later, preconditioning drives some of the innovation in stochastic gradient descent optimization algorithms."
    },
    {
      "chunk_id": "a8350e62ccd2_0",
      "chapter": "gd",
      "heading": "Gradient Descent with Line Search",
      "text": "One of the key problems in gradient descent is that we might overshoot the goal or make insufficient progress. A simple fix for the problem is to use line search in conjunction with gradient descent. That is, we use the direction given by $\\nabla f(\\mathbf{x})$ and then perform binary search as to which learning rate $\\eta$ minimizes $f(\\mathbf{x} - \\eta \\nabla f(\\mathbf{x}))$.\n\nThis algorithm converges rapidly (for an analysis and proof see e.g., :citet:`Boyd.Vandenberghe.2004`). However, for the purpose of deep learning this is not quite so feasible, since each step of the line search would require us to evaluate the objective function on the entire dataset. This is way too costly to accomplish."
    },
    {
      "chunk_id": "2862d5157b09_0",
      "chapter": "gd",
      "heading": "Summary",
      "text": "* Learning rates matter. Too large and we diverge, too small and we do not make progress.\n* Gradient descent can get stuck in local minima.\n* In high dimensions adjusting the learning rate is complicated.\n* Preconditioning can help with scale adjustment.\n* Newton's method is a lot faster once it has started working properly in convex problems.\n* Beware of using Newton's method without any adjustments for nonconvex problems."
    },
    {
      "chunk_id": "f5c63a8e2b28_0",
      "chapter": "gd",
      "heading": "Exercises",
      "text": "1. Experiment with different learning rates and objective functions for gradient descent.\n1. Implement line search to minimize a convex function in the interval $[a, b]$.\n    1. Do you need derivatives for binary search, i.e., to decide whether to pick $[a, (a+b)/2]$ or $[(a+b)/2, b]$.\n    1. How rapid is the rate of convergence for the algorithm?\n    1. Implement the algorithm and apply it to minimizing $\\log (\\exp(x) + \\exp(-2x -3))$.\n1. Design an objective function defined on $\\mathbb{R}^2$ where gradient descent is exceedingly slow. Hint: scale different coordinates differently.\n1. Implement the lightweight version of Newton's method using preconditioning:\n    1. Use diagonal Hessian as preconditioner.\n    1. Use the absolute values of that rather than the actual (possibly signed) values.\n    1. Apply this to the problem above.\n1. Apply the algorithm above to a number of objective functions (convex or not). What happens if you rotate coordinates by $45$ degrees?\n\n[Discussions](https://discuss.d2l.ai/t/351)"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Optimization Algorithms\n:label:`chap_optimization`\n\nIf you read the book in sequence up to this point you already used a number of optimization algorithms to train deep learning models.\nThey were the tools that allowed us to continue updating model parameters and to minimize the value of the loss function, as evaluated on the training set. Indeed, anyone content with treating optimization as a black box device to minimize objective functions in a simple setting might well content oneself with the knowledge that there exists an array of incantations of such a procedure (with names such as \"SGD\" and \"Adam\").\n\nTo do well, however, some deeper knowledge is required.\nOptimization algorithms are important for deep learning.\nOn the one hand, training a complex deep learning model can take hours, days, or even weeks.\nThe performance of the optimization algorithm directly affects the model's training efficiency.\nOn the other hand, understanding the principles of different optimization algorithms and the role of their hyperparameters\nwill enable us to tune the hyperparameters in a targeted manner to improve the performance of deep learning models.\n\nIn this chapter, we explore common deep learning optimization algorithms in depth.\nAlmost all optimization problems arising in deep learning are *nonconvex*.\nNonetheless, the design and analysis of algorithms in the context of *convex* problems have proven to be very instructive.\nIt is for that reason that this chapter includes a primer on convex optimization and the proof for a very simple stochastic gradient descent algorithm on a convex objective function.\n\n```toc\n:maxdepth: 2\n\noptimization-intro\nconvexity\ngd\nsgd\nminibatch-sgd\nmomentum\nadagrad\nrmsprop\nadadelta\nadam\nlr-scheduler\n```"
    },
    {
      "chunk_id": "264a07bb05cc_0",
      "chapter": "lr-scheduler",
      "heading": "lr-scheduler",
      "text": "# Learning Rate Scheduling\n:label:`sec_scheduler`\n\nSo far we primarily focused on optimization *algorithms* for how to update the weight vectors rather than on the *rate* at which they are being updated. Nonetheless, adjusting the learning rate is often just as important as the actual algorithm. There are a number of aspects to consider:\n\n* Most obviously the *magnitude* of the learning rate matters. If it is too large, optimization diverges, if it is too small, it takes too long to train or we end up with a suboptimal result. We saw previously that the condition number of the problem matters (see e.g., :numref:`sec_momentum` for details). Intuitively it is the ratio of the amount of change in the least sensitive direction vs. the most sensitive one. * Secondly, the rate of decay is just as important. If the learning rate remains large we may simply end up bouncing around the minimum and thus not reach optimality. :numref:`sec_minibatch_sgd` discussed this in some detail and we analyzed performance guarantees in :numref:`sec_sgd`. In short, we want the rate to decay, but probably more slowly than $\\mathcal{O}(t^{-\\frac{1}{2}})$ which would be a good choice for convex problems. * Another aspect that is equally important is *initialization*. This pertains both to how the parameters are set initially (review :numref:`sec_numerical_stability` for details) and also how they evolve initially. This goes under the moniker of *warmup*, i.e., how rapidly we start moving towards the solution initially. Large steps in the beginning might not be beneficial, in particular since the initial set of parameters is random. The initial update directions might be quite meaningless, too. * Lastly, there are a number of optimization variants that perform cyclical learning rate adjustment. This is beyond the scope of the current chapter. We recommend the reader to review details in :citet:`Izmailov.Podoprikhin.Garipov.ea.2018`, e.g., how to obtain better solutions by averaging over an entire *path* of parameters."
    },
    {
      "chunk_id": "264a07bb05cc_1",
      "chapter": "lr-scheduler",
      "heading": "lr-scheduler",
      "text": "This is beyond the scope of the current chapter. We recommend the reader to review details in :citet:`Izmailov.Podoprikhin.Garipov.ea.2018`, e.g., how to obtain better solutions by averaging over an entire *path* of parameters. Given the fact that there is a lot of detail needed to manage learning rates, most deep learning frameworks have tools to deal with this automatically. In the current chapter we will review the effects that different schedules have on accuracy and also show how this can be managed efficiently via a *learning rate scheduler*."
    },
    {
      "chunk_id": "54988d49f81f_0",
      "chapter": "lr-scheduler",
      "heading": "Toy Problem",
      "text": "We begin with a toy problem that is cheap enough to compute easily, yet sufficiently nontrivial to illustrate some of the key aspects. For that we pick a slightly modernized version of LeNet (`relu` instead of `sigmoid` activation, MaxPooling rather than AveragePooling), as applied to Fashion-MNIST. Moreover, we hybridize the network for performance. Since most of the code is standard we just introduce the basics without further detailed discussion. See :numref:`chap_cnn` for a refresher as needed."
    },
    {
      "chunk_id": "54988d49f81f_1",
      "chapter": "lr-scheduler",
      "heading": "Toy Problem",
      "text": "Moreover, we hybridize the network for performance. Since most of the code is standard we just introduce the basics without further detailed discussion. See :numref:`chap_cnn` for a refresher as needed. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, lr_scheduler, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()\n\nnet = nn.HybridSequential()\nnet.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='relu'),\n        nn.MaxPool2D(pool_size=2, strides=2),\n        nn.Conv2D(channels=16, kernel_size=5, activation='relu'),\n        nn.MaxPool2D(pool_size=2, strides=2),\n        nn.Dense(120, activation='relu'),\n        nn.Dense(84, activation='relu'),\n        nn.Dense(10))\nnet.hybridize()\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\ndevice = d2l.try_gpu()\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n\n# The code is almost identical to `d2l.train_ch6` defined in the\n# lenet section of chapter convolutional neural networks\ndef train(net, train_iter, test_iter, num_epochs, loss, trainer, device):\n    net.initialize(force_reinit=True, ctx=device, init=init.Xavier())\n    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],\n                            legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n        for i, (X, y) in enumerate(train_iter):\n            X, y = X.as_in_ctx(device), y.as_in_ctx(device)\n            with autograd.record():\n                y_hat = net(X)\n                l = loss(y_hat, y)\n            l.backward()\n            trainer.step(X.shape[0])\n            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])\n            train_loss = metric[0] / metric[2]\n            train_acc = metric[1] / metric[2]\n            if (i + 1) % 50 == 0:\n                animator.add(epoch + i / len(train_iter),\n                             (train_loss, train_acc, None))\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n          f'test acc {test_acc:.3f}')\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport math\nimport torch\nfrom torch import nn\nfrom torch.optim import lr_scheduler\n\ndef net_fn():\n    model = nn.Sequential(\n        nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        nn.Flatten(),\n        nn.Linear(16 * 5 * 5, 120), nn.ReLU(),\n        nn.Linear(120, 84), nn.ReLU(),\n        nn.Linear(84, 10))\n\n    return model\n\nloss = nn.CrossEntropyLoss()\ndevice = d2l.try_gpu()\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n\n# The code is almost identical to `d2l.train_ch6` defined in the\n# lenet section of chapter convolutional neural networks\ndef train(net, train_iter, test_iter, num_epochs, loss, trainer, device,\n          scheduler=None):\n    net.to(device)\n    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],\n                            legend=['train loss', 'train acc', 'test acc'])\n\n    for epoch in range(num_epochs):\n        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n        for i, (X, y) in enumerate(train_iter):\n            net.train()\n            trainer.zero_grad()\n            X, y = X.to(device), y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            l.backward()\n            trainer.step()\n            with torch.no_grad():\n                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n            train_loss = metric[0] / metric[2]\n            train_acc = metric[1] / metric[2]\n            if (i + 1) % 50 == 0:\n                animator.add(epoch + i / len(train_iter),\n                             (train_loss, train_acc, None))\n\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch+1, (None, None, test_acc))\n\n        if scheduler:\n            if scheduler.__module__ == lr_scheduler.__name__:\n                # Using PyTorch In-Built scheduler\n                scheduler.step()\n            else:\n                # Using custom defined scheduler\n                for param_group in trainer.param_groups:\n                    param_group['lr'] = scheduler(epoch)\n\n    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n          f'test acc {test_acc:.3f}')\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport math\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\ndef net():\n    return tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu',\n                               padding='same'),\n        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n        tf.keras.layers.Conv2D(filters=16, kernel_size=5,\n                               activation='relu'),\n        tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(120, activation='relu'),\n        tf.keras.layers.Dense(84, activation='sigmoid'),\n        tf.keras.layers.Dense(10)])\n\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n\n# The code is almost identical to `d2l.train_ch6` defined in the\n# lenet section of chapter convolutional neural networks\ndef train(net_fn, train_iter, test_iter, num_epochs, lr,\n              device=d2l.try_gpu(), custom_callback = False):\n    device_name = device._device_name\n    strategy = tf.distribute.OneDeviceStrategy(device_name)\n    with strategy.scope():\n        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        net = net_fn()\n        net.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    callback = d2l.TrainCallback(net, train_iter, test_iter, num_epochs,\n                             device_name)\n    if custom_callback is False:\n        net.fit(train_iter, epochs=num_epochs, verbose=0,\n                callbacks=[callback])\n    else:\n         net.fit(train_iter, epochs=num_epochs, verbose=0,\n                 callbacks=[callback, custom_callback])\n    return net\n```\n\nLet's have a look at what happens if we invoke this algorithm with default settings, such as a learning rate of $0.3$ and train for $30$ iterations."
    },
    {
      "chunk_id": "54988d49f81f_2",
      "chapter": "lr-scheduler",
      "heading": "Toy Problem",
      "text": "Note how the training accuracy keeps on increasing while progress in terms of test accuracy stalls beyond a point. The gap between both curves indicates overfitting. ```{.python .input}\n#@tab mxnet\nlr, num_epochs = 0.3, 30\nnet.initialize(force_reinit=True, ctx=device, init=init.Xavier())\ntrainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\n```\n\n```{.python .input}\n#@tab pytorch\nlr, num_epochs = 0.3, 30\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\n```\n\n```{.python .input}\n#@tab tensorflow\nlr, num_epochs = 0.3, 30\ntrain(net, train_iter, test_iter, num_epochs, lr)\n```"
    },
    {
      "chunk_id": "d0a8be5e514a_0",
      "chapter": "lr-scheduler",
      "heading": "Schedulers",
      "text": "One way of adjusting the learning rate is to set it explicitly at each step. This is conveniently achieved by the `set_learning_rate` method. We could adjust it downward after every epoch (or even after every minibatch), e.g., in a dynamic manner in response to how optimization is progressing. ```{.python .input}\n#@tab mxnet\ntrainer.set_learning_rate(0.1)\nprint(f'learning rate is now {trainer.learning_rate:.2f}')\n```\n\n```{.python .input}\n#@tab pytorch\nlr = 0.1\ntrainer.param_groups[0][\"lr\"] = lr\nprint(f'learning rate is now {trainer.param_groups[0][\"lr\"]:.2f}')\n```\n\n```{.python .input}\n#@tab tensorflow\nlr = 0.1\ndummy_model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\ndummy_model.compile(tf.keras.optimizers.SGD(learning_rate=lr), loss='mse')\nprint(f'learning rate is now ,', dummy_model.optimizer.lr.numpy())\n```\n\nMore generally we want to define a scheduler. When invoked with the number of updates it returns the appropriate value of the learning rate. Let's define a simple one that sets the learning rate to $\\eta = \\eta_0 (t + 1)^{-\\frac{1}{2}}$. ```{.python .input}\n#@tab all\nclass SquareRootScheduler:\n    def __init__(self, lr=0.1):\n        self.lr = lr\n\n    def __call__(self, num_update):\n        return self.lr * pow(num_update + 1.0, -0.5)\n```\n\nLet's plot its behavior over a range of values. ```{.python .input}\n#@tab all\nscheduler = SquareRootScheduler(lr=0.1)\nd2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n```\n\nNow let's see how this plays out for training on Fashion-MNIST. We simply provide the scheduler as an additional argument to the training algorithm."
    },
    {
      "chunk_id": "d0a8be5e514a_1",
      "chapter": "lr-scheduler",
      "heading": "Schedulers",
      "text": "We simply provide the scheduler as an additional argument to the training algorithm. ```{.python .input}\n#@tab mxnet\ntrainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\n      scheduler)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrain(net, train_iter, test_iter, num_epochs, lr,\n      custom_callback=LearningRateScheduler(scheduler))\n```\n\nThis worked quite a bit better than previously. Two things stand out: the curve was rather more smooth than previously. Secondly, there was less overfitting. Unfortunately it is not a well-resolved question as to why certain strategies lead to less overfitting in *theory*. There is some argument that a smaller stepsize will lead to parameters that are closer to zero and thus simpler. However, this does not explain the phenomenon entirely since we do not really stop early but simply reduce the learning rate gently."
    },
    {
      "chunk_id": "80cacdeec289_0",
      "chapter": "lr-scheduler",
      "heading": "Policies",
      "text": "While we cannot possibly cover the entire variety of learning rate schedulers, we attempt to give a brief overview of popular policies below. Common choices are polynomial decay and piecewise constant schedules. Beyond that, cosine learning rate schedules have been found to work well empirically on some problems. Lastly, on some problems it is beneficial to warm up the optimizer prior to using large learning rates."
    },
    {
      "chunk_id": "c2553084a69c_0",
      "chapter": "lr-scheduler",
      "heading": "Factor Scheduler",
      "text": "One alternative to a polynomial decay would be a multiplicative one, that is $\\eta_{t+1} \\leftarrow \\eta_t \\cdot \\alpha$ for $\\alpha \\in (0, 1)$. To prevent the learning rate from decaying beyond a reasonable lower bound the update equation is often modified to $\\eta_{t+1} \\leftarrow \\mathop{\\mathrm{max}}(\\eta_{\\mathrm{min}}, \\eta_t \\cdot \\alpha)$.\n\n```{.python .input}\n#@tab all\nclass FactorScheduler:\n    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1):\n        self.factor = factor\n        self.stop_factor_lr = stop_factor_lr\n        self.base_lr = base_lr\n\n    def __call__(self, num_update):\n        self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)\n        return self.base_lr\n\nscheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)\nd2l.plot(d2l.arange(50), [scheduler(t) for t in range(50)])\n```\n\nThis can also be accomplished by a built-in scheduler in MXNet via the `lr_scheduler.FactorScheduler` object. It takes a few more parameters, such as warmup period, warmup mode (linear or constant), the maximum number of desired updates, etc.; Going forward we will use the built-in schedulers as appropriate and only explain their functionality here. As illustrated, it is fairly straightforward to build your own scheduler if needed."
    },
    {
      "chunk_id": "a6aad3f3d48d_0",
      "chapter": "lr-scheduler",
      "heading": "Multi Factor Scheduler",
      "text": "A common strategy for training deep networks is to keep the learning rate piecewise constant and to decrease it by a given amount every so often. That is, given a set of times when to decrease the rate, such as $s = \\{5, 10, 20\\}$ decrease $\\eta_{t+1} \\leftarrow \\eta_t \\cdot \\alpha$ whenever $t \\in s$. Assuming that the values are halved at each step we can implement this as follows. ```{.python .input}\n#@tab mxnet\nscheduler = lr_scheduler.MultiFactorScheduler(step=[15, 30], factor=0.5,\n                                              base_lr=0.5)\nd2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n```\n\n```{.python .input}\n#@tab pytorch\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=0.5)\nscheduler = lr_scheduler.MultiStepLR(trainer, milestones=[15, 30], gamma=0.5)\n\ndef get_lr(trainer, scheduler):\n    lr = scheduler.get_last_lr()[0]\n    trainer.step()\n    scheduler.step()\n    return lr\n\nd2l.plot(d2l.arange(num_epochs), [get_lr(trainer, scheduler)\n                                  for t in range(num_epochs)])\n```\n\n```{.python .input}\n#@tab tensorflow\nclass MultiFactorScheduler:\n    def __init__(self, step, factor, base_lr):\n        self.step = step\n        self.factor = factor\n        self.base_lr = base_lr\n\n    def __call__(self, epoch):\n        if epoch in self.step:\n            self.base_lr = self.base_lr * self.factor\n            return self.base_lr\n        else:\n            return self.base_lr\n\nscheduler = MultiFactorScheduler(step=[15, 30], factor=0.5, base_lr=0.5)\nd2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n```\n\nThe intuition behind this piecewise constant learning rate schedule is that one lets optimization proceed until a stationary point has been reached in terms of the distribution of weight vectors. Then (and only then) do we decrease the rate such as to obtain a higher quality proxy to a good local minimum. The example below shows how this can produce ever slightly better solutions."
    },
    {
      "chunk_id": "a6aad3f3d48d_1",
      "chapter": "lr-scheduler",
      "heading": "Multi Factor Scheduler",
      "text": "Then (and only then) do we decrease the rate such as to obtain a higher quality proxy to a good local minimum. The example below shows how this can produce ever slightly better solutions. ```{.python .input}\n#@tab mxnet\ntrainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\n```\n\n```{.python .input}\n#@tab pytorch\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\n      scheduler)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrain(net, train_iter, test_iter, num_epochs, lr,\n      custom_callback=LearningRateScheduler(scheduler))\n```"
    },
    {
      "chunk_id": "fa566f7abaac_0",
      "chapter": "lr-scheduler",
      "heading": "Cosine Scheduler",
      "text": "A rather perplexing heuristic was proposed by :citet:`Loshchilov.Hutter.2016`. It relies on the observation that we might not want to decrease the learning rate too drastically in the beginning and moreover, that we might want to \"refine\" the solution in the end using a very small learning rate. This results in a cosine-like schedule with the following functional form for learning rates in the range $t \\in [0, T]$. $$\\eta_t = \\eta_T + \\frac{\\eta_0 - \\eta_T}{2} \\left(1 + \\cos(\\pi t/T)\\right)$$\n\n\nHere $\\eta_0$ is the initial learning rate, $\\eta_T$ is the target rate at time $T$. Furthermore, for $t > T$ we simply pin the value to $\\eta_T$ without increasing it again. In the following example, we set the max update step $T = 20$."
    },
    {
      "chunk_id": "fa566f7abaac_1",
      "chapter": "lr-scheduler",
      "heading": "Cosine Scheduler",
      "text": "Furthermore, for $t > T$ we simply pin the value to $\\eta_T$ without increasing it again. In the following example, we set the max update step $T = 20$. ```{.python .input}\n#@tab mxnet\nscheduler = lr_scheduler.CosineScheduler(max_update=20, base_lr=0.3,\n                                         final_lr=0.01)\nd2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n```\n\n```{.python .input}\n#@tab pytorch, tensorflow\nclass CosineScheduler:\n    def __init__(self, max_update, base_lr=0.01, final_lr=0,\n               warmup_steps=0, warmup_begin_lr=0):\n        self.base_lr_orig = base_lr\n        self.max_update = max_update\n        self.final_lr = final_lr\n        self.warmup_steps = warmup_steps\n        self.warmup_begin_lr = warmup_begin_lr\n        self.max_steps = self.max_update - self.warmup_steps\n\n    def get_warmup_lr(self, epoch):\n        increase = (self.base_lr_orig - self.warmup_begin_lr) \\\n                       * float(epoch) / float(self.warmup_steps)\n        return self.warmup_begin_lr + increase\n\n    def __call__(self, epoch):\n        if epoch < self.warmup_steps:\n            return self.get_warmup_lr(epoch)\n        if epoch <= self.max_update:\n            self.base_lr = self.final_lr + (\n                self.base_lr_orig - self.final_lr) * (1 + math.cos(\n                math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2\n        return self.base_lr\n\nscheduler = CosineScheduler(max_update=20, base_lr=0.3, final_lr=0.01)\nd2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n```\n\nIn the context of computer vision this schedule *can* lead to improved results. Note, though, that such improvements are not guaranteed (as can be seen below)."
    },
    {
      "chunk_id": "fa566f7abaac_2",
      "chapter": "lr-scheduler",
      "heading": "Cosine Scheduler",
      "text": "Note, though, that such improvements are not guaranteed (as can be seen below). ```{.python .input}\n#@tab mxnet\ntrainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\n      scheduler)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrain(net, train_iter, test_iter, num_epochs, lr,\n      custom_callback=LearningRateScheduler(scheduler))\n```"
    },
    {
      "chunk_id": "2410f94d3c49_0",
      "chapter": "lr-scheduler",
      "heading": "Warmup",
      "text": "In some cases initializing the parameters is not sufficient to guarantee a good solution. This is particularly a problem for some advanced network designs that may lead to unstable optimization problems. We could address this by choosing a sufficiently small learning rate to prevent divergence in the beginning. Unfortunately this means that progress is slow. Conversely, a large learning rate initially leads to divergence. A rather simple fix for this dilemma is to use a warmup period during which the learning rate *increases* to its initial maximum and to cool down the rate until the end of the optimization process. For simplicity one typically uses a linear increase for this purpose. This leads to a schedule of the form indicated below. ```{.python .input}\n#@tab mxnet\nscheduler = lr_scheduler.CosineScheduler(20, warmup_steps=5, base_lr=0.3,\n                                         final_lr=0.01)\nd2l.plot(np.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n```\n\n```{.python .input}\n#@tab pytorch, tensorflow\nscheduler = CosineScheduler(20, warmup_steps=5, base_lr=0.3, final_lr=0.01)\nd2l.plot(d2l.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n```\n\nNote that the network converges better initially (in particular observe the performance during the first 5 epochs). ```{.python .input}\n#@tab mxnet\ntrainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'lr_scheduler': scheduler})\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\n```\n\n```{.python .input}\n#@tab pytorch\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\n      scheduler)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrain(net, train_iter, test_iter, num_epochs, lr,\n      custom_callback=LearningRateScheduler(scheduler))\n```\n\nWarmup can be applied to any scheduler (not just cosine)."
    },
    {
      "chunk_id": "2410f94d3c49_1",
      "chapter": "lr-scheduler",
      "heading": "Warmup",
      "text": "For a more detailed discussion of learning rate schedules and many more experiments see also :cite:`Gotmare.Keskar.Xiong.ea.2018`. In particular they find that a warmup phase limits the amount of divergence of parameters in very deep networks. This makes intuitively sense since we would expect significant divergence due to random initialization in those parts of the network that take the most time to make progress in the beginning."
    },
    {
      "chunk_id": "6a23e4f4de15_0",
      "chapter": "lr-scheduler",
      "heading": "Summary",
      "text": "* Decreasing the learning rate during training can lead to improved accuracy and (most perplexingly) reduced overfitting of the model.\n* A piecewise decrease of the learning rate whenever progress has plateaued is effective in practice. Essentially this ensures that we converge efficiently to a suitable solution and only then reduce the inherent variance of the parameters by reducing the learning rate.\n* Cosine schedulers are popular for some computer vision problems. See e.g., [GluonCV](http://gluon-cv.mxnet.io) for details of such a scheduler.\n* A warmup period before optimization can prevent divergence.\n* Optimization serves multiple purposes in deep learning. Besides minimizing the training objective, different choices of optimization algorithms and learning rate scheduling can lead to rather different amounts of generalization and overfitting on the test set (for the same amount of training error)."
    },
    {
      "chunk_id": "2e5a7d32b78b_0",
      "chapter": "lr-scheduler",
      "heading": "Exercises",
      "text": "1. Experiment with the optimization behavior for a given fixed learning rate. What is the best model you can obtain this way?\n1. How does convergence change if you change the exponent of the decrease in the learning rate? Use `PolyScheduler` for your convenience in the experiments.\n1. Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet. How does it affect performance relative to other schedulers?\n1. How long should warmup last?\n1. Can you connect optimization and sampling? Start by using results from :citet:`Welling.Teh.2011` on Stochastic Gradient Langevin Dynamics.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/359)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1080)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1081)\n:end_tab:"
    },
    {
      "chunk_id": "5491dd87b6b6_0",
      "chapter": "minibatch-sgd",
      "heading": "minibatch-sgd",
      "text": "# Minibatch Stochastic Gradient Descent\n:label:`sec_minibatch_sgd`\n\nSo far we encountered two extremes in the approach to gradient-based learning: :numref:`sec_gd` uses the full dataset to compute gradients and to update parameters, one pass at a time. Conversely :numref:`sec_sgd` processes one training example at a time to make progress.\nEither of them has its own drawbacks.\nGradient descent is not particularly *data efficient* whenever data is very similar.\nStochastic gradient descent is not particularly *computationally efficient* since CPUs and GPUs cannot exploit the full power of vectorization.\nThis suggests that there might be something in between,\nand in fact, that is what we have been using so far in the examples we discussed."
    },
    {
      "chunk_id": "c0e279759183_0",
      "chapter": "minibatch-sgd",
      "heading": "Vectorization and Caches",
      "text": "At the heart of the decision to use minibatches is computational efficiency. This is most easily understood when considering parallelization to multiple GPUs and multiple servers. In this case we need to send at least one image to each GPU. With 8 GPUs per server and 16 servers we already arrive at a minibatch size no smaller than 128. Things are a bit more subtle when it comes to single GPUs or even CPUs. These devices have multiple types of memory, often multiple types of computational units and different bandwidth constraints between them. For instance, a CPU has a small number of registers and then the L1, L2, and in some cases even L3 cache (which is shared among different processor cores). These caches are of increasing size and latency (and at the same time they are of decreasing bandwidth). Suffice to say, the processor is capable of performing many more operations than what the main memory interface is able to provide. First, a 2GHz CPU with 16 cores and AVX-512 vectorization can process up to $2 \\cdot 10^9 \\cdot 16 \\cdot 32 = 10^{12}$ bytes per second. The capability of GPUs easily exceeds this number by a factor of 100. On the other hand, a midrange server processor might not have much more than 100 GB/s bandwidth, i.e., less than one tenth of what would be required to keep the processor fed. To make matters worse, not all memory access is created equal: memory interfaces are typically 64 bit wide or wider (e.g., on GPUs up to 384 bit), hence reading a single byte incurs the cost of a much wider access. Second, there is significant overhead for the first access whereas sequential access is relatively cheap (this is often called a burst read). There are many more things to keep in mind, such as caching when we have multiple sockets, chiplets, and other structures. See this [Wikipedia article](https://en.wikipedia.org/wiki/Cache_hierarchy)\nfor a more in-depth discussion."
    },
    {
      "chunk_id": "c0e279759183_1",
      "chapter": "minibatch-sgd",
      "heading": "Vectorization and Caches",
      "text": "There are many more things to keep in mind, such as caching when we have multiple sockets, chiplets, and other structures. See this [Wikipedia article](https://en.wikipedia.org/wiki/Cache_hierarchy)\nfor a more in-depth discussion. The way to alleviate these constraints is to use a hierarchy of CPU caches that are actually fast enough to supply the processor with data. This is *the* driving force behind batching in deep learning. To keep matters simple, consider matrix-matrix multiplication, say $\\mathbf{A} = \\mathbf{B}\\mathbf{C}$. We have a number of options for calculating $\\mathbf{A}$. For instance, we could try the following:\n\n1. We could compute $\\mathbf{A}_{ij} = \\mathbf{B}_{i,:} \\mathbf{C}_{:,j}$, i.e., we could compute it elementwise by means of dot products. 1. We could compute $\\mathbf{A}_{:,j} = \\mathbf{B} \\mathbf{C}_{:,j}$, i.e., we could compute it one column at a time. Likewise we could compute $\\mathbf{A}$ one row $\\mathbf{A}_{i,:}$ at a time. 1. We could simply compute $\\mathbf{A} = \\mathbf{B} \\mathbf{C}$. 1. We could break $\\mathbf{B}$ and $\\mathbf{C}$ into smaller block matrices and compute $\\mathbf{A}$ one block at a time. If we follow the first option, we will need to copy one row and one column vector into the CPU each time we want to compute an element $\\mathbf{A}_{ij}$. Even worse, due to the fact that matrix elements are aligned sequentially we are thus required to access many disjoint locations for one of the two vectors as we read them from memory. The second option is much more favorable. In it, we are able to keep the column vector $\\mathbf{C}_{:,j}$ in the CPU cache while we keep on traversing through $\\mathbf{B}$. This halves the memory bandwidth requirement with correspondingly faster access. Of course, option 3 is most desirable. Unfortunately, most matrices might not entirely fit into cache (this is what we are discussing after all). However, option 4 offers a practically useful alternative: we can move blocks of the matrix into cache and multiply them locally."
    },
    {
      "chunk_id": "c0e279759183_2",
      "chapter": "minibatch-sgd",
      "heading": "Vectorization and Caches",
      "text": "Unfortunately, most matrices might not entirely fit into cache (this is what we are discussing after all). However, option 4 offers a practically useful alternative: we can move blocks of the matrix into cache and multiply them locally. Optimized libraries take care of this for us. Let's have a look at how efficient these operations are in practice. Beyond computational efficiency, the overhead introduced by Python and by the deep learning framework itself is considerable. Recall that each time we execute a command the Python interpreter sends a command to the MXNet engine which needs to insert it into the computational graph and deal with it during scheduling. Such overhead can be quite detrimental. In short, it is highly advisable to use vectorization (and matrices) whenever possible. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nimport time\nnpx.set_np()\n\nA = np.zeros((256, 256))\nB = np.random.normal(0, 1, (256, 256))\nC = np.random.normal(0, 1, (256, 256))\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport numpy as np\nimport time\nimport torch\nfrom torch import nn\n\nA = torch.zeros(256, 256)\nB = torch.randn(256, 256)\nC = torch.randn(256, 256)\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nA = tf.Variable(d2l.zeros((256, 256)))\nB = tf.Variable(d2l.normal([256, 256], 0, 1))\nC = tf.Variable(d2l.normal([256, 256], 0, 1))\n```\n\nSince we will benchmark the running time frequently in the rest of the book, let's define a timer."
    },
    {
      "chunk_id": "c0e279759183_3",
      "chapter": "minibatch-sgd",
      "heading": "Vectorization and Caches",
      "text": "```{.python .input}\n#@tab all\nclass Timer:  #@save\n    \"\"\"Record multiple running times.\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start()\n\n    def start(self):\n        \"\"\"Start the timer.\"\"\"\n        self.tik = time.time()\n\n    def stop(self):\n        \"\"\"Stop the timer and record the time in a list.\"\"\"\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n\n    def avg(self):\n        \"\"\"Return the average time.\"\"\"\n        return sum(self.times) / len(self.times)\n\n    def sum(self):\n        \"\"\"Return the sum of time.\"\"\"\n        return sum(self.times)\n\n    def cumsum(self):\n        \"\"\"Return the accumulated time.\"\"\"\n        return np.array(self.times).cumsum().tolist()\n\ntimer = Timer()\n```\n\nElement-wise assignment simply iterates over all rows and columns of $\\mathbf{B}$ and $\\mathbf{C}$ respectively to assign the value to $\\mathbf{A}$. ```{.python .input}\n#@tab mxnet\n# Compute A = BC one element at a time\ntimer.start()\nfor i in range(256):\n    for j in range(256):\n        A[i, j] = np.dot(B[i, :], C[:, j])\nA.wait_to_read()\ntimer.stop()\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute A = BC one element at a time\ntimer.start()\nfor i in range(256):\n    for j in range(256):\n        A[i, j] = torch.dot(B[i, :], C[:, j])\ntimer.stop()\n```\n\n```{.python .input}\n#@tab tensorflow\n# Compute A = BC one element at a time\ntimer.start()\nfor i in range(256):\n    for j in range(256):\n        A[i, j].assign(tf.tensordot(B[i, :], C[:, j], axes=1))\ntimer.stop()\n```\n\nA faster strategy is to perform column-wise assignment."
    },
    {
      "chunk_id": "c0e279759183_4",
      "chapter": "minibatch-sgd",
      "heading": "Vectorization and Caches",
      "text": "```{.python .input}\n#@tab mxnet\n# Compute A = BC one column at a time\ntimer.start()\nfor j in range(256):\n    A[:, j] = np.dot(B, C[:, j])\nA.wait_to_read()\ntimer.stop()\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute A = BC one column at a time\ntimer.start()\nfor j in range(256):\n    A[:, j] = torch.mv(B, C[:, j])\ntimer.stop()\n```\n\n```{.python .input}\n#@tab tensorflow\ntimer.start()\nfor j in range(256):\n    A[:, j].assign(tf.tensordot(B, C[:, j], axes=1))\ntimer.stop()\n```\n\nLast, the most effective manner is to perform the entire operation in one block. Note that multiplying any two matrices $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{C} \\in \\mathbb{R}^{n \\times p}$ takes approximately $2mnp$ floating point operations,\nwhen scalar multiplication and addition are counted as separate operations (fused in practice). Thus, multiplying two $256 \\times 256$ matrices\ntakes $0.03$ billion floating point operations. Let's see what the respective speed of the operations is. ```{.python .input}\n#@tab mxnet\n# Compute A = BC in one go\ntimer.start()\nA = np.dot(B, C)\nA.wait_to_read()\ntimer.stop()\n\ngigaflops = [0.03 / i for i in timer.times]\nprint(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')\n```\n\n```{.python .input}\n#@tab pytorch\n# Compute A = BC in one go\ntimer.start()\nA = torch.mm(B, C)\ntimer.stop()\n\ngigaflops = [0.03 / i for i in timer.times]\nprint(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')\n```\n\n```{.python .input}\n#@tab tensorflow\ntimer.start()\nA.assign(tf.tensordot(B, C, axes=1))\ntimer.stop()\n\ngigaflops = [0.03 / i for i in timer.times]\nprint(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')\n```"
    },
    {
      "chunk_id": "681e993e69b1_0",
      "chapter": "minibatch-sgd",
      "heading": "Minibatches",
      "text": ":label:`sec_minibatches`\n\nIn the past we took it for granted that we would read *minibatches* of data rather than single observations to update parameters. We now give a brief justification for it. Processing single observations requires us to perform many single matrix-vector (or even vector-vector) multiplications, which is quite expensive and which incurs a significant overhead on behalf of the underlying deep learning framework. This applies both to evaluating a network when applied to data (often referred to as inference) and when computing gradients to update parameters. That is, this applies whenever we perform $\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g}_t$ where\n\n$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} f(\\mathbf{x}_{t}, \\mathbf{w})$$\n\nWe can increase the *computational* efficiency of this operation by applying it to a minibatch of observations at a time. That is, we replace the gradient $\\mathbf{g}_t$ over a single observation by one over a small batch\n\n$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w})$$\n\nLet's see what this does to the statistical properties of $\\mathbf{g}_t$: since both $\\mathbf{x}_t$ and also all elements of the minibatch $\\mathcal{B}_t$ are drawn uniformly at random from the training set, the expectation of the gradient remains unchanged. The variance, on the other hand, is reduced significantly. Since the minibatch gradient is composed of $b \\stackrel{\\textrm{def}}{=} |\\mathcal{B}_t|$ independent gradients which are being averaged, its standard deviation is reduced by a factor of $b^{-\\frac{1}{2}}$. This, by itself, is a good thing, since it means that the updates are more reliably aligned with the full gradient. Naively this would indicate that choosing a large minibatch $\\mathcal{B}_t$ would be universally desirable. Alas, after some point, the additional reduction in standard deviation is minimal when compared to the linear increase in computational cost."
    },
    {
      "chunk_id": "681e993e69b1_1",
      "chapter": "minibatch-sgd",
      "heading": "Minibatches",
      "text": "Naively this would indicate that choosing a large minibatch $\\mathcal{B}_t$ would be universally desirable. Alas, after some point, the additional reduction in standard deviation is minimal when compared to the linear increase in computational cost. In practice we pick a minibatch that is large enough to offer good computational efficiency while still fitting into the memory of a GPU. To illustrate the savings let's have a look at some code. In it we perform the same matrix-matrix multiplication, but this time broken up into \"minibatches\" of 64 columns at a time. ```{.python .input}\n#@tab mxnet\ntimer.start()\nfor j in range(0, 256, 64):\n    A[:, j:j+64] = np.dot(B, C[:, j:j+64])\ntimer.stop()\nprint(f'performance in Gigaflops: block {0.03 / timer.times[3]:.3f}')\n```\n\n```{.python .input}\n#@tab pytorch\ntimer.start()\nfor j in range(0, 256, 64):\n    A[:, j:j+64] = torch.mm(B, C[:, j:j+64])\ntimer.stop()\nprint(f'performance in Gigaflops: block {0.03 / timer.times[3]:.3f}')\n```\n\n```{.python .input}\n#@tab tensorflow\ntimer.start()\nfor j in range(0, 256, 64):\n    A[:, j:j+64].assign(tf.tensordot(B, C[:, j:j+64], axes=1))\ntimer.stop()\nprint(f'performance in Gigaflops: block {0.03 / timer.times[3]:.3f}')\n```\n\nAs we can see, the computation on the minibatch is essentially as efficient as on the full matrix. A word of caution is in order. In :numref:`sec_batch_norm` we used a type of regularization that was heavily dependent on the amount of variance in a minibatch. As we increase the latter, the variance decreases and with it the benefit of the noise-injection due to batch normalization. See e.g., :citet:`Ioffe.2017` for details on how to rescale and compute the appropriate terms."
    },
    {
      "chunk_id": "46497f4ffcfa_0",
      "chapter": "minibatch-sgd",
      "heading": "Reading the Dataset",
      "text": "Let's have a look at how minibatches are efficiently generated from data. In the following we use a dataset developed by NASA to test the wing [noise from different aircraft](https://archive.ics.uci.edu/dataset/291/airfoil+self+noise) to compare these optimization algorithms. For convenience we only use the first $1,500$ examples. The data is whitened for preprocessing, i.e., we remove the mean and rescale the variance to $1$ per coordinate."
    },
    {
      "chunk_id": "46497f4ffcfa_1",
      "chapter": "minibatch-sgd",
      "heading": "Reading the Dataset",
      "text": "For convenience we only use the first $1,500$ examples. The data is whitened for preprocessing, i.e., we remove the mean and rescale the variance to $1$ per coordinate. ```{.python .input}\n#@tab mxnet\n#@save\nd2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n\n#@save\ndef get_data_ch11(batch_size=10, n=1500):\n    data = np.genfromtxt(d2l.download('airfoil'),\n                         dtype=np.float32, delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    data_iter = d2l.load_array(\n        (data[:n, :-1], data[:n, -1]), batch_size, is_train=True)\n    return data_iter, data.shape[1]-1\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nd2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n\n#@save\ndef get_data_ch11(batch_size=10, n=1500):\n    data = np.genfromtxt(d2l.download('airfoil'),\n                         dtype=np.float32, delimiter='\\t')\n    data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\n    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n                               batch_size, is_train=True)\n    return data_iter, data.shape[1]-1\n```\n\n```{.python .input}\n#@tab tensorflow\n#@save\nd2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n\n#@save\ndef get_data_ch11(batch_size=10, n=1500):\n    data = np.genfromtxt(d2l.download('airfoil'),\n                         dtype=np.float32, delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n                               batch_size, is_train=True)\n    return data_iter, data.shape[1]-1\n```"
    },
    {
      "chunk_id": "852851396fa0_0",
      "chapter": "minibatch-sgd",
      "heading": "Implementation from Scratch",
      "text": "Recall the minibatch stochastic gradient descent implementation from :numref:`sec_linear_scratch`. In the following we provide a slightly more general implementation. For convenience it has the same call signature as the other optimization algorithms introduced later in this chapter. Specifically, we add the status\ninput `states` and place the hyperparameter in dictionary `hyperparams`. In\naddition, we will average the loss of each minibatch example in the training\nfunction, so the gradient in the optimization algorithm does not need to be\ndivided by the batch size. ```{.python .input}\n#@tab mxnet\ndef sgd(params, states, hyperparams):\n    for p in params:\n        p[:] -= hyperparams['lr'] * p.grad\n```\n\n```{.python .input}\n#@tab pytorch\ndef sgd(params, states, hyperparams):\n    for p in params:\n        p.data.sub_(hyperparams['lr'] * p.grad)\n        p.grad.data.zero_()\n```\n\n```{.python .input}\n#@tab tensorflow\ndef sgd(params, grads, states, hyperparams):\n    for param, grad in zip(params, grads):\n        param.assign_sub(hyperparams['lr']*grad)\n```\n\nNext, we implement a generic training function to facilitate the use of the other optimization algorithms introduced later in this chapter. It initializes a linear regression model and can be used to train the model with minibatch stochastic gradient descent and other algorithms introduced subsequently."
    },
    {
      "chunk_id": "852851396fa0_1",
      "chapter": "minibatch-sgd",
      "heading": "Implementation from Scratch",
      "text": "It initializes a linear regression model and can be used to train the model with minibatch stochastic gradient descent and other algorithms introduced subsequently. ```{.python .input}\n#@tab mxnet\n#@save\ndef train_ch11(trainer_fn, states, hyperparams, data_iter,\n               feature_dim, num_epochs=2):\n    # Initialization\n    w = np.random.normal(scale=0.01, size=(feature_dim, 1))\n    b = np.zeros(1)\n    w.attach_grad()\n    b.attach_grad()\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    # Train\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, d2l.Timer()\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n            with autograd.record():\n                l = loss(net(X), y).mean()\n            l.backward()\n            trainer_fn([w, b], states, hyperparams)\n            n += X.shape[0]\n            if n % 200 == 0:\n                timer.stop()\n                animator.add(n/X.shape[0]/len(data_iter),\n                             (d2l.evaluate_loss(net, data_iter, loss),))\n                timer.start()\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n    return timer.cumsum(), animator.Y[0]\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef train_ch11(trainer_fn, states, hyperparams, data_iter,\n               feature_dim, num_epochs=2):\n    # Initialization\n    w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\n                     requires_grad=True)\n    b = torch.zeros((1), requires_grad=True)\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    # Train\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, d2l.Timer()\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n            l = loss(net(X), y).mean()\n            l.backward()\n            trainer_fn([w, b], states, hyperparams)\n            n += X.shape[0]\n            if n % 200 == 0:\n                timer.stop()\n                animator.add(n/X.shape[0]/len(data_iter),\n                             (d2l.evaluate_loss(net, data_iter, loss),))\n                timer.start()\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n    return timer.cumsum(), animator.Y[0]\n```\n\n```{.python .input}\n#@tab tensorflow\n#@save\ndef train_ch11(trainer_fn, states, hyperparams, data_iter,\n               feature_dim, num_epochs=2):\n    # Initialization\n    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1),\n                                   mean=0, stddev=0.01),trainable=True)\n    b = tf.Variable(tf.zeros(1), trainable=True)\n\n    # Train\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, d2l.Timer()\n\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n          with tf.GradientTape() as g:\n            l = tf.math.reduce_mean(loss(net(X), y))\n\n          dw, db = g.gradient(l, [w, b])\n          trainer_fn([w, b], [dw, db], states, hyperparams)\n          n += X.shape[0]\n          if n % 200 == 0:\n              timer.stop()\n              p = n/X.shape[0]\n              q = p/tf.data.experimental.cardinality(data_iter).numpy()\n              r = (d2l.evaluate_loss(net, data_iter, loss),)\n              animator.add(q, r)\n              timer.start()\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n    return timer.cumsum(), animator.Y[0]\n```\n\nLet's see how optimization proceeds for batch gradient descent."
    },
    {
      "chunk_id": "852851396fa0_2",
      "chapter": "minibatch-sgd",
      "heading": "Implementation from Scratch",
      "text": "This can be achieved by setting the minibatch size to 1500 (i.e., to the total number of examples). As a result the model parameters are updated only once per epoch. There is little progress. In fact, after 6 steps progress stalls. ```{.python .input}\n#@tab all\ndef train_sgd(lr, batch_size, num_epochs=2):\n    data_iter, feature_dim = get_data_ch11(batch_size)\n    return train_ch11(\n        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)\n\ngd_res = train_sgd(1, 1500, 10)\n```\n\nWhen the batch size equals 1, we use stochastic gradient descent for optimization. For simplicity of implementation we picked a constant (albeit small) learning rate. In stochastic gradient descent, the model parameters are updated whenever an example is processed. In our case this amounts to 1500 updates per epoch. As we can see, the decline in the value of the objective function slows down after one epoch. Although both the procedures processed 1500 examples within one epoch, stochastic gradient descent consumes more time than gradient descent in our experiment. This is because stochastic gradient descent updated the parameters more frequently and since it is less efficient to process single observations one at a time. ```{.python .input}\n#@tab all\nsgd_res = train_sgd(0.005, 1)\n```\n\nFinally, when the batch size equals 100, we use minibatch stochastic gradient descent for optimization. The time required per epoch is shorter than the time needed for stochastic gradient descent and the time for batch gradient descent. ```{.python .input}\n#@tab all\nmini1_res = train_sgd(.4, 100)\n```\n\nReducing the batch size to 10, the time for each epoch increases because the workload for each batch is less efficient to execute. ```{.python .input}\n#@tab all\nmini2_res = train_sgd(.05, 10)\n```\n\nNow we can compare the time vs. loss for the previous four experiments."
    },
    {
      "chunk_id": "852851396fa0_3",
      "chapter": "minibatch-sgd",
      "heading": "Implementation from Scratch",
      "text": "```{.python .input}\n#@tab all\nmini2_res = train_sgd(.05, 10)\n```\n\nNow we can compare the time vs. loss for the previous four experiments. As can be seen, although stochastic gradient descent converges faster than GD in terms of number of examples processed, it uses more time to reach the same loss than GD because computing the gradient example by example is not as efficient. Minibatch stochastic gradient descent is able to trade-off convergence speed and computation efficiency. A minibatch size of 10 is more efficient than stochastic gradient descent; a minibatch size of 100 even outperforms GD in terms of runtime. ```{.python .input}\n#@tab all\nd2l.set_figsize([6, 3])\nd2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),\n         'time (sec)', 'loss', xlim=[1e-2, 10],\n         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\nd2l.plt.gca().set_xscale('log')\n```"
    },
    {
      "chunk_id": "7d2307ea0359_0",
      "chapter": "minibatch-sgd",
      "heading": "Concise Implementation",
      "text": "In Gluon, we can use the `Trainer` class to call optimization algorithms. This is used to implement a generic training function. We will use this throughout the current chapter."
    },
    {
      "chunk_id": "7d2307ea0359_1",
      "chapter": "minibatch-sgd",
      "heading": "Concise Implementation",
      "text": "In Gluon, we can use the `Trainer` class to call optimization algorithms. This is used to implement a generic training function. We will use this throughout the current chapter. ```{.python .input}\n#@tab mxnet\n#@save\ndef train_concise_ch11(tr_name, hyperparams, data_iter, num_epochs=2):\n    # Initialization\n    net = nn.Sequential()\n    net.add(nn.Dense(1))\n    net.initialize(init.Normal(sigma=0.01))\n    trainer = gluon.Trainer(net.collect_params(), tr_name, hyperparams)\n    loss = gluon.loss.L2Loss()\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, d2l.Timer()\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n            with autograd.record():\n                l = loss(net(X), y)\n            l.backward()\n            trainer.step(X.shape[0])\n            n += X.shape[0]\n            if n % 200 == 0:\n                timer.stop()\n                animator.add(n/X.shape[0]/len(data_iter),\n                             (d2l.evaluate_loss(net, data_iter, loss),))\n                timer.start()\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\ndef train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):\n    # Initialization\n    net = nn.Sequential(nn.Linear(5, 1))\n    def init_weights(module):\n        if type(module) == nn.Linear:\n            torch.nn.init.normal_(module.weight, std=0.01)\n    net.apply(init_weights)\n\n    optimizer = trainer_fn(net.parameters(), **hyperparams)\n    loss = nn.MSELoss(reduction='none')\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, d2l.Timer()\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n            optimizer.zero_grad()\n            out = net(X)\n            y = y.reshape(out.shape)\n            l = loss(out, y)\n            l.mean().backward()\n            optimizer.step()\n            n += X.shape[0]\n            if n % 200 == 0:\n                timer.stop()\n                # `MSELoss` computes squared error without the 1/2 factor\n                animator.add(n/X.shape[0]/len(data_iter),\n                             (d2l.evaluate_loss(net, data_iter, loss) / 2,))\n                timer.start()\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n```\n\n```{.python .input}\n#@tab tensorflow\n#@save\ndef train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=2):\n    # Initialization\n    net = tf.keras.Sequential()\n    net.add(tf.keras.layers.Dense(1,\n            kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    optimizer = trainer_fn(**hyperparams)\n    loss = tf.keras.losses.MeanSquaredError()\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, d2l.Timer()\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n            with tf.GradientTape() as g:\n                out = net(X)\n                l = loss(y, out)\n                params = net.trainable_variables\n                grads = g.gradient(l, params)\n            optimizer.apply_gradients(zip(grads, params))\n            n += X.shape[0]\n            if n % 200 == 0:\n                timer.stop()\n                p = n/X.shape[0]\n                q = p/tf.data.experimental.cardinality(data_iter).numpy()\n                # `MeanSquaredError` computes squared error without the 1/2\n                # factor\n                r = (d2l.evaluate_loss(net, data_iter, loss) / 2,)\n                animator.add(q, r)\n                timer.start()\n    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/epoch')\n```\n\nUsing Gluon to repeat the last experiment shows identical behavior."
    },
    {
      "chunk_id": "7d2307ea0359_2",
      "chapter": "minibatch-sgd",
      "heading": "Concise Implementation",
      "text": "```{.python .input}\n#@tab mxnet\ndata_iter, _ = get_data_ch11(10)\ntrain_concise_ch11('sgd', {'learning_rate': 0.05}, data_iter)\n```\n\n```{.python .input}\n#@tab pytorch\ndata_iter, _ = get_data_ch11(10)\ntrainer = torch.optim.SGD\ntrain_concise_ch11(trainer, {'lr': 0.01}, data_iter)\n```\n\n```{.python .input}\n#@tab tensorflow\ndata_iter, _ = get_data_ch11(10)\ntrainer = tf.keras.optimizers.SGD\ntrain_concise_ch11(trainer, {'learning_rate': 0.05}, data_iter)\n```"
    },
    {
      "chunk_id": "1bceac949869_0",
      "chapter": "minibatch-sgd",
      "heading": "Summary",
      "text": "* Vectorization makes code more efficient due to reduced overhead arising from the deep learning framework and due to better memory locality and caching on CPUs and GPUs.\n* There is a trade-off between statistical efficiency arising from stochastic gradient descent and computational efficiency arising from processing large batches of data at a time.\n* Minibatch stochastic gradient descent offers the best of both worlds: computational and statistical efficiency.\n* In minibatch stochastic gradient descent we process batches of data obtained by a random permutation of the training data (i.e., each observation is processed only once per epoch, albeit in random order).\n* It is advisable to decay the learning rates during training.\n* In general, minibatch stochastic gradient descent is faster than stochastic gradient descent and gradient descent for convergence to a smaller risk, when measured in terms of clock time."
    },
    {
      "chunk_id": "8ebb8203e7f5_0",
      "chapter": "minibatch-sgd",
      "heading": "Exercises",
      "text": "1. Modify the batch size and learning rate and observe the rate of decline for the value of the objective function and the time consumed in each epoch.\n1. Read the MXNet documentation and use the `Trainer` class `set_learning_rate` function to reduce the learning rate of the minibatch stochastic gradient descent to 1/10 of its previous value after each epoch.\n1. Compare minibatch stochastic gradient descent with a variant that actually *samples with replacement* from the training set. What happens?\n1. An evil genie replicates your dataset without telling you (i.e., each observation occurs twice and your dataset grows to twice its original size, but nobody told you). How does the behavior of stochastic gradient descent, minibatch stochastic gradient descent and that of gradient descent change?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/353)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1068)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1069)\n:end_tab:"
    },
    {
      "chunk_id": "3673c83710b3_0",
      "chapter": "momentum",
      "heading": "momentum",
      "text": "# Momentum\n:label:`sec_momentum`\n\nIn :numref:`sec_sgd` we reviewed what happens when performing stochastic gradient descent, i.e., when performing optimization where only a noisy variant of the gradient is available. In particular, we noticed that for noisy gradients we need to be extra cautious when it comes to choosing the learning rate in the face of noise. If we decrease it too rapidly, convergence stalls. If we are too lenient, we fail to converge to a good enough solution since noise keeps on driving us away from optimality."
    },
    {
      "chunk_id": "96755d4a583c_0",
      "chapter": "momentum",
      "heading": "Basics",
      "text": "In this section, we will explore more effective optimization algorithms, especially for certain types of optimization problems that are common in practice."
    },
    {
      "chunk_id": "877e926b20ad_0",
      "chapter": "momentum",
      "heading": "Leaky Averages",
      "text": "The previous section saw us discussing minibatch SGD as a means for accelerating computation. It also had the nice side-effect that averaging gradients reduced the amount of variance. The minibatch stochastic gradient descent can be calculated by:\n\n$$\\mathbf{g}_{t, t-1} = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w}_{t-1}) = \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{h}_{i, t-1}. $$\n\nTo keep the notation simple, here we used $\\mathbf{h}_{i, t-1} = \\partial_{\\mathbf{w}} f(\\mathbf{x}_i, \\mathbf{w}_{t-1})$ as the stochastic gradient descent for sample $i$ using the weights updated at time $t-1$. It would be nice if we could benefit from the effect of variance reduction even beyond averaging gradients on a minibatch. One option to accomplish this task is to replace the gradient computation by a \"leaky average\":\n\n$$\\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}$$\n\nfor some $\\beta \\in (0, 1)$. This effectively replaces the instantaneous gradient by one that is been averaged over multiple *past* gradients. $\\mathbf{v}$ is called *velocity*. It accumulates past gradients similar to how a heavy ball rolling down the objective function landscape integrates over past forces. To see what is happening in more detail let's expand $\\mathbf{v}_t$ recursively into\n\n$$\\begin{aligned}\n\\mathbf{v}_t = \\beta^2 \\mathbf{v}_{t-2} + \\beta \\mathbf{g}_{t-1, t-2} + \\mathbf{g}_{t, t-1}\n= \\ldots, = \\sum_{\\tau = 0}^{t-1} \\beta^{\\tau} \\mathbf{g}_{t-\\tau, t-\\tau-1}. \\end{aligned}$$\n\nLarge $\\beta$ amounts to a long-range average, whereas small $\\beta$ amounts to only a slight correction relative to a gradient method. The new gradient replacement no longer points into the direction of steepest descent on a particular instance any longer but rather in the direction of a weighted average of past gradients. This allows us to realize most of the benefits of averaging over a batch without the cost of actually computing the gradients on it."
    },
    {
      "chunk_id": "877e926b20ad_1",
      "chapter": "momentum",
      "heading": "Leaky Averages",
      "text": "This allows us to realize most of the benefits of averaging over a batch without the cost of actually computing the gradients on it. We will revisit this averaging procedure in more detail later. The above reasoning formed the basis for what is now known as *accelerated* gradient methods, such as gradients with momentum. They enjoy the additional benefit of being much more effective in cases where the optimization problem is ill-conditioned (i.e., where there are some directions where progress is much slower than in others, resembling a narrow canyon). Furthermore, they allow us to average over subsequent gradients to obtain more stable directions of descent. Indeed, the aspect of acceleration even for noise-free convex problems is one of the key reasons why momentum works and why it works so well. As one would expect, due to its efficacy momentum is a well-studied subject in optimization for deep learning and beyond. See e.g., the beautiful [expository article](https://distill.pub/2017/momentum/) by :citet:`Goh.2017` for an in-depth analysis and interactive animation. It was proposed by :citet:`Polyak.1964`. :citet:`Nesterov.2018` has a detailed theoretical discussion in the context of convex optimization. Momentum in deep learning has been known to be beneficial for a long time. See e.g., the discussion by :citet:`Sutskever.Martens.Dahl.ea.2013` for details."
    },
    {
      "chunk_id": "6e3ddb2e0cd8_0",
      "chapter": "momentum",
      "heading": "An Ill-conditioned Problem",
      "text": "To get a better understanding of the geometric properties of the momentum method we revisit gradient descent, albeit with a significantly less pleasant objective function. Recall that in :numref:`sec_gd` we used $f(\\mathbf{x}) = x_1^2 + 2 x_2^2$, i.e., a moderately distorted ellipsoid objective. We distort this function further by stretching it out in the $x_1$ direction via\n\n$$f(\\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2.$$\n\nAs before $f$ has its minimum at $(0, 0)$. This function is *very* flat in the direction of $x_1$. Let's see what happens when we perform gradient descent as before on this new function. We pick a learning rate of $0.4$. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n\neta = 0.4\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\ndef gd_2d(x1, x2, s1, s2):\n    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\n\neta = 0.4\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\ndef gd_2d(x1, x2, s1, s2):\n    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n\neta = 0.4\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\ndef gd_2d(x1, x2, s1, s2):\n    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))\n```\n\nBy construction, the gradient in the $x_2$ direction is *much* higher and changes much more rapidly than in the horizontal $x_1$ direction. Thus we are stuck between two undesirable choices: if we pick a small learning rate we ensure that the solution does not diverge in the $x_2$ direction but we are saddled with slow convergence in the $x_1$ direction. Conversely, with a large learning rate we progress rapidly in the $x_1$ direction but diverge in $x_2$."
    },
    {
      "chunk_id": "6e3ddb2e0cd8_1",
      "chapter": "momentum",
      "heading": "An Ill-conditioned Problem",
      "text": "Conversely, with a large learning rate we progress rapidly in the $x_1$ direction but diverge in $x_2$. The example below illustrates what happens even after a slight increase in learning rate from $0.4$ to $0.6$. Convergence in the $x_1$ direction improves but the overall solution quality is much worse. ```{.python .input}\n#@tab all\neta = 0.6\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))\n```"
    },
    {
      "chunk_id": "03a419588488_0",
      "chapter": "momentum",
      "heading": "The Momentum Method",
      "text": "The momentum method allows us to solve the gradient descent problem described\nabove. Looking at the optimization trace above we might intuit that averaging gradients over the past would work well. After all, in the $x_1$ direction this will aggregate well-aligned gradients, thus increasing the distance we cover with every step. Conversely, in the $x_2$ direction where gradients oscillate, an aggregate gradient will reduce step size due to oscillations that cancel each other out.\nUsing $\\mathbf{v}_t$ instead of the gradient $\\mathbf{g}_t$ yields the following update equations:\n\n$$\n\\begin{aligned}\n\\mathbf{v}_t &\\leftarrow \\beta \\mathbf{v}_{t-1} + \\mathbf{g}_{t, t-1}, \\\\\n\\mathbf{x}_t &\\leftarrow \\mathbf{x}_{t-1} - \\eta_t \\mathbf{v}_t.\n\\end{aligned}\n$$\n\nNote that for $\\beta = 0$ we recover regular gradient descent. Before delving deeper into the mathematical properties let's have a quick look at how the algorithm behaves in practice.\n\n```{.python .input}\n#@tab all\ndef momentum_2d(x1, x2, v1, v2):\n    v1 = beta * v1 + 0.2 * x1\n    v2 = beta * v2 + 4 * x2\n    return x1 - eta * v1, x2 - eta * v2, v1, v2\n\neta, beta = 0.6, 0.5\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\n```\n\nAs we can see, even with the same learning rate that we used before, momentum still converges well. Let's see what happens when we decrease the momentum parameter. Halving it to $\\beta = 0.25$ leads to a trajectory that barely converges at all. Nonetheless, it is a lot better than without momentum (when the solution diverges).\n\n```{.python .input}\n#@tab all\neta, beta = 0.6, 0.25\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\n```\n\nNote that we can combine momentum with stochastic gradient descent and in particular, minibatch stochastic gradient descent. The only change is that in that case we replace the gradients $\\mathbf{g}_{t, t-1}$ with $\\mathbf{g}_t$. Last, for convenience we initialize $\\mathbf{v}_0 = 0$ at time $t=0$. Let's look at what leaky averaging actually does to the updates."
    },
    {
      "chunk_id": "7980dac2f48b_0",
      "chapter": "momentum",
      "heading": "Effective Sample Weight",
      "text": "Recall that $\\mathbf{v}_t = \\sum_{\\tau = 0}^{t-1} \\beta^{\\tau} \\mathbf{g}_{t-\\tau, t-\\tau-1}$. In the limit the terms add up to $\\sum_{\\tau=0}^\\infty \\beta^\\tau = \\frac{1}{1-\\beta}$. In other words, rather than taking a step of size $\\eta$ in gradient descent or stochastic gradient descent we take a step of size $\\frac{\\eta}{1-\\beta}$ while at the same time, dealing with a potentially much better behaved descent direction. These are two benefits in one. To illustrate how weighting behaves for different choices of $\\beta$ consider the diagram below.\n\n```{.python .input}\n#@tab all\nd2l.set_figsize()\nbetas = [0.95, 0.9, 0.6, 0]\nfor beta in betas:\n    x = d2l.numpy(d2l.arange(40))\n    d2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')\nd2l.plt.xlabel('time')\nd2l.plt.legend();\n```"
    },
    {
      "chunk_id": "89e280e4a4e2_0",
      "chapter": "momentum",
      "heading": "Practical Experiments",
      "text": "Let's see how momentum works in practice, i.e., when used within the context of a proper optimizer. For this we need a somewhat more scalable implementation."
    },
    {
      "chunk_id": "f2145bdf6e3f_0",
      "chapter": "momentum",
      "heading": "Implementation from Scratch",
      "text": "Compared with (minibatch) stochastic gradient descent the momentum method needs to maintain a set of  auxiliary variables, i.e., velocity. It has the same shape as the gradients (and variables of the optimization problem). In the implementation below we call these variables `states`. ```{.python .input}\n#@tab mxnet,pytorch\ndef init_momentum_states(feature_dim):\n    v_w = d2l.zeros((feature_dim, 1))\n    v_b = d2l.zeros(1)\n    return (v_w, v_b)\n```\n\n```{.python .input}\n#@tab tensorflow\ndef init_momentum_states(features_dim):\n    v_w = tf.Variable(d2l.zeros((features_dim, 1)))\n    v_b = tf.Variable(d2l.zeros(1))\n    return (v_w, v_b)\n```\n\n```{.python .input}\n#@tab mxnet\ndef sgd_momentum(params, states, hyperparams):\n    for p, v in zip(params, states):\n        v[:] = hyperparams['momentum'] * v + p.grad\n        p[:] -= hyperparams['lr'] * v\n```\n\n```{.python .input}\n#@tab pytorch\ndef sgd_momentum(params, states, hyperparams):\n    for p, v in zip(params, states):\n        with torch.no_grad():\n            v[:] = hyperparams['momentum'] * v + p.grad\n            p[:] -= hyperparams['lr'] * v\n        p.grad.data.zero_()\n```\n\n```{.python .input}\n#@tab tensorflow\ndef sgd_momentum(params, grads, states, hyperparams):\n    for p, v, g in zip(params, states, grads):\n            v[:].assign(hyperparams['momentum'] * v + g)\n            p[:].assign(p - hyperparams['lr'] * v)\n```\n\nLet's see how this works in practice. ```{.python .input}\n#@tab all\ndef train_momentum(lr, momentum, num_epochs=2):\n    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),\n                   {'lr': lr, 'momentum': momentum}, data_iter,\n                   feature_dim, num_epochs)\n\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\ntrain_momentum(0.02, 0.5)\n```\n\nWhen we increase the momentum hyperparameter `momentum` to 0.9, it amounts to a significantly larger effective sample size of $\\frac{1}{1 - 0.9} = 10$. We reduce the learning rate slightly to $0.01$ to keep matters under control."
    },
    {
      "chunk_id": "f2145bdf6e3f_1",
      "chapter": "momentum",
      "heading": "Implementation from Scratch",
      "text": "We reduce the learning rate slightly to $0.01$ to keep matters under control. ```{.python .input}\n#@tab all\ntrain_momentum(0.01, 0.9)\n```\n\nReducing the learning rate further addresses any issue of non-smooth optimization problems. Setting it to $0.005$ yields good convergence properties. ```{.python .input}\n#@tab all\ntrain_momentum(0.005, 0.9)\n```"
    },
    {
      "chunk_id": "0ee4b1747786_0",
      "chapter": "momentum",
      "heading": "Concise Implementation",
      "text": "There is very little to do in Gluon since the standard `sgd` solver already had momentum built in. Setting matching parameters yields a very similar trajectory.\n\n```{.python .input}\n#@tab mxnet\nd2l.train_concise_ch11('sgd', {'learning_rate': 0.005, 'momentum': 0.9},\n                       data_iter)\n```\n\n```{.python .input}\n#@tab pytorch\ntrainer = torch.optim.SGD\nd2l.train_concise_ch11(trainer, {'lr': 0.005, 'momentum': 0.9}, data_iter)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrainer = tf.keras.optimizers.SGD\nd2l.train_concise_ch11(trainer, {'learning_rate': 0.005, 'momentum': 0.9},\n                       data_iter)\n```"
    },
    {
      "chunk_id": "6cf01d3435ce_0",
      "chapter": "momentum",
      "heading": "Theoretical Analysis",
      "text": "So far the 2D example of $f(x) = 0.1 x_1^2 + 2 x_2^2$ seemed rather contrived. We will now see that this is actually quite representative of the types of problem one might encounter, at least in the case of minimizing convex quadratic objective functions."
    },
    {
      "chunk_id": "1735f75008e1_0",
      "chapter": "momentum",
      "heading": "Quadratic Convex Functions",
      "text": "Consider the function\n\n$$h(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} + \\mathbf{x}^\\top \\mathbf{c} + b.$$\n\nThis is a general quadratic function. For positive definite matrices $\\mathbf{Q} \\succ 0$, i.e., for matrices with positive eigenvalues this has a minimizer at $\\mathbf{x}^* = -\\mathbf{Q}^{-1} \\mathbf{c}$ with minimum value $b - \\frac{1}{2} \\mathbf{c}^\\top \\mathbf{Q}^{-1} \\mathbf{c}$. Hence we can rewrite $h$ as\n\n$$h(\\mathbf{x}) = \\frac{1}{2} (\\mathbf{x} - \\mathbf{Q}^{-1} \\mathbf{c})^\\top \\mathbf{Q} (\\mathbf{x} - \\mathbf{Q}^{-1} \\mathbf{c}) + b - \\frac{1}{2} \\mathbf{c}^\\top \\mathbf{Q}^{-1} \\mathbf{c}.$$\n\nThe gradient is given by $\\partial_{\\mathbf{x}} h(\\mathbf{x}) = \\mathbf{Q} (\\mathbf{x} - \\mathbf{Q}^{-1} \\mathbf{c})$. That is, it is given by the distance between $\\mathbf{x}$ and the minimizer, multiplied by $\\mathbf{Q}$. Consequently also the velocity  is a linear combination of terms $\\mathbf{Q} (\\mathbf{x}_t - \\mathbf{Q}^{-1} \\mathbf{c})$. Since $\\mathbf{Q}$ is positive definite it can be decomposed into its eigensystem via $\\mathbf{Q} = \\mathbf{O}^\\top \\boldsymbol{\\Lambda} \\mathbf{O}$ for an orthogonal (rotation) matrix $\\mathbf{O}$ and a diagonal matrix $\\boldsymbol{\\Lambda}$ of positive eigenvalues. This allows us to perform a change of variables from $\\mathbf{x}$ to $\\mathbf{z} \\stackrel{\\textrm{def}}{=} \\mathbf{O} (\\mathbf{x} - \\mathbf{Q}^{-1} \\mathbf{c})$ to obtain a much simplified expression:\n\n$$h(\\mathbf{z}) = \\frac{1}{2} \\mathbf{z}^\\top \\boldsymbol{\\Lambda} \\mathbf{z} + b'.$$\n\nHere $b' = b - \\frac{1}{2} \\mathbf{c}^\\top \\mathbf{Q}^{-1} \\mathbf{c}$. Since $\\mathbf{O}$ is only an orthogonal matrix this does not perturb the gradients in a meaningful way. Expressed in terms of $\\mathbf{z}$ gradient descent becomes\n\n$$\\mathbf{z}_t = \\mathbf{z}_{t-1} - \\boldsymbol{\\Lambda} \\mathbf{z}_{t-1} = (\\mathbf{I} - \\boldsymbol{\\Lambda}) \\mathbf{z}_{t-1}.$$\n\nThe important fact in this expression is that gradient descent *does not mix* between different eigenspaces."
    },
    {
      "chunk_id": "1735f75008e1_1",
      "chapter": "momentum",
      "heading": "Quadratic Convex Functions",
      "text": "That is, when expressed in terms of the eigensystem of $\\mathbf{Q}$ the optimization problem proceeds in a coordinate-wise manner. This also holds for\n\n$$\\begin{aligned}\n\\mathbf{v}_t & = \\beta \\mathbf{v}_{t-1} + \\boldsymbol{\\Lambda} \\mathbf{z}_{t-1} \\\\\n\\mathbf{z}_t & = \\mathbf{z}_{t-1} - \\eta \\left(\\beta \\mathbf{v}_{t-1} + \\boldsymbol{\\Lambda} \\mathbf{z}_{t-1}\\right) \\\\\n    & = (\\mathbf{I} - \\eta \\boldsymbol{\\Lambda}) \\mathbf{z}_{t-1} - \\eta \\beta \\mathbf{v}_{t-1}. \\end{aligned}$$\n\nIn doing this we just proved the following theorem: gradient descent with and without momentum for a convex quadratic function decomposes into coordinate-wise optimization in the direction of the eigenvectors of the quadratic matrix."
    },
    {
      "chunk_id": "55b3ed5d4dcc_0",
      "chapter": "momentum",
      "heading": "Scalar Functions",
      "text": "Given the above result let's see what happens when we minimize the function $f(x) = \\frac{\\lambda}{2} x^2$. For gradient descent we have\n\n$$x_{t+1} = x_t - \\eta \\lambda x_t = (1 - \\eta \\lambda) x_t.$$\n\nWhenever $|1 - \\eta \\lambda| < 1$ this optimization converges at an exponential rate since after $t$ steps we have $x_t = (1 - \\eta \\lambda)^t x_0$. This shows how the rate of convergence improves initially as we increase the learning rate $\\eta$ until $\\eta \\lambda = 1$. Beyond that things diverge and for $\\eta \\lambda > 2$ the optimization problem diverges. ```{.python .input}\n#@tab all\nlambdas = [0.1, 1, 10, 19]\neta = 0.1\nd2l.set_figsize((6, 4))\nfor lam in lambdas:\n    t = d2l.numpy(d2l.arange(20))\n    d2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')\nd2l.plt.xlabel('time')\nd2l.plt.legend();\n```\n\nTo analyze convergence in the case of momentum we begin by rewriting the update equations in terms of two scalars: one for $x$ and one for velocity $v$. This yields:\n\n$$\n\\begin{bmatrix} v_{t+1} \\\\ x_{t+1} \\end{bmatrix} =\n\\begin{bmatrix} \\beta & \\lambda \\\\ -\\eta \\beta & (1 - \\eta \\lambda) \\end{bmatrix}\n\\begin{bmatrix} v_{t} \\\\ x_{t} \\end{bmatrix} = \\mathbf{R}(\\beta, \\eta, \\lambda) \\begin{bmatrix} v_{t} \\\\ x_{t} \\end{bmatrix}. $$\n\nWe used $\\mathbf{R}$ to denote the $2 \\times 2$ governing convergence behavior. After $t$ steps the initial choice $[v_0, x_0]$ becomes $\\mathbf{R}(\\beta, \\eta, \\lambda)^t [v_0, x_0]$. Hence, it is up to the eigenvalues of $\\mathbf{R}$ to determine the speed of convergence. See the [Distill post](https://distill.pub/2017/momentum/) of :citet:`Goh.2017` for a great animation and :citet:`Flammarion.Bach.2015` for a detailed analysis. One can show that $0 < \\eta \\lambda < 2 + 2 \\beta$ velocity converges. This is a larger range of feasible parameters when compared to $0 < \\eta \\lambda < 2$ for gradient descent. It also suggests that in general large values of $\\beta$ are desirable."
    },
    {
      "chunk_id": "55b3ed5d4dcc_1",
      "chapter": "momentum",
      "heading": "Scalar Functions",
      "text": "One can show that $0 < \\eta \\lambda < 2 + 2 \\beta$ velocity converges. This is a larger range of feasible parameters when compared to $0 < \\eta \\lambda < 2$ for gradient descent. It also suggests that in general large values of $\\beta$ are desirable. Further details require a fair amount of technical detail and we suggest that the interested reader consult the original publications."
    },
    {
      "chunk_id": "73ea7ec9311c_0",
      "chapter": "momentum",
      "heading": "Summary",
      "text": "* Momentum replaces gradients with a leaky average over past gradients. This accelerates convergence significantly.\n* It is desirable for both noise-free gradient descent and (noisy) stochastic gradient descent.\n* Momentum prevents stalling of the optimization process that is much more likely to occur for stochastic gradient descent.\n* The effective number of gradients is given by $\\frac{1}{1-\\beta}$ due to exponentiated downweighting of past data.\n* In the case of convex quadratic problems this can be analyzed explicitly in detail.\n* Implementation is quite straightforward but it requires us to store an additional state vector (velocity $\\mathbf{v}$)."
    },
    {
      "chunk_id": "9f7d4ea2c20d_0",
      "chapter": "momentum",
      "heading": "Exercises",
      "text": "1. Use other combinations of momentum hyperparameters and learning rates and observe and analyze the different experimental results.\n1. Try out gradient descent and momentum for a quadratic problem where you have multiple eigenvalues, i.e., $f(x) = \\frac{1}{2} \\sum_i \\lambda_i x_i^2$, e.g., $\\lambda_i = 2^{-i}$. Plot how the values of $x$ decrease for the initialization $x_i = 1$.\n1. Derive minimum value and minimizer for $h(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} + \\mathbf{x}^\\top \\mathbf{c} + b$.\n1. What changes when we perform stochastic gradient descent with momentum? What happens when we use minibatch stochastic gradient descent with momentum? Experiment with the parameters?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/354)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1070)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1071)\n:end_tab:"
    },
    {
      "chunk_id": "01291ec34f34_0",
      "chapter": "optimization-intro",
      "heading": "optimization-intro",
      "text": "# Optimization and Deep Learning\n:label:`sec_optimization-intro`\n\nIn this section, we will discuss the relationship between optimization and deep learning as well as the challenges of using optimization in deep learning.\nFor a deep learning problem, we will usually define a *loss function* first. Once we have the loss function, we can use an optimization algorithm in attempt to minimize the loss.\nIn optimization, a loss function is often referred to as the *objective function* of the optimization problem. By tradition and convention most optimization algorithms are concerned with *minimization*. If we ever need to maximize an objective there is a simple solution: just flip the sign on the objective."
    },
    {
      "chunk_id": "a1c0cafacf01_0",
      "chapter": "optimization-intro",
      "heading": "Goal of Optimization",
      "text": "Although optimization provides a way to minimize the loss function for deep\nlearning, in essence, the goals of optimization and deep learning are\nfundamentally different. The former is primarily concerned with minimizing an\nobjective whereas the latter is concerned with finding a suitable model, given a\nfinite amount of data. In :numref:`sec_generalization_basics`,\nwe discussed the difference between these two goals in detail. For instance,\ntraining error and generalization error generally differ: since the objective\nfunction of the optimization algorithm is usually a loss function based on the\ntraining dataset, the goal of optimization is to reduce the training error. However, the goal of deep learning (or more broadly, statistical inference) is to\nreduce the generalization error. To accomplish the latter we need to pay\nattention to overfitting in addition to using the optimization algorithm to\nreduce the training error. ```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mpl_toolkits import mplot3d\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nimport torch\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nimport tensorflow as tf\n```\n\nTo illustrate the aforementioned different goals,\nlet's consider\nthe empirical risk and the risk. As described\nin :numref:`subsec_empirical-risk-and-risk`,\nthe empirical risk\nis an average loss\non the training dataset\nwhile the risk is the expected loss\non the entire population of data. Below we define two functions:\nthe risk function `f`\nand the empirical risk function `g`. Suppose that we have only a finite amount of training data. As a result, here `g` is less smooth than `f`."
    },
    {
      "chunk_id": "a1c0cafacf01_1",
      "chapter": "optimization-intro",
      "heading": "Goal of Optimization",
      "text": "Below we define two functions:\nthe risk function `f`\nand the empirical risk function `g`. Suppose that we have only a finite amount of training data. As a result, here `g` is less smooth than `f`. ```{.python .input}\n#@tab all\ndef f(x):\n    return x * d2l.cos(np.pi * x)\n\ndef g(x):\n    return f(x) + 0.2 * d2l.cos(5 * np.pi * x)\n```\n\nThe graph below illustrates that the minimum of the empirical risk on a training dataset may be at a different location from the minimum of the risk (generalization error). ```{.python .input}\n#@tab all\ndef annotate(text, xy, xytext):  #@save\n    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,\n                           arrowprops=dict(arrowstyle='->'))\n\nx = d2l.arange(0.5, 1.5, 0.01)\nd2l.set_figsize((4.5, 2.5))\nd2l.plot(x, [f(x), g(x)], 'x', 'risk')\nannotate('min of\\nempirical risk', (1.0, -1.2), (0.5, -1.1))\nannotate('min of risk', (1.1, -1.05), (0.95, -0.5))\n```"
    },
    {
      "chunk_id": "6f7000029ff5_0",
      "chapter": "optimization-intro",
      "heading": "Optimization Challenges in Deep Learning",
      "text": "In this chapter, we are going to focus specifically on the performance of optimization algorithms in minimizing the objective function, rather than a\nmodel's generalization error.\nIn :numref:`sec_linear_regression`\nwe distinguished between analytical solutions and numerical solutions in\noptimization problems.\nIn deep learning, most objective functions are\ncomplicated and do not have analytical solutions. Instead, we must use numerical\noptimization algorithms.\nThe optimization algorithms in this chapter\nall fall into this\ncategory.\n\nThere are many challenges in deep learning optimization. Some of the most vexing ones are local minima, saddle points, and vanishing gradients.\nLet's have a look at them."
    },
    {
      "chunk_id": "41638e7682a7_0",
      "chapter": "optimization-intro",
      "heading": "Local Minima",
      "text": "For any objective function $f(x)$,\nif the value of $f(x)$ at $x$ is smaller than the values of $f(x)$ at any other points in the vicinity of $x$, then $f(x)$ could be a local minimum.\nIf the value of $f(x)$ at $x$ is the minimum of the objective function over the entire domain,\nthen $f(x)$ is the global minimum.\n\nFor example, given the function\n\n$$f(x) = x \\cdot \\textrm{cos}(\\pi x) \\textrm{ for } -1.0 \\leq x \\leq 2.0,$$\n\nwe can approximate the local minimum and global minimum of this function.\n\n```{.python .input}\n#@tab all\nx = d2l.arange(-1.0, 2.0, 0.01)\nd2l.plot(x, [f(x), ], 'x', 'f(x)')\nannotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))\nannotate('global minimum', (1.1, -0.95), (0.6, 0.8))\n```\n\nThe objective function of deep learning models usually has many local optima.\nWhen the numerical solution of an optimization problem is near the local optimum, the numerical solution obtained by the final iteration may only minimize the objective function *locally*, rather than *globally*, as the gradient of the objective function's solutions approaches or becomes zero.\nOnly some degree of noise might knock the parameter out of the local minimum. In fact, this is one of the beneficial properties of\nminibatch stochastic gradient descent where the natural variation of gradients over minibatches is able to dislodge the parameters from local minima."
    },
    {
      "chunk_id": "087d8d964f0f_0",
      "chapter": "optimization-intro",
      "heading": "Saddle Points",
      "text": "Besides local minima, saddle points are another reason for gradients to vanish. A *saddle point* is any location where all gradients of a function vanish but which is neither a global nor a local minimum. Consider the function $f(x) = x^3$. Its first and second derivative vanish for $x=0$. Optimization might stall at this point, even though it is not a minimum. ```{.python .input}\n#@tab all\nx = d2l.arange(-2.0, 2.0, 0.01)\nd2l.plot(x, [x**3], 'x', 'f(x)')\nannotate('saddle point', (0, -0.2), (-0.52, -5.0))\n```\n\nSaddle points in higher dimensions are even more insidious, as the example below shows. Consider the function $f(x, y) = x^2 - y^2$. It has its saddle point at $(0, 0)$. This is a maximum with respect to $y$ and a minimum with respect to $x$. Moreover, it *looks* like a saddle, which is where this mathematical property got its name. ```{.python .input}\n#@tab mxnet\nx, y = d2l.meshgrid(\n    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))\nz = x**2 - y**2\n\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x.asnumpy(), y.asnumpy(), z.asnumpy(),\n                  **{'rstride': 10, 'cstride': 10})\nax.plot([0], [0], [0], 'rx')\nticks = [-1, 0, 1]\nd2l.plt.xticks(ticks)\nd2l.plt.yticks(ticks)\nax.set_zticks(ticks)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y');\n```\n\n```{.python .input}\n#@tab pytorch, tensorflow\nx, y = d2l.meshgrid(\n    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))\nz = x**2 - y**2\n\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})\nax.plot([0], [0], [0], 'rx')\nticks = [-1, 0, 1]\nd2l.plt.xticks(ticks)\nd2l.plt.yticks(ticks)\nax.set_zticks(ticks)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y');\n```\n\nWe assume that the input of a function is a $k$-dimensional vector and its\noutput is a scalar, so its Hessian matrix will have $k$ eigenvalues."
    },
    {
      "chunk_id": "087d8d964f0f_1",
      "chapter": "optimization-intro",
      "heading": "Saddle Points",
      "text": "The solution of the\nfunction could be a local minimum, a local maximum, or a saddle point at a\nposition where the function gradient is zero:\n\n* When the eigenvalues of the function's Hessian matrix at the zero-gradient position are all positive, we have a local minimum for the function. * When the eigenvalues of the function's Hessian matrix at the zero-gradient position are all negative, we have a local maximum for the function. * When the eigenvalues of the function's Hessian matrix at the zero-gradient position are negative and positive, we have a saddle point for the function. For high-dimensional problems the likelihood that at least *some* of the eigenvalues are negative is quite high. This makes saddle points more likely than local minima. We will discuss some exceptions to this situation in the next section when introducing convexity. In short, convex functions are those where the eigenvalues of the Hessian are never negative. Sadly, though, most deep learning problems do not fall into this category. Nonetheless it is a great tool to study optimization algorithms."
    },
    {
      "chunk_id": "fd24e3c22ad7_0",
      "chapter": "optimization-intro",
      "heading": "Vanishing Gradients",
      "text": "Probably the most insidious problem to encounter is the vanishing gradient.\nRecall our commonly-used activation functions and their derivatives in :numref:`subsec_activation-functions`.\nFor instance, assume that we want to minimize the function $f(x) = \\tanh(x)$ and we happen to get started at $x = 4$. As we can see, the gradient of $f$ is close to nil.\nMore specifically, $f'(x) = 1 - \\tanh^2(x)$ and thus $f'(4) = 0.0013$.\nConsequently, optimization will get stuck for a long time before we make progress. This turns out to be one of the reasons that training deep learning models was quite tricky prior to the introduction of the ReLU activation function.\n\n```{.python .input}\n#@tab all\nx = d2l.arange(-2.0, 5.0, 0.01)\nd2l.plot(x, [d2l.tanh(x)], 'x', 'f(x)')\nannotate('vanishing gradient', (4, 1), (2, 0.0))\n```\n\nAs we saw, optimization for deep learning is full of challenges. Fortunately there exists a robust range of algorithms that perform well and that are easy to use even for beginners. Furthermore, it is not really necessary to find *the* best solution. Local optima or even approximate solutions thereof are still very useful."
    },
    {
      "chunk_id": "cfca954e7b02_0",
      "chapter": "optimization-intro",
      "heading": "Summary",
      "text": "* Minimizing the training error does *not* guarantee that we find the best set of parameters to minimize the generalization error.\n* The optimization problems may have many local minima.\n* The problem may have even more saddle points, as generally the problems are not convex.\n* Vanishing gradients can cause optimization to stall. Often a reparametrization of the problem helps. Good initialization of the parameters can be beneficial, too."
    },
    {
      "chunk_id": "3ef81a0c83e5_0",
      "chapter": "optimization-intro",
      "heading": "Exercises",
      "text": "1. Consider a simple MLP with a single hidden layer of, say, $d$ dimensions in the hidden layer and a single output. Show that for any local minimum there are at least $d!$ equivalent solutions that behave identically.\n1. Assume that we have a symmetric random matrix $\\mathbf{M}$ where the entries\n   $M_{ij} = M_{ji}$ are each drawn from some probability distribution\n   $p_{ij}$. Furthermore assume that $p_{ij}(x) = p_{ij}(-x)$, i.e., that the\n   distribution is symmetric (see e.g., :citet:`Wigner.1958` for details).\n    1. Prove that the distribution over eigenvalues is also symmetric. That is, for any eigenvector $\\mathbf{v}$ the probability that the associated eigenvalue $\\lambda$ satisfies $P(\\lambda > 0) = P(\\lambda < 0)$.\n    1. Why does the above *not* imply $P(\\lambda > 0) = 0.5$?\n1. What other challenges involved in deep learning optimization can you think of?\n1. Assume that you want to balance a (real) ball on a (real) saddle.\n    1. Why is this hard?\n    1. Can you exploit this effect also for optimization algorithms?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/349)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/487)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/489)\n:end_tab:"
    },
    {
      "chunk_id": "e6e5d149db71_0",
      "chapter": "rmsprop",
      "heading": "rmsprop",
      "text": "# RMSProp\n:label:`sec_rmsprop`\n\n\nOne of the key issues in :numref:`sec_adagrad` is that the learning rate decreases at a predefined schedule of effectively $\\mathcal{O}(t^{-\\frac{1}{2}})$. While this is generally appropriate for convex problems, it might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet, the coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.\n\n:citet:`Tieleman.Hinton.2012` proposed the RMSProp algorithm as a simple fix to decouple rate scheduling from coordinate-adaptive learning rates. The issue is that Adagrad accumulates the squares of the gradient $\\mathbf{g}_t$ into a state vector $\\mathbf{s}_t = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2$. As a result $\\mathbf{s}_t$ keeps on growing without bound due to the lack of normalization, essentially linearly as the algorithm converges.\n\nOne way of fixing this problem would be to use $\\mathbf{s}_t / t$. For reasonable distributions of $\\mathbf{g}_t$ this will converge. Unfortunately it might take a very long time until the limit behavior starts to matter since the procedure remembers the full trajectory of values. An alternative is to use a leaky average in the same way we used in the momentum method, i.e., $\\mathbf{s}_t \\leftarrow \\gamma \\mathbf{s}_{t-1} + (1-\\gamma) \\mathbf{g}_t^2$ for some parameter $\\gamma > 0$. Keeping all other parts unchanged yields RMSProp."
    },
    {
      "chunk_id": "7b6cca29b139_0",
      "chapter": "rmsprop",
      "heading": "The Algorithm",
      "text": "Let's write out the equations in detail.\n\n$$\\begin{aligned}\n    \\mathbf{s}_t & \\leftarrow \\gamma \\mathbf{s}_{t-1} + (1 - \\gamma) \\mathbf{g}_t^2, \\\\\n    \\mathbf{x}_t & \\leftarrow \\mathbf{x}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\odot \\mathbf{g}_t.\n\\end{aligned}$$\n\nThe constant $\\epsilon > 0$ is typically set to $10^{-6}$ to ensure that we do not suffer from division by zero or overly large step sizes. Given this expansion we are now free to control the learning rate $\\eta$ independently of the scaling that is applied on a per-coordinate basis. In terms of leaky averages we can apply the same reasoning as previously applied in the case of the momentum method. Expanding the definition of $\\mathbf{s}_t$ yields\n\n$$\n\\begin{aligned}\n\\mathbf{s}_t & = (1 - \\gamma) \\mathbf{g}_t^2 + \\gamma \\mathbf{s}_{t-1} \\\\\n& = (1 - \\gamma) \\left(\\mathbf{g}_t^2 + \\gamma \\mathbf{g}_{t-1}^2 + \\gamma^2 \\mathbf{g}_{t-2} + \\ldots, \\right).\n\\end{aligned}\n$$\n\nAs before in :numref:`sec_momentum` we use $1 + \\gamma + \\gamma^2 + \\ldots, = \\frac{1}{1-\\gamma}$. Hence the sum of weights is normalized to $1$ with a half-life time of an observation of $\\gamma^{-1}$. Let's visualize the weights for the past 40 time steps for various choices of $\\gamma$.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import np, npx\n\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\nfrom d2l import torch as d2l\nimport torch\nimport math\n```\n\n```{.python .input}\n#@tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport math\n```\n\n```{.python .input}\n#@tab all\nd2l.set_figsize()\ngammas = [0.95, 0.9, 0.8, 0.7]\nfor gamma in gammas:\n    x = d2l.numpy(d2l.arange(40))\n    d2l.plt.plot(x, (1-gamma) * gamma ** x, label=f'gamma = {gamma:.2f}')\nd2l.plt.xlabel('time');\n```"
    },
    {
      "chunk_id": "a326faf003a3_0",
      "chapter": "rmsprop",
      "heading": "Implementation from Scratch",
      "text": "As before we use the quadratic function $f(\\mathbf{x})=0.1x_1^2+2x_2^2$ to observe the trajectory of RMSProp. Recall that in :numref:`sec_adagrad`, when we used Adagrad with a learning rate of 0.4, the variables moved only very slowly in the later stages of the algorithm since the learning rate decreased too quickly. Since $\\eta$ is controlled separately this does not happen with RMSProp. ```{.python .input}\n#@tab all\ndef rmsprop_2d(x1, x2, s1, s2):\n    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6\n    s1 = gamma * s1 + (1 - gamma) * g1 ** 2\n    s2 = gamma * s2 + (1 - gamma) * g2 ** 2\n    x1 -= eta / math.sqrt(s1 + eps) * g1\n    x2 -= eta / math.sqrt(s2 + eps) * g2\n    return x1, x2, s1, s2\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\neta, gamma = 0.4, 0.9\nd2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))\n```\n\nNext, we implement RMSProp to be used in a deep network. This is equally straightforward."
    },
    {
      "chunk_id": "a326faf003a3_1",
      "chapter": "rmsprop",
      "heading": "Implementation from Scratch",
      "text": "This is equally straightforward. ```{.python .input}\n#@tab mxnet,pytorch\ndef init_rmsprop_states(feature_dim):\n    s_w = d2l.zeros((feature_dim, 1))\n    s_b = d2l.zeros(1)\n    return (s_w, s_b)\n```\n\n```{.python .input}\n#@tab tensorflow\ndef init_rmsprop_states(feature_dim):\n    s_w = tf.Variable(d2l.zeros((feature_dim, 1)))\n    s_b = tf.Variable(d2l.zeros(1))\n    return (s_w, s_b)\n```\n\n```{.python .input}\n#@tab mxnet\ndef rmsprop(params, states, hyperparams):\n    gamma, eps = hyperparams['gamma'], 1e-6\n    for p, s in zip(params, states):\n        s[:] = gamma * s + (1 - gamma) * np.square(p.grad)\n        p[:] -= hyperparams['lr'] * p.grad / np.sqrt(s + eps)\n```\n\n```{.python .input}\n#@tab pytorch\ndef rmsprop(params, states, hyperparams):\n    gamma, eps = hyperparams['gamma'], 1e-6\n    for p, s in zip(params, states):\n        with torch.no_grad():\n            s[:] = gamma * s + (1 - gamma) * torch.square(p.grad)\n            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)\n        p.grad.data.zero_()\n```\n\n```{.python .input}\n#@tab tensorflow\ndef rmsprop(params, grads, states, hyperparams):\n    gamma, eps = hyperparams['gamma'], 1e-6\n    for p, s, g in zip(params, states, grads):\n        s[:].assign(gamma * s + (1 - gamma) * tf.math.square(g))\n        p[:].assign(p - hyperparams['lr'] * g / tf.math.sqrt(s + eps))\n```\n\nWe set the initial learning rate to 0.01 and the weighting term $\\gamma$ to 0.9. That is, $\\mathbf{s}$ aggregates on average over the past $1/(1-\\gamma) = 10$ observations of the square gradient. ```{.python .input}\n#@tab all\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim),\n               {'lr': 0.01, 'gamma': 0.9}, data_iter, feature_dim);\n```"
    },
    {
      "chunk_id": "24c65cf5431b_0",
      "chapter": "rmsprop",
      "heading": "Concise Implementation",
      "text": "Since RMSProp is a rather popular algorithm it is also available in the `Trainer` instance. All we need to do is instantiate it using an algorithm named `rmsprop`, assigning $\\gamma$ to the parameter `gamma1`.\n\n```{.python .input}\n#@tab mxnet\nd2l.train_concise_ch11('rmsprop', {'learning_rate': 0.01, 'gamma1': 0.9},\n                       data_iter)\n```\n\n```{.python .input}\n#@tab pytorch\ntrainer = torch.optim.RMSprop\nd2l.train_concise_ch11(trainer, {'lr': 0.01, 'alpha': 0.9},\n                       data_iter)\n```\n\n```{.python .input}\n#@tab tensorflow\ntrainer = tf.keras.optimizers.RMSprop\nd2l.train_concise_ch11(trainer, {'learning_rate': 0.01, 'rho': 0.9},\n                       data_iter)\n```"
    },
    {
      "chunk_id": "11d58dcae40c_0",
      "chapter": "rmsprop",
      "heading": "Summary",
      "text": "* RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale coefficients.\n* RMSProp shares with momentum the leaky averaging. However, RMSProp uses the technique to adjust the coefficient-wise preconditioner.\n* The learning rate needs to be scheduled by the experimenter in practice.\n* The coefficient $\\gamma$ determines how long the history is when adjusting the per-coordinate scale."
    },
    {
      "chunk_id": "42b87100820f_0",
      "chapter": "rmsprop",
      "heading": "Exercises",
      "text": "1. What happens experimentally if we set $\\gamma = 1$? Why?\n1. Rotate the optimization problem to minimize $f(\\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2$. What happens to the convergence?\n1. Try out what happens to RMSProp on a real machine learning problem, such as training on Fashion-MNIST. Experiment with different choices for adjusting the learning rate.\n1. Would you want to adjust $\\gamma$ as optimization progresses? How sensitive is RMSProp to this?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/356)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1074)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1075)\n:end_tab:"
    },
    {
      "chunk_id": "955b4f478f72_0",
      "chapter": "sgd",
      "heading": "sgd",
      "text": "# Stochastic Gradient Descent\n:label:`sec_sgd`\n\nIn earlier chapters we kept using stochastic gradient descent in our training procedure, however, without explaining why it works.\nTo shed some light on it,\nwe just described the basic principles of gradient descent\nin :numref:`sec_gd`.\nIn this section, we go on to discuss\n*stochastic gradient descent* in greater detail.\n\n```{.python .input}\n#@tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n#@tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport math\nimport torch\n```\n\n```{.python .input}\n#@tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport math\nimport tensorflow as tf\n```"
    },
    {
      "chunk_id": "b62f1f5dac22_0",
      "chapter": "sgd",
      "heading": "Stochastic Gradient Updates",
      "text": "In deep learning, the objective function is usually the average of the loss functions for each example in the training dataset. Given a training dataset of $n$ examples,\nwe assume that $f_i(\\mathbf{x})$ is the loss function\nwith respect to the training example of index $i$,\nwhere $\\mathbf{x}$ is the parameter vector. Then we arrive at the objective function\n\n$$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x}).$$\n\nThe gradient of the objective function at $\\mathbf{x}$ is computed as\n\n$$\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}).$$\n\nIf gradient descent is used, the computational cost for each independent variable iteration is $\\mathcal{O}(n)$, which grows linearly with $n$. Therefore, when the  training dataset is larger, the cost of gradient descent for each iteration will be higher. Stochastic gradient descent (SGD) reduces computational cost at each iteration. At each iteration of stochastic gradient descent, we uniformly sample an index $i\\in\\{1,\\ldots, n\\}$ for data examples at random, and compute the gradient $\\nabla f_i(\\mathbf{x})$ to update $\\mathbf{x}$:\n\n$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}),$$\n\nwhere $\\eta$ is the learning rate. We can see that the computational cost for each iteration drops from $\\mathcal{O}(n)$ of the gradient descent to the constant $\\mathcal{O}(1)$. Moreover, we want to emphasize that the stochastic gradient $\\nabla f_i(\\mathbf{x})$ is an unbiased estimate of the full gradient $\\nabla f(\\mathbf{x})$ because\n\n$$\\mathbb{E}_i \\nabla f_i(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x}).$$\n\nThis means that, on average, the stochastic gradient is a good estimate of the gradient. Now, we will compare it with gradient descent by adding random noise with a mean of 0 and a variance of 1 to the gradient to simulate a stochastic gradient descent."
    },
    {
      "chunk_id": "b62f1f5dac22_1",
      "chapter": "sgd",
      "heading": "Stochastic Gradient Updates",
      "text": "Now, we will compare it with gradient descent by adding random noise with a mean of 0 and a variance of 1 to the gradient to simulate a stochastic gradient descent. ```{.python .input}\n#@tab all\ndef f(x1, x2):  # Objective function\n    return x1 ** 2 + 2 * x2 ** 2\n\ndef f_grad(x1, x2):  # Gradient of the objective function\n    return 2 * x1, 4 * x2\n```\n\n```{.python .input}\n#@tab mxnet\ndef sgd(x1, x2, s1, s2, f_grad):\n    g1, g2 = f_grad(x1, x2)\n    # Simulate noisy gradient\n    g1 += d2l.normal(0.0, 1, (1,))\n    g2 += d2l.normal(0.0, 1, (1,))\n    eta_t = eta * lr()\n    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n```\n\n```{.python .input}\n#@tab pytorch\ndef sgd(x1, x2, s1, s2, f_grad):\n    g1, g2 = f_grad(x1, x2)\n    # Simulate noisy gradient\n    g1 += torch.normal(0.0, 1, (1,)).item()\n    g2 += torch.normal(0.0, 1, (1,)).item()\n    eta_t = eta * lr()\n    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n```\n\n```{.python .input}\n#@tab tensorflow\ndef sgd(x1, x2, s1, s2, f_grad):\n    g1, g2 = f_grad(x1, x2)\n    # Simulate noisy gradient\n    g1 += d2l.normal([1], 0.0, 1)\n    g2 += d2l.normal([1], 0.0, 1)\n    eta_t = eta * lr()\n    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\n```\n\n```{.python .input}\n#@tab all\ndef constant_lr():\n    return 1\n\neta = 0.1\nlr = constant_lr  # Constant learning rate\nd2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))\n```\n\nAs we can see, the trajectory of the variables in the stochastic gradient descent is much more noisy than the one we observed in gradient descent in :numref:`sec_gd`. This is due to the stochastic nature of the gradient. That is, even when we arrive near the minimum, we are still subject to the uncertainty injected by the instantaneous gradient via $\\eta \\nabla f_i(\\mathbf{x})$. Even after 50 steps the quality is still not so good. Even worse, it will not improve after additional steps (we encourage you to experiment with a larger number of steps to confirm this). This leaves us with the only alternative: change the learning rate $\\eta$."
    },
    {
      "chunk_id": "b62f1f5dac22_2",
      "chapter": "sgd",
      "heading": "Stochastic Gradient Updates",
      "text": "Even worse, it will not improve after additional steps (we encourage you to experiment with a larger number of steps to confirm this). This leaves us with the only alternative: change the learning rate $\\eta$. However, if we pick this too small, we will not make any meaningful progress initially. On the other hand, if we pick it too large, we will not get a good solution, as seen above. The only way to resolve these conflicting goals is to reduce the learning rate *dynamically* as optimization progresses. This is also the reason for adding a learning rate function `lr` into the `sgd` step function. In the example above any functionality for learning rate scheduling lies dormant as we set the associated `lr` function to be constant."
    },
    {
      "chunk_id": "b1f9a1d4c561_0",
      "chapter": "sgd",
      "heading": "Dynamic Learning Rate",
      "text": "Replacing $\\eta$ with a time-dependent learning rate $\\eta(t)$ adds to the complexity of controlling convergence of an optimization algorithm. In particular, we need to figure out how rapidly $\\eta$ should decay. If it is too quick, we will stop optimizing prematurely. If we decrease it too slowly, we waste too much time on optimization. The following are a few basic strategies that are used in adjusting $\\eta$ over time (we will discuss more advanced strategies later):\n\n$$\n\\begin{aligned}\n    \\eta(t) & = \\eta_i \\textrm{ if } t_i \\leq t \\leq t_{i+1}  && \\textrm{piecewise constant} \\\\\n    \\eta(t) & = \\eta_0 \\cdot e^{-\\lambda t} && \\textrm{exponential decay} \\\\\n    \\eta(t) & = \\eta_0 \\cdot (\\beta t + 1)^{-\\alpha} && \\textrm{polynomial decay}\n\\end{aligned}\n$$\n\nIn the first *piecewise constant* scenario we decrease the learning rate, e.g., whenever progress in optimization stalls. This is a common strategy for training deep networks. Alternatively we could decrease it much more aggressively by an *exponential decay*. Unfortunately this often leads to premature stopping before the algorithm has converged. A popular choice is *polynomial decay* with $\\alpha = 0.5$. In the case of convex optimization there are a number of proofs that show that this rate is well behaved. Let's see what the exponential decay looks like in practice. ```{.python .input}\n#@tab all\ndef exponential_lr():\n    # Global variable that is defined outside this function and updated inside\n    global t\n    t += 1\n    return math.exp(-0.1 * t)\n\nt = 1\nlr = exponential_lr\nd2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))\n```\n\nAs expected, the variance in the parameters is significantly reduced. However, this comes at the expense of failing to converge to the optimal solution $\\mathbf{x} = (0, 0)$. Even after 1000 iteration steps are we are still very far away from the optimal solution. Indeed, the algorithm fails to converge at all."
    },
    {
      "chunk_id": "b1f9a1d4c561_1",
      "chapter": "sgd",
      "heading": "Dynamic Learning Rate",
      "text": "However, this comes at the expense of failing to converge to the optimal solution $\\mathbf{x} = (0, 0)$. Even after 1000 iteration steps are we are still very far away from the optimal solution. Indeed, the algorithm fails to converge at all. On the other hand, if we use a polynomial decay where the learning rate decays with the inverse square root of the number of steps, convergence gets better after only 50 steps. ```{.python .input}\n#@tab all\ndef polynomial_lr():\n    # Global variable that is defined outside this function and updated inside\n    global t\n    t += 1\n    return (1 + 0.1 * t) ** (-0.5)\n\nt = 1\nlr = polynomial_lr\nd2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))\n```\n\nThere exist many more choices for how to set the learning rate. For instance, we could start with a small rate, then rapidly ramp up and then decrease it again, albeit more slowly. We could even alternate between smaller and larger learning rates. There exists a large variety of such schedules. For now let's focus on learning rate schedules for which a comprehensive theoretical analysis is possible, i.e., on learning rates in a convex setting. For general nonconvex problems it is very difficult to obtain meaningful convergence guarantees, since in general minimizing nonlinear nonconvex problems is NP hard. For a survey see e.g., the excellent [lecture notes](https://www.stat.cmu.edu/%7Eryantibs/convexopt-F15/lectures/26-nonconvex.pdf) of Tibshirani 2015."
    },
    {
      "chunk_id": "0acc3f554152_0",
      "chapter": "sgd",
      "heading": "Convergence Analysis for Convex Objectives",
      "text": "The following convergence analysis of stochastic gradient descent for convex objective functions\nis optional and primarily serves to convey more intuition about the problem. We limit ourselves to one of the simplest proofs :cite:`Nesterov.Vial.2000`. Significantly more advanced proof techniques exist, e.g., whenever the objective function is particularly well behaved. Suppose that the objective function $f(\\boldsymbol{\\xi}, \\mathbf{x})$ is convex in $\\mathbf{x}$\nfor all $\\boldsymbol{\\xi}$. More concretely,\nwe consider the stochastic gradient descent update:\n\n$$\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - \\eta_t \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x}),$$\n\nwhere $f(\\boldsymbol{\\xi}_t, \\mathbf{x})$\nis the objective function\nwith respect to the training example $\\boldsymbol{\\xi}_t$\ndrawn from some distribution\nat step $t$ and $\\mathbf{x}$ is the model parameter. Denote by\n\n$$R(\\mathbf{x}) = E_{\\boldsymbol{\\xi}}[f(\\boldsymbol{\\xi}, \\mathbf{x})]$$\n\nthe expected risk and by $R^*$ its minimum with regard to $\\mathbf{x}$. Last let $\\mathbf{x}^*$ be the minimizer (we assume that it exists within the domain where $\\mathbf{x}$ is defined). In this case we can track the distance between the current parameter $\\mathbf{x}_t$ at time $t$ and the risk minimizer $\\mathbf{x}^*$ and see whether it improves over time:\n\n$$\\begin{aligned}    &\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 \\\\ =& \\|\\mathbf{x}_{t} - \\eta_t \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x}) - \\mathbf{x}^*\\|^2 \\\\    =& \\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 + \\eta_t^2 \\|\\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\|^2 - 2 \\eta_t    \\left\\langle \\mathbf{x}_t - \\mathbf{x}^*, \\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\right\\rangle."
    },
    {
      "chunk_id": "0acc3f554152_1",
      "chapter": "sgd",
      "heading": "Convergence Analysis for Convex Objectives",
      "text": "\\end{aligned}$$\n:eqlabel:`eq_sgd-xt+1-xstar`\n\nWe assume that the $\\ell_2$ norm of stochastic gradient $\\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})$ is bounded  by some  constant $L$, hence we have that\n\n$$\\eta_t^2 \\|\\partial_\\mathbf{x} f(\\boldsymbol{\\xi}_t, \\mathbf{x})\\|^2 \\leq \\eta_t^2 L^2.$$\n:eqlabel:`eq_sgd-L`\n\n\nWe are mostly interested in how the distance between $\\mathbf{x}_t$ and $\\mathbf{x}^*$ changes *in expectation*. In fact, for any specific sequence of steps the distance might well increase, depending on whichever $\\boldsymbol{\\xi}_t$ we encounter. Hence we need to bound the dot product. Since for any convex function $f$ it holds that\n$f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\langle f'(\\mathbf{x}), \\mathbf{y} - \\mathbf{x} \\rangle$\nfor all $\\mathbf{x}$ and $\\mathbf{y}$,\nby convexity we have\n\n$$f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*) \\geq f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) + \\left\\langle \\mathbf{x}^* - \\mathbf{x}_t, \\partial_{\\mathbf{x}} f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) \\right\\rangle.$$\n:eqlabel:`eq_sgd-f-xi-xstar`\n\nPlugging both inequalities :eqref:`eq_sgd-L` and :eqref:`eq_sgd-f-xi-xstar` into :eqref:`eq_sgd-xt+1-xstar` we obtain a bound on the distance between parameters at time $t+1$ as follows:\n\n$$\\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2 - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2 \\geq 2 \\eta_t (f(\\boldsymbol{\\xi}_t, \\mathbf{x}_t) - f(\\boldsymbol{\\xi}_t, \\mathbf{x}^*)) - \\eta_t^2 L^2.$$\n:eqlabel:`eqref_sgd-xt-diff`\n\nThis means that we make progress as long as the  difference between current loss and the optimal loss outweighs $\\eta_t L^2/2$. Since this difference is bound to converge to zero it follows that the learning rate $\\eta_t$ also needs to *vanish*. Next we take expectations over :eqref:`eqref_sgd-xt-diff`. This yields\n\n$$E\\left[\\|\\mathbf{x}_{t} - \\mathbf{x}^*\\|^2\\right] - E\\left[\\|\\mathbf{x}_{t+1} - \\mathbf{x}^*\\|^2\\right] \\geq 2 \\eta_t [E[R(\\mathbf{x}_t)] - R^*] -  \\eta_t^2 L^2.$$\n\nThe last step involves summing over the inequalities for $t \\in \\{1, \\ldots, T\\}$."
    },
    {
      "chunk_id": "0acc3f554152_2",
      "chapter": "sgd",
      "heading": "Convergence Analysis for Convex Objectives",
      "text": "Since the sum telescopes and by dropping the lower term we obtain\n\n$$\\|\\mathbf{x}_1 - \\mathbf{x}^*\\|^2 \\geq 2 \\left (\\sum_{t=1}^T   \\eta_t \\right) [E[R(\\mathbf{x}_t)] - R^*] - L^2 \\sum_{t=1}^T \\eta_t^2.$$\n:eqlabel:`eq_sgd-x1-xstar`\n\nNote that we exploited that $\\mathbf{x}_1$ is given and thus the expectation can be dropped. Last define\n\n$$\\bar{\\mathbf{x}} \\stackrel{\\textrm{def}}{=} \\frac{\\sum_{t=1}^T \\eta_t \\mathbf{x}_t}{\\sum_{t=1}^T \\eta_t}.$$\n\nSince\n\n$$E\\left(\\frac{\\sum_{t=1}^T \\eta_t R(\\mathbf{x}_t)}{\\sum_{t=1}^T \\eta_t}\\right) = \\frac{\\sum_{t=1}^T \\eta_t E[R(\\mathbf{x}_t)]}{\\sum_{t=1}^T \\eta_t} = E[R(\\mathbf{x}_t)],$$\n\nby Jensen's inequality (setting $i=t$, $\\alpha_i = \\eta_t/\\sum_{t=1}^T \\eta_t$ in :eqref:`eq_jensens-inequality`) and convexity of $R$ it follows that $E[R(\\mathbf{x}_t)] \\geq E[R(\\bar{\\mathbf{x}})]$, thus\n\n$$\\sum_{t=1}^T \\eta_t E[R(\\mathbf{x}_t)] \\geq \\sum_{t=1}^T \\eta_t  E\\left[R(\\bar{\\mathbf{x}})\\right].$$\n\nPlugging this into the inequality :eqref:`eq_sgd-x1-xstar` yields the bound\n\n$$\n\\left[E[\\bar{\\mathbf{x}}]\\right] - R^* \\leq \\frac{r^2 + L^2 \\sum_{t=1}^T \\eta_t^2}{2 \\sum_{t=1}^T \\eta_t},\n$$\n\nwhere $r^2 \\stackrel{\\textrm{def}}{=} \\|\\mathbf{x}_1 - \\mathbf{x}^*\\|^2$ is a bound on the distance between the initial choice of parameters and the final outcome. In short, the speed of convergence depends on how\nthe norm of stochastic gradient is bounded ($L$) and how far away from optimality the initial parameter value is ($r$). Note that the bound is in terms of $\\bar{\\mathbf{x}}$ rather than $\\mathbf{x}_T$. This is the case since $\\bar{\\mathbf{x}}$ is a smoothed version of the optimization path. Whenever $r, L$, and $T$ are known we can pick the learning rate $\\eta = r/(L \\sqrt{T})$. This yields as upper bound $rL/\\sqrt{T}$. That is, we converge with rate $\\mathcal{O}(1/\\sqrt{T})$ to the optimal solution."
    },
    {
      "chunk_id": "382ccfdb5cd7_0",
      "chapter": "sgd",
      "heading": "Stochastic Gradients and Finite Samples",
      "text": "So far we have played a bit fast and loose when it comes to talking about stochastic gradient descent. We posited that we draw instances $x_i$, typically with labels $y_i$ from some distribution $p(x, y)$ and that we use this to update the model parameters in some manner. In particular, for a finite sample size we simply argued that the discrete distribution $p(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}(x) \\delta_{y_i}(y)$\nfor some functions $\\delta_{x_i}$ and $\\delta_{y_i}$\nallows us to perform stochastic gradient descent over it.\n\nHowever, this is not really what we did. In the toy examples in the current section we simply added noise to an otherwise non-stochastic gradient, i.e., we pretended to have pairs $(x_i, y_i)$. It turns out that this is justified here (see the exercises for a detailed discussion). More troubling is that in all previous discussions we clearly did not do this. Instead we iterated over all instances *exactly once*. To see why this is preferable consider the converse, namely that we are sampling $n$ observations from the discrete distribution *with replacement*. The probability of choosing an element $i$ at random is $1/n$. Thus to choose it *at least* once is\n\n$$P(\\textrm{choose~} i) = 1 - P(\\textrm{omit~} i) = 1 - (1-1/n)^n \\approx 1-e^{-1} \\approx 0.63.$$\n\nA similar reasoning shows that the probability of picking some sample (i.e., training example) *exactly once* is given by\n\n$${n \\choose 1} \\frac{1}{n} \\left(1-\\frac{1}{n}\\right)^{n-1} = \\frac{n}{n-1} \\left(1-\\frac{1}{n}\\right)^{n} \\approx e^{-1} \\approx 0.37.$$\n\nSampling with replacement leads to an increased variance and decreased data efficiency relative to sampling *without replacement*. Hence, in practice we perform the latter (and this is the default choice throughout this book). Last note that repeated passes through the training dataset traverse it in a *different* random order."
    },
    {
      "chunk_id": "7d776d5f7fec_0",
      "chapter": "sgd",
      "heading": "Summary",
      "text": "* For convex problems we can prove that for a wide choice of learning rates stochastic gradient descent will converge to the optimal solution.\n* For deep learning this is generally not the case. However, the analysis of convex problems gives us useful insight into how to approach optimization, namely to reduce the learning rate progressively, albeit not too quickly.\n* Problems occur when the learning rate is too small or too large. In practice  a suitable learning rate is often found only after multiple experiments.\n* When there are more examples in the training dataset, it costs more to compute each iteration for gradient descent, so stochastic gradient descent is preferred in these cases.\n* Optimality guarantees for stochastic gradient descent are in general not available in nonconvex cases since the number of local minima that require checking might well be exponential."
    },
    {
      "chunk_id": "e7dbacb5029b_0",
      "chapter": "sgd",
      "heading": "Exercises",
      "text": "1. Experiment with different learning rate schedules for stochastic gradient descent and with different numbers of iterations. In particular, plot the distance from the optimal solution $(0, 0)$ as a function of the number of iterations.\n1. Prove that for the function $f(x_1, x_2) = x_1^2 + 2 x_2^2$ adding normal noise to the gradient is equivalent to minimizing a loss function $f(\\mathbf{x}, \\mathbf{w}) = (x_1 - w_1)^2 + 2 (x_2 - w_2)^2$ where $\\mathbf{x}$ is drawn from a normal distribution.\n1. Compare convergence of stochastic gradient descent when you sample from $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$ with replacement and when you sample without replacement.\n1. How would you change the stochastic gradient descent solver if some gradient (or rather some coordinate associated with it) was consistently larger than all the other gradients?\n1. Assume that $f(x) = x^2 (1 + \\sin x)$. How many local minima does $f$ have? Can you change $f$ in such a way that to minimize it one needs to evaluate all the local minima?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/352)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/497)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1067)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Preface\n\nJust a few years ago, there were no legions of deep learning scientists\ndeveloping intelligent products and services at major companies and startups.\nWhen we entered the field, machine learning\ndid not command headlines in daily newspapers.\nOur parents had no idea what machine learning was,\nlet alone why we might prefer it to a career in medicine or law.\nMachine learning was a blue skies academic discipline\nwhose industrial significance was limited\nto a narrow set of real-world applications,\nincluding speech recognition and computer vision.\nMoreover, many of these applications\nrequired so much domain knowledge\nthat they were often regarded as entirely separate areas\nfor which machine learning was one small component.\nAt that time, neural networks---the\npredecessors of the deep learning methods\nthat we focus on in this book---were\ngenerally regarded as outmoded.\n\n\nYet in just a few years, deep learning has taken the world by surprise,\ndriving rapid progress in such diverse fields\nas computer vision, natural language processing,\nautomatic speech recognition, reinforcement learning,\nand biomedical informatics.\nMoreover, the success of deep learning\nin so many tasks of practical interest\nhas even catalyzed developments\nin theoretical machine learning and statistics.\nWith these advances in hand,\nwe can now build cars that drive themselves\nwith more autonomy than ever before\n(though less autonomy than some companies might have you believe),\ndialogue systems that debug code by asking clarifying questions,\nand software agents beating the best human players in the world at board games such as Go, a feat once thought to be decades away.\nAlready, these tools exert ever-wider influence on industry and society,\nchanging the way movies are made, diseases are diagnosed,\nand playing a growing role in basic sciences---from astrophysics, to climate modeling, to weather prediction, to biomedicine."
    },
    {
      "chunk_id": "4c07e4b27f6e_0",
      "chapter": "index",
      "heading": "About This Book",
      "text": "This book represents our attempt to make deep learning approachable,\nteaching you the *concepts*, the *context*, and the *code*."
    },
    {
      "chunk_id": "6afb59c2c813_0",
      "chapter": "index",
      "heading": "One Medium Combining Code, Math, and HTML",
      "text": "For any computing technology to reach its full impact,\nit must be well understood, well documented, and supported by\nmature, well-maintained tools. The key ideas should be clearly distilled,\nminimizing the onboarding time needed\nto bring new practitioners up to date. Mature libraries should automate common tasks,\nand exemplar code should make it easy for practitioners\nto modify, apply, and extend common applications to suit their needs. As an example, take dynamic web applications. Despite a large number of companies, such as Amazon,\ndeveloping successful database-driven web applications in the 1990s,\nthe potential of this technology to aid creative entrepreneurs\nwas realized to a far greater degree only in the past ten years,\nowing in part to the development of powerful, well-documented frameworks. Testing the potential of deep learning presents unique challenges\nbecause any single application brings together various disciplines. Applying deep learning requires simultaneously understanding\n(i) the motivations for casting a problem in a particular way;\n(ii) the mathematical form of a given model;\n(iii) the optimization algorithms for fitting the models to data;\n(iv) the statistical principles that tell us\nwhen we should expect our models\nto generalize to unseen data\nand practical methods for certifying\nthat they have, in fact, generalized;\nand (v) the engineering techniques\nrequired to train models efficiently,\nnavigating the pitfalls of numerical computing\nand getting the most out of available hardware. Teaching the critical thinking skills\nrequired to formulate problems,\nthe mathematics to solve them,\nand the software tools to implement those solutions\nall in one place presents formidable challenges. Our goal in this book is to present a unified resource\nto bring would-be practitioners up to speed."
    },
    {
      "chunk_id": "6afb59c2c813_1",
      "chapter": "index",
      "heading": "One Medium Combining Code, Math, and HTML",
      "text": "Our goal in this book is to present a unified resource\nto bring would-be practitioners up to speed. When we started this book project,\nthere were no resources that simultaneously\n(i) remained up to date;\n(ii) covered the breadth of modern machine learning practices\nwith sufficient technical depth;\nand (iii) interleaved exposition of\nthe quality one expects of a textbook\nwith the clean runnable code\nthat one expects of a hands-on tutorial. We found plenty of code examples illustrating\nhow to use a given deep learning framework\n(e.g., how to do basic numerical computing with matrices in TensorFlow)\nor for implementing particular techniques\n(e.g., code snippets for LeNet, AlexNet, ResNet, etc.)\nscattered across various blog posts and GitHub repositories. However, these examples typically focused on\n*how* to implement a given approach,\nbut left out the discussion of\n*why* certain algorithmic decisions are made. While some interactive resources\nhave popped up sporadically\nto address a particular topic,\ne.g., the engaging blog posts\npublished on the website [Distill](http://distill.pub), or personal blogs,\nthey only covered selected topics in deep learning,\nand often lacked associated code. On the other hand, while several deep learning textbooks\nhave emerged---e.g., :citet:`Goodfellow.Bengio.Courville.2016`,\nwhich offers a comprehensive survey\non the basics of deep learning---these\nresources do not marry the descriptions\nto realizations of the concepts in code,\nsometimes leaving readers clueless\nas to how to implement them. Moreover, too many resources\nare hidden behind the paywalls\nof commercial course providers."
    },
    {
      "chunk_id": "6afb59c2c813_2",
      "chapter": "index",
      "heading": "One Medium Combining Code, Math, and HTML",
      "text": "Moreover, too many resources\nare hidden behind the paywalls\nof commercial course providers. We set out to create a resource that could\n(i) be freely available for everyone;\n(ii) offer sufficient technical depth\nto provide a starting point on the path\nto actually becoming an applied machine learning scientist;\n(iii) include runnable code, showing readers\n*how* to solve problems in practice;\n(iv) allow for rapid updates, both by us\nand also by the community at large;\nand (v) be complemented by a [forum](https://discuss.d2l.ai/c/5)\nfor interactive discussion of technical details and to answer questions. These goals were often in conflict. Equations, theorems, and citations\nare best managed and laid out in LaTeX. Code is best described in Python. And webpages are native in HTML and JavaScript. Furthermore, we want the content to be\naccessible both as executable code, as a physical book,\nas a downloadable PDF, and on the Internet as a website. No workflows seemed suited to these demands,\nso we decided to assemble our own (:numref:`sec_how_to_contribute`). We settled on GitHub to share the source\nand to facilitate community contributions;\nJupyter notebooks for mixing code, equations and text;\nSphinx as a rendering engine;\nand Discourse as a discussion platform. While our system is not perfect,\nthese choices strike a compromise\namong the competing concerns. We believe that *Dive into Deep Learning*\nmight be the first book published\nusing such an integrated workflow."
    },
    {
      "chunk_id": "07382ccfffae_0",
      "chapter": "index",
      "heading": "Learning by Doing",
      "text": "Many textbooks present concepts in succession,\ncovering each in exhaustive detail. For example,\nthe excellent textbook of\n:citet:`Bishop.2006`\nteaches each topic so thoroughly\nthat getting to the chapter\non linear regression requires\na nontrivial amount of work. While experts love this book\nprecisely for its thoroughness,\nfor true beginners, this property limits\nits usefulness as an introductory text. In this book, we teach most concepts *just in time*. In other words, you will learn concepts at the very moment\nthat they are needed to accomplish some practical end. While we take some time at the outset to teach\nfundamental preliminaries, like linear algebra and probability,\nwe want you to taste the satisfaction of training your first model\nbefore worrying about more esoteric concepts. Aside from a few preliminary notebooks that provide a crash course\nin the basic mathematical background,\neach subsequent chapter both introduces a reasonable number of new concepts\nand provides several self-contained working examples, using real datasets. This presented an organizational challenge. Some models might logically be grouped together in a single notebook. And some ideas might be best taught\nby executing several models in succession. By contrast, there is a big advantage to adhering\nto a policy of *one working example, one notebook*:\nThis makes it as easy as possible for you to\nstart your own research projects by leveraging our code. Just copy a notebook and start modifying it. Throughout, we interleave the runnable code\nwith background material as needed. In general, we err on the side of making tools\navailable before explaining them fully\n(often filling in the background later). For instance, we might use *stochastic gradient descent*\nbefore explaining why it is useful\nor offering some intuition for why it works. This helps to give practitioners the necessary\nammunition to solve problems quickly,\nat the expense of requiring the reader\nto trust us with some curatorial decisions."
    },
    {
      "chunk_id": "07382ccfffae_1",
      "chapter": "index",
      "heading": "Learning by Doing",
      "text": "This helps to give practitioners the necessary\nammunition to solve problems quickly,\nat the expense of requiring the reader\nto trust us with some curatorial decisions. This book teaches deep learning concepts from scratch. Sometimes, we delve into fine details about models\nthat would typically be hidden from users\nby modern deep learning frameworks. This comes up especially in the basic tutorials,\nwhere we want you to understand everything\nthat happens in a given layer or optimizer. In these cases, we often present\ntwo versions of the example:\none where we implement everything from scratch,\nrelying only on NumPy-like functionality\nand automatic differentiation,\nand a more practical example,\nwhere we write succinct code\nusing the high-level APIs of deep learning frameworks. After explaining how some component works,\nwe rely on the high-level API in subsequent tutorials."
    },
    {
      "chunk_id": "157c1e44d655_0",
      "chapter": "index",
      "heading": "Content and Structure",
      "text": "The book can be divided into roughly three parts,\ndealing with preliminaries,\ndeep learning techniques,\nand advanced topics\nfocused on real systems\nand applications (:numref:`fig_book_org`). ![Book structure.](../img/book-org.svg)\n:label:`fig_book_org`\n\n\n* **Part 1: Basics and Preliminaries**. :numref:`chap_introduction` is\nan introduction to deep learning. Then, in :numref:`chap_preliminaries`,\nwe quickly bring you up to speed\non the prerequisites required\nfor hands-on deep learning,\nsuch as how to store and manipulate data,\nand how to apply various numerical operations\nbased on elementary concepts from linear algebra,\ncalculus, and probability. :numref:`chap_regression` and :numref:`chap_perceptrons`\ncover the most fundamental concepts and techniques in deep learning,\nincluding regression and classification;\nlinear models; multilayer perceptrons;\nand overfitting and regularization. * **Part 2: Modern Deep Learning Techniques**. :numref:`chap_computation` describes\nthe key computational components\nof deep learning systems\nand lays the groundwork\nfor our subsequent implementations\nof more complex models. Next, :numref:`chap_cnn` and :numref:`chap_modern_cnn`\npresent convolutional neural networks (CNNs),\npowerful tools that form the backbone\nof most modern computer vision systems. Similarly, :numref:`chap_rnn` and :numref:`chap_modern_rnn`\nintroduce recurrent neural networks (RNNs),\nmodels that exploit sequential (e.g., temporal)\nstructure in data and are commonly used\nfor natural language processing\nand time series prediction. In :numref:`chap_attention-and-transformers`,\nwe describe a relatively new class of models,\nbased on so-called *attention mechanisms*,\nthat has displaced RNNs as the dominant architecture\nfor most natural language processing tasks. These sections will bring you up to speed\non the most powerful and general tools\nthat are widely used by deep learning practitioners. * **Part 3: Scalability, Efficiency, and Applications** (available [online](https://d2l.ai))."
    },
    {
      "chunk_id": "157c1e44d655_1",
      "chapter": "index",
      "heading": "Content and Structure",
      "text": "These sections will bring you up to speed\non the most powerful and general tools\nthat are widely used by deep learning practitioners. * **Part 3: Scalability, Efficiency, and Applications** (available [online](https://d2l.ai)). In Chapter 12,\nwe discuss several common optimization algorithms\nused to train deep learning models. Next, in Chapter 13,\nwe examine several key factors\nthat influence the computational performance\nof deep learning code. Then, in Chapter 14,\nwe illustrate major applications\nof deep learning in computer vision. Finally, in Chapter 15 and Chapter 16,\nwe demonstrate how to pretrain language representation models\nand apply them to natural language processing tasks."
    },
    {
      "chunk_id": "97465422d0b2_0",
      "chapter": "index",
      "heading": "Code",
      "text": ":label:`sec_code`\n\nMost sections of this book feature executable code. We believe that some intuitions are best developed\nvia trial and error,\ntweaking the code in small ways and observing the results. Ideally, an elegant mathematical theory might tell us\nprecisely how to tweak our code to achieve a desired result. However, deep learning practitioners today\nmust often tread where no solid theory provides guidance. Despite our best attempts, formal explanations\nfor the efficacy of various techniques are\nstill lacking, for a variety of reasons: the mathematics to characterize these models\ncan be so difficult;\nthe explanation likely depends on properties\nof the data that currently lack clear definitions;\nand serious inquiry on these topics\nhas only recently kicked into high gear. We are hopeful that as the theory of deep learning progresses,\neach future edition of this book will provide insights\nthat eclipse those presently available. To avoid unnecessary repetition, we capture\nsome of our most frequently imported and used\nfunctions and classes in the `d2l` package. Throughout, we mark blocks of code\n(such as functions, classes,\nor collection of import statements) with `#@save`\nto indicate that they will be accessed later\nvia the `d2l` package. We offer a detailed overview\nof these classes and functions in :numref:`sec_d2l`."
    },
    {
      "chunk_id": "97465422d0b2_1",
      "chapter": "index",
      "heading": "Code",
      "text": "We offer a detailed overview\nof these classes and functions in :numref:`sec_d2l`. The `d2l` package is lightweight and only requires\nthe following dependencies:\n\n```{.python .input}\n#@tab all\n#@save\nimport inspect\nimport collections\nfrom collections import defaultdict\nfrom IPython import display\nimport math\nfrom matplotlib import pyplot as plt\nfrom matplotlib_inline import backend_inline\nimport os\nimport pandas as pd\nimport random\nimport re\nimport shutil\nimport sys\nimport tarfile\nimport time\nimport requests\nimport zipfile\nimport hashlib\nd2l = sys.modules[__name__]\n```\n\n:begin_tab:`mxnet`\nMost of the code in this book is based on Apache MXNet,\nan open-source framework for deep learning\nthat is the preferred choice\nof AWS (Amazon Web Services),\nas well as many colleges and companies. All of the code in this book has passed tests\nunder the newest MXNet version. However, due to the rapid development of deep learning,\nsome code *in the print edition*\nmay not work properly in future versions of MXNet. We plan to keep the online version up to date. In case you encounter any problems,\nplease consult :ref:`chap_installation`\nto update your code and runtime environment. Below lists dependencies in our MXNet implementation. :end_tab:\n\n:begin_tab:`pytorch`\nMost of the code in this book is based on PyTorch,\na popular open-source framework\nthat has been enthusiastically embraced\nby the deep learning research community. All of the code in this book has passed tests\nunder the latest stable version of PyTorch. However, due to the rapid development of deep learning,\nsome code *in the print edition*\nmay not work properly in future versions of PyTorch. We plan to keep the online version up to date. In case you encounter any problems,\nplease consult :ref:`chap_installation`\nto update your code and runtime environment. Below lists dependencies in our PyTorch implementation."
    },
    {
      "chunk_id": "97465422d0b2_2",
      "chapter": "index",
      "heading": "Code",
      "text": "We plan to keep the online version up to date. In case you encounter any problems,\nplease consult :ref:`chap_installation`\nto update your code and runtime environment. Below lists dependencies in our PyTorch implementation. :end_tab:\n\n:begin_tab:`tensorflow`\nMost of the code in this book is based on TensorFlow,\nan open-source framework for deep learning\nthat is widely adopted in industry\nand popular among researchers. All of the code in this book has passed tests\nunder the latest stable version TensorFlow. However, due to the rapid development of deep learning,\nsome code *in the print edition*\nmay not work properly in future versions of TensorFlow. We plan to keep the online version up to date. In case you encounter any problems,\nplease consult :ref:`chap_installation`\nto update your code and runtime environment. Below lists dependencies in our TensorFlow implementation. :end_tab:\n\n:begin_tab:`jax`\nMost of the code in this book is based on Jax,\nan open-source framework enabling composable function\ntransformations such as differentiation of arbitrary\nPython and NumPy functions, as well as JIT compliation,\nvectorization and much more! It is becoming popular in\nthe machine learning research space and has an\neasy-to-learn NumPy-like API. Actually, JAX tries\nto achieve 1:1 parity with NumPy, so switching your\ncode could be as simple as changing a single import statement! However, due to the rapid development of deep learning,\nsome code *in the print edition*\nmay not work properly in future versions of Jax. We plan to keep the online version up to date. In case you encounter any problems,\nplease consult :ref:`chap_installation`\nto update your code and runtime environment. Below lists dependencies in our JAX implementation."
    },
    {
      "chunk_id": "97465422d0b2_3",
      "chapter": "index",
      "heading": "Code",
      "text": "We plan to keep the online version up to date. In case you encounter any problems,\nplease consult :ref:`chap_installation`\nto update your code and runtime environment. Below lists dependencies in our JAX implementation. :end_tab:\n\n```{.python .input}\n#@tab mxnet\n#@save\nfrom mxnet import autograd, context, gluon, image, init, np, npx\nfrom mxnet.gluon import nn, rnn\n```\n\n```{.python .input}\n#@tab pytorch\n#@save\nimport numpy as np\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nfrom scipy.spatial import distance_matrix\n```\n\n```{.python .input}\n#@tab tensorflow\n#@save\nimport numpy as np\nimport tensorflow as tf\n```\n\n```{.python .input}\n#@tab jax\n#@save\nfrom dataclasses import field\nfrom functools import partial\nimport flax\nfrom flax import linen as nn\nfrom flax.training import train_state\nimport jax\nfrom jax import numpy as jnp\nfrom jax import grad, vmap\nimport numpy as np\nimport optax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom types import FunctionType\nfrom typing import Any\n```"
    },
    {
      "chunk_id": "7c037ddaf79e_0",
      "chapter": "index",
      "heading": "Target Audience",
      "text": "This book is for students (undergraduate or graduate),\nengineers, and researchers, who seek a solid grasp\nof the practical techniques of deep learning.\nBecause we explain every concept from scratch,\nno previous background in deep learning or machine learning is required.\nFully explaining the methods of deep learning\nrequires some mathematics and programming,\nbut we will only assume that you enter with some basics,\nincluding modest amounts of linear algebra,\ncalculus, probability, and Python programming.\nJust in case you have forgotten anything,\nthe [online Appendix](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/index.html) provides a refresher\non most of the mathematics\nyou will find in this book.\nUsually, we will prioritize\nintuition and ideas\nover mathematical rigor.\nIf you would like to extend these foundations\nbeyond the prerequisites to understand our book,\nwe happily recommend some other terrific resources:\n*Linear Analysis* by :citet:`Bollobas.1999`\ncovers linear algebra and functional analysis in great depth.\n*All of Statistics* :cite:`Wasserman.2013`\nprovides a marvelous introduction to statistics.\nJoe Blitzstein's [books](https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1138369918)\nand [courses](https://projects.iq.harvard.edu/stat110/home)\non probability and inference are pedagogical gems.\nAnd if you have not used Python before,\nyou may want to peruse this [Python tutorial](http://learnpython.org/)."
    },
    {
      "chunk_id": "c3f9b99a4ac9_0",
      "chapter": "index",
      "heading": "Notebooks, Website, GitHub, and Forum",
      "text": "All our notebooks can be downloaded\nfrom the [D2L.ai website](https://d2l.ai)\nand from [GitHub](https://github.com/d2l-ai/d2l-en).\nAssociated with this book we have launched a discussion forum\nat [discuss.d2l.ai](https://discuss.d2l.ai/c/5).\nWhenever you have questions on any section of the book,\nyou can find a link to the associated discussion page\nat the end of each notebook."
    },
    {
      "chunk_id": "c571513cffee_0",
      "chapter": "index",
      "heading": "Acknowledgments",
      "text": "We are indebted to the hundreds of contributors for both\nthe English and the Chinese drafts. They helped improve the content and offered valuable feedback. This book was originally implemented with MXNet as the primary framework. We thank Anirudh Dagar and Yuan Tang for adapting a majority part of earlier MXNet code into PyTorch and TensorFlow implementations, respectively. Since July 2021, we have redesigned and reimplemented this book in PyTorch, MXNet, and TensorFlow, choosing PyTorch as the primary framework. We thank Anirudh Dagar for adapting a majority part of more recent PyTorch code into JAX implementations. We thank Gaosheng Wu, Liujun Hu, Ge Zhang, and Jiehang Xie from Baidu for adapting a majority part of more recent PyTorch code into PaddlePaddle implementations in the Chinese draft. We thank Shuai Zhang for integrating the LaTeX style from the press into the PDF building. On GitHub, we thank every contributor of this English draft\nfor making it better for everyone."
    },
    {
      "chunk_id": "c571513cffee_1",
      "chapter": "index",
      "heading": "Acknowledgments",
      "text": "We thank Shuai Zhang for integrating the LaTeX style from the press into the PDF building. On GitHub, we thank every contributor of this English draft\nfor making it better for everyone. Their GitHub IDs or names are (in no particular order):\nalxnorden, avinashingit, bowen0701, brettkoonce, Chaitanya Prakash Bapat,\ncryptonaut, Davide Fiocco, edgarroman, gkutiel, John Mitro, Liang Pu,\nRahul Agarwal, Mohamed Ali Jamaoui, Michael (Stu) Stewart, Mike M\u00fcller,\nNRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sundeepteki,\ntopecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB, Yuhong Chen,\nEvgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen, pmuens,\nAndrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Abhishek Gupta,\nuwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth Buddareddygari, brianhendee,\nmani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete L\u00fcer, Surbhi Vijayvargeeya,\nMuhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pedro Larroy,\nlgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gardner,\nMaximilian B\u00f6ther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong,\nSteve Sedlmeyer, Ruslan Baratov, Rafael Schlatter, liusy182, Giannis Pappas,\nati-ozgur, qbaza, dchoi77, Adam Gerson, Phuc Le, Mark Atwood, christabella, vn09,\nHaibin Lin, jjangga0214, RichyChen, noelo, hansent, Giel Dops, dvincent1337, WhiteD3vil,\nPeter Kulits, codypenta, joseppinilla, ahmaurya, karolszk, heytitle, Peter Goetz, rigtorp,\nTiep Vu, sfilip, mlxd, Kale-ab Tessera, Sanjar Adilov, MatteoFerrara, hsneto,\nKatarzyna Biesialska, Gregory Bruss, Duy\u2013Thanh Doan, paulaurel, graytowne, Duc Pham,\nsl7423, Jaedong Hwang, Yida Wang, cys4, clhm, Jean Kaddour, austinmw, trebeljahr, tbaums,\nCuong V."
    },
    {
      "chunk_id": "c571513cffee_2",
      "chapter": "index",
      "heading": "Acknowledgments",
      "text": "Nguyen, pavelkomarov, vzlamal, NotAnotherSystem, J-Arun-Mani, jancio, eldarkurtic,\nthe-great-shazbot, doctorcolossus, gducharme, cclauss, Daniel-Mietchen, hoonose, biagiom,\nabhinavsp0730, jonathanhrandall, ysraell, Nodar Okroshiashvili, UgurKap, Jiyang Kang,\nStevenJokes, Tomer Kaftan, liweiwp, netyster, ypandya, NishantTharani, heiligerl, SportsTHU,\nHoa Nguyen, manuel-arno-korfmann-webentwicklung, aterzis-personal, nxby, Xiaoting He, Josiah Yoder,\nmathresearch, mzz2017, jroberayalas, iluu, ghejc, BSharmi, vkramdev, simonwardjones, LakshKD,\nTalNeoran, djliden, Nikhil95, Oren Barkan, guoweis, haozhu233, pratikhack, Yue Ying, tayfununal,\nsteinsag, charleybeller, Andrew Lumsdaine, Jiekui Zhang, Deepak Pathak, Florian Donhauser, Tim Gates,\nAdriaan Tijsseling, Ron Medina, Gaurav Saha, Murat Semerci, Lei Mao, Levi McClenny, Joshua Broyde,\njake221, jonbally, zyhazwraith, Brian Pulfer, Nick Tomasino, Lefan Zhang, Hongshen Yang, Vinney Cavallo,\nyuntai, Yuanxiang Zhu, amarazov, pasricha, Ben Greenawald, Shivam Upadhyay, Quanshangze Du, Biswajit Sahoo,\nParthe Pandit, Ishan Kumar, HomunculusK, Lane Schwartz, varadgunjal, Jason Wiener, Armin Gholampoor,\nShreshtha13, eigen-arnav, Hyeonggyu Kim, EmilyOng, B\u00e1lint Mucs\u00e1nyi, Chase DuBois, Juntian Tao,\nWenxiang Xu, Lifu Huang, filevich, quake2005, nils-werner, Yiming Li, Marsel Khisamutdinov,\nFrancesco \"Fuma\" Fumagalli, Peilin Sun, Vincent Gurgul, qingfengtommy, Janmey Shukla, Mo Shan,\nKaan Sancak, regob, AlexSauer, Gopalakrishna Ramachandra, Tobias Uelwer, Chao Wang, Tian Cao,\nNicolas Corthorn, akash5474, kxxt, zxydi1992, Jacob Britton, Shuangchi He, zhmou, krahets, Jie-Han Chen,\nAtishay Garg, Marcel Flygare, adtygan, Nik Vaessen, bolded, Louis Schlessinger, Balaji Varatharajan,\natgctg, Kaixin Li, Victor Barbaros, Riccardo Musto, Elizabeth Ho, azimjonn, Guilherme Miotto, Alessandro Finamore,\nJoji Joseph, Anthony Biel, Zeming Zhao, shjustinbaek, gab-chen, nantekoto, Yutaro Nishiyama, Oren Amsalem,\nTian-MaoMao, Amin Allahyar, Gijs van Tulder, Mikhail Berkov, iamorphen, Matthew Caseres, Andrew Walsh,\npggPL, RohanKarthikeyan, Ryan Choi, and Likun Lei."
    },
    {
      "chunk_id": "c571513cffee_3",
      "chapter": "index",
      "heading": "Acknowledgments",
      "text": "We thank Amazon Web Services, especially Wen-Ming Ye, George Karypis, Swami Sivasubramanian, Peter DeSantis, Adam Selipsky,\nand Andrew Jassy for their generous support in writing this book. Without the available time, resources, discussions with colleagues,\nand continuous encouragement, this book would not have happened. During the preparation of the book for publication,\nCambridge University Press has offered excellent support. We thank our commissioning editor David Tranah\nfor his help and professionalism."
    },
    {
      "chunk_id": "91d22918ddd9_0",
      "chapter": "index",
      "heading": "Summary",
      "text": "Deep learning has revolutionized pattern recognition,\nintroducing technology that now powers a wide range of  technologies,\nin such diverse fields as computer vision,\nnatural language processing,\nand automatic speech recognition.\nTo successfully apply deep learning,\nyou must understand how to cast a problem,\nthe basic mathematics of modeling,\nthe algorithms for fitting your models to data,\nand the engineering techniques to implement it all.\nThis book presents a comprehensive resource,\nincluding prose, figures, mathematics, and code, all in one place."
    },
    {
      "chunk_id": "020f0e0a8156_0",
      "chapter": "index",
      "heading": "Exercises",
      "text": "1. Register an account on the discussion forum of this book [discuss.d2l.ai](https://discuss.d2l.ai/).\n1. Install Python on your computer.\n1. Follow the links at the bottom of the section to the forum, where you will be able to seek out help and discuss the book and find answers to your questions by engaging the authors and broader community.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/18)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/20)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/186)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17963)\n:end_tab:"
    },
    {
      "chunk_id": "c4fbf517f146_0",
      "chapter": "autograd",
      "heading": "autograd",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Automatic Differentiation\n:label:`sec_autograd`\n\nRecall from :numref:`sec_calculus` \nthat calculating derivatives is the crucial step\nin all the optimization algorithms\nthat we will use to train deep networks.\nWhile the calculations are straightforward,\nworking them out by hand can be tedious and error-prone, \nand these issues only grow\nas our models become more complex.\n\nFortunately all modern deep learning frameworks\ntake this work off our plates\nby offering *automatic differentiation*\n(often shortened to *autograd*). \nAs we pass data through each successive function,\nthe framework builds a *computational graph* \nthat tracks how each value depends on others.\nTo calculate derivatives, \nautomatic differentiation \nworks backwards through this graph\napplying the chain rule. \nThe computational algorithm for applying the chain rule\nin this fashion is called *backpropagation*.\n\nWhile autograd libraries have become\na hot concern over the past decade,\nthey have a long history. \nIn fact the earliest references to autograd\ndate back over half of a century :cite:`Wengert.1964`.\nThe core ideas behind modern backpropagation\ndate to a PhD thesis from 1980 :cite:`Speelpenning.1980`\nand were further developed in the late 1980s :cite:`Griewank.1989`.\nWhile backpropagation has become the default method \nfor computing gradients, it is not the only option. \nFor instance, the Julia programming language employs \nforward propagation :cite:`Revels.Lubin.Papamarkou.2016`. \nBefore exploring methods, \nlet's first master the autograd package.\n\n```{.python .input}\n%%tab mxnet\nfrom mxnet import autograd, np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "7a0493cc5bb4_0",
      "chapter": "autograd",
      "heading": "A Simple Function",
      "text": "Let's assume that we are interested\nin (**differentiating the function\n$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\nwith respect to the column vector $\\mathbf{x}$.**)\nTo start, we assign `x` an initial value. ```{.python .input  n=1}\n%%tab mxnet\nx = np.arange(4.0)\nx\n```\n\n```{.python .input  n=7}\n%%tab pytorch\nx = torch.arange(4.0)\nx\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.range(4, dtype=tf.float32)\nx\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.arange(4.0)\nx\n```\n\n:begin_tab:`mxnet, pytorch, tensorflow`\n[**Before we calculate the gradient\nof $y$ with respect to $\\mathbf{x}$,\nwe need a place to store it.**]\nIn general, we avoid allocating new memory\nevery time we take a derivative \nbecause deep learning requires \nsuccessively computing derivatives\nwith respect to the same parameters\na great many times,\nand we might risk running out of memory. Note that the gradient of a scalar-valued function\nwith respect to a vector $\\mathbf{x}$\nis vector-valued with \nthe same shape as $\\mathbf{x}$."
    },
    {
      "chunk_id": "7a0493cc5bb4_1",
      "chapter": "autograd",
      "heading": "A Simple Function",
      "text": "Note that the gradient of a scalar-valued function\nwith respect to a vector $\\mathbf{x}$\nis vector-valued with \nthe same shape as $\\mathbf{x}$. :end_tab:\n\n```{.python .input  n=8}\n%%tab mxnet\n# We allocate memory for a tensor's gradient by invoking `attach_grad`\nx.attach_grad()\n# After we calculate a gradient taken with respect to `x`, we will be able to\n# access it via the `grad` attribute, whose values are initialized with 0s\nx.grad\n```\n\n```{.python .input  n=9}\n%%tab pytorch\n# Can also create x = torch.arange(4.0, requires_grad=True)\nx.requires_grad_(True)\nx.grad  # The gradient is None by default\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.Variable(x)\n```\n\n(**We now calculate our function of `x` and assign the result to `y`.**)\n\n```{.python .input  n=10}\n%%tab mxnet\n# Our code is inside an `autograd.record` scope to build the computational\n# graph\nwith autograd.record():\n    y = 2 * np.dot(x, x)\ny\n```\n\n```{.python .input  n=11}\n%%tab pytorch\ny = 2 * torch.dot(x, x)\ny\n```\n\n```{.python .input}\n%%tab tensorflow\n# Record all computations onto a tape\nwith tf.GradientTape() as t:\n    y = 2 * tf.tensordot(x, x, axes=1)\ny\n```\n\n```{.python .input}\n%%tab jax\ny = lambda x: 2 * jnp.dot(x, x)\ny(x)\n```\n\n:begin_tab:`mxnet`\n[**We can now take the gradient of `y`\nwith respect to `x`**] by calling \nits `backward` method. Next, we can access the gradient \nvia `x`'s `grad` attribute. :end_tab:\n\n:begin_tab:`pytorch`\n[**We can now take the gradient of `y`\nwith respect to `x`**] by calling \nits `backward` method. Next, we can access the gradient \nvia `x`'s `grad` attribute. :end_tab:\n\n:begin_tab:`tensorflow`\n[**We can now calculate the gradient of `y`\nwith respect to `x`**] by calling \nthe `gradient` method. :end_tab:\n\n:begin_tab:`jax`\n[**We can now take the gradient of `y`\nwith respect to `x`**] by passing through the\n`grad` transform."
    },
    {
      "chunk_id": "7a0493cc5bb4_2",
      "chapter": "autograd",
      "heading": "A Simple Function",
      "text": ":end_tab:\n\n:begin_tab:`jax`\n[**We can now take the gradient of `y`\nwith respect to `x`**] by passing through the\n`grad` transform. :end_tab:\n\n```{.python .input}\n%%tab mxnet\ny.backward()\nx.grad\n```\n\n```{.python .input  n=12}\n%%tab pytorch\ny.backward()\nx.grad\n```\n\n```{.python .input}\n%%tab tensorflow\nx_grad = t.gradient(y, x)\nx_grad\n```\n\n```{.python .input}\n%%tab jax\nfrom jax import grad\n# The `grad` transform returns a Python function that\n# computes the gradient of the original function\nx_grad = grad(y)(x)\nx_grad\n```\n\n(**We already know that the gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\nwith respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.**)\nWe can now verify that the automatic gradient computation\nand the expected result are identical. ```{.python .input  n=13}\n%%tab mxnet\nx.grad == 4 * x\n```\n\n```{.python .input  n=14}\n%%tab pytorch\nx.grad == 4 * x\n```\n\n```{.python .input}\n%%tab tensorflow\nx_grad == 4 * x\n```\n\n```{.python .input}\n%%tab jax\nx_grad == 4 * x\n```\n\n:begin_tab:`mxnet`\n[**Now let's calculate \nanother function of `x`\nand take its gradient.**] \nNote that MXNet resets the gradient buffer \nwhenever we record a new gradient. :end_tab:\n\n:begin_tab:`pytorch`\n[**Now let's calculate \nanother function of `x`\nand take its gradient.**]\nNote that PyTorch does not automatically \nreset the gradient buffer \nwhen we record a new gradient. Instead, the new gradient\nis added to the already-stored gradient. This behavior comes in handy\nwhen we want to optimize the sum \nof multiple objective functions. To reset the gradient buffer,\nwe can call `x.grad.zero_()` as follows:\n:end_tab:\n\n:begin_tab:`tensorflow`\n[**Now let's calculate \nanother function of `x`\nand take its gradient.**]\nNote that TensorFlow resets the gradient buffer \nwhenever we record a new gradient."
    },
    {
      "chunk_id": "7a0493cc5bb4_3",
      "chapter": "autograd",
      "heading": "A Simple Function",
      "text": ":end_tab:\n\n```{.python .input}\n%%tab mxnet\nwith autograd.record():\n    y = x.sum()\ny.backward()\nx.grad  # Overwritten by the newly calculated gradient\n```\n\n```{.python .input  n=20}\n%%tab pytorch\nx.grad.zero_()  # Reset the gradient\ny = x.sum()\ny.backward()\nx.grad\n```\n\n```{.python .input}\n%%tab tensorflow\nwith tf.GradientTape() as t:\n    y = tf.reduce_sum(x)\nt.gradient(y, x)  # Overwritten by the newly calculated gradient\n```\n\n```{.python .input}\n%%tab jax\ny = lambda x: x.sum()\ngrad(y)(x)\n```"
    },
    {
      "chunk_id": "473618de40c1_0",
      "chapter": "autograd",
      "heading": "Backward for Non-Scalar Variables",
      "text": "When `y` is a vector, \nthe most natural representation \nof the derivative of  `y`\nwith respect to a vector `x` \nis a matrix called the *Jacobian*\nthat contains the partial derivatives\nof each component of `y` \nwith respect to each component of `x`. Likewise, for higher-order `y` and `x`,\nthe result of differentiation could be an even higher-order tensor. While Jacobians do show up in some\nadvanced machine learning techniques,\nmore commonly we want to sum up \nthe gradients of each component of `y`\nwith respect to the full vector `x`,\nyielding a vector of the same shape as `x`. For example, we often have a vector \nrepresenting the value of our loss function\ncalculated separately for each example among\na *batch* of training examples. Here, we just want to (**sum up the gradients\ncomputed individually for each example**). :begin_tab:`mxnet`\nMXNet handles this problem by reducing all tensors to scalars \nby summing before computing a gradient. In other words, rather than returning the Jacobian \n$\\partial_{\\mathbf{x}} \\mathbf{y}$,\nit returns the gradient of the sum\n$\\partial_{\\mathbf{x}} \\sum_i y_i$. :end_tab:\n\n:begin_tab:`pytorch`\nBecause deep learning frameworks vary \nin how they interpret gradients of\nnon-scalar tensors,\nPyTorch takes some steps to avoid confusion. Invoking `backward` on a non-scalar elicits an error \nunless we tell PyTorch how to reduce the object to a scalar. More formally, we need to provide some vector $\\mathbf{v}$ \nsuch that `backward` will compute \n$\\mathbf{v}^\\top \\partial_{\\mathbf{x}} \\mathbf{y}$ \nrather than $\\partial_{\\mathbf{x}} \\mathbf{y}$. This next part may be confusing,\nbut for reasons that will become clear later, \nthis argument (representing $\\mathbf{v}$) is named `gradient`. For a more detailed description, see Yang Zhang's \n[Medium post](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29). :end_tab:\n\n:begin_tab:`tensorflow`\nBy default, TensorFlow returns the gradient of the sum."
    },
    {
      "chunk_id": "473618de40c1_1",
      "chapter": "autograd",
      "heading": "Backward for Non-Scalar Variables",
      "text": ":end_tab:\n\n:begin_tab:`tensorflow`\nBy default, TensorFlow returns the gradient of the sum. In other words, rather than returning \nthe Jacobian $\\partial_{\\mathbf{x}} \\mathbf{y}$,\nit returns the gradient of the sum\n$\\partial_{\\mathbf{x}} \\sum_i y_i$. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nwith autograd.record():\n    y = x * x  \ny.backward()\nx.grad  # Equals the gradient of y = sum(x * x)\n```\n\n```{.python .input}\n%%tab pytorch\nx.grad.zero_()\ny = x * x\ny.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\nx.grad\n```\n\n```{.python .input}\n%%tab tensorflow\nwith tf.GradientTape() as t:\n    y = x * x\nt.gradient(y, x)  # Same as y = tf.reduce_sum(x * x)\n```\n\n```{.python .input}\n%%tab jax\ny = lambda x: x * x\n# grad is only defined for scalar output functions\ngrad(lambda x: y(x).sum())(x)\n```"
    },
    {
      "chunk_id": "1002b66629ed_0",
      "chapter": "autograd",
      "heading": "Detaching Computation",
      "text": "Sometimes, we wish to [**move some calculations\noutside of the recorded computational graph.**]\nFor example, say that we use the input \nto create some auxiliary intermediate terms \nfor which we do not want to compute a gradient. In this case, we need to *detach* \nthe respective computational graph\nfrom the final result. The following toy example makes this clearer: \nsuppose we have `z = x * y` and `y = x * x` \nbut we want to focus on the *direct* influence of `x` on `z` \nrather than the influence conveyed via `y`. In this case, we can create a new variable `u`\nthat takes the same value as `y` \nbut whose *provenance* (how it was created)\nhas been wiped out. Thus `u` has no ancestors in the graph\nand gradients do not flow through `u` to `x`. For example, taking the gradient of `z = x * u`\nwill yield the result `u`,\n(not `3 * x * x` as you might have \nexpected since `z = x * x * x`). ```{.python .input}\n%%tab mxnet\nwith autograd.record():\n    y = x * x\n    u = y.detach()\n    z = u * x\nz.backward()\nx.grad == u\n```\n\n```{.python .input  n=21}\n%%tab pytorch\nx.grad.zero_()\ny = x * x\nu = y.detach()\nz = u * x\n\nz.sum().backward()\nx.grad == u\n```\n\n```{.python .input}\n%%tab tensorflow\n# Set persistent=True to preserve the compute graph. # This lets us run t.gradient more than once\nwith tf.GradientTape(persistent=True) as t:\n    y = x * x\n    u = tf.stop_gradient(y)\n    z = u * x\n\nx_grad = t.gradient(z, x)\nx_grad == u\n```\n\n```{.python .input}\n%%tab jax\nimport jax\n\ny = lambda x: x * x\n# jax.lax primitives are Python wrappers around XLA operations\nu = jax.lax.stop_gradient(y(x))\nz = lambda x: u * x\n\ngrad(lambda x: z(x).sum())(x) == y(x)\n```\n\nNote that while this procedure\ndetaches `y`'s ancestors\nfrom the graph leading to `z`, \nthe computational graph leading to `y` \npersists and thus we can calculate\nthe gradient of `y` with respect to `x`."
    },
    {
      "chunk_id": "1002b66629ed_1",
      "chapter": "autograd",
      "heading": "Detaching Computation",
      "text": "```{.python .input}\n%%tab mxnet\ny.backward()\nx.grad == 2 * x\n```\n\n```{.python .input}\n%%tab pytorch\nx.grad.zero_()\ny.sum().backward()\nx.grad == 2 * x\n```\n\n```{.python .input}\n%%tab tensorflow\nt.gradient(y, x) == 2 * x\n```\n\n```{.python .input}\n%%tab jax\ngrad(lambda x: y(x).sum())(x) == 2 * x\n```"
    },
    {
      "chunk_id": "880bf7b04e5b_0",
      "chapter": "autograd",
      "heading": "Gradients and Python Control Flow",
      "text": "So far we reviewed cases where the path from input to output \nwas well defined via a function such as `z = x * x * x`. Programming offers us a lot more freedom in how we compute results. For instance, we can make them depend on auxiliary variables \nor condition choices on intermediate results. One benefit of using automatic differentiation\nis that [**even if**] building the computational graph of \n(**a function required passing through a maze of Python control flow**)\n(e.g., conditionals, loops, and arbitrary function calls),\n(**we can still calculate the gradient of the resulting variable.**)\nTo illustrate this, consider the following code snippet where \nthe number of iterations of the `while` loop\nand the evaluation of the `if` statement\nboth depend on the value of the input `a`. ```{.python .input}\n%%tab mxnet\ndef f(a):\n    b = a * 2\n    while np.linalg.norm(b) < 1000:\n        b = b * 2\n    if b.sum() > 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n```\n\n```{.python .input}\n%%tab pytorch\ndef f(a):\n    b = a * 2\n    while b.norm() < 1000:\n        b = b * 2\n    if b.sum() > 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n```\n\n```{.python .input}\n%%tab tensorflow\ndef f(a):\n    b = a * 2\n    while tf.norm(b) < 1000:\n        b = b * 2\n    if tf.reduce_sum(b) > 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n```\n\n```{.python .input}\n%%tab jax\ndef f(a):\n    b = a * 2\n    while jnp.linalg.norm(b) < 1000:\n        b = b * 2\n    if b.sum() > 0:\n        c = b\n    else:\n        c = 100 * b\n    return c\n```\n\nBelow, we call this function, passing in a random value, as input. Since the input is a random variable, \nwe do not know what form \nthe computational graph will take. However, whenever we execute `f(a)` \non a specific input, we realize \na specific computational graph\nand can subsequently run `backward`."
    },
    {
      "chunk_id": "880bf7b04e5b_1",
      "chapter": "autograd",
      "heading": "Gradients and Python Control Flow",
      "text": "Since the input is a random variable, \nwe do not know what form \nthe computational graph will take. However, whenever we execute `f(a)` \non a specific input, we realize \na specific computational graph\nand can subsequently run `backward`. ```{.python .input}\n%%tab mxnet\na = np.random.normal()\na.attach_grad()\nwith autograd.record():\n    d = f(a)\nd.backward()\n```\n\n```{.python .input}\n%%tab pytorch\na = torch.randn(size=(), requires_grad=True)\nd = f(a)\nd.backward()\n```\n\n```{.python .input}\n%%tab tensorflow\na = tf.Variable(tf.random.normal(shape=()))\nwith tf.GradientTape() as t:\n    d = f(a)\nd_grad = t.gradient(d, a)\nd_grad\n```\n\n```{.python .input}\n%%tab jax\nfrom jax import random\na = random.normal(random.PRNGKey(1), ())\nd = f(a)\nd_grad = grad(f)(a)\n```\n\nEven though our function `f` is, for demonstration purposes, a bit contrived,\nits dependence on the input is quite simple: \nit is a *linear* function of `a` \nwith piecewise defined scale. As such, `f(a) / a` is a vector of constant entries \nand, moreover, `f(a) / a` needs to match \nthe gradient of `f(a)` with respect to `a`. ```{.python .input}\n%%tab mxnet\na.grad == d / a\n```\n\n```{.python .input}\n%%tab pytorch\na.grad == d / a\n```\n\n```{.python .input}\n%%tab tensorflow\nd_grad == d / a\n```\n\n```{.python .input}\n%%tab jax\nd_grad == d / a\n```\n\nDynamic control flow is very common in deep learning. For instance, when processing text, the computational graph\ndepends on the length of the input. In these cases, automatic differentiation \nbecomes vital for statistical modeling \nsince it is impossible to compute the gradient *a priori*."
    },
    {
      "chunk_id": "ea32ae95d194_0",
      "chapter": "autograd",
      "heading": "Discussion",
      "text": "You have now gotten a taste of the power of automatic differentiation. \nThe development of libraries for calculating derivatives\nboth automatically and efficiently \nhas been a massive productivity booster\nfor deep learning practitioners,\nliberating them so they can focus on less menial.\nMoreover, autograd lets us design massive models\nfor which pen and paper gradient computations \nwould be prohibitively time consuming.\nInterestingly, while we use autograd to *optimize* models\n(in a statistical sense)\nthe *optimization* of autograd libraries themselves\n(in a computational sense)\nis a rich subject\nof vital interest to framework designers.\nHere, tools from compilers and graph manipulation \nare leveraged to compute results \nin the most expedient and memory-efficient manner. \n\nFor now, try to remember these basics: (i) attach gradients to those variables with respect to which we desire derivatives; (ii) record the computation of the target value; (iii) execute the backpropagation function; and  (iv) access the resulting gradient."
    },
    {
      "chunk_id": "7d37941f18a8_0",
      "chapter": "autograd",
      "heading": "Exercises",
      "text": "1. Why is the second derivative much more expensive to compute than the first derivative?\n1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result. \n1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$. \n1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously. \n1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation. \n1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved. \n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/34)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/35)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/200)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17970)\n:end_tab:"
    },
    {
      "chunk_id": "8b3d2c1ad1fb_0",
      "chapter": "calculus",
      "heading": "calculus",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Calculus\n:label:`sec_calculus`\n\nFor a long time, how to calculate \nthe area of a circle remained a mystery. Then, in Ancient Greece, the mathematician Archimedes\ncame up with the clever idea \nto inscribe a series of polygons \nwith increasing numbers of vertices\non the inside of a circle\n(:numref:`fig_circle_area`). For a polygon with $n$ vertices,\nwe obtain $n$ triangles. The height of each triangle approaches the radius $r$ \nas we partition the circle more finely. At the same time, its base approaches $2 \\pi r/n$, \nsince the ratio between arc and secant approaches 1 \nfor a large number of vertices. Thus, the area of the polygon approaches\n$n \\cdot r \\cdot \\frac{1}{2} (2 \\pi r/n) = \\pi r^2$. ![Finding the area of a circle as a limit procedure.](../img/polygon-circle.svg)\n:label:`fig_circle_area`\n\nThis limiting procedure is at the root of both \n*differential calculus* and *integral calculus*. The former can tell us how to increase\nor decrease a function's value by\nmanipulating its arguments. This comes in handy for the *optimization problems*\nthat we face in deep learning,\nwhere we repeatedly update our parameters \nin order to decrease the loss function. Optimization addresses how to fit our models to training data,\nand calculus is its key prerequisite. However, do not forget that our ultimate goal\nis to perform well on *previously unseen* data. That problem is called *generalization*\nand will be a key focus of other chapters."
    },
    {
      "chunk_id": "8b3d2c1ad1fb_1",
      "chapter": "calculus",
      "heading": "calculus",
      "text": "However, do not forget that our ultimate goal\nis to perform well on *previously unseen* data. That problem is called *generalization*\nand will be a key focus of other chapters. ```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom matplotlib_inline import backend_inline\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nfrom matplotlib_inline import backend_inline\nimport numpy as np\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nfrom matplotlib_inline import backend_inline\nimport numpy as np\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nfrom matplotlib_inline import backend_inline\nimport numpy as np\n```"
    },
    {
      "chunk_id": "a5ea99e8ac0f_0",
      "chapter": "calculus",
      "heading": "Derivatives and Differentiation",
      "text": "Put simply, a *derivative* is the rate of change\nin a function with respect to changes in its arguments. Derivatives can tell us how rapidly a loss function\nwould increase or decrease were we \nto *increase* or *decrease* each parameter\nby an infinitesimally small amount. Formally, for functions $f: \\mathbb{R} \\rightarrow \\mathbb{R}$,\nthat map from scalars to scalars,\n[**the *derivative* of $f$ at a point $x$ is defined as**]\n\n(**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$**)\n:eqlabel:`eq_derivative`\n\nThis term on the right hand side is called a *limit* \nand it tells us what happens \nto the value of an expression\nas a specified variable \napproaches a particular value. This limit tells us what \nthe ratio between a perturbation $h$\nand the change in the function value \n$f(x + h) - f(x)$ converges to \nas we shrink its size to zero. When $f'(x)$ exists, $f$ is said \nto be *differentiable* at $x$;\nand when $f'(x)$ exists for all $x$\non a set, e.g., the interval $[a,b]$, \nwe say that $f$ is differentiable on this set. Not all functions are differentiable,\nincluding many that we wish to optimize,\nsuch as accuracy and the area under the\nreceiving operating characteristic (AUC). However, because computing the derivative of the loss \nis a crucial step in nearly all \nalgorithms for training deep neural networks,\nwe often optimize a differentiable *surrogate* instead. We can interpret the derivative \n$f'(x)$\nas the *instantaneous* rate of change \nof $f(x)$ with respect to $x$. Let's develop some intuition with an example."
    },
    {
      "chunk_id": "a5ea99e8ac0f_1",
      "chapter": "calculus",
      "heading": "Derivatives and Differentiation",
      "text": "We can interpret the derivative \n$f'(x)$\nas the *instantaneous* rate of change \nof $f(x)$ with respect to $x$. Let's develop some intuition with an example. (**Define $u = f(x) = 3x^2-4x$.**)\n\n```{.python .input}\n%%tab mxnet\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n```\n\n```{.python .input}\n%%tab pytorch\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n```\n\n```{.python .input}\n%%tab tensorflow\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n```\n\n```{.python .input}\n%%tab jax\ndef f(x):\n    return 3 * x ** 2 - 4 * x\n```\n\n[**Setting $x=1$, we see that $\\frac{f(x+h) - f(x)}{h}$**] (**approaches $2$\nas $h$ approaches $0$.**)\nWhile this experiment lacks \nthe rigor of a mathematical proof,\nwe can quickly see that indeed $f'(1) = 2$. ```{.python .input}\n%%tab all\nfor h in 10.0**np.arange(-1, -6, -1):\n    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')\n```\n\nThere are several equivalent notational conventions for derivatives. Given $y = f(x)$, the following expressions are equivalent:\n\n$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n\nwhere the symbols $\\frac{d}{dx}$ and $D$ are *differentiation operators*. Below, we present the derivatives of some common functions:\n\n$$\\begin{aligned} \\frac{d}{dx} C & = 0 && \\textrm{for any constant $C$} \\\\ \\frac{d}{dx} x^n & = n x^{n-1} && \\textrm{for } n \\neq 0 \\\\ \\frac{d}{dx} e^x & = e^x \\\\ \\frac{d}{dx} \\ln x & = x^{-1}. \\end{aligned}$$\n\nFunctions composed from differentiable functions \nare often themselves differentiable. The following rules come in handy \nfor working with compositions \nof any differentiable functions \n$f$ and $g$, and constant $C$."
    },
    {
      "chunk_id": "a5ea99e8ac0f_2",
      "chapter": "calculus",
      "heading": "Derivatives and Differentiation",
      "text": "\\end{aligned}$$\n\nFunctions composed from differentiable functions \nare often themselves differentiable. The following rules come in handy \nfor working with compositions \nof any differentiable functions \n$f$ and $g$, and constant $C$. $$\\begin{aligned} \\frac{d}{dx} [C f(x)] & = C \\frac{d}{dx} f(x) && \\textrm{Constant multiple rule} \\\\ \\frac{d}{dx} [f(x) + g(x)] & = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x) && \\textrm{Sum rule} \\\\ \\frac{d}{dx} [f(x) g(x)] & = f(x) \\frac{d}{dx} g(x) + g(x) \\frac{d}{dx} f(x) && \\textrm{Product rule} \\\\ \\frac{d}{dx} \\frac{f(x)}{g(x)} & = \\frac{g(x) \\frac{d}{dx} f(x) - f(x) \\frac{d}{dx} g(x)}{g^2(x)} && \\textrm{Quotient rule} \\end{aligned}$$\n\nUsing this, we can apply the rules \nto find the derivative of $3 x^2 - 4x$ via\n\n$$\\frac{d}{dx} [3 x^2 - 4x] = 3 \\frac{d}{dx} x^2 - 4 \\frac{d}{dx} x = 6x - 4.$$\n\nPlugging in $x = 1$ shows that, indeed, \nthe derivative equals $2$ at this location. Note that derivatives tell us \nthe *slope* of a function \nat a particular location."
    },
    {
      "chunk_id": "4408859d22c3_0",
      "chapter": "calculus",
      "heading": "Visualization Utilities",
      "text": "[**We can visualize the slopes of functions using the `matplotlib` library**]. We need to define a few functions. As its name indicates, `use_svg_display` \ntells `matplotlib` to output graphics \nin SVG format for crisper images. The comment `#@save` is a special modifier \nthat allows us to save any function, \nclass, or other code block to the `d2l` package \nso that we can invoke it later \nwithout repeating the code, \ne.g., via `d2l.use_svg_display()`. ```{.python .input}\n%%tab all\ndef use_svg_display():  #@save\n    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n    backend_inline.set_matplotlib_formats('svg')\n```\n\nConveniently, we can set figure sizes with `set_figsize`. Since the import statement `from matplotlib import pyplot as plt` \nwas marked via `#@save` in the `d2l` package, we can call `d2l.plt`. ```{.python .input}\n%%tab all\ndef set_figsize(figsize=(3.5, 2.5)):  #@save\n    \"\"\"Set the figure size for matplotlib.\"\"\"\n    use_svg_display()\n    d2l.plt.rcParams['figure.figsize'] = figsize\n```\n\nThe `set_axes` function can associate axes\nwith properties, including labels, ranges,\nand scales. ```{.python .input}\n%%tab all\n#@save\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    \"\"\"Set the axes for matplotlib.\"\"\"\n    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale), axes.set_yscale(yscale)\n    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n    if legend:\n        axes.legend(legend)\n    axes.grid()\n```\n\nWith these three functions, we can define a `plot` function \nto overlay multiple curves. Much of the code here is just ensuring \nthat the sizes and shapes of inputs match."
    },
    {
      "chunk_id": "4408859d22c3_1",
      "chapter": "calculus",
      "heading": "Visualization Utilities",
      "text": "Much of the code here is just ensuring \nthat the sizes and shapes of inputs match. ```{.python .input}\n%%tab all\n#@save\ndef plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n         ylim=None, xscale='linear', yscale='linear',\n         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n    \"\"\"Plot data points.\"\"\"\n\n    def has_one_axis(X):  # True if X (tensor or list) has 1 axis\n        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n                and not hasattr(X[0], \"__len__\"))\n    \n    if has_one_axis(X): X = [X]\n    if Y is None:\n        X, Y = [[]] * len(X), X\n    elif has_one_axis(Y):\n        Y = [Y]\n    if len(X) != len(Y):\n        X = X * len(Y)\n        \n    set_figsize(figsize)\n    if axes is None:\n        axes = d2l.plt.gca()\n    axes.cla()\n    for x, y, fmt in zip(X, Y, fmts):\n        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)\n    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n```\n\nNow we can [**plot the function $u = f(x)$ and its tangent line $y = 2x - 3$ at $x=1$**],\nwhere the coefficient $2$ is the slope of the tangent line. ```{.python .input}\n%%tab all\nx = np.arange(0, 3, 0.1)\nplot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n```"
    },
    {
      "chunk_id": "2611366b87f3_0",
      "chapter": "calculus",
      "heading": "Partial Derivatives and Gradients",
      "text": ":label:`subsec_calculus-grad`\n\nThus far, we have been differentiating\nfunctions of just one variable. In deep learning, we also need to work\nwith functions of *many* variables. We briefly introduce notions of the derivative\nthat apply to such *multivariate* functions. Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with $n$ variables. The *partial derivative* of $y$ \nwith respect to its $i^\\textrm{th}$ parameter $x_i$ is\n\n$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n\n\nTo calculate $\\frac{\\partial y}{\\partial x_i}$, \nwe can treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ as constants \nand calculate the derivative of $y$ with respect to $x_i$. The following notational conventions for partial derivatives \nare all common and all mean the same thing:\n\n$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = \\partial_{x_i} f = \\partial_i f = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n\nWe can concatenate partial derivatives \nof a multivariate function \nwith respect to all its variables \nto obtain a vector that is called\nthe *gradient* of the function. Suppose that the input of function \n$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ \nis an $n$-dimensional vector \n$\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ \nand the output is a scalar. The gradient of the function $f$ \nwith respect to $\\mathbf{x}$ \nis a vector of $n$ partial derivatives:\n\n$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\left[\\partial_{x_1} f(\\mathbf{x}), \\partial_{x_2} f(\\mathbf{x}), \\ldots\n\\partial_{x_n} f(\\mathbf{x})\\right]^\\top.$$ \n\nWhen there is no ambiguity,\n$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ \nis typically replaced \nby $\\nabla f(\\mathbf{x})$. The following rules come in handy \nfor differentiating multivariate functions:\n\n* For all $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ we have $\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$ and $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$."
    },
    {
      "chunk_id": "2611366b87f3_1",
      "chapter": "calculus",
      "heading": "Partial Derivatives and Gradients",
      "text": "* For square matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ we have that $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$ and in particular\n$\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$. Similarly, for any matrix $\\mathbf{X}$, \nwe have $\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_\\textrm{F}^2 = 2\\mathbf{X}$."
    },
    {
      "chunk_id": "c5296ba7bdce_0",
      "chapter": "calculus",
      "heading": "Chain Rule",
      "text": "In deep learning, the gradients of concern\nare often difficult to calculate\nbecause we are working with \ndeeply nested functions \n(of functions (of functions...)).\nFortunately, the *chain rule* takes care of this. \nReturning to functions of a single variable,\nsuppose that $y = f(g(x))$\nand that the underlying functions \n$y=f(u)$ and $u=g(x)$ \nare both differentiable.\nThe chain rule states that \n\n\n$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n\n\n\nTurning back to multivariate functions,\nsuppose that $y = f(\\mathbf{u})$ has variables\n$u_1, u_2, \\ldots, u_m$, \nwhere each $u_i = g_i(\\mathbf{x})$ \nhas variables $x_1, x_2, \\ldots, x_n$,\ni.e.,  $\\mathbf{u} = g(\\mathbf{x})$.\nThen the chain rule states that\n\n$$\\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial y}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x_{i}} + \\frac{\\partial y}{\\partial u_{2}} \\frac{\\partial u_{2}}{\\partial x_{i}} + \\ldots + \\frac{\\partial y}{\\partial u_{m}} \\frac{\\partial u_{m}}{\\partial x_{i}} \\ \\textrm{ and so } \\ \\nabla_{\\mathbf{x}} y =  \\mathbf{A} \\nabla_{\\mathbf{u}} y,$$\n\nwhere $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ is a *matrix*\nthat contains the derivative of vector $\\mathbf{u}$\nwith respect to vector $\\mathbf{x}$.\nThus, evaluating the gradient requires \ncomputing a vector--matrix product. \nThis is one of the key reasons why linear algebra \nis such an integral building block \nin building deep learning systems."
    },
    {
      "chunk_id": "6665b3b45a95_0",
      "chapter": "calculus",
      "heading": "Discussion",
      "text": "While we have just scratched the surface of a deep topic,\na number of concepts already come into focus: \nfirst, the composition rules for differentiation\ncan be applied routinely, enabling\nus to compute gradients *automatically*.\nThis task requires no creativity and thus \nwe can focus our cognitive powers elsewhere.\nSecond, computing the derivatives of vector-valued functions \nrequires us to multiply matrices as we trace \nthe dependency graph of variables from output to input. \nIn particular, this graph is traversed in a *forward* direction \nwhen we evaluate a function \nand in a *backwards* direction \nwhen we compute gradients. \nLater chapters will formally introduce backpropagation,\na computational procedure for applying the chain rule.\n\nFrom the viewpoint of optimization, gradients allow us \nto determine how to move the parameters of a model\nin order to lower the loss,\nand each step of the optimization algorithms used \nthroughout this book will require calculating the gradient."
    },
    {
      "chunk_id": "0d5423cad269_0",
      "chapter": "calculus",
      "heading": "Exercises",
      "text": "1. So far we took the rules for derivatives for granted. \n   Using the definition and limits prove the properties \n   for (i) $f(x) = c$, (ii) $f(x) = x^n$, (iii) $f(x) = e^x$ and (iv) $f(x) = \\log x$.\n1. In the same vein, prove the product, sum, and quotient rule from first principles. \n1. Prove that the constant multiple rule follows as a special case of the product rule. \n1. Calculate the derivative of $f(x) = x^x$. \n1. What does it mean that $f'(x) = 0$ for some $x$? \n   Give an example of a function $f$ \n   and a location $x$ for which this might hold. \n1. Plot the function $y = f(x) = x^3 - \\frac{1}{x}$ \n   and plot its tangent line at $x = 1$.\n1. Find the gradient of the function \n   $f(\\mathbf{x}) = 3x_1^2 + 5e^{x_2}$.\n1. What is the gradient of the function \n   $f(\\mathbf{x}) = \\|\\mathbf{x}\\|_2$? What happens for $\\mathbf{x} = \\mathbf{0}$?\n1. Can you write out the chain rule for the case \n   where $u = f(x, y, z)$ and $x = x(a, b)$, $y = y(a, b)$, and $z = z(a, b)$?\n1. Given a function $f(x)$ that is invertible, \n   compute the derivative of its inverse $f^{-1}(x)$. \n   Here we have that $f^{-1}(f(x)) = x$ and conversely $f(f^{-1}(y)) = y$. \n   Hint: use these properties in your derivation. \n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/32)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/33)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/197)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17969)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "#  Preliminaries\n:label:`chap_preliminaries`\n\nTo prepare for your dive into deep learning,\nyou will need a few survival skills:\n(i) techniques for storing and manipulating data;\n(ii) libraries for ingesting \nand preprocessing data from a variety of sources;\n(iii) knowledge of the basic linear algebraic operations\nthat we apply to high-dimensional data elements;\n(iv) just enough calculus to determine\nwhich direction to adjust each parameter\nin order to decrease the loss function;\n(v) the ability to automatically compute derivatives\nso that you can forget much of \nthe calculus you just learned;\n(vi) some basic fluency in probability,\nour primary language for reasoning under uncertainty;\nand (vii) some aptitude for finding answers \nin the official documentation when you get stuck.\n\nIn short, this chapter provides a rapid introduction \nto the basics that you will need to follow \n*most* of the technical content in this book.\n\n```toc\n:maxdepth: 2\n\nndarray\npandas\nlinear-algebra\ncalculus\nautograd\nprobability\nlookup-api\n```"
    },
    {
      "chunk_id": "c79ff7345b50_0",
      "chapter": "linear-algebra",
      "heading": "linear-algebra",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Linear Algebra\n:label:`sec_linear-algebra`\n\nBy now, we can load datasets into tensors\nand manipulate these tensors\nwith basic mathematical operations.\nTo start building sophisticated models,\nwe will also need a few tools from linear algebra.\nThis section offers a gentle introduction\nto the most essential concepts,\nstarting from scalar arithmetic\nand ramping up to matrix multiplication.\n\n```{.python .input}\n%%tab mxnet\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "ade9d0412a08_0",
      "chapter": "linear-algebra",
      "heading": "Scalars",
      "text": "Most everyday mathematics\nconsists of manipulating\nnumbers one at a time.\nFormally, we call these values *scalars*.\nFor example, the temperature in Palo Alto\nis a balmy $72$ degrees Fahrenheit.\nIf you wanted to convert the temperature to Celsius\nyou would evaluate the expression\n$c = \\frac{5}{9}(f - 32)$, setting $f$ to $72$.\nIn this equation, the values\n$5$, $9$, and $32$ are constant scalars.\nThe variables $c$ and $f$\nin general represent unknown scalars.\n\nWe denote scalars\nby ordinary lower-cased letters\n(e.g., $x$, $y$, and $z$)\nand the space of all (continuous)\n*real-valued* scalars by $\\mathbb{R}$.\nFor expedience, we will skip past\nrigorous definitions of *spaces*:\njust remember that the expression $x \\in \\mathbb{R}$\nis a formal way to say that $x$ is a real-valued scalar.\nThe symbol $\\in$ (pronounced \"in\")\ndenotes membership in a set.\nFor example, $x, y \\in \\{0, 1\\}$\nindicates that $x$ and $y$ are variables\nthat can only take values $0$ or $1$.\n\n(**Scalars are implemented as tensors\nthat contain only one element.**)\nBelow, we assign two scalars\nand perform the familiar addition, multiplication,\ndivision, and exponentiation operations.\n\n```{.python .input}\n%%tab mxnet\nx = np.array(3.0)\ny = np.array(2.0)\n\nx + y, x * y, x / y, x ** y\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.tensor(3.0)\ny = torch.tensor(2.0)\n\nx + y, x * y, x / y, x**y\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.constant(3.0)\ny = tf.constant(2.0)\n\nx + y, x * y, x / y, x**y\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.array(3.0)\ny = jnp.array(2.0)\n\nx + y, x * y, x / y, x**y\n```"
    },
    {
      "chunk_id": "20a7c36ddd04_0",
      "chapter": "linear-algebra",
      "heading": "Vectors",
      "text": "For current purposes, [**you can think of a vector as a fixed-length array of scalars.**]\nAs with their code counterparts,\nwe call these scalars the *elements* of the vector\n(synonyms include *entries* and *components*). When vectors represent examples from real-world datasets,\ntheir values hold some real-world significance. For example, if we were training a model to predict\nthe risk of a loan defaulting,\nwe might associate each applicant with a vector\nwhose components correspond to quantities\nlike their income, length of employment,\nor number of previous defaults. If we were studying the risk of heart attack,\neach vector might represent a patient\nand its components might correspond to\ntheir most recent vital signs, cholesterol levels,\nminutes of exercise per day, etc. We denote vectors by bold lowercase letters,\n(e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$). Vectors are implemented as $1^{\\textrm{st}}$-order tensors. In general, such tensors can have arbitrary lengths,\nsubject to memory limitations. Caution: in Python, as in most programming languages, vector indices start at $0$, also known as *zero-based indexing*, whereas in linear algebra subscripts begin at $1$ (one-based indexing). ```{.python .input}\n%%tab mxnet\nx = np.arange(3)\nx\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.arange(3)\nx\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.range(3)\nx\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.arange(3)\nx\n```\n\nWe can refer to an element of a vector by using a subscript. For example, $x_2$ denotes the second element of $\\mathbf{x}$. Since $x_2$ is a scalar, we do not bold it. By default, we visualize vectors\nby stacking their elements vertically:\n\n$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix}.$$\n:eqlabel:`eq_vec_def`\n\nHere $x_1, \\ldots, x_n$ are elements of the vector. Later on, we will distinguish between such *column vectors*\nand *row vectors* whose elements are stacked horizontally."
    },
    {
      "chunk_id": "20a7c36ddd04_1",
      "chapter": "linear-algebra",
      "heading": "Vectors",
      "text": "Later on, we will distinguish between such *column vectors*\nand *row vectors* whose elements are stacked horizontally. Recall that [**we access a tensor's elements via indexing.**]\n\n```{.python .input}\n%%tab all\nx[2]\n```\n\nTo indicate that a vector contains $n$ elements,\nwe write $\\mathbf{x} \\in \\mathbb{R}^n$. Formally, we call $n$ the *dimensionality* of the vector. [**In code, this corresponds to the tensor's length**],\naccessible via Python's built-in `len` function. ```{.python .input}\n%%tab all\nlen(x)\n```\n\nWe can also access the length via the `shape` attribute. The shape is a tuple that indicates a tensor's length along each axis. (**Tensors with just one axis have shapes with just one element.**)\n\n```{.python .input}\n%%tab all\nx.shape\n```\n\nOftentimes, the word \"dimension\" gets overloaded\nto mean both the number of axes\nand the length along a particular axis. To avoid this confusion,\nwe use *order* to refer to the number of axes\nand *dimensionality* exclusively to refer\nto the number of components."
    },
    {
      "chunk_id": "f2bb9b3f0d1d_0",
      "chapter": "linear-algebra",
      "heading": "Matrices",
      "text": "Just as scalars are $0^{\\textrm{th}}$-order tensors\nand vectors are $1^{\\textrm{st}}$-order tensors,\nmatrices are $2^{\\textrm{nd}}$-order tensors. We denote matrices by bold capital letters\n(e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$),\nand represent them in code by tensors with two axes. The expression $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\nindicates that a matrix $\\mathbf{A}$\ncontains $m \\times n$ real-valued scalars,\narranged as $m$ rows and $n$ columns. When $m = n$, we say that a matrix is *square*. Visually, we can illustrate any matrix as a table. To refer to an individual element,\nwe subscript both the row and column indices, e.g.,\n$a_{ij}$ is the value that belongs to $\\mathbf{A}$'s\n$i^{\\textrm{th}}$ row and $j^{\\textrm{th}}$ column:\n\n$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n:eqlabel:`eq_matrix_def`\n\n\nIn code, we represent a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\nby a $2^{\\textrm{nd}}$-order tensor with shape ($m$, $n$). [**We can convert any appropriately sized $m \\times n$ tensor\ninto an $m \\times n$ matrix**]\nby passing the desired shape to `reshape`:\n\n```{.python .input}\n%%tab mxnet\nA = np.arange(6).reshape(3, 2)\nA\n```\n\n```{.python .input}\n%%tab pytorch\nA = torch.arange(6).reshape(3, 2)\nA\n```\n\n```{.python .input}\n%%tab tensorflow\nA = tf.reshape(tf.range(6), (3, 2))\nA\n```\n\n```{.python .input}\n%%tab jax\nA = jnp.arange(6).reshape(3, 2)\nA\n```\n\nSometimes we want to flip the axes. When we exchange a matrix's rows and columns,\nthe result is called its *transpose*. Formally, we signify a matrix $\\mathbf{A}$'s transpose\nby $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$,\nthen $b_{ij} = a_{ji}$ for all $i$ and $j$."
    },
    {
      "chunk_id": "f2bb9b3f0d1d_1",
      "chapter": "linear-algebra",
      "heading": "Matrices",
      "text": "When we exchange a matrix's rows and columns,\nthe result is called its *transpose*. Formally, we signify a matrix $\\mathbf{A}$'s transpose\nby $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$,\nthen $b_{ij} = a_{ji}$ for all $i$ and $j$. Thus, the transpose of an $m \\times n$ matrix\nis an $n \\times m$ matrix:\n\n$$\n\\mathbf{A}^\\top =\n\\begin{bmatrix}\n    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n    a_{1n} & a_{2n} & \\dots  & a_{mn}\n\\end{bmatrix}. $$\n\nIn code, we can access any (**matrix's transpose**) as follows:\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nA.T\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.transpose(A)\n```\n\n[**Symmetric matrices are the subset of square matrices\nthat are equal to their own transposes:\n$\\mathbf{A} = \\mathbf{A}^\\top$.**]\nThe following matrix is symmetric:\n\n```{.python .input}\n%%tab mxnet\nA = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nA == A.T\n```\n\n```{.python .input}\n%%tab pytorch\nA = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nA == A.T\n```\n\n```{.python .input}\n%%tab tensorflow\nA = tf.constant([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nA == tf.transpose(A)\n```\n\n```{.python .input}\n%%tab jax\nA = jnp.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nA == A.T\n```\n\nMatrices are useful for representing datasets. Typically, rows correspond to individual records\nand columns correspond to distinct attributes."
    },
    {
      "chunk_id": "6cd507da439d_0",
      "chapter": "linear-algebra",
      "heading": "Tensors",
      "text": "While you can go far in your machine learning journey\nwith only scalars, vectors, and matrices,\neventually you may need to work with\nhigher-order [**tensors**].\nTensors (**give us a generic way of describing\nextensions to $n^{\\textrm{th}}$-order arrays.**)\nWe call software objects of the *tensor class* \"tensors\"\nprecisely because they too can have arbitrary numbers of axes.\nWhile it may be confusing to use the word\n*tensor* for both the mathematical object\nand its realization in code,\nour meaning should usually be clear from context.\nWe denote general tensors by capital letters\nwith a special font face\n(e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$)\nand their indexing mechanism\n(e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$)\nfollows naturally from that of matrices.\n\nTensors will become more important\nwhen we start working with images.\nEach image arrives as a $3^{\\textrm{rd}}$-order tensor\nwith axes corresponding to the height, width, and *channel*.\nAt each spatial location, the intensities\nof each color (red, green, and blue)\nare stacked along the channel.\nFurthermore, a collection of images is represented\nin code by a $4^{\\textrm{th}}$-order tensor,\nwhere distinct images are indexed\nalong the first axis.\nHigher-order tensors are constructed, as were vectors and matrices,\nby growing the number of shape components.\n\n```{.python .input}\n%%tab mxnet\nnp.arange(24).reshape(2, 3, 4)\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.arange(24).reshape(2, 3, 4)\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reshape(tf.range(24), (2, 3, 4))\n```\n\n```{.python .input}\n%%tab jax\njnp.arange(24).reshape(2, 3, 4)\n```"
    },
    {
      "chunk_id": "01edb71aee48_0",
      "chapter": "linear-algebra",
      "heading": "Basic Properties of Tensor Arithmetic",
      "text": "Scalars, vectors, matrices,\nand higher-order tensors\nall have some handy properties.\nFor example, elementwise operations\nproduce outputs that have the\nsame shape as their operands.\n\n```{.python .input}\n%%tab mxnet\nA = np.arange(6).reshape(2, 3)\nB = A.copy()  # Assign a copy of A to B by allocating new memory\nA, A + B\n```\n\n```{.python .input}\n%%tab pytorch\nA = torch.arange(6, dtype=torch.float32).reshape(2, 3)\nB = A.clone()  # Assign a copy of A to B by allocating new memory\nA, A + B\n```\n\n```{.python .input}\n%%tab tensorflow\nA = tf.reshape(tf.range(6, dtype=tf.float32), (2, 3))\nB = A  # No cloning of A to B by allocating new memory\nA, A + B\n```\n\n```{.python .input}\n%%tab jax\nA = jnp.arange(6, dtype=jnp.float32).reshape(2, 3)\nB = A\nA, A + B\n```\n\nThe [**elementwise product of two matrices\nis called their *Hadamard product***] (denoted $\\odot$).\nWe can spell out the entries\nof the Hadamard product of two matrices\n$\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$:\n\n\n\n$$\n\\mathbf{A} \\odot \\mathbf{B} =\n\\begin{bmatrix}\n    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n\\end{bmatrix}.\n$$\n\n```{.python .input}\n%%tab all\nA * B\n```\n\n[**Adding or multiplying a scalar and a tensor**] produces a result\nwith the same shape as the original tensor.\nHere, each element of the tensor is added to (or multiplied by) the scalar.\n\n```{.python .input}\n%%tab mxnet\na = 2\nX = np.arange(24).reshape(2, 3, 4)\na + X, (a * X).shape\n```\n\n```{.python .input}\n%%tab pytorch\na = 2\nX = torch.arange(24).reshape(2, 3, 4)\na + X, (a * X).shape\n```\n\n```{.python .input}\n%%tab tensorflow\na = 2\nX = tf.reshape(tf.range(24), (2, 3, 4))\na + X, (a * X).shape\n```\n\n```{.python .input}\n%%tab jax\na = 2\nX = jnp.arange(24).reshape(2, 3, 4)\na + X, (a * X).shape\n```"
    },
    {
      "chunk_id": "413ec518eb25_0",
      "chapter": "linear-algebra",
      "heading": "Reduction",
      "text": ":label:`subsec_lin-alg-reduction`\n\nOften, we wish to calculate [**the sum of a tensor's elements.**]\nTo express the sum of the elements in a vector $\\mathbf{x}$ of length $n$,\nwe write $\\sum_{i=1}^n x_i$. There is a simple function for it:\n\n```{.python .input}\n%%tab mxnet\nx = np.arange(3)\nx, x.sum()\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.arange(3, dtype=torch.float32)\nx, x.sum()\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.range(3, dtype=tf.float32)\nx, tf.reduce_sum(x)\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.arange(3, dtype=jnp.float32)\nx, x.sum()\n```\n\nTo express [**sums over the elements of tensors of arbitrary shape**],\nwe simply sum over all its axes. For example, the sum of the elements\nof an $m \\times n$ matrix $\\mathbf{A}$\ncould be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$. ```{.python .input}\n%%tab mxnet, pytorch, jax\nA.shape, A.sum()\n```\n\n```{.python .input}\n%%tab tensorflow\nA.shape, tf.reduce_sum(A)\n```\n\nBy default, invoking the sum function\n*reduces* a tensor along all of its axes,\neventually producing a scalar. Our libraries also allow us to [**specify the axes\nalong which the tensor should be reduced.**]\nTo sum over all elements along the rows (axis 0),\nwe specify `axis=0` in `sum`. Since the input matrix reduces along axis 0\nto generate the output vector,\nthis axis is missing from the shape of the output. ```{.python .input}\n%%tab mxnet, pytorch, jax\nA.shape, A.sum(axis=0).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nA.shape, tf.reduce_sum(A, axis=0).shape\n```\n\nSpecifying `axis=1` will reduce the column dimension (axis 1) by summing up elements of all the columns. ```{.python .input}\n%%tab mxnet, pytorch, jax\nA.shape, A.sum(axis=1).shape\n```\n\n```{.python .input}\n%%tab tensorflow\nA.shape, tf.reduce_sum(A, axis=1).shape\n```\n\nReducing a matrix along both rows and columns via summation\nis equivalent to summing up all the elements of the matrix."
    },
    {
      "chunk_id": "413ec518eb25_1",
      "chapter": "linear-algebra",
      "heading": "Reduction",
      "text": "```{.python .input}\n%%tab mxnet, pytorch, jax\nA.sum(axis=[0, 1]) == A.sum()  # Same as A.sum()\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reduce_sum(A, axis=[0, 1]), tf.reduce_sum(A)  # Same as tf.reduce_sum(A)\n```\n\n[**A related quantity is the *mean*, also called the *average*.**]\nWe calculate the mean by dividing the sum\nby the total number of elements. Because computing the mean is so common,\nit gets a dedicated library function\nthat works analogously to `sum`. ```{.python .input}\n%%tab mxnet, jax\nA.mean(), A.sum() / A.size\n```\n\n```{.python .input}\n%%tab pytorch\nA.mean(), A.sum() / A.numel()\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reduce_mean(A), tf.reduce_sum(A) / tf.size(A).numpy()\n```\n\nLikewise, the function for calculating the mean\ncan also reduce a tensor along specific axes. ```{.python .input}\n%%tab mxnet, pytorch, jax\nA.mean(axis=0), A.sum(axis=0) / A.shape[0]\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reduce_mean(A, axis=0), tf.reduce_sum(A, axis=0) / A.shape[0]\n```"
    },
    {
      "chunk_id": "7718e5e9f214_0",
      "chapter": "linear-algebra",
      "heading": "Non-Reduction Sum",
      "text": ":label:`subsec_lin-alg-non-reduction`\n\nSometimes it can be useful to [**keep the number of axes unchanged**]\nwhen invoking the function for calculating the sum or mean.\nThis matters when we want to use the broadcast mechanism.\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nsum_A = A.sum(axis=1, keepdims=True)\nsum_A, sum_A.shape\n```\n\n```{.python .input}\n%%tab tensorflow\nsum_A = tf.reduce_sum(A, axis=1, keepdims=True)\nsum_A, sum_A.shape\n```\n\nFor instance, since `sum_A` keeps its two axes after summing each row,\nwe can (**divide `A` by `sum_A` with broadcasting**)\nto create a matrix where each row sums up to $1$.\n\n```{.python .input}\n%%tab all\nA / sum_A\n```\n\nIf we want to calculate [**the cumulative sum of elements of `A` along some axis**],\nsay `axis=0` (row by row), we can call the `cumsum` function.\nBy design, this function does not reduce the input tensor along any axis.\n\n```{.python .input}\n%%tab mxnet, pytorch, jax\nA.cumsum(axis=0)\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.cumsum(A, axis=0)\n```"
    },
    {
      "chunk_id": "0ade5c43e00b_0",
      "chapter": "linear-algebra",
      "heading": "Dot Products",
      "text": "So far, we have only performed elementwise operations, sums, and averages. And if this was all we could do, linear algebra\nwould not deserve its own section. Fortunately, this is where things get more interesting. One of the most fundamental operations is the dot product. Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$,\ntheir *dot product* $\\mathbf{x}^\\top \\mathbf{y}$ (also known as *inner product*, $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$)\nis a sum over the products of the elements at the same position:\n$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$. [~~The *dot product* of two vectors is a sum over the products of the elements at the same position~~]\n\n```{.python .input}\n%%tab mxnet\ny = np.ones(3)\nx, y, np.dot(x, y)\n```\n\n```{.python .input}\n%%tab pytorch\ny = torch.ones(3, dtype = torch.float32)\nx, y, torch.dot(x, y)\n```\n\n```{.python .input}\n%%tab tensorflow\ny = tf.ones(3, dtype=tf.float32)\nx, y, tf.tensordot(x, y, axes=1)\n```\n\n```{.python .input}\n%%tab jax\ny = jnp.ones(3, dtype = jnp.float32)\nx, y, jnp.dot(x, y)\n```\n\nEquivalently, (**we can calculate the dot product of two vectors\nby performing an elementwise multiplication followed by a sum:**)\n\n```{.python .input}\n%%tab mxnet\nnp.sum(x * y)\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.sum(x * y)\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reduce_sum(x * y)\n```\n\n```{.python .input}\n%%tab jax\njnp.sum(x * y)\n```\n\nDot products are useful in a wide range of contexts. For example, given some set of values,\ndenoted by a vector $\\mathbf{x}  \\in \\mathbb{R}^n$,\nand a set of weights, denoted by $\\mathbf{w} \\in \\mathbb{R}^n$,\nthe weighted sum of the values in $\\mathbf{x}$\naccording to the weights $\\mathbf{w}$\ncould be expressed as the dot product $\\mathbf{x}^\\top \\mathbf{w}$. When the weights are nonnegative\nand sum to $1$, i.e., $\\left(\\sum_{i=1}^{n} {w_i} = 1\\right)$,\nthe dot product expresses a *weighted average*. After normalizing two vectors to have unit length,\nthe dot products express the cosine of the angle between them."
    },
    {
      "chunk_id": "0ade5c43e00b_1",
      "chapter": "linear-algebra",
      "heading": "Dot Products",
      "text": "When the weights are nonnegative\nand sum to $1$, i.e., $\\left(\\sum_{i=1}^{n} {w_i} = 1\\right)$,\nthe dot product expresses a *weighted average*. After normalizing two vectors to have unit length,\nthe dot products express the cosine of the angle between them. Later in this section, we will formally introduce this notion of *length*."
    },
    {
      "chunk_id": "dbe1d7d081e2_0",
      "chapter": "linear-algebra",
      "heading": "Matrix--Vector Products",
      "text": "Now that we know how to calculate dot products,\nwe can begin to understand the *product*\nbetween an $m \\times n$ matrix $\\mathbf{A}$\nand an $n$-dimensional vector $\\mathbf{x}$. To start off, we visualize our matrix\nin terms of its row vectors\n\n$$\\mathbf{A}=\n\\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_m \\\\\n\\end{bmatrix},$$\n\nwhere each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$\nis a row vector representing the $i^\\textrm{th}$ row\nof the matrix $\\mathbf{A}$. [**The matrix--vector product $\\mathbf{A}\\mathbf{x}$\nis simply a column vector of length $m$,\nwhose $i^\\textrm{th}$ element is the dot product\n$\\mathbf{a}^\\top_i \\mathbf{x}$:**]\n\n$$\n\\mathbf{A}\\mathbf{x}\n= \\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_m \\\\\n\\end{bmatrix}\\mathbf{x}\n= \\begin{bmatrix}\n \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n\\vdots\\\\\n \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n\\end{bmatrix}. $$\n\nWe can think of multiplication with a matrix\n$\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$\nas a transformation that projects vectors\nfrom $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$. These transformations are remarkably useful. For example, we can represent rotations\nas multiplications by certain square matrices. Matrix--vector products also describe\nthe key calculation involved in computing\nthe outputs of each layer in a neural network\ngiven the outputs from the previous layer. :begin_tab:`mxnet`\nTo express a matrix--vector product in code,\nwe use the same `dot` function. The operation is inferred\nbased on the type of the arguments. Note that the column dimension of `A`\n(its length along axis 1)\nmust be the same as the dimension of `x` (its length). :end_tab:\n\n:begin_tab:`pytorch`\nTo express a matrix--vector product in code,\nwe use the `mv` function. Note that the column dimension of `A`\n(its length along axis 1)\nmust be the same as the dimension of `x` (its length)."
    },
    {
      "chunk_id": "dbe1d7d081e2_1",
      "chapter": "linear-algebra",
      "heading": "Matrix--Vector Products",
      "text": ":end_tab:\n\n:begin_tab:`pytorch`\nTo express a matrix--vector product in code,\nwe use the `mv` function. Note that the column dimension of `A`\n(its length along axis 1)\nmust be the same as the dimension of `x` (its length). Python has a convenience operator `@`\nthat can execute both matrix--vector\nand matrix--matrix products\n(depending on its arguments). Thus we can write `A@x`. :end_tab:\n\n:begin_tab:`tensorflow`\nTo express a matrix--vector product in code,\nwe use the `matvec` function. Note that the column dimension of `A`\n(its length along axis 1)\nmust be the same as the dimension of `x` (its length). :end_tab:\n\n```{.python .input}\n%%tab mxnet\nA.shape, x.shape, np.dot(A, x)\n```\n\n```{.python .input}\n%%tab pytorch\nA.shape, x.shape, torch.mv(A, x), A@x\n```\n\n```{.python .input}\n%%tab tensorflow\nA.shape, x.shape, tf.linalg.matvec(A, x)\n```\n\n```{.python .input}\n%%tab jax\nA.shape, x.shape, jnp.matmul(A, x)\n```"
    },
    {
      "chunk_id": "69f5855bbf6c_0",
      "chapter": "linear-algebra",
      "heading": "Matrix--Matrix Multiplication",
      "text": "Once you have gotten the hang of dot products and matrix--vector products,\nthen *matrix--matrix multiplication* should be straightforward. Say that we have two matrices\n$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$\nand $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n\n$$\\mathbf{A}=\\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n\\end{bmatrix},\\quad\n\\mathbf{B}=\\begin{bmatrix}\n b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n\\end{bmatrix}.$$\n\n\nLet $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ denote\nthe row vector representing the $i^\\textrm{th}$ row\nof the matrix $\\mathbf{A}$\nand let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ denote\nthe column vector from the $j^\\textrm{th}$ column\nof the matrix $\\mathbf{B}$:\n\n$$\\mathbf{A}=\n\\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_n \\\\\n\\end{bmatrix},\n\\quad \\mathbf{B}=\\begin{bmatrix}\n \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n\\end{bmatrix}."
    },
    {
      "chunk_id": "69f5855bbf6c_1",
      "chapter": "linear-algebra",
      "heading": "Matrix--Matrix Multiplication",
      "text": "$$\n\n\nTo form the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$,\nwe simply compute each element $c_{ij}$\nas the dot product between\nthe $i^{\\textrm{th}}$ row of $\\mathbf{A}$\nand the $j^{\\textrm{th}}$ column of $\\mathbf{B}$,\ni.e., $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n\n$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\\\\n\\mathbf{a}^\\top_{2} \\\\\n\\vdots \\\\\n\\mathbf{a}^\\top_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n \\vdots & \\vdots & \\ddots &\\vdots\\\\\n\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n\\end{bmatrix}. $$\n\n[**We can think of the matrix--matrix multiplication $\\mathbf{AB}$\nas performing $m$ matrix--vector products\nor $m \\times n$ dot products\nand stitching the results together\nto form an $n \\times m$ matrix.**]\nIn the following snippet,\nwe perform matrix multiplication on `A` and `B`. Here,\u00a0`A` is a matrix with two rows and three columns,\nand `B` is a matrix with three rows and four columns. After multiplication, we obtain a matrix with two rows and four columns. ```{.python .input}\n%%tab mxnet\nB = np.ones(shape=(3, 4))\nnp.dot(A, B)\n```\n\n```{.python .input}\n%%tab pytorch\nB = torch.ones(3, 4)\ntorch.mm(A, B), A@B\n```\n\n```{.python .input}\n%%tab tensorflow\nB = tf.ones((3, 4), tf.float32)\ntf.matmul(A, B)\n```\n\n```{.python .input}\n%%tab jax\nB = jnp.ones((3, 4))\njnp.matmul(A, B)\n```\n\nThe term *matrix--matrix multiplication* is\noften simplified to *matrix multiplication*,\nand should not be confused with the Hadamard product."
    },
    {
      "chunk_id": "5aadc93bbac4_0",
      "chapter": "linear-algebra",
      "heading": "Norms",
      "text": ":label:`subsec_lin-algebra-norms`\n\nSome of the most useful operators in linear algebra are *norms*. Informally, the norm of a vector tells us how *big* it is. For instance, the $\\ell_2$ norm measures\nthe (Euclidean) length of a vector. Here, we are employing a notion of *size* that concerns the magnitude of a vector's components\n(not its dimensionality). A norm is a function $\\| \\cdot \\|$ that maps a vector\nto a scalar and satisfies the following three properties:\n\n1. Given any vector $\\mathbf{x}$, if we scale (all elements of) the vector\n   by a scalar $\\alpha \\in \\mathbb{R}$, its norm scales accordingly:\n   $$\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.$$\n2. For any vectors $\\mathbf{x}$ and $\\mathbf{y}$:\n   norms satisfy the triangle inequality:\n   $$\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.$$\n3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n   $$\\|\\mathbf{x}\\| > 0 \\textrm{ for all } \\mathbf{x} \\neq 0.$$\n\nMany functions are valid norms and different norms\nencode different notions of size. The Euclidean norm that we all learned in elementary school geometry\nwhen calculating the hypotenuse of a right triangle\nis the square root of the sum of squares of a vector's elements. Formally, this is called [**the $\\ell_2$ *norm***] and expressed as\n\n(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.$$**)\n\nThe method `norm` calculates the $\\ell_2$ norm. ```{.python .input}\n%%tab mxnet\nu = np.array([3, -4])\nnp.linalg.norm(u)\n```\n\n```{.python .input}\n%%tab pytorch\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)\n```\n\n```{.python .input}\n%%tab tensorflow\nu = tf.constant([3.0, -4.0])\ntf.norm(u)\n```\n\n```{.python .input}\n%%tab jax\nu = jnp.array([3.0, -4.0])\njnp.linalg.norm(u)\n```\n\n[**The $\\ell_1$ norm**] is also common\nand the associated measure is called the Manhattan distance."
    },
    {
      "chunk_id": "5aadc93bbac4_1",
      "chapter": "linear-algebra",
      "heading": "Norms",
      "text": "By definition, the $\\ell_1$ norm sums\nthe absolute values of a vector's elements:\n\n(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n\nCompared to the $\\ell_2$ norm, it is less sensitive to outliers. To compute the $\\ell_1$ norm,\nwe compose the absolute value\nwith the sum operation. ```{.python .input}\n%%tab mxnet\nnp.abs(u).sum()\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.abs(u).sum()\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reduce_sum(tf.abs(u))\n```\n\n```{.python .input}\n%%tab jax\njnp.linalg.norm(u, ord=1) # same as jnp.abs(u).sum()\n```\n\nBoth the $\\ell_2$ and $\\ell_1$ norms are special cases\nof the more general $\\ell_p$ *norms*:\n\n$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n\nIn the case of matrices, matters are more complicated. After all, matrices can be viewed both as collections of individual entries\n*and* as objects that operate on vectors and transform them into other vectors. For instance, we can ask by how much longer\nthe matrix--vector product $\\mathbf{X} \\mathbf{v}$\ncould be relative to $\\mathbf{v}$. This line of thought leads to what is called the *spectral* norm. For now, we introduce [**the *Frobenius norm*,\nwhich is much easier to compute**] and defined as\nthe square root of the sum of the squares\nof a matrix's elements:\n\n[**$$\\|\\mathbf{X}\\|_\\textrm{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**]\n\nThe Frobenius norm behaves as if it were\nan $\\ell_2$ norm of a matrix-shaped vector. Invoking the following function will calculate\nthe Frobenius norm of a matrix. ```{.python .input}\n%%tab mxnet\nnp.linalg.norm(np.ones((4, 9)))\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.norm(torch.ones((4, 9)))\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.norm(tf.ones((4, 9)))\n```\n\n```{.python .input}\n%%tab jax\njnp.linalg.norm(jnp.ones((4, 9)))\n```\n\nWhile we do not want to get too far ahead of ourselves,\nwe already can plant some intuition about why these concepts are useful."
    },
    {
      "chunk_id": "5aadc93bbac4_2",
      "chapter": "linear-algebra",
      "heading": "Norms",
      "text": "In deep learning, we are often trying to solve optimization problems:\n*maximize* the probability assigned to observed data;\n*maximize* the revenue associated with a recommender model;\n*minimize* the distance between predictions\nand the ground truth observations;\n*minimize* the distance between representations\nof photos of the same person\nwhile *maximizing* the distance between representations\nof photos of different people. These distances, which constitute\nthe objectives of deep learning algorithms,\nare often expressed as norms."
    },
    {
      "chunk_id": "62727373a463_0",
      "chapter": "linear-algebra",
      "heading": "Discussion",
      "text": "In this section, we have reviewed all the linear algebra\nthat you will need to understand\na significant chunk of modern deep learning. There is a lot more to linear algebra, though,\nand much of it is useful for machine learning. For example, matrices can be decomposed into factors,\nand these decompositions can reveal\nlow-dimensional structure in real-world datasets. There are entire subfields of machine learning\nthat focus on using matrix decompositions\nand their generalizations to high-order tensors\nto discover structure in datasets\nand solve prediction problems. But this book focuses on deep learning. And we believe you will be more inclined\nto learn more mathematics\nonce you have gotten your hands dirty\napplying machine learning to real datasets. So while we reserve the right\nto introduce more mathematics later on,\nwe wrap up this section here. If you are eager to learn more linear algebra,\nthere are many excellent books and online resources. For a more advanced crash course, consider checking out\n:citet:`Strang.1993`, :citet:`Kolter.2008`, and :citet:`Petersen.Pedersen.ea.2008`. To recap:\n\n* Scalars, vectors, matrices, and tensors are\n  the basic mathematical objects used in linear algebra\n  and have zero, one, two, and an arbitrary number of axes, respectively. * Tensors can be sliced or reduced along specified axes\n  via indexing, or operations such as `sum` and `mean`, respectively. * Elementwise products are called Hadamard products. By contrast, dot products, matrix--vector products, and matrix--matrix products\n  are not elementwise operations and in general return objects\n  having shapes that are different from the the operands. * Compared to Hadamard products, matrix--matrix products\n  take considerably longer to compute (cubic rather than quadratic time). * Norms capture various notions of the magnitude of a vector (or matrix),\n  and are commonly applied to the difference of two vectors\n  to measure their distance apart."
    },
    {
      "chunk_id": "62727373a463_1",
      "chapter": "linear-algebra",
      "heading": "Discussion",
      "text": "* Norms capture various notions of the magnitude of a vector (or matrix),\n  and are commonly applied to the difference of two vectors\n  to measure their distance apart. * Common vector norms include the $\\ell_1$ and $\\ell_2$ norms,\n   and common matrix norms include the *spectral* and *Frobenius* norms."
    },
    {
      "chunk_id": "146ad9e2dd42_0",
      "chapter": "linear-algebra",
      "heading": "Exercises",
      "text": "1. Prove that the transpose of the transpose of a matrix is the matrix itself: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$. 1. Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that sum and transposition commute: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$. 1. Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Can you prove the result by using only the results of the previous two exercises? 1. We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`? Write your answer without implementing any code, then check your answer using code. 1. For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis? 1. Run `A / A.sum(axis=1)` and see what happens. Can you analyze the results? 1. When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally? 1. Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs along axes 0, 1, and 2? 1. Feed a tensor with three or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape? 1. Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{14}}$, initialized with Gaussian random variables. You want to compute the product $\\mathbf{A} \\mathbf{B} \\mathbf{C}$. Is there any difference in memory footprint and speed, depending on whether you compute $(\\mathbf{A} \\mathbf{B}) \\mathbf{C}$ or $\\mathbf{A} (\\mathbf{B} \\mathbf{C})$? Why? 1. Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}}$."
    },
    {
      "chunk_id": "146ad9e2dd42_1",
      "chapter": "linear-algebra",
      "heading": "Exercises",
      "text": "Why? 1. Consider three large matrices, say $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$, $\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ and $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}}$. Is there any difference in speed depending on whether you compute $\\mathbf{A} \\mathbf{B}$ or $\\mathbf{A} \\mathbf{C}^\\top$? Why? What changes if you initialize $\\mathbf{C} = \\mathbf{B}^\\top$ without cloning memory? Why? 1. Consider three matrices, say $\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{100 \\times 200}$. Construct a tensor with three axes by stacking $[\\mathbf{A}, \\mathbf{B}, \\mathbf{C}]$. What is the dimensionality? Slice out the second coordinate of the third axis to recover $\\mathbf{B}$. Check that your answer is correct. :begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/30)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/31)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/196)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17968)\n:end_tab:"
    },
    {
      "chunk_id": "37720e8db26e_0",
      "chapter": "lookup-api",
      "heading": "lookup-api",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Documentation\n:begin_tab:`mxnet`\nWhile we cannot possibly introduce every single MXNet function and class \n(and the information might become outdated quickly), \nthe [API documentation](https://mxnet.apache.org/versions/1.8.0/api) \nand additional [tutorials](https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/) and examples \nprovide such documentation. \nThis section provides some guidance for how to explore the MXNet API.\n:end_tab:\n\n:begin_tab:`pytorch`\nWhile we cannot possibly introduce every single PyTorch function and class \n(and the information might become outdated quickly), \nthe [API documentation](https://pytorch.org/docs/stable/index.html) and additional [tutorials](https://pytorch.org/tutorials/beginner/basics/intro.html) and examples \nprovide such documentation.\nThis section provides some guidance for how to explore the PyTorch API.\n:end_tab:\n\n:begin_tab:`tensorflow`\nWhile we cannot possibly introduce every single TensorFlow function and class \n(and the information might become outdated quickly), \nthe [API documentation](https://www.tensorflow.org/api_docs) and additional [tutorials](https://www.tensorflow.org/tutorials) and examples \nprovide such documentation. \nThis section provides some guidance for how to explore the TensorFlow API.\n:end_tab:\n\n```{.python .input}\n%%tab mxnet\nfrom mxnet import np\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nimport jax\n```"
    },
    {
      "chunk_id": "336e8eab7a09_0",
      "chapter": "lookup-api",
      "heading": "Functions and Classes in a Module",
      "text": "To know which functions and classes can be called in a module,\nwe invoke the `dir` function. For instance, we can\n(**query all properties in the module for generating random numbers**):\n\n```{.python .input  n=1}\n%%tab mxnet\nprint(dir(np.random))\n```\n\n```{.python .input  n=1}\n%%tab pytorch\nprint(dir(torch.distributions))\n```\n\n```{.python .input  n=1}\n%%tab tensorflow\nprint(dir(tf.random))\n```\n\n```{.python .input}\n%%tab jax\nprint(dir(jax.random))\n```\n\nGenerally, we can ignore functions that start and end with `__` (special objects in Python) \nor functions that start with a single `_`(usually internal functions). \nBased on the remaining function or attribute names, \nwe might hazard a guess that this module offers \nvarious methods for generating random numbers, \nincluding sampling from the uniform distribution (`uniform`), \nnormal distribution (`normal`), and multinomial distribution (`multinomial`)."
    },
    {
      "chunk_id": "6dae0c80ba78_0",
      "chapter": "lookup-api",
      "heading": "Specific Functions and Classes",
      "text": "For specific instructions on how to use a given function or class,\nwe can invoke the  `help` function. As an example, let's\n[**explore the usage instructions for tensors' `ones` function**].\n\n```{.python .input}\n%%tab mxnet\nhelp(np.ones)\n```\n\n```{.python .input}\n%%tab pytorch\nhelp(torch.ones)\n```\n\n```{.python .input}\n%%tab tensorflow\nhelp(tf.ones)\n```\n\n```{.python .input}\n%%tab jax\nhelp(jax.numpy.ones)\n```\n\nFrom the documentation, we can see that the `ones` function \ncreates a new tensor with the specified shape \nand sets all the elements to the value of 1. \nWhenever possible, you should (**run a quick test**) \nto confirm your interpretation:\n\n```{.python .input}\n%%tab mxnet\nnp.ones(4)\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.ones(4)\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.ones(4)\n```\n\n```{.python .input}\n%%tab jax\njax.numpy.ones(4)\n```\n\nIn the Jupyter notebook, we can use `?` to display the document in another\nwindow. For example, `list?` will create content\nthat is almost identical to `help(list)`,\ndisplaying it in a new browser window.\nIn addition, if we use two question marks, such as `list??`,\nthe Python code implementing the function will also be displayed.\n\nThe official documentation provides plenty of descriptions and examples that are beyond this book. \nWe emphasize important use cases \nthat will get you started quickly with practical problems, \nrather than completeness of coverage. \nWe also encourage you to study the source code of the libraries \nto see examples of high-quality implementations of production code. \nBy doing this you will become a better engineer \nin addition to becoming a better scientist.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/38)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/39)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/199)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17972)\n:end_tab:"
    },
    {
      "chunk_id": "9fbbbe66680b_0",
      "chapter": "ndarray",
      "heading": "ndarray",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Data Manipulation\n:label:`sec_ndarray`\n\nIn order to get anything done, \nwe need some way to store and manipulate data.\nGenerally, there are two important things \nwe need to do with data: \n(i) acquire them; \nand (ii) process them once they are inside the computer. \nThere is no point in acquiring data \nwithout some way to store it, \nso to start, let's get our hands dirty\nwith $n$-dimensional arrays, \nwhich we also call *tensors*.\nIf you already know the NumPy \nscientific computing package, \nthis will be a breeze.\nFor all modern deep learning frameworks,\nthe *tensor class* (`ndarray` in MXNet, \n`Tensor` in PyTorch and TensorFlow) \nresembles NumPy's `ndarray`,\nwith a few killer features added.\nFirst, the tensor class\nsupports automatic differentiation.\nSecond, it leverages GPUs\nto accelerate numerical computation,\nwhereas NumPy only runs on CPUs.\nThese properties make neural networks\nboth easy to code and fast to run."
    },
    {
      "chunk_id": "594504fef465_0",
      "chapter": "ndarray",
      "heading": "Getting Started",
      "text": ":begin_tab:`mxnet`\nTo start, we import the `np` (`numpy`) and\n`npx` (`numpy_extension`) modules from MXNet. Here, the `np` module includes \nfunctions supported by NumPy,\nwhile the `npx` module contains a set of extensions\ndeveloped to empower deep learning \nwithin a NumPy-like environment. When using tensors, we almost always \ninvoke the `set_np` function:\nthis is for compatibility of tensor processing \nby other components of MXNet. :end_tab:\n\n:begin_tab:`pytorch`\n(**To start, we import the PyTorch library. Note that the package name is `torch`.**)\n:end_tab:\n\n:begin_tab:`tensorflow`\nTo start, we import `tensorflow`. For brevity, practitioners \noften assign the alias `tf`. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nimport jax\nfrom jax import numpy as jnp\n```\n\n[**A tensor represents a (possibly multidimensional) array of numerical values.**]\nIn the one-dimensional case, i.e., when only one axis is needed for the data,\na tensor is called a *vector*. With two axes, a tensor is called a *matrix*. With $k > 2$ axes, we drop the specialized names\nand just refer to the object as a $k^\\textrm{th}$-*order tensor*. :begin_tab:`mxnet`\nMXNet provides a variety of functions \nfor creating new tensors \nprepopulated with values. For example, by invoking `arange(n)`,\nwe can create a vector of evenly spaced values,\nstarting at 0 (included) \nand ending at `n` (not included). By default, the interval size is $1$. Unless otherwise specified, \nnew tensors are stored in main memory \nand designated for CPU-based computation. :end_tab:\n\n:begin_tab:`pytorch`\nPyTorch provides a variety of functions \nfor creating new tensors \nprepopulated with values. For example, by invoking `arange(n)`,\nwe can create a vector of evenly spaced values,\nstarting at 0 (included) \nand ending at `n` (not included). By default, the interval size is $1$."
    },
    {
      "chunk_id": "594504fef465_1",
      "chapter": "ndarray",
      "heading": "Getting Started",
      "text": "For example, by invoking `arange(n)`,\nwe can create a vector of evenly spaced values,\nstarting at 0 (included) \nand ending at `n` (not included). By default, the interval size is $1$. Unless otherwise specified, \nnew tensors are stored in main memory \nand designated for CPU-based computation. :end_tab:\n\n:begin_tab:`tensorflow`\nTensorFlow provides a variety of functions \nfor creating new tensors \nprepopulated with values. For example, by invoking `range(n)`,\nwe can create a vector of evenly spaced values,\nstarting at 0 (included) \nand ending at `n` (not included). By default, the interval size is $1$. Unless otherwise specified, \nnew tensors are stored in main memory \nand designated for CPU-based computation. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nx = np.arange(12)\nx\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.arange(12, dtype=torch.float32)\nx\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.range(12, dtype=tf.float32)\nx\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.arange(12)\nx\n```\n\n:begin_tab:`mxnet`\nEach of these values is called\nan *element* of the tensor. The tensor `x` contains 12 elements. We can inspect the total number of elements \nin a tensor via its `size` attribute. :end_tab:\n\n:begin_tab:`pytorch`\nEach of these values is called\nan *element* of the tensor. The tensor `x` contains 12 elements. We can inspect the total number of elements \nin a tensor via its `numel` method. :end_tab:\n\n:begin_tab:`tensorflow`\nEach of these values is called\nan *element* of the tensor. The tensor `x` contains 12 elements. We can inspect the total number of elements \nin a tensor via the `size` function. :end_tab:\n\n```{.python .input}\n%%tab mxnet, jax\nx.size\n```\n\n```{.python .input}\n%%tab pytorch\nx.numel()\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.size(x)\n```\n\n(**We can access a tensor's *shape***) \n(the length along each axis)\nby inspecting its `shape` attribute. Because we are dealing with a vector here,\nthe `shape` contains just a single element\nand is identical to the size."
    },
    {
      "chunk_id": "594504fef465_2",
      "chapter": "ndarray",
      "heading": "Getting Started",
      "text": "Because we are dealing with a vector here,\nthe `shape` contains just a single element\nand is identical to the size. ```{.python .input}\n%%tab all\nx.shape\n```\n\nWe can [**change the shape of a tensor\nwithout altering its size or values**],\nby invoking `reshape`. For example, we can transform \nour vector `x` whose shape is (12,) \nto a matrix `X`  with shape (3, 4). This new tensor retains all elements\nbut reconfigures them into a matrix. Notice that the elements of our vector\nare laid out one row at a time and thus\n`x[3] == X[0, 3]`. ```{.python .input}\n%%tab mxnet, pytorch, jax\nX = x.reshape(3, 4)\nX\n```\n\n```{.python .input}\n%%tab tensorflow\nX = tf.reshape(x, (3, 4))\nX\n```\n\nNote that specifying every shape component\nto `reshape` is redundant. Because we already know our tensor's size,\nwe can work out one component of the shape given the rest. For example, given a tensor of size $n$\nand target shape ($h$, $w$),\nwe know that $w = n/h$. To automatically infer one component of the shape,\nwe can place a `-1` for the shape component\nthat should be inferred automatically. In our case, instead of calling `x.reshape(3, 4)`,\nwe could have equivalently called `x.reshape(-1, 4)` or `x.reshape(3, -1)`. Practitioners often need to work with tensors\ninitialized to contain all 0s or 1s. [**We can construct a tensor with all elements set to 0**] (~~or one~~)\nand a shape of (2, 3, 4) via the `zeros` function. ```{.python .input}\n%%tab mxnet\nnp.zeros((2, 3, 4))\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.zeros((2, 3, 4))\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.zeros((2, 3, 4))\n```\n\n```{.python .input}\n%%tab jax\njnp.zeros((2, 3, 4))\n```\n\nSimilarly, we can create a tensor \nwith all 1s by invoking `ones`."
    },
    {
      "chunk_id": "594504fef465_3",
      "chapter": "ndarray",
      "heading": "Getting Started",
      "text": "```{.python .input}\n%%tab mxnet\nnp.ones((2, 3, 4))\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.ones((2, 3, 4))\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.ones((2, 3, 4))\n```\n\n```{.python .input}\n%%tab jax\njnp.ones((2, 3, 4))\n```\n\nWe often wish to \n[**sample each element randomly (and independently)**] \nfrom a given probability distribution. For example, the parameters of neural networks\nare often initialized randomly. The following snippet creates a tensor \nwith elements drawn from \na standard Gaussian (normal) distribution\nwith mean 0 and standard deviation 1. ```{.python .input}\n%%tab mxnet\nnp.random.normal(0, 1, size=(3, 4))\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.randn(3, 4)\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.random.normal(shape=[3, 4])\n```\n\n```{.python .input}\n%%tab jax\n# Any call of a random function in JAX requires a key to be\n# specified, feeding the same key to a random function will\n# always result in the same sample being generated\njax.random.normal(jax.random.PRNGKey(0), (3, 4))\n```\n\nFinally, we can construct tensors by\n[**supplying the exact values for each element**] \nby supplying (possibly nested) Python list(s) \ncontaining numerical literals. Here, we construct a matrix with a list of lists,\nwhere the outermost list corresponds to axis 0,\nand the inner list corresponds to axis 1. ```{.python .input}\n%%tab mxnet\nnp.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n```\n\n```{.python .input}\n%%tab jax\njnp.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n```"
    },
    {
      "chunk_id": "7b707c9e1bbc_0",
      "chapter": "ndarray",
      "heading": "Indexing and Slicing",
      "text": "As with  Python lists,\nwe can access tensor elements \nby indexing (starting with 0). To access an element based on its position\nrelative to the end of the list,\nwe can use negative indexing. Finally, we can access whole ranges of indices \nvia slicing (e.g., `X[start:stop]`), \nwhere the returned value includes \nthe first index (`start`) *but not the last* (`stop`). Finally, when only one index (or slice)\nis specified for a $k^\\textrm{th}$-order tensor,\nit is applied along axis 0. Thus, in the following code,\n[**`[-1]` selects the last row and `[1:3]`\nselects the second and third rows**]. ```{.python .input}\n%%tab all\nX[-1], X[1:3]\n```\n\n:begin_tab:`mxnet, pytorch`\nBeyond reading them, (**we can also *write* elements of a matrix by specifying indices.**)\n:end_tab:\n\n:begin_tab:`tensorflow`\n`Tensors` in TensorFlow are immutable, and cannot be assigned to. `Variables` in TensorFlow are mutable containers of state that support\nassignments. Keep in mind that gradients in TensorFlow do not flow backwards\nthrough `Variable` assignments. Beyond assigning a value to the entire `Variable`, we can write elements of a\n`Variable` by specifying indices. :end_tab:\n\n```{.python .input}\n%%tab mxnet, pytorch\nX[1, 2] = 17\nX\n```\n\n```{.python .input}\n%%tab tensorflow\nX_var = tf.Variable(X)\nX_var[1, 2].assign(9)\nX_var\n```\n\n```{.python .input}\n%%tab jax\n# JAX arrays are immutable. jax.numpy.ndarray.at index\n# update operators create a new array with the corresponding\n# modifications made\nX_new_1 = X.at[1, 2].set(17)\nX_new_1\n```\n\nIf we want [**to assign multiple elements the same value,\nwe apply the indexing on the left-hand side \nof the assignment operation.**]\nFor instance, `[:2, :]`  accesses \nthe first and second rows,\nwhere `:` takes all the elements along axis 1 (column). While we discussed indexing for matrices,\nthis also works for vectors\nand for tensors of more than two dimensions."
    },
    {
      "chunk_id": "7b707c9e1bbc_1",
      "chapter": "ndarray",
      "heading": "Indexing and Slicing",
      "text": "While we discussed indexing for matrices,\nthis also works for vectors\nand for tensors of more than two dimensions. ```{.python .input}\n%%tab mxnet, pytorch\nX[:2, :] = 12\nX\n```\n\n```{.python .input}\n%%tab tensorflow\nX_var = tf.Variable(X)\nX_var[:2, :].assign(tf.ones(X_var[:2,:].shape, dtype=tf.float32) * 12)\nX_var\n```\n\n```{.python .input}\n%%tab jax\nX_new_2 = X_new_1.at[:2, :].set(12)\nX_new_2\n```"
    },
    {
      "chunk_id": "c684ae12f179_0",
      "chapter": "ndarray",
      "heading": "Operations",
      "text": "Now that we know how to construct tensors\nand how to read from and write to their elements,\nwe can begin to manipulate them\nwith various mathematical operations. Among the most useful of these \nare the *elementwise* operations. These apply a standard scalar operation\nto each element of a tensor. For functions that take two tensors as inputs,\nelementwise operations apply some standard binary operator\non each pair of corresponding elements. We can create an elementwise function \nfrom any function that maps \nfrom a scalar to a scalar. In mathematical notation, we denote such\n*unary* scalar operators (taking one input)\nby the signature \n$f: \\mathbb{R} \\rightarrow \\mathbb{R}$. This just means that the function maps\nfrom any real number onto some other real number. Most standard operators, including unary ones like $e^x$, can be applied elementwise. ```{.python .input}\n%%tab mxnet\nnp.exp(x)\n```\n\n```{.python .input}\n%%tab pytorch\ntorch.exp(x)\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.exp(x)\n```\n\n```{.python .input}\n%%tab jax\njnp.exp(x)\n```\n\nLikewise, we denote *binary* scalar operators,\nwhich map pairs of real numbers\nto a (single) real number\nvia the signature \n$f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$. Given any two vectors $\\mathbf{u}$ \nand $\\mathbf{v}$ *of the same shape*,\nand a binary operator $f$, we can produce a vector\n$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\nby setting $c_i \\gets f(u_i, v_i)$ for all $i$,\nwhere $c_i, u_i$, and $v_i$ are the $i^\\textrm{th}$ elements\nof vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$. Here, we produced the vector-valued\n$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\nby *lifting* the scalar function\nto an elementwise vector operation. The common standard arithmetic operators\nfor addition (`+`), subtraction (`-`), \nmultiplication (`*`), division (`/`), \nand exponentiation (`**`)\nhave all been *lifted* to elementwise operations\nfor identically-shaped tensors of arbitrary shape."
    },
    {
      "chunk_id": "c684ae12f179_1",
      "chapter": "ndarray",
      "heading": "Operations",
      "text": "The common standard arithmetic operators\nfor addition (`+`), subtraction (`-`), \nmultiplication (`*`), division (`/`), \nand exponentiation (`**`)\nhave all been *lifted* to elementwise operations\nfor identically-shaped tensors of arbitrary shape. ```{.python .input}\n%%tab mxnet\nx = np.array([1, 2, 4, 8])\ny = np.array([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n```\n\n```{.python .input}\n%%tab pytorch\nx = torch.tensor([1.0, 2, 4, 8])\ny = torch.tensor([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n```\n\n```{.python .input}\n%%tab tensorflow\nx = tf.constant([1.0, 2, 4, 8])\ny = tf.constant([2.0, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n```\n\n```{.python .input}\n%%tab jax\nx = jnp.array([1.0, 2, 4, 8])\ny = jnp.array([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n```\n\nIn addition to elementwise computations,\nwe can also perform linear algebraic operations,\nsuch as dot products and matrix multiplications. We will elaborate on these\nin :numref:`sec_linear-algebra`. We can also [***concatenate* multiple tensors,**]\nstacking them end-to-end to form a larger one. We just need to provide a list of tensors\nand tell the system along which axis to concatenate. The example below shows what happens when we concatenate\ntwo matrices along rows (axis 0)\ninstead of columns (axis 1). We can see that the first output's axis-0 length ($6$)\nis the sum of the two input tensors' axis-0 lengths ($3 + 3$);\nwhile the second output's axis-1 length ($8$)\nis the sum of the two input tensors' axis-1 lengths ($4 + 4$)."
    },
    {
      "chunk_id": "c684ae12f179_2",
      "chapter": "ndarray",
      "heading": "Operations",
      "text": "We can see that the first output's axis-0 length ($6$)\nis the sum of the two input tensors' axis-0 lengths ($3 + 3$);\nwhile the second output's axis-1 length ($8$)\nis the sum of the two input tensors' axis-1 lengths ($4 + 4$). ```{.python .input}\n%%tab mxnet\nX = np.arange(12).reshape(3, 4)\nY = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\nnp.concatenate([X, Y], axis=0), np.concatenate([X, Y], axis=1)\n```\n\n```{.python .input}\n%%tab pytorch\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\nY = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n```\n\n```{.python .input}\n%%tab tensorflow\nX = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4))\nY = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\ntf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1)\n```\n\n```{.python .input}\n%%tab jax\nX = jnp.arange(12, dtype=jnp.float32).reshape((3, 4))\nY = jnp.array([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\njnp.concatenate((X, Y), axis=0), jnp.concatenate((X, Y), axis=1)\n```\n\nSometimes, we want to \n[**construct a binary tensor via *logical statements*.**]\nTake `X == Y` as an example. For each position `i, j`, if `X[i, j]` and `Y[i, j]` are equal, \nthen the corresponding entry in the result takes value `1`,\notherwise it takes value `0`. ```{.python .input}\n%%tab all\nX == Y\n```\n\n[**Summing all the elements in the tensor**] yields a tensor with only one element. ```{.python .input}\n%%tab mxnet, pytorch, jax\nX.sum()\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.reduce_sum(X)\n```"
    },
    {
      "chunk_id": "8538525cf073_0",
      "chapter": "ndarray",
      "heading": "Broadcasting",
      "text": ":label:`subsec_broadcasting`\n\nBy now, you know how to perform \nelementwise binary operations\non two tensors of the same shape. \nUnder certain conditions,\neven when shapes differ, \nwe can still [**perform elementwise binary operations\nby invoking the *broadcasting mechanism*.**]\nBroadcasting works according to \nthe following two-step procedure:\n(i) expand one or both arrays\nby copying elements along axes with length 1\nso that after this transformation,\nthe two tensors have the same shape;\n(ii) perform an elementwise operation\non the resulting arrays.\n\n```{.python .input}\n%%tab mxnet\na = np.arange(3).reshape(3, 1)\nb = np.arange(2).reshape(1, 2)\na, b\n```\n\n```{.python .input}\n%%tab pytorch\na = torch.arange(3).reshape((3, 1))\nb = torch.arange(2).reshape((1, 2))\na, b\n```\n\n```{.python .input}\n%%tab tensorflow\na = tf.reshape(tf.range(3), (3, 1))\nb = tf.reshape(tf.range(2), (1, 2))\na, b\n```\n\n```{.python .input}\n%%tab jax\na = jnp.arange(3).reshape((3, 1))\nb = jnp.arange(2).reshape((1, 2))\na, b\n```\n\nSince `a` and `b` are $3\\times1$ \nand $1\\times2$ matrices, respectively,\ntheir shapes do not match up.\nBroadcasting produces a larger $3\\times2$ matrix \nby replicating matrix `a` along the columns\nand matrix `b` along the rows\nbefore adding them elementwise.\n\n```{.python .input}\n%%tab all\na + b\n```"
    },
    {
      "chunk_id": "7f15aac9c829_0",
      "chapter": "ndarray",
      "heading": "Saving Memory",
      "text": "[**Running operations can cause new memory to be\nallocated to host results.**]\nFor example, if we write `Y = X + Y`,\nwe dereference the tensor that `Y` used to point to\nand instead point `Y` at the newly allocated memory. We can demonstrate this issue with Python's `id()` function,\nwhich gives us the exact address \nof the referenced object in memory. Note that after we run `Y = Y + X`,\n`id(Y)` points to a different location. That is because Python first evaluates `Y + X`,\nallocating new memory for the result \nand then points `Y` to this new location in memory. ```{.python .input}\n%%tab all\nbefore = id(Y)\nY = Y + X\nid(Y) == before\n```\n\nThis might be undesirable for two reasons. First, we do not want to run around\nallocating memory unnecessarily all the time. In machine learning, we often have\nhundreds of megabytes of parameters\nand update all of them multiple times per second. Whenever possible, we want to perform these updates *in place*. Second, we might point at the \nsame parameters from multiple variables. If we do not update in place, \nwe must be careful to update all of these references,\nlest we spring a memory leak \nor inadvertently refer to stale parameters. :begin_tab:`mxnet, pytorch`\nFortunately, (**performing in-place operations**) is easy. We can assign the result of an operation\nto a previously allocated array `Y`\nby using slice notation: `Y[:] = <expression>`. To illustrate this concept, \nwe overwrite the values of tensor `Z`,\nafter initializing it, using `zeros_like`,\nto have the same shape as `Y`. :end_tab:\n\n:begin_tab:`tensorflow`\n`Variables` are mutable containers of state in TensorFlow. They provide\na way to store your model parameters. We can assign the result of an operation\nto a `Variable` with `assign`. To illustrate this concept, \nwe overwrite the values of `Variable` `Z`\nafter initializing it, using `zeros_like`,\nto have the same shape as `Y`."
    },
    {
      "chunk_id": "7f15aac9c829_1",
      "chapter": "ndarray",
      "heading": "Saving Memory",
      "text": "We can assign the result of an operation\nto a `Variable` with `assign`. To illustrate this concept, \nwe overwrite the values of `Variable` `Z`\nafter initializing it, using `zeros_like`,\nto have the same shape as `Y`. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nZ = np.zeros_like(Y)\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\n```\n\n```{.python .input}\n%%tab pytorch\nZ = torch.zeros_like(Y)\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\n```\n\n```{.python .input}\n%%tab tensorflow\nZ = tf.Variable(tf.zeros_like(Y))\nprint('id(Z):', id(Z))\nZ.assign(X + Y)\nprint('id(Z):', id(Z))\n```\n\n```{.python .input}\n%%tab jax\n# JAX arrays do not allow in-place operations\n```\n\n:begin_tab:`mxnet, pytorch`\n[**If the value of `X` is not reused in subsequent computations,\nwe can also use `X[:] = X + Y` or `X += Y`\nto reduce the memory overhead of the operation.**]\n:end_tab:\n\n:begin_tab:`tensorflow`\nEven once you store state persistently in a `Variable`, \nyou may want to reduce your memory usage further by avoiding excess\nallocations for tensors that are not your model parameters. Because TensorFlow `Tensors` are immutable \nand gradients do not flow through `Variable` assignments, \nTensorFlow does not provide an explicit way to run\nan individual operation in-place. However, TensorFlow provides the `tf.function` decorator \nto wrap computation inside of a TensorFlow graph \nthat gets compiled and optimized before running. This allows TensorFlow to prune unused values, \nand to reuse prior allocations that are no longer needed. This minimizes the memory overhead of TensorFlow computations. :end_tab:\n\n```{.python .input}\n%%tab mxnet, pytorch\nbefore = id(X)\nX += Y\nid(X) == before\n```\n\n```{.python .input}\n%%tab tensorflow\n@tf.function\ndef computation(X, Y):\n    Z = tf.zeros_like(Y)  # This unused value will be pruned out\n    A = X + Y  # Allocations will be reused when no longer needed\n    B = A + Y\n    C = B + Y\n    return C + Y\n\ncomputation(X, Y)\n```"
    },
    {
      "chunk_id": "63cf24383da5_0",
      "chapter": "ndarray",
      "heading": "Conversion to Other Python Objects",
      "text": ":begin_tab:`mxnet, tensorflow`\n[**Converting to a NumPy tensor (`ndarray`)**], or vice versa, is easy.\nThe converted result does not share memory.\nThis minor inconvenience is actually quite important:\nwhen you perform operations on the CPU or on GPUs,\nyou do not want to halt computation, waiting to see\nwhether the NumPy package of Python \nmight want to be doing something else\nwith the same chunk of memory.\n:end_tab:\n\n:begin_tab:`pytorch`\n[**Converting to a NumPy tensor (`ndarray`)**], or vice versa, is easy.\nThe torch tensor and NumPy array \nwill share their underlying memory, \nand changing one through an in-place operation \nwill also change the other.\n:end_tab:\n\n```{.python .input}\n%%tab mxnet\nA = X.asnumpy()\nB = np.array(A)\ntype(A), type(B)\n```\n\n```{.python .input}\n%%tab pytorch\nA = X.numpy()\nB = torch.from_numpy(A)\ntype(A), type(B)\n```\n\n```{.python .input}\n%%tab tensorflow\nA = X.numpy()\nB = tf.constant(A)\ntype(A), type(B)\n```\n\n```{.python .input}\n%%tab jax\nA = jax.device_get(X)\nB = jax.device_put(A)\ntype(A), type(B)\n```\n\nTo (**convert a size-1 tensor to a Python scalar**),\nwe can invoke the `item` function or Python's built-in functions.\n\n```{.python .input}\n%%tab mxnet\na = np.array([3.5])\na, a.item(), float(a), int(a)\n```\n\n```{.python .input}\n%%tab pytorch\na = torch.tensor([3.5])\na, a.item(), float(a), int(a)\n```\n\n```{.python .input}\n%%tab tensorflow\na = tf.constant([3.5]).numpy()\na, a.item(), float(a), int(a)\n```\n\n```{.python .input}\n%%tab jax\na = jnp.array([3.5])\na, a.item(), float(a), int(a)\n```"
    },
    {
      "chunk_id": "0805bd690767_0",
      "chapter": "ndarray",
      "heading": "Summary",
      "text": "The tensor class is the main interface for storing and manipulating data in deep learning libraries.\nTensors provide a variety of functionalities including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects."
    },
    {
      "chunk_id": "77f1cb753ec2_0",
      "chapter": "ndarray",
      "heading": "Exercises",
      "text": "1. Run the code in this section. Change the conditional statement `X == Y` to `X < Y` or `X > Y`, and then see what kind of tensor you can get.\n1. Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/26)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/27)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/187)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17966)\n:end_tab:"
    },
    {
      "chunk_id": "1b9917ef156e_0",
      "chapter": "pandas",
      "heading": "pandas",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Data Preprocessing\n:label:`sec_pandas`\n\nSo far, we have been working with synthetic data\nthat arrived in ready-made tensors.\nHowever, to apply deep learning in the wild\nwe must extract messy data \nstored in arbitrary formats,\nand preprocess it to suit our needs.\nFortunately, the *pandas* [library](https://pandas.pydata.org/) \ncan do much of the heavy lifting.\nThis section, while no substitute \nfor a proper *pandas* [tutorial](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html),\nwill give you a crash course\non some of the most common routines."
    },
    {
      "chunk_id": "cc6779cdc93e_0",
      "chapter": "pandas",
      "heading": "Reading the Dataset",
      "text": "Comma-separated values (CSV) files are ubiquitous \nfor the storing of tabular (spreadsheet-like) data.\nIn them, each line corresponds to one record\nand consists of several (comma-separated) fields, e.g.,\n\"Albert Einstein,March 14 1879,Ulm,Federal polytechnic school,field of gravitational physics\".\nTo demonstrate how to load CSV files with `pandas`, \nwe (**create a CSV file below**) `../data/house_tiny.csv`. \nThis file represents a dataset of homes,\nwhere each row corresponds to a distinct home\nand the columns correspond to the number of rooms (`NumRooms`),\nthe roof type (`RoofType`), and the price (`Price`).\n\n```{.python .input}\n%%tab all\nimport os\n\nos.makedirs(os.path.join('..', 'data'), exist_ok=True)\ndata_file = os.path.join('..', 'data', 'house_tiny.csv')\nwith open(data_file, 'w') as f:\n    f.write('''NumRooms,RoofType,Price\nNA,NA,127500\n2,NA,106000\n4,Slate,178100\nNA,NA,140000''')\n```\n\nNow let's import `pandas` and load the dataset with `read_csv`.\n\n```{.python .input}\n%%tab all\nimport pandas as pd\n\ndata = pd.read_csv(data_file)\nprint(data)\n```"
    },
    {
      "chunk_id": "9ae237aa3bc4_0",
      "chapter": "pandas",
      "heading": "Data Preparation",
      "text": "In supervised learning, we train models\nto predict a designated *target* value,\ngiven some set of *input* values. \nOur first step in processing the dataset\nis to separate out columns corresponding\nto input versus target values. \nWe can select columns either by name or\nvia integer-location based indexing (`iloc`).\n\nYou might have noticed that `pandas` replaced\nall CSV entries with value `NA`\nwith a special `NaN` (*not a number*) value. \nThis can also happen whenever an entry is empty,\ne.g., \"3,,,270000\".\nThese are called *missing values* \nand they are the \"bed bugs\" of data science,\na persistent menace that you will confront\nthroughout your career. \nDepending upon the context, \nmissing values might be handled\neither via *imputation* or *deletion*.\nImputation replaces missing values \nwith estimates of their values\nwhile deletion simply discards \neither those rows or those columns\nthat contain missing values. \n\nHere are some common imputation heuristics.\n[**For categorical input fields, \nwe can treat `NaN` as a category.**]\nSince the `RoofType` column takes values `Slate` and `NaN`,\n`pandas` can convert this column \ninto two columns `RoofType_Slate` and `RoofType_nan`.\nA row whose roof type is `Slate` will set values \nof `RoofType_Slate` and `RoofType_nan` to 1 and 0, respectively.\nThe converse holds for a row with a missing `RoofType` value.\n\n```{.python .input}\n%%tab all\ninputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\ninputs = pd.get_dummies(inputs, dummy_na=True)\nprint(inputs)\n```\n\nFor missing numerical values, \none common heuristic is to \n[**replace the `NaN` entries with \nthe mean value of the corresponding column**].\n\n```{.python .input}\n%%tab all\ninputs = inputs.fillna(inputs.mean())\nprint(inputs)\n```"
    },
    {
      "chunk_id": "3ee4dada01d0_0",
      "chapter": "pandas",
      "heading": "Conversion to the Tensor Format",
      "text": "Now that [**all the entries in `inputs` and `targets` are numerical,\nwe can load them into a tensor**] (recall :numref:`sec_ndarray`).\n\n```{.python .input}\n%%tab mxnet\nfrom mxnet import np\n\nX, y = np.array(inputs.to_numpy(dtype=float)), np.array(targets.to_numpy(dtype=float))\nX, y\n```\n\n```{.python .input}\n%%tab pytorch\nimport torch\n\nX = torch.tensor(inputs.to_numpy(dtype=float))\ny = torch.tensor(targets.to_numpy(dtype=float))\nX, y\n```\n\n```{.python .input}\n%%tab tensorflow\nimport tensorflow as tf\n\nX = tf.constant(inputs.to_numpy(dtype=float))\ny = tf.constant(targets.to_numpy(dtype=float))\nX, y\n```\n\n```{.python .input}\n%%tab jax\nfrom jax import numpy as jnp\n\nX = jnp.array(inputs.to_numpy(dtype=float))\ny = jnp.array(targets.to_numpy(dtype=float))\nX, y\n```"
    },
    {
      "chunk_id": "e79510af145b_0",
      "chapter": "pandas",
      "heading": "Discussion",
      "text": "You now know how to partition data columns, \nimpute missing variables, \nand load `pandas` data into tensors. \nIn :numref:`sec_kaggle_house`, you will\npick up some more data processing skills. \nWhile this crash course kept things simple,\ndata processing can get hairy.\nFor example, rather than arriving in a single CSV file,\nour dataset might be spread across multiple files\nextracted from a relational database.\nFor instance, in an e-commerce application,\ncustomer addresses might live in one table\nand purchase data in another.\nMoreover, practitioners face myriad data types\nbeyond categorical and numeric, for example,\ntext strings, images,\naudio data, and point clouds. \nOftentimes, advanced tools and efficient algorithms \nare required in order to prevent data processing from becoming\nthe biggest bottleneck in the machine learning pipeline. \nThese problems will arise when we get to \ncomputer vision and natural language processing. \nFinally, we must pay attention to data quality.\nReal-world datasets are often plagued \nby outliers, faulty measurements from sensors, and recording errors, \nwhich must be addressed before \nfeeding the data into any model. \nData visualization tools such as [seaborn](https://seaborn.pydata.org/), \n[Bokeh](https://docs.bokeh.org/), or [matplotlib](https://matplotlib.org/)\ncan help you to manually inspect the data \nand develop intuitions about \nthe type of problems you may need to address."
    },
    {
      "chunk_id": "e382fdf609c6_0",
      "chapter": "pandas",
      "heading": "Exercises",
      "text": "1. Try loading datasets, e.g., Abalone from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets) and inspect their properties. What fraction of them has missing values? What fraction of the variables is numerical, categorical, or text?\n1. Try indexing and selecting data columns by name rather than by column number. The pandas documentation on [indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) has further details on how to do this.\n1. How large a dataset do you think you could load this way? What might be the limitations? Hint: consider the time to read the data, representation, processing, and memory footprint. Try this out on your laptop. What happens if you try it out on a server? \n1. How would you deal with data that has a very large number of categories? What if the category labels are all unique? Should you include the latter?\n1. What alternatives to pandas can you think of? How about [loading NumPy tensors from a file](https://numpy.org/doc/stable/reference/generated/numpy.load.html)? Check out [Pillow](https://python-pillow.org/), the Python Imaging Library. \n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/28)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/29)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/195)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17967)\n:end_tab:"
    },
    {
      "chunk_id": "bcf9a528c8a7_0",
      "chapter": "probability",
      "heading": "probability",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n# Probability and Statistics\n:label:`sec_prob`\n\nOne way or another,\nmachine learning is all about uncertainty. In supervised learning, we want to predict\nsomething unknown (the *target*)\ngiven something known (the *features*). Depending on our objective,\nwe might attempt to predict\nthe most likely value of the target. Or we might predict the value with the smallest\nexpected distance from the target. And sometimes we wish not only\nto predict a specific value\nbut to *quantify our uncertainty*. For example, given some features\ndescribing a patient,\nwe might want to know *how likely* they are\nto suffer a heart attack in the next year. In unsupervised learning,\nwe often care about uncertainty. To determine whether a set of measurements are anomalous,\nit helps to know how likely one is\nto observe values in a population of interest. Furthermore, in reinforcement learning,\nwe wish to develop agents\nthat act intelligently in various environments. This requires reasoning about\nhow an environment might be expected to change\nand what rewards one might expect to encounter\nin response to each of the available actions. *Probability* is the mathematical field\nconcerned with reasoning under uncertainty. Given a probabilistic model of some process,\nwe can reason about the likelihood of various events. The use of probabilities to describe\nthe frequencies of repeatable events\n(like coin tosses)\nis fairly uncontroversial. In fact, *frequentist* scholars adhere\nto an interpretation of probability\nthat applies *only* to such repeatable events. By contrast *Bayesian* scholars\nuse the language of probability more broadly\nto formalize reasoning under uncertainty. Bayesian probability is characterized\nby two unique features:\n(i) assigning degrees of belief\nto non-repeatable events,\ne.g., what is the *probability*\nthat a dam will collapse?;\nand (ii) subjectivity."
    },
    {
      "chunk_id": "bcf9a528c8a7_1",
      "chapter": "probability",
      "heading": "probability",
      "text": "Bayesian probability is characterized\nby two unique features:\n(i) assigning degrees of belief\nto non-repeatable events,\ne.g., what is the *probability*\nthat a dam will collapse?;\nand (ii) subjectivity. While Bayesian\nprobability provides unambiguous rules\nfor how one should update their beliefs\nin light of new evidence,\nit allows for different individuals\nto start off with different *prior* beliefs. *Statistics* helps us to reason backwards,\nstarting off with collection and organization of data\nand backing out to what inferences\nwe might draw about the process\nthat generated the data. Whenever we analyze a dataset, hunting for patterns\nthat we hope might characterize a broader population,\nwe are employing statistical thinking. Many courses, majors, theses, careers, departments,\ncompanies, and institutions have been devoted\nto the study of probability and statistics. While this section only scratches the surface,\nwe will provide the foundation\nthat you need to begin building models. ```{.python .input}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.numpy.random import multinomial\nimport random\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport random\nimport torch\nfrom torch.distributions.multinomial import Multinomial\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport random\nimport tensorflow as tf\nfrom tensorflow_probability import distributions as tfd\n```\n\n```{.python .input}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport random\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\n```"
    },
    {
      "chunk_id": "a9c87a7c3fdb_0",
      "chapter": "probability",
      "heading": "A Simple Example: Tossing Coins",
      "text": "Imagine that we plan to toss a coin\nand want to quantify how likely\nwe are to see heads (vs. tails). If the coin is *fair*,\nthen both outcomes\n(heads and tails),\nare equally likely. Moreover if we plan to toss the coin $n$ times\nthen the fraction of heads\nthat we *expect* to see\nshould exactly match\nthe *expected* fraction of tails. One intuitive way to see this\nis by symmetry:\nfor every possible outcome\nwith $n_\\textrm{h}$ heads and $n_\\textrm{t} = (n - n_\\textrm{h})$ tails,\nthere is an equally likely outcome\nwith $n_\\textrm{t}$ heads and $n_\\textrm{h}$ tails. Note that this is only possible\nif on average we expect to see\n$1/2$ of tosses come up heads\nand $1/2$ come up tails. Of course, if you conduct this experiment\nmany times with $n=1000000$ tosses each,\nyou might never see a trial\nwhere $n_\\textrm{h} = n_\\textrm{t}$ exactly. Formally, the quantity $1/2$ is called a *probability*\nand here it captures the certainty with which\nany given toss will come up heads. Probabilities assign scores between $0$ and $1$\nto outcomes of interest, called *events*. Here the event of interest is $\\textrm{heads}$\nand we denote the corresponding probability $P(\\textrm{heads})$. A probability of $1$ indicates absolute certainty\n(imagine a trick coin where both sides were heads)\nand a probability of $0$ indicates impossibility\n(e.g., if both sides were tails). The frequencies $n_\\textrm{h}/n$ and $n_\\textrm{t}/n$ are not probabilities\nbut rather *statistics*. Probabilities are *theoretical* quantities\nthat underly the data generating process. Here, the probability $1/2$\nis a property of the coin itself. By contrast, statistics are *empirical* quantities\nthat are computed as functions of the observed data. Our interests in probabilistic and statistical quantities\nare inextricably intertwined. We often design special statistics called *estimators*\nthat, given a dataset, produce *estimates*\nof model parameters such as probabilities."
    },
    {
      "chunk_id": "a9c87a7c3fdb_1",
      "chapter": "probability",
      "heading": "A Simple Example: Tossing Coins",
      "text": "Our interests in probabilistic and statistical quantities\nare inextricably intertwined. We often design special statistics called *estimators*\nthat, given a dataset, produce *estimates*\nof model parameters such as probabilities. Moreover, when those estimators satisfy\na nice property called *consistency*,\nour estimates will converge\nto the corresponding probability. In turn, these inferred probabilities\ntell about the likely statistical properties\nof data from the same population\nthat we might encounter in the future. Suppose that we stumbled upon a real coin\nfor which we did not know\nthe true $P(\\textrm{heads})$. To investigate this quantity\nwith statistical methods,\nwe need to (i) collect some data;\nand (ii) design an estimator. Data acquisition here is easy;\nwe can toss the coin many times\nand record all the outcomes. Formally, drawing realizations\nfrom some underlying random process\nis called *sampling*. As you might have guessed,\none natural estimator\nis the ratio of\nthe number of observed *heads*\nto the total number of tosses. Now, suppose that the coin was in fact fair,\ni.e., $P(\\textrm{heads}) = 0.5$. To simulate tosses of a fair coin,\nwe can invoke any random number generator. There are some easy ways to draw samples\nof an event with probability $0.5$. For example Python's `random.random`\nyields numbers in the interval $[0,1]$\nwhere the probability of lying\nin any sub-interval $[a, b] \\subset [0,1]$\nis equal to $b-a$."
    },
    {
      "chunk_id": "a9c87a7c3fdb_2",
      "chapter": "probability",
      "heading": "A Simple Example: Tossing Coins",
      "text": "There are some easy ways to draw samples\nof an event with probability $0.5$. For example Python's `random.random`\nyields numbers in the interval $[0,1]$\nwhere the probability of lying\nin any sub-interval $[a, b] \\subset [0,1]$\nis equal to $b-a$. Thus we can get out `0` and `1` with probability `0.5` each\nby testing whether the returned float number is greater than `0.5`:\n\n```{.python .input}\n%%tab all\nnum_tosses = 100\nheads = sum([random.random() > 0.5 for _ in range(num_tosses)])\ntails = num_tosses - heads\nprint(\"heads, tails: \", [heads, tails])\n```\n\nMore generally, we can simulate multiple draws\nfrom any variable with a finite number\nof possible outcomes\n(like the toss of a coin or roll of a die)\nby calling the multinomial function,\nsetting the first argument\nto the number of draws\nand the second as a list of probabilities\nassociated with each of the possible outcomes. To simulate ten tosses of a fair coin,\nwe assign probability vector `[0.5, 0.5]`,\ninterpreting index 0 as heads\nand index 1 as tails. The function returns a vector\nwith length equal to the number\nof possible outcomes (here, 2),\nwhere the first component tells us\nthe number of occurrences of heads\nand the second component tells us\nthe number of occurrences of tails. ```{.python .input}\n%%tab mxnet\nfair_probs = [0.5, 0.5]\nmultinomial(100, fair_probs)\n```\n\n```{.python .input}\n%%tab pytorch\nfair_probs = torch.tensor([0.5, 0.5])\nMultinomial(100, fair_probs).sample()\n```\n\n```{.python .input}\n%%tab tensorflow\nfair_probs = tf.ones(2) / 2\ntfd.Multinomial(100, fair_probs).sample()\n```\n\n```{.python .input}\n%%tab jax\nfair_probs = [0.5, 0.5]\n# jax.random does not have multinomial distribution implemented\nnp.random.multinomial(100, fair_probs)\n```\n\nEach time you run this sampling process,\nyou will receive a new random value\nthat may differ from the previous outcome. Dividing by the number of tosses\ngives us the *frequency*\nof each outcome in our data. Note that these frequencies,\njust like the probabilities\nthat they are intended\nto estimate, sum to $1$."
    },
    {
      "chunk_id": "a9c87a7c3fdb_3",
      "chapter": "probability",
      "heading": "A Simple Example: Tossing Coins",
      "text": "Dividing by the number of tosses\ngives us the *frequency*\nof each outcome in our data. Note that these frequencies,\njust like the probabilities\nthat they are intended\nto estimate, sum to $1$. ```{.python .input}\n%%tab mxnet\nmultinomial(100, fair_probs) / 100\n```\n\n```{.python .input}\n%%tab pytorch\nMultinomial(100, fair_probs).sample() / 100\n```\n\n```{.python .input}\n%%tab tensorflow\ntfd.Multinomial(100, fair_probs).sample() / 100\n```\n\n```{.python .input}\n%%tab jax\nnp.random.multinomial(100, fair_probs) / 100\n```\n\nHere, even though our simulated coin is fair\n(we ourselves set the probabilities `[0.5, 0.5]`),\nthe counts of heads and tails may not be identical. That is because we only drew a relatively small number of samples. If we did not implement the simulation ourselves,\nand only saw the outcome,\nhow would we know if the coin were slightly unfair\nor if the possible deviation from $1/2$ was\njust an artifact of the small sample size? Let's see what happens when we simulate 10,000 tosses. ```{.python .input}\n%%tab mxnet\ncounts = multinomial(10000, fair_probs).astype(np.float32)\ncounts / 10000\n```\n\n```{.python .input}\n%%tab pytorch\ncounts = Multinomial(10000, fair_probs).sample()\ncounts / 10000\n```\n\n```{.python .input}\n%%tab tensorflow\ncounts = tfd.Multinomial(10000, fair_probs).sample()\ncounts / 10000\n```\n\n```{.python .input}\n%%tab jax\ncounts = np.random.multinomial(10000, fair_probs).astype(np.float32)\ncounts / 10000\n```\n\nIn general, for averages of repeated events (like coin tosses),\nas the number of repetitions grows,\nour estimates are guaranteed to converge\nto the true underlying probabilities. The mathematical formulation of this phenomenon\nis called the *law of large numbers*\nand the *central limit theorem*\ntells us that in many situations,\nas the sample size $n$ grows,\nthese errors should go down\nat a rate of $(1/\\sqrt{n})$. Let's get some more intuition by studying\nhow our estimate evolves as we grow\nthe number of tosses from 1 to 10,000."
    },
    {
      "chunk_id": "a9c87a7c3fdb_4",
      "chapter": "probability",
      "heading": "A Simple Example: Tossing Coins",
      "text": "Let's get some more intuition by studying\nhow our estimate evolves as we grow\nthe number of tosses from 1 to 10,000. ```{.python .input}\n%%tab pytorch\ncounts = Multinomial(1, fair_probs).sample((10000,))\ncum_counts = counts.cumsum(dim=0)\nestimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\nestimates = estimates.numpy()\n\nd2l.set_figsize((4.5, 3.5))\nd2l.plt.plot(estimates[:, 0], label=(\"P(coin=heads)\"))\nd2l.plt.plot(estimates[:, 1], label=(\"P(coin=tails)\"))\nd2l.plt.axhline(y=0.5, color='black', linestyle='dashed')\nd2l.plt.gca().set_xlabel('Samples')\nd2l.plt.gca().set_ylabel('Estimated probability')\nd2l.plt.legend();\n```\n\n```{.python .input}\n%%tab mxnet\ncounts = multinomial(1, fair_probs, size=10000)\ncum_counts = counts.astype(np.float32).cumsum(axis=0)\nestimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)\n```\n\n```{.python .input}\n%%tab tensorflow\ncounts = tfd.Multinomial(1, fair_probs).sample(10000)\ncum_counts = tf.cumsum(counts, axis=0)\nestimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)\nestimates = estimates.numpy()\n```\n\n```{.python .input}\n%%tab jax\ncounts = np.random.multinomial(1, fair_probs, size=10000).astype(np.float32)\ncum_counts = counts.cumsum(axis=0)\nestimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow, jax\nd2l.set_figsize((4.5, 3.5))\nd2l.plt.plot(estimates[:, 0], label=(\"P(coin=heads)\"))\nd2l.plt.plot(estimates[:, 1], label=(\"P(coin=tails)\"))\nd2l.plt.axhline(y=0.5, color='black', linestyle='dashed')\nd2l.plt.gca().set_xlabel('Samples')\nd2l.plt.gca().set_ylabel('Estimated probability')\nd2l.plt.legend();\n```\n\nEach solid curve corresponds to one of the two values of the coin\nand gives our estimated probability that the coin turns up that value\nafter each group of experiments. The dashed black line gives the true underlying probability. As we get more data by conducting more experiments,\nthe curves converge towards the true probability."
    },
    {
      "chunk_id": "a9c87a7c3fdb_5",
      "chapter": "probability",
      "heading": "A Simple Example: Tossing Coins",
      "text": "The dashed black line gives the true underlying probability. As we get more data by conducting more experiments,\nthe curves converge towards the true probability. You might already begin to see the shape\nof some of the more advanced questions\nthat preoccupy statisticians:\nHow quickly does this convergence happen? If we had already tested many coins\nmanufactured at the same plant,\nhow might we incorporate this information?"
    },
    {
      "chunk_id": "86f17041d0bd_0",
      "chapter": "probability",
      "heading": "A More Formal Treatment",
      "text": "We have already gotten pretty far: posing\na probabilistic model,\ngenerating synthetic data,\nrunning a statistical estimator,\nempirically assessing convergence,\nand reporting error metrics (checking the deviation). However, to go much further,\nwe will need to be more precise. When dealing with randomness,\nwe denote the set of possible outcomes $\\mathcal{S}$\nand call it the *sample space* or *outcome space*. Here, each element is a distinct possible *outcome*. In the case of rolling a single coin,\n$\\mathcal{S} = \\{\\textrm{heads}, \\textrm{tails}\\}$. For a single die, $\\mathcal{S} = \\{1, 2, 3, 4, 5, 6\\}$. When flipping two coins, possible outcomes are\n$\\{(\\textrm{heads}, \\textrm{heads}), (\\textrm{heads}, \\textrm{tails}), (\\textrm{tails}, \\textrm{heads}),  (\\textrm{tails}, \\textrm{tails})\\}$. *Events* are subsets of the sample space. For instance, the event \"the first coin toss comes up heads\"\ncorresponds to the set $\\{(\\textrm{heads}, \\textrm{heads}), (\\textrm{heads}, \\textrm{tails})\\}$. Whenever the outcome $z$ of a random experiment satisfies\n$z \\in \\mathcal{A}$, then event $\\mathcal{A}$ has occurred. For a single roll of a die, we could define the events\n\"seeing a $5$\" ($\\mathcal{A} = \\{5\\}$)\nand \"seeing an odd number\"  ($\\mathcal{B} = \\{1, 3, 5\\}$). In this case, if the die came up $5$,\nwe would say that both $\\mathcal{A}$ and $\\mathcal{B}$ occurred. On the other hand, if $z = 3$,\nthen $\\mathcal{A}$ did not occur\nbut $\\mathcal{B}$ did. A *probability* function maps events\nonto real values ${P: \\mathcal{A} \\subseteq \\mathcal{S} \\rightarrow [0,1]}$."
    },
    {
      "chunk_id": "86f17041d0bd_1",
      "chapter": "probability",
      "heading": "A More Formal Treatment",
      "text": "On the other hand, if $z = 3$,\nthen $\\mathcal{A}$ did not occur\nbut $\\mathcal{B}$ did. A *probability* function maps events\nonto real values ${P: \\mathcal{A} \\subseteq \\mathcal{S} \\rightarrow [0,1]}$. The probability, denoted $P(\\mathcal{A})$, of an event $\\mathcal{A}$\nin the given sample space $\\mathcal{S}$,\nhas the following properties:\n\n* The probability of any event $\\mathcal{A}$ is a nonnegative real number, i.e., $P(\\mathcal{A}) \\geq 0$;\n* The probability of the entire sample space is $1$, i.e., $P(\\mathcal{S}) = 1$;\n* For any countable sequence of events $\\mathcal{A}_1, \\mathcal{A}_2, \\ldots$ that are *mutually exclusive* (i.e., $\\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset$ for all $i \\neq j$), the probability that any of them happens is equal to the sum of their individual probabilities, i.e., $P(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} P(\\mathcal{A}_i)$. These axioms of probability theory,\nproposed by :citet:`Kolmogorov.1933`,\ncan be applied to rapidly derive a number of important consequences. For instance, it follows immediately\nthat the probability of any event $\\mathcal{A}$\n*or* its complement $\\mathcal{A}'$ occurring is 1\n(because $\\mathcal{A} \\cup \\mathcal{A}' = \\mathcal{S}$). We can also prove that $P(\\emptyset) = 0$\nbecause $1 = P(\\mathcal{S} \\cup \\mathcal{S}') = P(\\mathcal{S} \\cup \\emptyset) = P(\\mathcal{S}) + P(\\emptyset) = 1 + P(\\emptyset)$. Consequently, the probability of any event $\\mathcal{A}$\n*and* its complement $\\mathcal{A}'$ occurring simultaneously\nis $P(\\mathcal{A} \\cap \\mathcal{A}') = 0$. Informally, this tells us that impossible events\nhave zero probability of occurring."
    },
    {
      "chunk_id": "63c956cba6bc_0",
      "chapter": "probability",
      "heading": "Random Variables",
      "text": "When we spoke about events like the roll of a die\ncoming up odds or the first coin toss coming up heads,\nwe were invoking the idea of a *random variable*. Formally, random variables are mappings\nfrom an underlying sample space\nto a set of (possibly many) values. You might wonder how a random variable\nis different from the sample space,\nsince both are collections of outcomes. Importantly, random variables can be much coarser\nthan the raw sample space. We can define a binary random variable like \"greater than 0.5\"\neven when the underlying sample space is infinite,\ne.g., points on the line segment between $0$ and $1$. Additionally, multiple random variables\ncan share the same underlying sample space. For example \"whether my home alarm goes off\"\nand \"whether my house was burgled\"\nare both binary random variables\nthat share an underlying sample space. Consequently, knowing the value taken by one random variable\ncan tell us something about the likely value of another random variable. Knowing that the alarm went off,\nwe might suspect that the house was likely burgled. Every value taken by a random variable corresponds\nto a subset of the underlying sample space. Thus the occurrence where the random variable $X$\ntakes value $v$, denoted by $X=v$, is an *event*\nand $P(X=v)$ denotes its probability. Sometimes this notation can get clunky,\nand we can abuse notation when the context is clear. For example, we might use $P(X)$ to refer broadly\nto the *distribution* of $X$, i.e.,\nthe function that tells us the probability\nthat $X$ takes any given value. Other times we write expressions\nlike $P(X,Y) = P(X) P(Y)$,\nas a shorthand to express a statement\nthat is true for all of the values\nthat the random variables $X$ and $Y$ can take, i.e.,\nfor all $i,j$ it holds that $P(X=i \\textrm{ and } Y=j) = P(X=i)P(Y=j)$. Other times, we abuse notation by writing\n$P(v)$ when the random variable is clear from the context."
    },
    {
      "chunk_id": "63c956cba6bc_1",
      "chapter": "probability",
      "heading": "Random Variables",
      "text": "Other times, we abuse notation by writing\n$P(v)$ when the random variable is clear from the context. Since an event in probability theory is a set of outcomes from the sample space,\nwe can specify a range of values for a random variable to take. For example, $P(1 \\leq X \\leq 3)$ denotes the probability of the event $\\{1 \\leq X \\leq 3\\}$. Note that there is a subtle difference\nbetween *discrete* random variables,\nlike flips of a coin or tosses of a die,\nand *continuous* ones,\nlike the weight and the height of a person\nsampled at random from the population. In this case we seldom really care about\nsomeone's exact height. Moreover, if we took precise enough measurements,\nwe would find that no two people on the planet\nhave the exact same height. In fact, with fine enough measurements,\nyou would never have the same height\nwhen you wake up and when you go to sleep. There is little point in asking about\nthe exact probability that someone\nis 1.801392782910287192 meters tall. Instead, we typically care more about being able to say\nwhether someone's height falls into a given interval,\nsay between 1.79 and 1.81 meters. In these cases we work with probability *densities*. The height of exactly 1.80 meters\nhas no probability, but nonzero density. To work out the probability assigned to an interval,\nwe must take an *integral* of the density\nover that interval."
    },
    {
      "chunk_id": "be231202fbe4_0",
      "chapter": "probability",
      "heading": "Multiple Random Variables",
      "text": "You might have noticed that we could not even\nmake it through the previous section without\nmaking statements involving interactions\namong multiple random variables\n(recall that $P(X,Y) = P(X) P(Y)$). Most of machine learning\nis concerned with such relationships. Here, the sample space would be\nthe population of interest,\nsay customers who transact with a business,\nphotographs on the Internet,\nor proteins known to biologists. Each random variable would represent\nthe (unknown) value of a different attribute. Whenever we sample an individual from the population,\nwe observe a realization of each of the random variables. Because the values taken by random variables\ncorrespond to subsets of the sample space\nthat could be overlapping, partially overlapping,\nor entirely disjoint,\nknowing the value taken by one random variable\ncan cause us to update our beliefs\nabout which values of another random variable are likely. If a patient walks into a hospital\nand we observe that they\nare having trouble breathing\nand have lost their sense of smell,\nthen we believe that they are more likely\nto have COVID-19 than we might\nif they had no trouble breathing\nand a perfectly ordinary sense of smell. When working with multiple random variables,\nwe can construct events corresponding\nto every combination of values\nthat the variables can jointly take. The probability function that assigns\nprobabilities to each of these combinations\n(e.g. $A=a$ and $B=b$)\nis called the *joint probability* function\nand simply returns the probability assigned\nto the intersection of the corresponding subsets\nof the sample space. The *joint probability* assigned to the event\nwhere random variables $A$ and $B$\ntake values $a$ and $b$, respectively,\nis denoted $P(A = a, B = b)$,\nwhere the comma indicates \"and\". Note that for any values $a$ and $b$,\nit follows that\n\n$$P(A=a, B=b) \\leq P(A=a) \\textrm{ and } P(A=a, B=b) \\leq P(B = b),$$\n\nsince for $A=a$ and $B=b$ to happen,\n$A=a$ has to happen *and* $B=b$ also has to happen."
    },
    {
      "chunk_id": "be231202fbe4_1",
      "chapter": "probability",
      "heading": "Multiple Random Variables",
      "text": "Note that for any values $a$ and $b$,\nit follows that\n\n$$P(A=a, B=b) \\leq P(A=a) \\textrm{ and } P(A=a, B=b) \\leq P(B = b),$$\n\nsince for $A=a$ and $B=b$ to happen,\n$A=a$ has to happen *and* $B=b$ also has to happen. Interestingly, the joint probability\ntells us all that we can know about these\nrandom variables in a probabilistic sense,\nand can be used to derive many other\nuseful quantities, including recovering the\nindividual distributions $P(A)$ and $P(B)$. To recover $P(A=a)$ we simply sum up\n$P(A=a, B=v)$ over all values $v$\nthat the random variable $B$ can take:\n$P(A=a) = \\sum_v P(A=a, B=v)$. The ratio $\\frac{P(A=a, B=b)}{P(A=a)} \\leq 1$\nturns out to be extremely important. It is called the *conditional probability*,\nand is denoted via the \"$\\mid$\" symbol:\n\n$$P(B=b \\mid A=a) = P(A=a,B=b)/P(A=a).$$\n\nIt tells us the new probability\nassociated with the event $B=b$,\nonce we condition on the fact $A=a$ took place. We can think of this conditional probability\nas restricting attention only to the subset\nof the sample space associated with $A=a$\nand then renormalizing so that\nall probabilities sum to 1. Conditional probabilities\nare in fact just ordinary probabilities\nand thus respect all of the axioms,\nas long as we condition all terms\non the same event and thus\nrestrict attention to the same sample space. For instance, for disjoint events\n$\\mathcal{B}$ and $\\mathcal{B}'$, we have that\n$P(\\mathcal{B} \\cup \\mathcal{B}' \\mid A = a) = P(\\mathcal{B} \\mid A = a) + P(\\mathcal{B}' \\mid A = a)$. Using the definition of conditional probabilities,\nwe can derive the famous result called *Bayes' theorem*. By construction, we have that $P(A, B) = P(B\\mid A) P(A)$\nand $P(A, B) = P(A\\mid B) P(B)$. Combining both equations yields\n$P(B\\mid A) P(A) = P(A\\mid B) P(B)$ and hence\n\n$$P(A \\mid B) = \\frac{P(B\\mid A) P(A)}{P(B)}.$$\n\n\n\n\n\n\nThis simple equation has profound implications because\nit allows us to reverse the order of conditioning. If we know how to estimate $P(B\\mid A)$, $P(A)$, and $P(B)$,\nthen we can estimate $P(A\\mid B)$."
    },
    {
      "chunk_id": "be231202fbe4_2",
      "chapter": "probability",
      "heading": "Multiple Random Variables",
      "text": "If we know how to estimate $P(B\\mid A)$, $P(A)$, and $P(B)$,\nthen we can estimate $P(A\\mid B)$. We often find it easier to estimate one term directly\nbut not the other and Bayes' theorem can come to the rescue here. For instance, if we know the prevalence of symptoms for a given disease,\nand the overall prevalences of the disease and symptoms, respectively,\nwe can determine how likely someone is\nto have the disease based on their symptoms. In some cases we might not have direct access to $P(B)$,\nsuch as the prevalence of symptoms. In this case a simplified version of Bayes' theorem comes in handy:\n\n$$P(A \\mid B) \\propto P(B \\mid A) P(A).$$\n\nSince we know that $P(A \\mid B)$ must be normalized to $1$, i.e., $\\sum_a P(A=a \\mid B) = 1$,\nwe can use it to compute\n\n$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{\\sum_a P(B \\mid A=a) P(A = a)}.$$\n\nIn Bayesian statistics, we think of an observer\nas possessing some (subjective) prior beliefs\nabout the plausibility of the available hypotheses\nencoded in the *prior* $P(H)$,\nand a *likelihood function* that says how likely\none is to observe any value of the collected evidence\nfor each of the hypotheses in the class $P(E \\mid H)$. Bayes' theorem is then interpreted as telling us\nhow to update the initial *prior* $P(H)$\nin light of the available evidence $E$\nto produce *posterior* beliefs\n$P(H \\mid E) = \\frac{P(E \\mid H) P(H)}{P(E)}$. Informally, this can be stated as\n\"posterior equals prior times likelihood, divided by the evidence\". Now, because the evidence $P(E)$ is the same for all hypotheses,\nwe can get away with simply normalizing over the hypotheses. Note that $\\sum_a P(A=a \\mid B) = 1$ also allows us to *marginalize* over random variables. That is, we can drop variables from a joint distribution such as $P(A, B)$. After all, we have that\n\n$$\\sum_a P(B \\mid A=a) P(A=a) = \\sum_a P(B, A=a) = P(B).$$\n\nIndependence is another fundamentally important concept\nthat forms the backbone of\nmany important ideas in statistics."
    },
    {
      "chunk_id": "be231202fbe4_3",
      "chapter": "probability",
      "heading": "Multiple Random Variables",
      "text": "After all, we have that\n\n$$\\sum_a P(B \\mid A=a) P(A=a) = \\sum_a P(B, A=a) = P(B).$$\n\nIndependence is another fundamentally important concept\nthat forms the backbone of\nmany important ideas in statistics. In short, two variables are *independent*\nif conditioning on the value of $A$ does not\ncause any change to the probability distribution\nassociated with $B$ and vice versa. More formally, independence, denoted $A \\perp B$,\nrequires that $P(A \\mid B) = P(A)$ and, consequently,\nthat $P(A,B) = P(A \\mid B) P(B) = P(A) P(B)$. Independence is often an appropriate assumption. For example, if the random variable $A$\nrepresents the outcome from tossing one fair coin\nand the random variable $B$\nrepresents the outcome from tossing another,\nthen knowing whether $A$ came up heads\nshould not influence the probability\nof $B$ coming up heads. Independence is especially useful when it holds among the successive\ndraws of our data from some underlying distribution\n(allowing us to make strong statistical conclusions)\nor when it holds among various variables in our data,\nallowing us to work with simpler models\nthat encode this independence structure. On the other hand, estimating the dependencies\namong random variables is often the very aim of learning. We care to estimate the probability of disease given symptoms\nspecifically because we believe\nthat diseases and symptoms are *not* independent. Note that because conditional probabilities are proper probabilities,\nthe concepts of independence and dependence also apply to them. Two random variables $A$ and $B$ are *conditionally independent*\ngiven a third variable $C$ if and only if $P(A, B \\mid C) = P(A \\mid C)P(B \\mid C)$. Interestingly, two variables can be independent in general\nbut become dependent when conditioning on a third. This often occurs when the two random variables $A$ and $B$\ncorrespond to causes of some third variable $C$."
    },
    {
      "chunk_id": "be231202fbe4_4",
      "chapter": "probability",
      "heading": "Multiple Random Variables",
      "text": "Interestingly, two variables can be independent in general\nbut become dependent when conditioning on a third. This often occurs when the two random variables $A$ and $B$\ncorrespond to causes of some third variable $C$. For example, broken bones and lung cancer might be independent\nin the general population but if we condition on being in the hospital\nthen we might find that broken bones are negatively correlated with lung cancer. That is because the broken bone *explains away* why some person is in the hospital\nand thus lowers the probability that they are hospitalized because of having lung cancer. And conversely, two dependent random variables\ncan become independent upon conditioning on a third. This often happens when two otherwise unrelated events\nhave a common cause. Shoe size and reading level are highly correlated\namong elementary school students,\nbut this correlation disappears if we condition on age."
    },
    {
      "chunk_id": "790d5ded5839_0",
      "chapter": "probability",
      "heading": "An Example",
      "text": ":label:`subsec_probability_hiv_app`\n\nLet's put our skills to the test. Assume that a doctor administers an HIV test to a patient. This test is fairly accurate and fails only with 1% probability\nif the patient is healthy but reported as diseased,\ni.e., healthy patients test positive in 1% of cases. Moreover, it never fails to detect HIV if the patient actually has it. We use $D_1 \\in \\{0, 1\\}$ to indicate the diagnosis\n($0$ if negative and $1$ if positive)\nand $H \\in \\{0, 1\\}$ to denote the HIV status. | Conditional probability | $H=1$ | $H=0$ |\n|:------------------------|------:|------:|\n| $P(D_1 = 1 \\mid H)$        |     1 |  0.01 |\n| $P(D_1 = 0 \\mid H)$        |     0 |  0.99 |\n\nNote that the column sums are all 1 (but the row sums do not),\nsince they are conditional probabilities. Let's compute the probability of the patient having HIV\nif the test comes back positive, i.e., $P(H = 1 \\mid D_1 = 1)$. Intuitively this is going to depend on how common the disease is,\nsince it affects the number of false alarms. Assume that the population is fairly free of the disease, e.g., $P(H=1) = 0.0015$. To apply Bayes' theorem, we need to apply marginalization\nto determine\n\n$$\\begin{aligned}\nP(D_1 = 1)\n=& P(D_1=1, H=0) + P(D_1=1, H=1)  \\\\\n=& P(D_1=1 \\mid H=0) P(H=0) + P(D_1=1 \\mid H=1) P(H=1) \\\\\n=& 0.011485. \\end{aligned}\n$$\n\nThis leads us to\n\n$$P(H = 1 \\mid D_1 = 1) = \\frac{P(D_1=1 \\mid H=1) P(H=1)}{P(D_1=1)} = 0.1306.$$\n\nIn other words, there is only a 13.06% chance\nthat the patient actually has HIV,\ndespite the test being pretty accurate. As we can see, probability can be counterintuitive. What should a patient do upon receiving such terrifying news? Likely, the patient would ask the physician\nto administer another test to get clarity. The second test has different characteristics\nand it is not as good as the first one."
    },
    {
      "chunk_id": "790d5ded5839_1",
      "chapter": "probability",
      "heading": "An Example",
      "text": "What should a patient do upon receiving such terrifying news? Likely, the patient would ask the physician\nto administer another test to get clarity. The second test has different characteristics\nand it is not as good as the first one. | Conditional probability | $H=1$ | $H=0$ |\n|:------------------------|------:|------:|\n| $P(D_2 = 1 \\mid H)$          |  0.98 |  0.03 |\n| $P(D_2 = 0 \\mid H)$          |  0.02 |  0.97 |\n\nUnfortunately, the second test comes back positive, too. Let's calculate the requisite probabilities to invoke Bayes' theorem\nby assuming conditional independence:\n\n$$\\begin{aligned}\nP(D_1 = 1, D_2 = 1 \\mid H = 0)\n& = P(D_1 = 1 \\mid H = 0) P(D_2 = 1 \\mid H = 0)\n=& 0.0003, \\\\\nP(D_1 = 1, D_2 = 1 \\mid H = 1)\n& = P(D_1 = 1 \\mid H = 1) P(D_2 = 1 \\mid H = 1)\n=& 0.98. \\end{aligned}\n$$\n\nNow we can apply marginalization to obtain the probability\nthat both tests come back positive:\n\n$$\\begin{aligned}\n&P(D_1 = 1, D_2 = 1)\\\\\n&= P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\\\\n&= P(D_1 = 1, D_2 = 1 \\mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \\mid H = 1)P(H=1)\\\\\n&= 0.00176955. \\end{aligned}\n$$\n\nFinally, the probability of the patient having HIV given that both tests are positive is\n\n$$P(H = 1 \\mid D_1 = 1, D_2 = 1)\n= \\frac{P(D_1 = 1, D_2 = 1 \\mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)}\n= 0.8307.$$\n\nThat is, the second test allowed us to gain much higher confidence that not all is well. Despite the second test being considerably less accurate than the first one,\nit still significantly improved our estimate. The assumption of both tests being conditionally independent of each other\nwas crucial for our ability to generate a more accurate estimate. Take the extreme case where we run the same test twice. In this situation we would expect the same outcome both times,\nhence no additional insight is gained from running the same test again."
    },
    {
      "chunk_id": "790d5ded5839_2",
      "chapter": "probability",
      "heading": "An Example",
      "text": "Take the extreme case where we run the same test twice. In this situation we would expect the same outcome both times,\nhence no additional insight is gained from running the same test again. The astute reader might have noticed that the diagnosis behaved\nlike a classifier hiding in plain sight\nwhere our ability to decide whether a patient is healthy\nincreases as we obtain more features (test outcomes)."
    },
    {
      "chunk_id": "07d4983d20ba_0",
      "chapter": "probability",
      "heading": "Expectations",
      "text": "Often, making decisions requires not just looking\nat the probabilities assigned to individual events\nbut composing them together into useful aggregates\nthat can provide us with guidance. For example, when random variables take continuous scalar values,\nwe often care about knowing what value to expect *on average*. This quantity is formally called an *expectation*. If we are making investments,\nthe first quantity of interest\nmight be the return we can expect,\naveraging over all the possible outcomes\n(and weighting by the appropriate probabilities). For instance, say that with 50% probability,\nan investment might fail altogether,\nwith 40% probability it might provide a 2$\\times$ return,\nand with 10% probability it might provide a 10$\\times$ return 10$\\times$. To calculate the expected return,\nwe sum over all returns, multiplying each\nby the probability that they will occur. This yields the expectation\n$0.5 \\cdot 0 + 0.4 \\cdot 2 + 0.1 \\cdot 10 = 1.8$. Hence the expected return is 1.8$\\times$. In general, the *expectation* (or average)\nof the random variable $X$ is defined as\n\n$$E[X] = E_{x \\sim P}[x] = \\sum_{x} x P(X = x).$$\n\nLikewise, for densities we obtain $E[X] = \\int x \\;dp(x)$. Sometimes we are interested in the expected value\nof some function of $x$. We can calculate these expectations as\n\n$$E_{x \\sim P}[f(x)] = \\sum_x f(x) P(x) \\textrm{ and } E_{x \\sim P}[f(x)] = \\int f(x) p(x) \\;dx$$\n\nfor discrete probabilities and densities, respectively. Returning to the investment example from above,\n$f$ might be the *utility* (happiness)\nassociated with the return. Behavior economists have long noted\nthat people associate greater disutility\nwith losing money than the utility gained\nfrom earning one dollar relative to their baseline. Moreover, the value of money tends to be sub-linear. Possessing 100k dollars versus zero dollars\ncan make the difference between paying the rent,\neating well, and enjoying quality healthcare\nversus suffering through homelessness."
    },
    {
      "chunk_id": "07d4983d20ba_1",
      "chapter": "probability",
      "heading": "Expectations",
      "text": "Moreover, the value of money tends to be sub-linear. Possessing 100k dollars versus zero dollars\ncan make the difference between paying the rent,\neating well, and enjoying quality healthcare\nversus suffering through homelessness. On the other hand, the gains due to possessing\n200k versus 100k are less dramatic. Reasoning like this motivates the clich\u00e9\nthat \"the utility of money is logarithmic\". If  the utility associated with a total loss were $-1$,\nand the utilities associated with returns of $1$, $2$, and $10$\nwere $1$, $2$ and $4$, respectively,\nthen the expected happiness of investing\nwould be $0.5 \\cdot (-1) + 0.4 \\cdot 2 + 0.1 \\cdot 4 = 0.7$\n(an expected loss of utility of 30%). If indeed this were your utility function,\nyou might be best off keeping the money in the bank. For financial decisions,\nwe might also want to measure\nhow *risky* an investment is. Here, we care not just about the expected value\nbut how much the actual values tend to *vary*\nrelative to this value. Note that we cannot just take\nthe expectation of the difference\nbetween the actual and expected values. This is because the expectation of a difference\nis the difference of the expectations,\ni.e., $E[X - E[X]] = E[X] - E[E[X]] = 0$. However, we can look at the expectation\nof any non-negative function of this difference. The *variance* of a random variable is calculated by looking\nat the expected value of the *squared* differences:\n\n$$\\textrm{Var}[X] = E\\left[(X - E[X])^2\\right] = E[X^2] - E[X]^2.$$\n\nHere the equality follows by expanding\n$(X - E[X])^2 = X^2 - 2 X E[X] + E[X]^2$\nand taking expectations for each term. The square root of the variance is another\nuseful quantity called the *standard deviation*. While this and the variance\nconvey the same information (either can be calculated from the other),\nthe standard deviation has the nice property\nthat it is expressed in the same units\nas the original quantity represented\nby the random variable."
    },
    {
      "chunk_id": "07d4983d20ba_2",
      "chapter": "probability",
      "heading": "Expectations",
      "text": "While this and the variance\nconvey the same information (either can be calculated from the other),\nthe standard deviation has the nice property\nthat it is expressed in the same units\nas the original quantity represented\nby the random variable. Lastly, the variance of a function\nof a random variable\nis defined analogously as\n\n$$\\textrm{Var}_{x \\sim P}[f(x)] = E_{x \\sim P}[f^2(x)] - E_{x \\sim P}[f(x)]^2.$$\n\nReturning to our investment example,\nwe can now compute the variance of the investment. It is given by $0.5 \\cdot 0 + 0.4 \\cdot 2^2 + 0.1 \\cdot 10^2 - 1.8^2 = 8.36$. For all intents and purposes this is a risky investment. Note that by mathematical convention mean and variance\nare often referenced as $\\mu$ and $\\sigma^2$. This is particularly the case whenever we use it\nto parametrize a Gaussian distribution. In the same way as we introduced expectations\nand variance for *scalar* random variables,\nwe can do so for vector-valued ones. Expectations are easy, since we can apply them elementwise. For instance, $\\boldsymbol{\\mu} \\stackrel{\\textrm{def}}{=} E_{\\mathbf{x} \\sim P}[\\mathbf{x}]$\nhas coordinates $\\mu_i = E_{\\mathbf{x} \\sim P}[x_i]$. *Covariances* are more complicated. We define them by taking expectations of the *outer product*\nof the difference between random variables and their mean:\n\n$$\\boldsymbol{\\Sigma} \\stackrel{\\textrm{def}}{=} \\textrm{Cov}_{\\mathbf{x} \\sim P}[\\mathbf{x}] = E_{\\mathbf{x} \\sim P}\\left[(\\mathbf{x} - \\boldsymbol{\\mu}) (\\mathbf{x} - \\boldsymbol{\\mu})^\\top\\right].$$\n\nThis matrix $\\boldsymbol{\\Sigma}$ is referred to as the covariance matrix. An easy way to see its effect is to consider some vector $\\mathbf{v}$\nof the same size as $\\mathbf{x}$."
    },
    {
      "chunk_id": "07d4983d20ba_3",
      "chapter": "probability",
      "heading": "Expectations",
      "text": "An easy way to see its effect is to consider some vector $\\mathbf{v}$\nof the same size as $\\mathbf{x}$. It follows that\n\n$$\\mathbf{v}^\\top \\boldsymbol{\\Sigma} \\mathbf{v} = E_{\\mathbf{x} \\sim P}\\left[\\mathbf{v}^\\top(\\mathbf{x} - \\boldsymbol{\\mu}) (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\mathbf{v}\\right] = \\textrm{Var}_{x \\sim P}[\\mathbf{v}^\\top \\mathbf{x}].$$\n\nAs such, $\\boldsymbol{\\Sigma}$ allows us to compute the variance\nfor any linear function of $\\mathbf{x}$\nby a simple matrix multiplication. The off-diagonal elements tell us how correlated the coordinates are:\na value of 0 means no correlation,\nwhere a larger positive value\nmeans that they are more strongly correlated."
    },
    {
      "chunk_id": "7cf8bc47b725_0",
      "chapter": "probability",
      "heading": "Discussion",
      "text": "In machine learning, there are many things to be uncertain about! We can be uncertain about the value of a label given an input. We can be uncertain about the estimated value of a parameter. We can even be uncertain about whether data arriving at deployment\nis even from the same distribution as the training data. By *aleatoric uncertainty*, we mean uncertainty\nthat is intrinsic to the problem,\nand due to genuine randomness\nunaccounted for by the observed variables. By *epistemic uncertainty*, we mean uncertainty\nover a model's parameters, the sort of uncertainty\nthat we can hope to reduce by collecting more data. We might have epistemic uncertainty\nconcerning the probability\nthat a coin turns up heads,\nbut even once we know this probability,\nwe are left with aleatoric uncertainty\nabout the outcome of any future toss. No matter how long we watch someone tossing a fair coin,\nwe will never be more or less than 50% certain\nthat the next toss will come up heads. These terms come from mechanical modeling,\n(see e.g., :citet:`Der-Kiureghian.Ditlevsen.2009` for a review on this aspect of [uncertainty quantification](https://en.wikipedia.org/wiki/Uncertainty_quantification)). It is worth noting, however, that these terms constitute a slight abuse of language. The term *epistemic* refers to anything concerning *knowledge*\nand thus, in the philosophical sense, all uncertainty is epistemic. We saw that sampling data from some unknown probability distribution\ncan provide us with information that can be used to estimate\nthe parameters of the data generating distribution. That said, the rate at which this is possible can be quite slow. In our coin tossing example (and many others)\nwe can do no better than to design estimators\nthat converge at a rate of $1/\\sqrt{n}$,\nwhere $n$ is the sample size (e.g., the number of tosses)."
    },
    {
      "chunk_id": "7cf8bc47b725_1",
      "chapter": "probability",
      "heading": "Discussion",
      "text": "That said, the rate at which this is possible can be quite slow. In our coin tossing example (and many others)\nwe can do no better than to design estimators\nthat converge at a rate of $1/\\sqrt{n}$,\nwhere $n$ is the sample size (e.g., the number of tosses). This means that by going from 10 to 1000 observations (usually a very achievable task)\nwe see a tenfold reduction of uncertainty,\nwhereas the next 1000 observations help comparatively little,\noffering only a 1.41 times reduction. This is a persistent feature of machine learning:\nwhile there are often easy gains, it takes a very large amount of data,\nand often with it an enormous amount of computation, to make further gains. For an empirical review of this fact for large scale language models see :citet:`Revels.Lubin.Papamarkou.2016`. We also sharpened our language and tools for statistical modeling. In the process of that we learned about conditional probabilities\nand about one of the most important equations in statistics---Bayes' theorem. It is an effective tool for decoupling information conveyed by data\nthrough a likelihood term $P(B \\mid A)$ that addresses\nhow well observations $B$ match a choice of parameters $A$,\nand a prior probability $P(A)$ which governs how plausible\na particular choice of $A$ was in the first place. In particular, we saw how this rule can be applied\nto assign probabilities to diagnoses,\nbased on the efficacy of the test *and*\nthe prevalence of the disease itself (i.e., our prior). Lastly, we introduced a first set of nontrivial questions\nabout the effect of a specific probability distribution,\nnamely expectations and variances. While there are many more than just linear and quadratic\nexpectations for a probability distribution,\nthese two already provide a good deal of knowledge\nabout the possible behavior of the distribution."
    },
    {
      "chunk_id": "7cf8bc47b725_2",
      "chapter": "probability",
      "heading": "Discussion",
      "text": "While there are many more than just linear and quadratic\nexpectations for a probability distribution,\nthese two already provide a good deal of knowledge\nabout the possible behavior of the distribution. For instance, [Chebyshev's inequality](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality)\nstates that $P(|X - \\mu| \\geq k \\sigma) \\leq 1/k^2$,\nwhere $\\mu$ is the expectation, $\\sigma^2$ is the variance of the distribution,\nand $k > 1$ is a confidence parameter of our choosing. It tells us that draws from a distribution lie\nwith at least 50% probability\nwithin a $[-\\sqrt{2} \\sigma, \\sqrt{2} \\sigma]$\ninterval centered on the expectation."
    },
    {
      "chunk_id": "e53fc0d58d24_0",
      "chapter": "probability",
      "heading": "Exercises",
      "text": "1. Give an example where observing more data can reduce the amount of uncertainty about the outcome to an arbitrarily low level. 1. Give an example where observing more data will only reduce the amount of uncertainty up to a point and then no further. Explain why this is the case and where you expect this point to occur. 1. We empirically demonstrated convergence to the mean for the toss of a coin. Calculate the variance of the estimate of the probability that we see a head after drawing $n$ samples. 1. How does the variance scale with the number of observations? 1. Use Chebyshev's inequality to bound the deviation from the expectation. 1. How does it relate to the central limit theorem? 1. Assume that we draw $m$ samples $x_i$ from a probability distribution with zero mean and unit variance. Compute the averages $z_m \\stackrel{\\textrm{def}}{=} m^{-1} \\sum_{i=1}^m x_i$. Can we apply Chebyshev's inequality for every $z_m$ independently? Why not? 1. Given two events with probability $P(\\mathcal{A})$ and $P(\\mathcal{B})$, compute upper and lower bounds on $P(\\mathcal{A} \\cup \\mathcal{B})$ and $P(\\mathcal{A} \\cap \\mathcal{B})$. Hint: graph the situation using a [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram). 1. Assume that we have a sequence of random variables, say $A$, $B$, and $C$, where $B$ only depends on $A$, and $C$ only depends on $B$, can you simplify the joint probability $P(A, B, C)$? Hint: this is a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain). 1. In :numref:`subsec_probability_hiv_app`, assume that the outcomes of the two tests are not independent. In particular assume that either test on its own has a false positive rate of 10% and a false negative rate of 1%. That is, assume that $P(D =1 \\mid H=0) = 0.1$ and that $P(D = 0 \\mid H=1) = 0.01$."
    },
    {
      "chunk_id": "e53fc0d58d24_1",
      "chapter": "probability",
      "heading": "Exercises",
      "text": "In particular assume that either test on its own has a false positive rate of 10% and a false negative rate of 1%. That is, assume that $P(D =1 \\mid H=0) = 0.1$ and that $P(D = 0 \\mid H=1) = 0.01$. Moreover, assume that for $H = 1$ (infected) the test outcomes are conditionally independent, i.e., that $P(D_1, D_2 \\mid H=1) = P(D_1 \\mid H=1) P(D_2 \\mid H=1)$ but that for healthy patients the outcomes are coupled via $P(D_1 = D_2 = 1 \\mid H=0) = 0.02$. 1. Work out the joint probability table for $D_1$ and $D_2$, given $H=0$ based on the information you have so far. 1. Derive the probability that the patient is diseased ($H=1$) after one test returns positive. You can assume the same baseline probability $P(H=1) = 0.0015$ as before. 1. Derive the probability that the patient is diseased ($H=1$) after both tests return positive. 1. Assume that you are an asset manager for an investment bank and you have a choice of stocks $s_i$ to invest in. Your portfolio needs to add up to $1$ with weights $\\alpha_i$ for each stock. The stocks have an average return $\\boldsymbol{\\mu} = E_{\\mathbf{s} \\sim P}[\\mathbf{s}]$ and covariance $\\boldsymbol{\\Sigma} = \\textrm{Cov}_{\\mathbf{s} \\sim P}[\\mathbf{s}]$. 1. Compute the expected return for a given portfolio $\\boldsymbol{\\alpha}$. 1. If you wanted to maximize the return of the portfolio, how should you choose your investment? 1. Compute the *variance* of the portfolio. 1. Formulate an optimization problem of maximizing the return while keeping the variance constrained to an upper bound. This is the Nobel-Prize winning [Markovitz portfolio](https://en.wikipedia.org/wiki/Markowitz_model) :cite:`Mangram.2013`. To solve it you will need a quadratic programming solver, something way beyond the scope of this book."
    },
    {
      "chunk_id": "e53fc0d58d24_2",
      "chapter": "probability",
      "heading": "Exercises",
      "text": "This is the Nobel-Prize winning [Markovitz portfolio](https://en.wikipedia.org/wiki/Markowitz_model) :cite:`Mangram.2013`. To solve it you will need a quadratic programming solver, something way beyond the scope of this book. :begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/36)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/37)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/198)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/17971)\n:end_tab:"
    },
    {
      "chunk_id": "df8e91244fc4_0",
      "chapter": "autorec",
      "heading": "autorec",
      "text": "# AutoRec: Rating Prediction with Autoencoders\n\nAlthough the matrix factorization model achieves decent performance on the rating prediction task, it is essentially a linear model. Thus, such models are not capable of capturing complex nonlinear and intricate relationships that may be predictive of users' preferences. In this section, we introduce a nonlinear neural network collaborative filtering model, AutoRec :cite:`Sedhain.Menon.Sanner.ea.2015`. It identifies collaborative filtering (CF) with an autoencoder architecture and aims to integrate nonlinear transformations into CF on the basis of explicit feedback. Neural networks have been proven to be capable of approximating any continuous function, making it suitable to address the limitation of matrix factorization and enrich the expressiveness of matrix factorization.\n\nOn the one hand, AutoRec has the same structure as an autoencoder which consists of an input layer, a hidden layer, and a reconstruction (output) layer.  An autoencoder is a neural network that learns to copy its input to its output in order to code the inputs into the hidden (and usually low-dimensional) representations. In AutoRec, instead of explicitly embedding users/items into low-dimensional space, it uses the column/row of the interaction matrix as input, then reconstructs the interaction matrix in the output layer.\n\nOn the other hand, AutoRec differs from a traditional autoencoder: rather than learning the hidden representations, AutoRec focuses on learning/reconstructing the output layer. It uses a partially observed interaction matrix as input, aiming to reconstruct a completed rating matrix. In the meantime, the missing entries of the input are filled in the output layer via reconstruction for the purpose of recommendation.\n\nThere are two variants of AutoRec: user-based and item-based. For brevity, here we only introduce the item-based AutoRec. User-based AutoRec can be derived accordingly."
    },
    {
      "chunk_id": "4799b0505223_0",
      "chapter": "autorec",
      "heading": "Model",
      "text": "Let $\\mathbf{R}_{*i}$ denote the $i^\\textrm{th}$ column of the rating matrix, where unknown ratings are set to zeros by default. The neural architecture is defined as:\n\n$$\nh(\\mathbf{R}_{*i}) = f(\\mathbf{W} \\cdot g(\\mathbf{V} \\mathbf{R}_{*i} + \\mu) + b)\n$$\n\nwhere $f(\\cdot)$ and $g(\\cdot)$ represent activation functions, $\\mathbf{W}$ and $\\mathbf{V}$ are weight matrices, $\\mu$ and $b$ are biases. Let $h( \\cdot )$ denote the whole network of AutoRec. The output $h(\\mathbf{R}_{*i})$ is the reconstruction of the $i^\\textrm{th}$ column of the rating matrix.\n\nThe following objective function aims to minimize the reconstruction error:\n\n$$\n\\underset{\\mathbf{W},\\mathbf{V},\\mu, b}{\\mathrm{argmin}} \\sum_{i=1}^M{\\parallel \\mathbf{R}_{*i} - h(\\mathbf{R}_{*i})\\parallel_{\\mathcal{O}}^2} +\\lambda(\\| \\mathbf{W} \\|_F^2 + \\| \\mathbf{V}\\|_F^2)\n$$\n\nwhere $\\| \\cdot \\|_{\\mathcal{O}}$ means only the contribution of observed ratings are considered, that is, only weights that are associated with observed inputs are updated during back-propagation.\n\n```{.python .input  n=3}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, np, npx\nfrom mxnet.gluon import nn\nimport mxnet as mx\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "3ad8918d1a37_0",
      "chapter": "autorec",
      "heading": "Implementing the Model",
      "text": "A typical autoencoder consists of an encoder and a decoder. The encoder projects the input to hidden representations and the decoder maps the hidden layer to the reconstruction layer. We follow this practice and create the encoder and decoder with fully connected layers. The activation of encoder is set to `sigmoid` by default and no activation is applied for decoder. Dropout is included after the encoding transformation to reduce over-fitting. The gradients of unobserved inputs are masked out to ensure that only observed ratings contribute to the model learning process.\n\n```{.python .input  n=2}\n#@tab mxnet\nclass AutoRec(nn.Block):\n    def __init__(self, num_hidden, num_users, dropout=0.05):\n        super(AutoRec, self).__init__()\n        self.encoder = nn.Dense(num_hidden, activation='sigmoid',\n                                use_bias=True)\n        self.decoder = nn.Dense(num_users, use_bias=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input):\n        hidden = self.dropout(self.encoder(input))\n        pred = self.decoder(hidden)\n        if autograd.is_training():  # Mask the gradient during training\n            return pred * np.sign(input)\n        else:\n            return pred\n```"
    },
    {
      "chunk_id": "766240d4f1af_0",
      "chapter": "autorec",
      "heading": "Reimplementing the Evaluator",
      "text": "Since the input and output have been changed, we need to reimplement the evaluation function, while we still use RMSE as the accuracy measure.\n\n```{.python .input  n=3}\n#@tab mxnet\ndef evaluator(network, inter_matrix, test_data, devices):\n    scores = []\n    for values in inter_matrix:\n        feat = gluon.utils.split_and_load(values, devices, even_split=False)\n        scores.extend([network(i).asnumpy() for i in feat])\n    recons = np.array([item for sublist in scores for item in sublist])\n    # Calculate the test RMSE\n    rmse = np.sqrt(np.sum(np.square(test_data - np.sign(test_data) * recons))\n                   / np.sum(np.sign(test_data)))\n    return float(rmse)\n```"
    },
    {
      "chunk_id": "9f587bb419a6_0",
      "chapter": "autorec",
      "heading": "Training and Evaluating the Model",
      "text": "Now, let's train and evaluate AutoRec on the MovieLens dataset. We can clearly see that the test RMSE is lower than the matrix factorization model, confirming the effectiveness of neural networks in the rating prediction task.\n\n```{.python .input  n=4}\n#@tab mxnet\ndevices = d2l.try_all_gpus()\n# Load the MovieLens 100K dataset\ndf, num_users, num_items = d2l.read_data_ml100k()\ntrain_data, test_data = d2l.split_data_ml100k(df, num_users, num_items)\n_, _, _, train_inter_mat = d2l.load_data_ml100k(train_data, num_users,\n                                                num_items)\n_, _, _, test_inter_mat = d2l.load_data_ml100k(test_data, num_users,\n                                               num_items)\ntrain_iter = gluon.data.DataLoader(train_inter_mat, shuffle=True,\n                                   last_batch=\"rollover\", batch_size=256,\n                                   num_workers=d2l.get_dataloader_workers())\ntest_iter = gluon.data.DataLoader(np.array(train_inter_mat), shuffle=False,\n                                  last_batch=\"keep\", batch_size=1024,\n                                  num_workers=d2l.get_dataloader_workers())\n# Model initialization, training, and evaluation\nnet = AutoRec(500, num_users)\nnet.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))\nlr, num_epochs, wd, optimizer = 0.002, 25, 1e-5, 'adam'\nloss = gluon.loss.L2Loss()\ntrainer = gluon.Trainer(net.collect_params(), optimizer,\n                        {\"learning_rate\": lr, 'wd': wd})\nd2l.train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,\n                        devices, evaluator, inter_mat=test_inter_mat)\n```"
    },
    {
      "chunk_id": "6c54d718b29b_0",
      "chapter": "autorec",
      "heading": "Summary",
      "text": "* We can frame the matrix factorization algorithm with autoencoders, while integrating non-linear layers and dropout regularization.\n* Experiments on the MovieLens 100K dataset show that AutoRec achieves superior performance than matrix factorization."
    },
    {
      "chunk_id": "7047c24467a0_0",
      "chapter": "autorec",
      "heading": "Exercises",
      "text": "* Vary the hidden dimension of AutoRec to see its impact on the model performance.\n* Try to add more hidden layers. Is it helpful to improve the model performance?\n* Can you find a better combination of decoder and encoder activation functions?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/401)\n:end_tab:"
    },
    {
      "chunk_id": "5d7eaf8f79e6_0",
      "chapter": "ctr",
      "heading": "ctr",
      "text": "# Feature-Rich Recommender Systems\n\nInteraction data is the most basic indication of users' preferences and interests. It plays a critical role in former introduced models. Yet, interaction data is usually extremely sparse and can be noisy at times. To address this issue, we can integrate side information such as features of items, profiles of users, and even in which context that the interaction occurred into the recommendation model. Utilizing these features are helpful in making recommendations in that these features can be an effective predictor of users interests especially when interaction data is lacking. As such, it is essential for recommendation models also have the capability to deal with those features and give the model some content/context awareness. To demonstrate this type of recommendation models, we introduce another task on click-through rate (CTR) for online advertisement recommendations :cite:`McMahan.Holt.Sculley.ea.2013` and present an anonymous advertising dataset. Targeted advertisement services have attracted widespread attention and are often framed as recommendation engines. Recommending advertisements that match users' personal taste and interest is important for click-through rate improvement. Digital marketers use online advertising to display advertisements to customers. Click-through rate is a metric that measures the number of clicks advertisers receive on their ads per number of impressions and it is expressed as a percentage calculated with the formula: \n\n$$ \\textrm{CTR} = \\frac{\\#\\textrm{Clicks}} {\\#\\textrm{Impressions}} \\times 100 \\% .$$\n\nClick-through rate is an important signal that indicates the effectiveness of prediction algorithms. Click-through rate prediction is a task of predicting the likelihood that something on a website will be clicked. Models on CTR prediction can not only be employed in targeted advertising systems but also in general item (e.g., movies, news, products) recommender systems, email campaigns, and even search engines."
    },
    {
      "chunk_id": "5d7eaf8f79e6_1",
      "chapter": "ctr",
      "heading": "ctr",
      "text": "Models on CTR prediction can not only be employed in targeted advertising systems but also in general item (e.g., movies, news, products) recommender systems, email campaigns, and even search engines. It is also closely related to user satisfaction, conversion rate, and can be helpful in setting campaign goals as it can help advertisers to set realistic expectations. ```{.python .input}\n#@tab mxnet\nfrom collections import defaultdict\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, np\nimport os\n```"
    },
    {
      "chunk_id": "42a16a51e60f_0",
      "chapter": "ctr",
      "heading": "An Online Advertising Dataset",
      "text": "With the considerable advancements of Internet and mobile technology, online advertising has become an important income resource and generates vast majority of revenue in the Internet industry. It is important to display relevant advertisements or advertisements that pique users' interests so that casual visitors can be converted into paying customers. The dataset we introduced is an online advertising dataset. It consists of 34 fields, with the first column representing the target variable that indicates if an ad was clicked (1) or not (0). All the other columns are categorical features. The columns might represent the advertisement id, site or application id, device id, time, user profiles and so on. The real semantics of the features are undisclosed due to anonymization and privacy concern.\n\nThe following code downloads the dataset from our server and saves it into the local data folder.\n\n```{.python .input  n=15}\n#@tab mxnet\n#@save\nd2l.DATA_HUB['ctr'] = (d2l.DATA_URL + 'ctr.zip',\n                       'e18327c48c8e8e5c23da714dd614e390d369843f')\n\ndata_dir = d2l.download_extract('ctr')\n```\n\nThere are a training set and a test set, consisting of 15000 and 3000 samples/lines, respectively."
    },
    {
      "chunk_id": "3e4fdd6db0b0_0",
      "chapter": "ctr",
      "heading": "Dataset Wrapper",
      "text": "For the convenience of data loading, we implement a `CTRDataset` which loads the advertising dataset from the CSV file and can be used by `DataLoader`."
    },
    {
      "chunk_id": "3e4fdd6db0b0_1",
      "chapter": "ctr",
      "heading": "Dataset Wrapper",
      "text": "For the convenience of data loading, we implement a `CTRDataset` which loads the advertising dataset from the CSV file and can be used by `DataLoader`. ```{.python .input  n=13}\n#@tab mxnet\n#@save\nclass CTRDataset(gluon.data.Dataset):\n    def __init__(self, data_path, feat_mapper=None, defaults=None,\n                 min_threshold=4, num_feat=34):\n        self.NUM_FEATS, self.count, self.data = num_feat, 0, {}\n        feat_cnts = defaultdict(lambda: defaultdict(int))\n        self.feat_mapper, self.defaults = feat_mapper, defaults\n        self.field_dims = np.zeros(self.NUM_FEATS, dtype=np.int64)\n        with open(data_path) as f:\n            for line in f:\n                instance = {}\n                values = line.rstrip('\\n').split('\\t')\n                if len(values) != self.NUM_FEATS + 1:\n                    continue\n                label = np.float32([0, 0])\n                label[int(values[0])] = 1\n                instance['y'] = [np.float32(values[0])]\n                for i in range(1, self.NUM_FEATS + 1):\n                    feat_cnts[i][values[i]] += 1\n                    instance.setdefault('x', []).append(values[i])\n                self.data[self.count] = instance\n                self.count = self.count + 1\n        if self.feat_mapper is None and self.defaults is None:\n            feat_mapper = {i: {feat for feat, c in cnt.items() if c >=\n                               min_threshold} for i, cnt in feat_cnts.items()}\n            self.feat_mapper = {i: {feat_v: idx for idx, feat_v in enumerate(feat_values)}\n                                for i, feat_values in feat_mapper.items()}\n            self.defaults = {i: len(feat_values) for i, feat_values in feat_mapper.items()}\n        for i, fm in self.feat_mapper.items():\n            self.field_dims[i - 1] = len(fm) + 1\n        self.offsets = np.array((0, *np.cumsum(self.field_dims).asnumpy()\n                                 [:-1]))\n        \n    def __len__(self):\n        return self.count\n    \n    def __getitem__(self, idx):\n        feat = np.array([self.feat_mapper[i + 1].get(v, self.defaults[i + 1])\n                         for i, v in enumerate(self.data[idx]['x'])])\n        return feat + self.offsets, self.data[idx]['y']\n```\n\nThe following example loads the training data and print out the first record."
    },
    {
      "chunk_id": "3e4fdd6db0b0_2",
      "chapter": "ctr",
      "heading": "Dataset Wrapper",
      "text": "```{.python .input  n=16}\n#@tab mxnet\ntrain_data = CTRDataset(os.path.join(data_dir, 'train.csv'))\ntrain_data[0]\n```\n\nAs can be seen, all the 34 fields are categorical features. Each value represents the one-hot index of the corresponding entry. The label $0$ means that it is not clicked. This `CTRDataset` can also be used to load other datasets such as the Criteo display advertising challenge [dataset](https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/) and the Avazu click-through rate prediction [dataset](https://www.kaggle.com/c/avazu-ctr-prediction)."
    },
    {
      "chunk_id": "a542b364bac8_0",
      "chapter": "ctr",
      "heading": "Summary",
      "text": "* Click-through rate is an important metric that is used to measure the effectiveness of advertising systems and recommender systems.\n* Click-through rate prediction is usually converted to a binary classification problem. The target is to predict whether an ad/item will be clicked or not based on given features."
    },
    {
      "chunk_id": "a30cd9baea63_0",
      "chapter": "ctr",
      "heading": "Exercises",
      "text": "* Can you load the Criteo and Avazu dataset with the provided `CTRDataset`. It is worth noting that the Criteo dataset consisting of real-valued features so you may have to revise the code a bit.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/405)\n:end_tab:"
    },
    {
      "chunk_id": "0070c811a134_0",
      "chapter": "deepfm",
      "heading": "deepfm",
      "text": "# Deep Factorization Machines\n\nLearning effective feature combinations is critical to the success of click-through rate prediction task. Factorization machines model feature interactions in a linear paradigm (e.g., bilinear interactions). This is often insufficient for real-world data where inherent feature crossing structures are usually very complex and nonlinear. What's worse, second-order feature interactions are generally used in factorization machines in practice. Modeling higher degrees of feature combinations with factorization machines is possible theoretically but it is usually not adopted due to numerical instability and high computational complexity.\n\nOne effective solution is using deep neural networks. Deep neural networks are powerful in feature representation learning and have the potential to learn sophisticated feature interactions. As such, it is natural to integrate deep neural networks to factorization machines. Adding nonlinear transformation layers to factorization machines gives it the capability to model both low-order feature combinations and high-order feature combinations. Moreover, non-linear inherent structures from inputs can also be captured with deep neural networks. In this section, we will introduce a representative model named deep factorization machines (DeepFM) :cite:`Guo.Tang.Ye.ea.2017` which combine FM and deep neural networks."
    },
    {
      "chunk_id": "bba3b5c182cb_0",
      "chapter": "deepfm",
      "heading": "Model Architectures",
      "text": "DeepFM consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is an MLP that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction. It is worth pointing out that the spirit of DeepFM resembles that of the Wide \\& Deep architecture which can capture both memorization and generalization. The advantages of DeepFM over the Wide \\& Deep model is that it reduces the effort of hand-crafted feature engineering by identifying feature combinations automatically. We omit the description of the FM component for brevity and denote the output as $\\hat{y}^{(FM)}$. Readers are referred to the last section for more details. Let $\\mathbf{e}_i \\in \\mathbb{R}^{k}$ denote the latent feature vector of the $i^\\textrm{th}$ field. The input of the deep component is the concatenation of the dense embeddings of all fields that are looked up with the sparse categorical feature input, denoted as:\n\n$$\n\\mathbf{z}^{(0)}  = [\\mathbf{e}_1, \\mathbf{e}_2, ..., \\mathbf{e}_f],\n$$\n\nwhere $f$ is the number of fields. It is then fed into the following neural network:\n\n$$\n\\mathbf{z}^{(l)}  = \\alpha(\\mathbf{W}^{(l)}\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l)}),\n$$\n\nwhere $\\alpha$ is the activation function. $\\mathbf{W}_{l}$ and $\\mathbf{b}_{l}$ are the weight and bias at the $l^\\textrm{th}$ layer. Let $y_{DNN}$ denote the output of the prediction. The ultimate prediction of DeepFM is the summation of the outputs from both FM and DNN. So we have:\n\n$$\n\\hat{y} = \\sigma(\\hat{y}^{(FM)} + \\hat{y}^{(DNN)}),\n$$\n\nwhere $\\sigma$ is the sigmoid function. The architecture of DeepFM is illustrated below. ![Illustration of the DeepFM model](../img/rec-deepfm.svg)\n\nIt is worth noting that DeepFM is not the only way to combine deep neural networks with FM."
    },
    {
      "chunk_id": "bba3b5c182cb_1",
      "chapter": "deepfm",
      "heading": "Model Architectures",
      "text": "The architecture of DeepFM is illustrated below. ![Illustration of the DeepFM model](../img/rec-deepfm.svg)\n\nIt is worth noting that DeepFM is not the only way to combine deep neural networks with FM. We can also add nonlinear layers over the feature interactions :cite:`He.Chua.2017`. ```{.python .input  n=2}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import init, gluon, np, npx\nfrom mxnet.gluon import nn\nimport os\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "0cf7b383e2ac_0",
      "chapter": "deepfm",
      "heading": "Implementation of DeepFM",
      "text": "The implementation of DeepFM is similar to that of FM. We keep the FM part unchanged and use an MLP block with `relu` as the activation function. Dropout is also used to regularize the model. The number of neurons of the MLP can be adjusted with the `mlp_dims` hyperparameter.\n\n```{.python .input  n=2}\n#@tab mxnet\nclass DeepFM(nn.Block):\n    def __init__(self, field_dims, num_factors, mlp_dims, drop_rate=0.1):\n        super(DeepFM, self).__init__()\n        num_inputs = int(sum(field_dims))\n        self.embedding = nn.Embedding(num_inputs, num_factors)\n        self.fc = nn.Embedding(num_inputs, 1)\n        self.linear_layer = nn.Dense(1, use_bias=True)\n        input_dim = self.embed_output_dim = len(field_dims) * num_factors\n        self.mlp = nn.Sequential()\n        for dim in mlp_dims:\n            self.mlp.add(nn.Dense(dim, 'relu', True, in_units=input_dim))\n            self.mlp.add(nn.Dropout(rate=drop_rate))\n            input_dim = dim\n        self.mlp.add(nn.Dense(in_units=input_dim, units=1))\n\n    def forward(self, x):\n        embed_x = self.embedding(x)\n        square_of_sum = np.sum(embed_x, axis=1) ** 2\n        sum_of_square = np.sum(embed_x ** 2, axis=1)\n        inputs = np.reshape(embed_x, (-1, self.embed_output_dim))\n        x = self.linear_layer(self.fc(x).sum(1)) \\\n            + 0.5 * (square_of_sum - sum_of_square).sum(1, keepdims=True) \\\n            + self.mlp(inputs)\n        x = npx.sigmoid(x)\n        return x\n```"
    },
    {
      "chunk_id": "21e21c5420ff_0",
      "chapter": "deepfm",
      "heading": "Training and Evaluating the Model",
      "text": "The data loading process is the same as that of FM. We set the MLP component of DeepFM to a three-layered dense network with the a pyramid structure (30-20-10). All other hyperparameters remain the same as FM.\n\n```{.python .input  n=4}\n#@tab mxnet\nbatch_size = 2048\ndata_dir = d2l.download_extract('ctr')\ntrain_data = d2l.CTRDataset(os.path.join(data_dir, 'train.csv'))\ntest_data = d2l.CTRDataset(os.path.join(data_dir, 'test.csv'),\n                           feat_mapper=train_data.feat_mapper,\n                           defaults=train_data.defaults)\nfield_dims = train_data.field_dims\ntrain_iter = gluon.data.DataLoader(\n    train_data, shuffle=True, last_batch='rollover', batch_size=batch_size,\n    num_workers=d2l.get_dataloader_workers())\ntest_iter = gluon.data.DataLoader(\n    test_data, shuffle=False, last_batch='rollover', batch_size=batch_size,\n    num_workers=d2l.get_dataloader_workers())\ndevices = d2l.try_all_gpus()\nnet = DeepFM(field_dims, num_factors=10, mlp_dims=[30, 20, 10])\nnet.initialize(init.Xavier(), ctx=devices)\nlr, num_epochs, optimizer = 0.01, 30, 'adam'\ntrainer = gluon.Trainer(net.collect_params(), optimizer,\n                        {'learning_rate': lr})\nloss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```\n\nCompared with FM, DeepFM converges faster and achieves better performance."
    },
    {
      "chunk_id": "a93a7e516597_0",
      "chapter": "deepfm",
      "heading": "Summary",
      "text": "* Integrating neural networks to FM enables it to model complex and high-order interactions.\n* DeepFM outperforms the original FM on the advertising dataset."
    },
    {
      "chunk_id": "d8f421bd318b_0",
      "chapter": "deepfm",
      "heading": "Exercises",
      "text": "* Vary the structure of the MLP to check its impact on model performance.\n* Change the dataset to Criteo and compare it with the original FM model.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/407)\n:end_tab:"
    },
    {
      "chunk_id": "4ae5b3e7c4fe_0",
      "chapter": "fm",
      "heading": "fm",
      "text": "# Factorization Machines\n\nFactorization machines (FM), proposed by :citet:`Rendle.2010`, is a supervised algorithm that can be used for classification, regression, and ranking tasks. It quickly took notice and became a popular and impactful method for making predictions and recommendations. Particularly, it is a generalization of the linear regression model and the matrix factorization model. Moreover, it is reminiscent of support vector machines with a polynomial kernel. The strengths of factorization machines over the linear regression and matrix factorization are: (1) it can model $\\chi$-way variable interactions, where $\\chi$ is the number of polynomial order and is usually set to two. (2) A fast optimization algorithm associated with factorization machines can reduce the polynomial computation time to linear complexity, making it extremely efficient especially for high dimensional sparse inputs.  For these reasons, factorization machines are widely employed in modern advertisement and products recommendations. The technical details and implementations are described below."
    },
    {
      "chunk_id": "37e6cf82a152_0",
      "chapter": "fm",
      "heading": "2-Way Factorization Machines",
      "text": "Formally, let $x \\in \\mathbb{R}^d$ denote the feature vectors of one sample, and $y$ denote the corresponding label which can be real-valued label or class label such as binary class \"click/non-click\". The model for a factorization machine of degree two is defined as:\n\n$$\n\\hat{y}(x) = \\mathbf{w}_0 + \\sum_{i=1}^d \\mathbf{w}_i x_i + \\sum_{i=1}^d\\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j\n$$\n\nwhere $\\mathbf{w}_0 \\in \\mathbb{R}$ is the global bias; $\\mathbf{w} \\in \\mathbb{R}^d$ denotes the weights of the i-th variable; $\\mathbf{V} \\in \\mathbb{R}^{d\\times k}$ represents the feature embeddings; $\\mathbf{v}_i$ represents the $i^\\textrm{th}$ row of $\\mathbf{V}$; $k$ is the dimensionality of latent factors; $\\langle\\cdot, \\cdot \\rangle$ is the dot product of two vectors.  $\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle$ model the interaction between the $i^\\textrm{th}$ and $j^\\textrm{th}$ feature. Some feature interactions can be easily understood so they can be designed by experts. However, most other feature interactions are hidden in data and difficult to identify. So modeling feature interactions automatically can greatly reduce the efforts in feature engineering. It is obvious that the first two terms correspond to the linear regression model and the last term is an extension of the matrix factorization model. If the feature $i$ represents an item and the feature $j$ represents a user, the third term is exactly the dot product between user and item embeddings. It is worth noting that FM can also generalize to higher orders (degree > 2). Nevertheless, the numerical stability might weaken the generalization."
    },
    {
      "chunk_id": "b3704f02726e_0",
      "chapter": "fm",
      "heading": "An Efficient Optimization Criterion",
      "text": "Optimizing the factorization machines in a  straight forward method leads to a complexity of $\\mathcal{O}(kd^2)$ as all pairwise interactions require to be computed. To solve this inefficiency problem, we can reorganize the third term of FM which could greatly reduce the computation cost, leading to a linear time complexity ($\\mathcal{O}(kd)$).  The reformulation of the pairwise interaction term is as follows:\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^d \\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j \\\\\n &= \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j - \\frac{1}{2}\\sum_{i=1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_i\\rangle x_i x_i \\\\\n &= \\frac{1}{2} \\big (\\sum_{i=1}^d \\sum_{j=1}^d \\sum_{l=1}^k\\mathbf{v}_{i, l} \\mathbf{v}_{j, l} x_i x_j - \\sum_{i=1}^d \\sum_{l=1}^k \\mathbf{v}_{i, l} \\mathbf{v}_{i, l} x_i x_i \\big)\\\\\n &=  \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i) (\\sum_{j=1}^d \\mathbf{v}_{j, l}x_j) - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2 \\big ) \\\\\n &= \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)\n \\end{aligned}\n$$\n\nWith this reformulation, the model complexity are decreased greatly. Moreover, for sparse features, only non-zero elements needs to be computed so that the overall complexity is linear to the number of non-zero features.\n\nTo learn the FM model, we can use the MSE loss for regression task, the cross-entropy loss for classification tasks, and the BPR loss for ranking task. Standard optimizers such as stochastic gradient descent and Adam are viable for optimization.\n\n```{.python .input  n=2}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import init, gluon, np, npx\nfrom mxnet.gluon import nn\nimport os\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "9cc5e1f57ddd_0",
      "chapter": "fm",
      "heading": "Model Implementation",
      "text": "The following code implement the factorization machines. It is clear to see that FM consists a linear regression block and an efficient feature interaction block. We apply a sigmoid function over the final score since we treat the CTR prediction as a classification task.\n\n```{.python .input  n=2}\n#@tab mxnet\nclass FM(nn.Block):\n    def __init__(self, field_dims, num_factors):\n        super(FM, self).__init__()\n        num_inputs = int(sum(field_dims))\n        self.embedding = nn.Embedding(num_inputs, num_factors)\n        self.fc = nn.Embedding(num_inputs, 1)\n        self.linear_layer = nn.Dense(1, use_bias=True)\n\n    def forward(self, x):\n        square_of_sum = np.sum(self.embedding(x), axis=1) ** 2\n        sum_of_square = np.sum(self.embedding(x) ** 2, axis=1)\n        x = self.linear_layer(self.fc(x).sum(1)) \\\n            + 0.5 * (square_of_sum - sum_of_square).sum(1, keepdims=True)\n        x = npx.sigmoid(x)\n        return x\n```"
    },
    {
      "chunk_id": "91f605220ffb_0",
      "chapter": "fm",
      "heading": "Load the Advertising Dataset",
      "text": "We use the CTR data wrapper from the last section to load the online advertising dataset.\n\n```{.python .input  n=3}\n#@tab mxnet\nbatch_size = 2048\ndata_dir = d2l.download_extract('ctr')\ntrain_data = d2l.CTRDataset(os.path.join(data_dir, 'train.csv'))\ntest_data = d2l.CTRDataset(os.path.join(data_dir, 'test.csv'),\n                           feat_mapper=train_data.feat_mapper,\n                           defaults=train_data.defaults)\ntrain_iter = gluon.data.DataLoader(\n    train_data, shuffle=True, last_batch='rollover', batch_size=batch_size,\n    num_workers=d2l.get_dataloader_workers())\ntest_iter = gluon.data.DataLoader(\n    test_data, shuffle=False, last_batch='rollover', batch_size=batch_size,\n    num_workers=d2l.get_dataloader_workers())\n```"
    },
    {
      "chunk_id": "e5578533aa07_0",
      "chapter": "fm",
      "heading": "Train the Model",
      "text": "Afterwards, we train the model. The learning rate is set to 0.02 and the embedding size is set to 20 by default. The `Adam` optimizer and the `SigmoidBinaryCrossEntropyLoss` loss are used for model training.\n\n```{.python .input  n=5}\n#@tab mxnet\ndevices = d2l.try_all_gpus()\nnet = FM(train_data.field_dims, num_factors=20)\nnet.initialize(init.Xavier(), ctx=devices)\nlr, num_epochs, optimizer = 0.02, 30, 'adam'\ntrainer = gluon.Trainer(net.collect_params(), optimizer,\n                        {'learning_rate': lr})\nloss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n```"
    },
    {
      "chunk_id": "ed0eaba8f3ec_0",
      "chapter": "fm",
      "heading": "Summary",
      "text": "* FM is a general framework that can be applied on a variety of tasks such as regression, classification, and ranking.\n* Feature interaction/crossing is important for prediction tasks and the 2-way interaction can be efficiently modeled with FM."
    },
    {
      "chunk_id": "1ff608e80d7f_0",
      "chapter": "fm",
      "heading": "Exercises",
      "text": "* Can you test FM on other dataset such as Avazu, MovieLens, and Criteo datasets?\n* Vary the embedding size to check its impact on performance, can you observe a similar pattern as that of matrix factorization?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/406)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Recommender Systems\n:label:`chap_recsys`\n\n\n**Shuai Zhang** (*Amazon*), **Aston Zhang** (*Amazon*), and **Yi Tay** (*Google*)\n\nRecommender systems are widely employed in industry and are ubiquitous in our daily lives. These systems are utilized in a number of areas such as online shopping sites (e.g., amazon.com), music/movie services site (e.g., Netflix and Spotify), mobile application stores (e.g., IOS app store and google play), online advertising, just to name a few. \n\nThe major goal of recommender systems is to help users discover relevant items such as movies to watch, text to read or products to buy, so as to create a delightful user experience. Moreover, recommender systems are among the most powerful machine learning systems that online retailers implement in order to drive incremental revenue. Recommender systems are replacements of search engines by reducing the efforts in proactive searches and surprising users with offers they never searched for. Many companies managed to position themselves ahead of their competitors with the help of more effective recommender systems. As such, recommender systems are central to not only our everyday lives but also highly indispensable in some industries.\n\n\nIn this chapter, we will cover the fundamentals and advancements of recommender systems, along with exploring some common fundamental techniques for building recommender systems with different data sources available and their implementations. Specifically, you will learn how to predict the rating a user might give to a prospective item, how to generate a recommendation list of items and how to predict the click-through rate from abundant features. These tasks are commonplace in real-world applications. By studying this chapter, you will get hands-on experience pertaining to solving real world recommendation problems with not only classical methods but the more advanced deep learning based models as well.\n\n```toc\n:maxdepth: 2\n\nrecsys-intro\nmovielens\nmf\nautorec\nranking\nneumf\nseqrec\nctr\nfm\ndeepfm\n```"
    },
    {
      "chunk_id": "d7942812c59e_0",
      "chapter": "mf",
      "heading": "mf",
      "text": "# Matrix Factorization\n\nMatrix Factorization :cite:`Koren.Bell.Volinsky.2009` is a well-established algorithm in the recommender systems literature. The first version of matrix factorization model is proposed by Simon Funk in a famous [blog\npost](https://sifter.org/%7Esimon/journal/20061211.html) in which he described the idea of factorizing the interaction matrix. It then became widely known due to the Netflix contest which was held in 2006. At that time, Netflix, a media-streaming and video-rental company, announced a contest to improve its recommender system performance. The best team that can improve on the Netflix baseline, i.e., Cinematch), by 10 percent would win a one million USD prize.  As such, this contest attracted\na lot of attention to the field of recommender system research. Subsequently, the grand prize was won by the BellKor's Pragmatic Chaos team, a combined team of BellKor, Pragmatic Theory, and BigChaos (you do not need to worry about these algorithms now). Although the final score was the result of an ensemble solution (i.e., a combination of many algorithms), the matrix factorization algorithm played a critical role in the final blend. The technical report of the Netflix Grand Prize solution :cite:`Toscher.Jahrer.Bell.2009` provides a detailed introduction to the adopted model. In this section, we will dive into the details of the matrix factorization model and its implementation."
    },
    {
      "chunk_id": "ddde1da699be_0",
      "chapter": "mf",
      "heading": "The Matrix Factorization Model",
      "text": "Matrix factorization is a class of collaborative filtering models. Specifically, the model factorizes the user-item interaction matrix (e.g., rating matrix) into the product of two lower-rank matrices, capturing the low-rank structure of the user-item interactions. Let $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$ denote the interaction matrix with $m$ users and $n$ items, and the values of $\\mathbf{R}$ represent explicit ratings. The user-item interaction will be factorized into a user latent matrix $\\mathbf{P} \\in \\mathbb{R}^{m \\times k}$ and an item latent matrix $\\mathbf{Q} \\in \\mathbb{R}^{n \\times k}$, where $k \\ll m, n$, is the latent factor size. Let $\\mathbf{p}_u$ denote the $u^\\textrm{th}$ row of $\\mathbf{P}$ and $\\mathbf{q}_i$ denote the $i^\\textrm{th}$ row of $\\mathbf{Q}$. For a given item $i$, the elements of $\\mathbf{q}_i$ measure the extent to which the item possesses those characteristics such as the genres and languages of a movie. For a given user $u$, the elements of $\\mathbf{p}_u$ measure the extent of interest the user has in items' corresponding characteristics. These latent factors might measure obvious dimensions as mentioned in those examples or are completely uninterpretable. The predicted ratings can be estimated by\n\n$$\\hat{\\mathbf{R}} = \\mathbf{PQ}^\\top$$\n\nwhere $\\hat{\\mathbf{R}}\\in \\mathbb{R}^{m \\times n}$ is the predicted rating matrix which has the same shape as $\\mathbf{R}$. One major problem of this prediction rule is that users/items biases can not be modeled. For example, some users tend to give higher ratings or some items always get lower ratings due to poorer quality. These biases are commonplace in real-world applications. To capture these biases, user specific and item specific bias terms are introduced."
    },
    {
      "chunk_id": "ddde1da699be_1",
      "chapter": "mf",
      "heading": "The Matrix Factorization Model",
      "text": "For example, some users tend to give higher ratings or some items always get lower ratings due to poorer quality. These biases are commonplace in real-world applications. To capture these biases, user specific and item specific bias terms are introduced. Specifically, the predicted rating user $u$ gives to item $i$ is calculated by\n\n$$\n\\hat{\\mathbf{R}}_{ui} = \\mathbf{p}_u\\mathbf{q}^\\top_i + b_u + b_i\n$$\n\nThen, we train the matrix factorization model by minimizing the mean squared error between predicted rating scores and real rating scores. The objective function is defined as follows:\n\n$$\n\\underset{\\mathbf{P}, \\mathbf{Q}, b}{\\mathrm{argmin}} \\sum_{(u, i) \\in \\mathcal{K}} \\| \\mathbf{R}_{ui} -\n\\hat{\\mathbf{R}}_{ui} \\|^2 + \\lambda (\\| \\mathbf{P} \\|^2_F + \\| \\mathbf{Q}\n\\|^2_F + b_u^2 + b_i^2 )\n$$\n\nwhere $\\lambda$ denotes the regularization rate. The regularizing term $\\lambda (\\| \\mathbf{P} \\|^2_F + \\| \\mathbf{Q}\n\\|^2_F + b_u^2 + b_i^2 )$ is used to avoid over-fitting by penalizing the magnitude of the parameters. The $(u, i)$ pairs for which $\\mathbf{R}_{ui}$ is known are stored in the set\n$\\mathcal{K}=\\{(u, i) \\mid \\mathbf{R}_{ui} \\textrm{ is known}\\}$. The model parameters can be learned with an optimization algorithm, such as Stochastic Gradient Descent and Adam. An intuitive illustration of the matrix factorization model is shown below:\n\n![Illustration of matrix factorization model](../img/rec-mf.svg)\n\nIn the rest of this section, we will explain the implementation of matrix factorization and train the model on the MovieLens dataset. ```{.python .input  n=2}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, np, npx\nfrom mxnet.gluon import nn\nimport mxnet as mx\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "667d36777464_0",
      "chapter": "mf",
      "heading": "Model Implementation",
      "text": "First, we implement the matrix factorization model described above. The user and item latent factors can be created with the `nn.Embedding`. The `input_dim` is the number of items/users and the `output_dim` is the dimension of the latent factors $k$.  We can also use `nn.Embedding` to create the user/item biases by setting the `output_dim` to one. In the `forward` function, user and item ids are used to look up the embeddings.\n\n```{.python .input  n=4}\n#@tab mxnet\nclass MF(nn.Block):\n    def __init__(self, num_factors, num_users, num_items, **kwargs):\n        super(MF, self).__init__(**kwargs)\n        self.P = nn.Embedding(input_dim=num_users, output_dim=num_factors)\n        self.Q = nn.Embedding(input_dim=num_items, output_dim=num_factors)\n        self.user_bias = nn.Embedding(num_users, 1)\n        self.item_bias = nn.Embedding(num_items, 1)\n\n    def forward(self, user_id, item_id):\n        P_u = self.P(user_id)\n        Q_i = self.Q(item_id)\n        b_u = self.user_bias(user_id)\n        b_i = self.item_bias(item_id)\n        outputs = (P_u * Q_i).sum(axis=1) + np.squeeze(b_u) + np.squeeze(b_i)\n        return outputs.flatten()\n```"
    },
    {
      "chunk_id": "31c4fb175c50_0",
      "chapter": "mf",
      "heading": "Evaluation Measures",
      "text": "We then implement the RMSE (root-mean-square error) measure, which is commonly used to measure the differences between rating scores predicted by the model and the actually observed ratings (ground truth) :cite:`Gunawardana.Shani.2015`. RMSE is defined as:\n\n$$\n\\textrm{RMSE} = \\sqrt{\\frac{1}{|\\mathcal{T}|}\\sum_{(u, i) \\in \\mathcal{T}}(\\mathbf{R}_{ui} -\\hat{\\mathbf{R}}_{ui})^2}\n$$\n\nwhere $\\mathcal{T}$ is the set consisting of pairs of users and items that you want to evaluate on. $|\\mathcal{T}|$ is the size of this set. We can use the RMSE function provided by `mx.metric`.\n\n```{.python .input  n=3}\n#@tab mxnet\ndef evaluator(net, test_iter, devices):\n    rmse = mx.metric.RMSE()  # Get the RMSE\n    rmse_list = []\n    for idx, (users, items, ratings) in enumerate(test_iter):\n        u = gluon.utils.split_and_load(users, devices, even_split=False)\n        i = gluon.utils.split_and_load(items, devices, even_split=False)\n        r_ui = gluon.utils.split_and_load(ratings, devices, even_split=False)\n        r_hat = [net(u, i) for u, i in zip(u, i)]\n        rmse.update(labels=r_ui, preds=r_hat)\n        rmse_list.append(rmse.get()[1])\n    return float(np.mean(np.array(rmse_list)))\n```"
    },
    {
      "chunk_id": "34da7347d377_0",
      "chapter": "mf",
      "heading": "Training and Evaluating the Model",
      "text": "In the training function, we adopt the $\\ell_2$ loss with weight decay. The weight decay mechanism has the same effect as the $\\ell_2$ regularization. ```{.python .input  n=4}\n#@tab mxnet\n#@save\ndef train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,\n                        devices=d2l.try_all_gpus(), evaluator=None,\n                        **kwargs):\n    timer = d2l.Timer()\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 2],\n                            legend=['train loss', 'test RMSE'])\n    for epoch in range(num_epochs):\n        metric, l = d2l.Accumulator(3), 0. for i, values in enumerate(train_iter):\n            timer.start()\n            input_data = []\n            values = values if isinstance(values, list) else [values]\n            for v in values:\n                input_data.append(gluon.utils.split_and_load(v, devices))\n            train_feat = input_data[:-1] if len(values) > 1 else input_data\n            train_label = input_data[-1]\n            with autograd.record():\n                preds = [net(*t) for t in zip(*train_feat)]\n                ls = [loss(p, s) for p, s in zip(preds, train_label)]\n            [l.backward() for l in ls]\n            l += sum([l.asnumpy() for l in ls]).mean() / len(devices)\n            trainer.step(values[0].shape[0])\n            metric.add(l, values[0].shape[0], values[0].size)\n            timer.stop()\n        if len(kwargs) > 0:  # It will be used in section AutoRec\n            test_rmse = evaluator(net, test_iter, kwargs['inter_mat'],\n                                  devices)\n        else:\n            test_rmse = evaluator(net, test_iter, devices)\n        train_l = l / (i + 1)\n        animator.add(epoch + 1, (train_l, test_rmse))\n    print(f'train loss {metric[0] / metric[1]:.3f}, '\n          f'test RMSE {test_rmse:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n          f'on {str(devices)}')\n```\n\nFinally, let's put all things together and train the model."
    },
    {
      "chunk_id": "34da7347d377_1",
      "chapter": "mf",
      "heading": "Training and Evaluating the Model",
      "text": "Here, we set the latent factor dimension to 30. ```{.python .input  n=5}\n#@tab mxnet\ndevices = d2l.try_all_gpus()\nnum_users, num_items, train_iter, test_iter = d2l.split_and_load_ml100k(\n    test_ratio=0.1, batch_size=512)\nnet = MF(30, num_users, num_items)\nnet.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))\nlr, num_epochs, wd, optimizer = 0.002, 20, 1e-5, 'adam'\nloss = gluon.loss.L2Loss()\ntrainer = gluon.Trainer(net.collect_params(), optimizer,\n                        {\"learning_rate\": lr, 'wd': wd})\ntrain_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,\n                    devices, evaluator)\n```\n\nBelow, we use the trained model to predict the rating that a user (ID 20) might give to an item (ID 30). ```{.python .input  n=6}\n#@tab mxnet\nscores = net(np.array([20], dtype='int', ctx=devices[0]),\n             np.array([30], dtype='int', ctx=devices[0]))\nscores\n```"
    },
    {
      "chunk_id": "277f07326bb0_0",
      "chapter": "mf",
      "heading": "Summary",
      "text": "* The matrix factorization model is widely used in recommender systems.  It can be used to predict ratings that a user might give to an item.\n* We can implement and train matrix factorization for recommender systems."
    },
    {
      "chunk_id": "2e8670d84413_0",
      "chapter": "mf",
      "heading": "Exercises",
      "text": "* Vary the size of latent factors. How does the size of latent factors influence the model performance?\n* Try different optimizers, learning rates, and weight decay rates.\n* Check the predicted rating scores of other users for a specific movie.\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/400)\n:end_tab:"
    },
    {
      "chunk_id": "1b1d3bc5d64c_0",
      "chapter": "movielens",
      "heading": "movielens",
      "text": "#  The MovieLens Dataset\n\nThere are a number of datasets that are available for recommendation research. Amongst them, the [MovieLens](https://movielens.org/) dataset is probably one of the more popular ones. MovieLens is a non-commercial web-based movie recommender system. It is created in 1997 and run by GroupLens, a research lab at the University of Minnesota, in order to gather movie rating data for research purposes.  MovieLens data has been critical for several research studies including personalized recommendation and social psychology."
    },
    {
      "chunk_id": "db681a6164bb_0",
      "chapter": "movielens",
      "heading": "Getting the Data",
      "text": "The MovieLens dataset is hosted by the [GroupLens](https://grouplens.org/datasets/movielens/) website. Several versions are available. We will use the MovieLens 100K dataset :cite:`Herlocker.Konstan.Borchers.ea.1999`.  This dataset is comprised of $100,000$ ratings, ranging from 1 to 5 stars, from 943 users on 1682 movies. It has been cleaned up so that each user has rated at least 20 movies. Some simple demographic information such as age, gender, genres for the users and items are also available.  We can download the [ml-100k.zip](http://files.grouplens.org/datasets/movielens/ml-100k.zip) and extract the `u.data` file, which contains all the $100,000$ ratings in the csv format. There are many other files in the folder, a detailed description for each file can be found in the [README](http://files.grouplens.org/datasets/movielens/ml-100k-README.txt) file of the dataset.\n\nTo begin with, let's import the packages required to run this section's experiments.\n\n```{.python .input  n=1}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, np\nimport os\nimport pandas as pd\n```\n\nThen, we download the MovieLens 100k dataset and load the interactions as `DataFrame`.\n\n```{.python .input  n=2}\n#@tab mxnet\n#@save\nd2l.DATA_HUB['ml-100k'] = (\n    'https://files.grouplens.org/datasets/movielens/ml-100k.zip',\n    'cd4dcac4241c8a4ad7badc7ca635da8a69dddb83')\n\n#@save\ndef read_data_ml100k():\n    data_dir = d2l.download_extract('ml-100k')\n    names = ['user_id', 'item_id', 'rating', 'timestamp']\n    data = pd.read_csv(os.path.join(data_dir, 'u.data'), sep='\\t',\n                       names=names, engine='python')\n    num_users = data.user_id.unique().shape[0]\n    num_items = data.item_id.unique().shape[0]\n    return data, num_users, num_items\n```"
    },
    {
      "chunk_id": "630787ad7deb_0",
      "chapter": "movielens",
      "heading": "Statistics of the Dataset",
      "text": "Let's load up the data and inspect the first five records manually. It is an effective way to learn the data structure and verify that they have been loaded properly.\n\n```{.python .input  n=3}\n#@tab mxnet\ndata, num_users, num_items = read_data_ml100k()\nsparsity = 1 - len(data) / (num_users * num_items)\nprint(f'number of users: {num_users}, number of items: {num_items}')\nprint(f'matrix sparsity: {sparsity:f}')\nprint(data.head(5))\n```\n\nWe can see that each line consists of four columns, including \"user id\" 1-943, \"item id\" 1-1682, \"rating\" 1-5 and \"timestamp\". We can construct an interaction matrix of size $n \\times m$, where $n$ and $m$ are the number of users and the number of items respectively. This dataset only records the existing ratings, so we can also call it rating matrix and we will use interaction matrix and rating matrix interchangeably in case that the values of this matrix represent exact ratings. Most of the values in the rating matrix are unknown as users have not rated the majority of movies. We also show the sparsity of this dataset. The sparsity is defined as `1 - number of nonzero entries / ( number of users * number of items)`. Clearly, the interaction matrix is extremely sparse (i.e., sparsity = 93.695%). Real world datasets may suffer from a greater extent of sparsity and has been a long-standing challenge in building recommender systems. A viable solution is to use additional side information such as user/item features to alleviate the sparsity.\n\nWe then plot the distribution of the count of different ratings. As expected, it appears to be a normal distribution, with most ratings centered at 3-4.\n\n```{.python .input  n=4}\n#@tab mxnet\nd2l.plt.hist(data['rating'], bins=5, ec='black')\nd2l.plt.xlabel('Rating')\nd2l.plt.ylabel('Count')\nd2l.plt.title('Distribution of Ratings in MovieLens 100K')\nd2l.plt.show()\n```"
    },
    {
      "chunk_id": "de102837681c_0",
      "chapter": "movielens",
      "heading": "Splitting the dataset",
      "text": "We split the dataset into training and test sets. The following function provides two split modes including `random` and `seq-aware`. In the `random` mode, the function splits the 100k interactions randomly without considering timestamp and uses the 90% of the data as training samples and the rest 10% as test samples by default. In the `seq-aware` mode, we leave out the item that a user rated most recently for test, and users' historical interactions as training set. User historical interactions are sorted from oldest to newest based on timestamp. This mode will be used in the sequence-aware recommendation section. ```{.python .input  n=5}\n#@tab mxnet\n#@save\ndef split_data_ml100k(data, num_users, num_items,\n                      split_mode='random', test_ratio=0.1):\n    \"\"\"Split the dataset in random mode or seq-aware mode.\"\"\"\n    if split_mode == 'seq-aware':\n        train_items, test_items, train_list = {}, {}, []\n        for line in data.itertuples():\n            u, i, rating, time = line[1], line[2], line[3], line[4]\n            train_items.setdefault(u, []).append((u, i, rating, time))\n            if u not in test_items or test_items[u][-1] < time:\n                test_items[u] = (i, rating, time)\n        for u in range(1, num_users + 1):\n            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))\n        test_data = [(key, *value) for key, value in test_items.items()]\n        train_data = [item for item in train_list if item not in test_data]\n        train_data = pd.DataFrame(train_data)\n        test_data = pd.DataFrame(test_data)\n    else:\n        mask = [True if x == 1 else False for x in np.random.uniform(\n            0, 1, (len(data))) < 1 - test_ratio]\n        neg_mask = [not x for x in mask]\n        train_data, test_data = data[mask], data[neg_mask]\n    return train_data, test_data\n```\n\nNote that it is good practice to use a validation set in practice, apart from only a test set. However, we omit that for the sake of brevity."
    },
    {
      "chunk_id": "de102837681c_1",
      "chapter": "movielens",
      "heading": "Splitting the dataset",
      "text": "However, we omit that for the sake of brevity. In this case, our test set can be regarded as our held-out validation set."
    },
    {
      "chunk_id": "0292cc3ed717_0",
      "chapter": "movielens",
      "heading": "Loading the data",
      "text": "After dataset splitting, we will convert the training set and test set into lists and dictionaries/matrix for the sake of convenience. The following function reads the dataframe line by line and enumerates the index of users/items start from zero. The function then returns lists of users, items, ratings and a dictionary/matrix that records the interactions. We can specify the type of feedback to either `explicit` or `implicit`. ```{.python .input  n=6}\n#@tab mxnet\n#@save\ndef load_data_ml100k(data, num_users, num_items, feedback='explicit'):\n    users, items, scores = [], [], []\n    inter = np.zeros((num_items, num_users)) if feedback == 'explicit' else {}\n    for line in data.itertuples():\n        user_index, item_index = int(line[1] - 1), int(line[2] - 1)\n        score = int(line[3]) if feedback == 'explicit' else 1\n        users.append(user_index)\n        items.append(item_index)\n        scores.append(score)\n        if feedback == 'implicit':\n            inter.setdefault(user_index, []).append(item_index)\n        else:\n            inter[item_index, user_index] = score\n    return users, items, scores, inter\n```\n\nAfterwards, we put the above steps together and it will be used in the next section. The results are wrapped with `Dataset` and `DataLoader`. Note that the `last_batch` of `DataLoader` for training data is set to the `rollover` mode (The remaining samples are rolled over to the next epoch.) and orders are shuffled."
    },
    {
      "chunk_id": "0292cc3ed717_1",
      "chapter": "movielens",
      "heading": "Loading the data",
      "text": "The results are wrapped with `Dataset` and `DataLoader`. Note that the `last_batch` of `DataLoader` for training data is set to the `rollover` mode (The remaining samples are rolled over to the next epoch.) and orders are shuffled. ```{.python .input  n=7}\n#@tab mxnet\n#@save\ndef split_and_load_ml100k(split_mode='seq-aware', feedback='explicit',\n                          test_ratio=0.1, batch_size=256):\n    data, num_users, num_items = read_data_ml100k()\n    train_data, test_data = split_data_ml100k(\n        data, num_users, num_items, split_mode, test_ratio)\n    train_u, train_i, train_r, _ = load_data_ml100k(\n        train_data, num_users, num_items, feedback)\n    test_u, test_i, test_r, _ = load_data_ml100k(\n        test_data, num_users, num_items, feedback)\n    train_set = gluon.data.ArrayDataset(\n        np.array(train_u), np.array(train_i), np.array(train_r))\n    test_set = gluon.data.ArrayDataset(\n        np.array(test_u), np.array(test_i), np.array(test_r))\n    train_iter = gluon.data.DataLoader(\n        train_set, shuffle=True, last_batch='rollover',\n        batch_size=batch_size)\n    test_iter = gluon.data.DataLoader(\n        test_set, batch_size=batch_size)\n    return num_users, num_items, train_iter, test_iter\n```"
    },
    {
      "chunk_id": "13802bb55258_0",
      "chapter": "movielens",
      "heading": "Summary",
      "text": "* MovieLens datasets are widely used for recommendation research. It is public available and free to use.\n* We define functions to download and preprocess the MovieLens 100k dataset for further use in later sections."
    },
    {
      "chunk_id": "48aec5509d45_0",
      "chapter": "movielens",
      "heading": "Exercises",
      "text": "* What other similar recommendation datasets can you find?\n* Go through the [https://movielens.org/](https://movielens.org/) site for more information about MovieLens.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/399)\n:end_tab:"
    },
    {
      "chunk_id": "8c930078d72a_0",
      "chapter": "neumf",
      "heading": "neumf",
      "text": "# Neural Collaborative Filtering for Personalized Ranking\n\nThis section moves beyond explicit feedback, introducing the neural collaborative filtering (NCF) framework for recommendation with implicit feedback. Implicit feedback is pervasive in recommender systems. Actions such as Clicks, buys, and watches are common implicit feedback which are easy to collect and indicative of users' preferences. The model we will introduce, titled NeuMF :cite:`He.Liao.Zhang.ea.2017`, short for neural matrix factorization, aims to address the personalized ranking task with implicit feedback. This model leverages the flexibility and non-linearity of neural networks to replace dot products of matrix factorization, aiming at enhancing the model expressiveness. In specific, this model is structured with two subnetworks including generalized matrix factorization (GMF) and MLP and models the interactions from two pathways instead of simple dot products. The outputs of these two networks are concatenated for the final prediction scores calculation. Unlike the rating prediction task in AutoRec, this model generates a ranked recommendation list to each user based on the implicit feedback. We will use the personalized ranking loss introduced in the last section to train this model."
    },
    {
      "chunk_id": "d60639674e4c_0",
      "chapter": "neumf",
      "heading": "The NeuMF model",
      "text": "As aforementioned, NeuMF fuses two subnetworks. The GMF is a generic neural network version of matrix factorization where the input is the elementwise product of user and item latent factors. It consists of two neural layers:\n\n$$\n\\mathbf{x} = \\mathbf{p}_u \\odot \\mathbf{q}_i \\\\\n\\hat{y}_{ui} = \\alpha(\\mathbf{h}^\\top \\mathbf{x}),\n$$\n\nwhere $\\odot$ denotes the Hadamard product of vectors. $\\mathbf{P} \\in \\mathbb{R}^{m \\times k}$  and $\\mathbf{Q} \\in \\mathbb{R}^{n \\times k}$ correspond to user and item latent matrix respectively. $\\mathbf{p}_u \\in \\mathbb{R}^{ k}$ is the $u^\\textrm{th}$ row of $P$ and $\\mathbf{q}_i \\in \\mathbb{R}^{ k}$ is the $i^\\textrm{th}$ row of $Q$. $\\alpha$ and $h$ denote the activation function and weight of the output layer. $\\hat{y}_{ui}$ is the prediction score of the user $u$ might give to the item $i$. Another component of this model is MLP. To enrich model flexibility, the MLP subnetwork does not share user and item embeddings with GMF. It uses the concatenation of user and item embeddings as input. With the complicated connections and nonlinear transformations, it is capable of estimating the intricate interactions between users and items. More precisely, the MLP subnetwork is defined as:\n\n$$\n\\begin{aligned}\nz^{(1)} &= \\phi_1(\\mathbf{U}_u, \\mathbf{V}_i) = \\left[ \\mathbf{U}_u, \\mathbf{V}_i \\right] \\\\\n\\phi^{(2)}(z^{(1)})  &= \\alpha^1(\\mathbf{W}^{(2)} z^{(1)} + b^{(2)}) \\\\\n&... \\\\\n\\phi^{(L)}(z^{(L-1)}) &= \\alpha^L(\\mathbf{W}^{(L)} z^{(L-1)} + b^{(L)})) \\\\\n\\hat{y}_{ui} &= \\alpha(\\mathbf{h}^\\top\\phi^L(z^{(L-1)}))\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}^*, \\mathbf{b}^*$ and $\\alpha^*$ denote the weight matrix, bias vector, and activation function. $\\phi^*$ denotes the function of the corresponding layer. $\\mathbf{z}^*$ denotes the output of corresponding layer. To fuse the results of GMF and MLP, instead of simple addition, NeuMF concatenates the second last layers of two subnetworks to create a feature vector which can be passed to the further layers."
    },
    {
      "chunk_id": "d60639674e4c_1",
      "chapter": "neumf",
      "heading": "The NeuMF model",
      "text": "$\\mathbf{z}^*$ denotes the output of corresponding layer. To fuse the results of GMF and MLP, instead of simple addition, NeuMF concatenates the second last layers of two subnetworks to create a feature vector which can be passed to the further layers. Afterwards, the outputs are projected with matrix $\\mathbf{h}$ and a sigmoid activation function. The prediction layer is formulated as:\n$$\n\\hat{y}_{ui} = \\sigma(\\mathbf{h}^\\top[\\mathbf{x}, \\phi^L(z^{(L-1)})]). $$\n\nThe following figure illustrates the model architecture of NeuMF. ![Illustration of the NeuMF model](../img/rec-neumf.svg)\n\n```{.python .input  n=1}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, np, npx\nfrom mxnet.gluon import nn\nimport mxnet as mx\nimport random\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "f3e3acc488f7_0",
      "chapter": "neumf",
      "heading": "Model Implementation",
      "text": "The following code implements the NeuMF model. It consists of a generalized matrix factorization model and an MLP with different user and item embedding vectors. The structure of the MLP is controlled with the parameter `nums_hiddens`. ReLU is used as the default activation function.\n\n```{.python .input  n=2}\n#@tab mxnet\nclass NeuMF(nn.Block):\n    def __init__(self, num_factors, num_users, num_items, nums_hiddens,\n                 **kwargs):\n        super(NeuMF, self).__init__(**kwargs)\n        self.P = nn.Embedding(num_users, num_factors)\n        self.Q = nn.Embedding(num_items, num_factors)\n        self.U = nn.Embedding(num_users, num_factors)\n        self.V = nn.Embedding(num_items, num_factors)\n        self.mlp = nn.Sequential()\n        for num_hiddens in nums_hiddens:\n            self.mlp.add(nn.Dense(num_hiddens, activation='relu',\n                                  use_bias=True))\n        self.prediction_layer = nn.Dense(1, activation='sigmoid', use_bias=False)\n\n    def forward(self, user_id, item_id):\n        p_mf = self.P(user_id)\n        q_mf = self.Q(item_id)\n        gmf = p_mf * q_mf\n        p_mlp = self.U(user_id)\n        q_mlp = self.V(item_id)\n        mlp = self.mlp(np.concatenate([p_mlp, q_mlp], axis=1))\n        con_res = np.concatenate([gmf, mlp], axis=1)\n        return self.prediction_layer(con_res)\n```"
    },
    {
      "chunk_id": "cfea67399f4e_0",
      "chapter": "neumf",
      "heading": "Customized Dataset with Negative Sampling",
      "text": "For pairwise ranking loss, an important step is negative sampling. For each user, the items that a user has not interacted with are candidate items (unobserved entries). The following function takes users identity and candidate items as input, and samples negative items randomly for each user from the candidate set of that user. During the training stage, the model ensures that the items that a user likes to be ranked higher than items he dislikes or has not interacted with.\n\n```{.python .input  n=3}\n#@tab mxnet\nclass PRDataset(gluon.data.Dataset):\n    def __init__(self, users, items, candidates, num_items):\n        self.users = users\n        self.items = items\n        self.cand = candidates\n        self.all = set([i for i in range(num_items)])\n\n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, idx):\n        neg_items = list(self.all - set(self.cand[int(self.users[idx])]))\n        indices = random.randint(0, len(neg_items) - 1)\n        return self.users[idx], self.items[idx], neg_items[indices]\n```"
    },
    {
      "chunk_id": "8d16a36b81b4_0",
      "chapter": "neumf",
      "heading": "Evaluator",
      "text": "In this section, we adopt the splitting by time strategy to construct the training and test sets. Two evaluation measures including hit rate at given cutting off $\\ell$ ($\\textrm{Hit}@\\ell$) and area under the ROC curve (AUC) are used to assess the model effectiveness. Hit rate at given position $\\ell$ for each user indicates that whether the recommended item is included in the top $\\ell$ ranked list. The formal definition is as follows:\n\n$$\n\\textrm{Hit}@\\ell = \\frac{1}{m} \\sum_{u \\in \\mathcal{U}} \\textbf{1}(rank_{u, g_u} <= \\ell),\n$$\n\nwhere $\\textbf{1}$ denotes an indicator function that is equal to one if the ground truth item is ranked in the top $\\ell$ list, otherwise it is equal to zero. $rank_{u, g_u}$ denotes the ranking of the ground truth item $g_u$ of the user $u$ in the recommendation list (The ideal ranking is 1). $m$ is the number of users. $\\mathcal{U}$ is the user set. The definition of AUC is as follows:\n\n$$\n\\textrm{AUC} = \\frac{1}{m} \\sum_{u \\in \\mathcal{U}} \\frac{1}{|\\mathcal{I} \\backslash S_u|} \\sum_{j \\in I \\backslash S_u} \\textbf{1}(rank_{u, g_u} < rank_{u, j}),\n$$\n\nwhere $\\mathcal{I}$ is the item set. $S_u$ is the candidate items of user $u$. Note that many other evaluation protocols such as precision, recall and normalized discounted cumulative gain (NDCG) can also be used. The following function calculates the hit counts and AUC for each user. ```{.python .input  n=4}\n#@tab mxnet\n#@save\ndef hit_and_auc(rankedlist, test_matrix, k):\n    hits_k = [(idx, val) for idx, val in enumerate(rankedlist[:k])\n              if val in set(test_matrix)]\n    hits_all = [(idx, val) for idx, val in enumerate(rankedlist)\n                if val in set(test_matrix)]\n    max = len(rankedlist) - 1\n    auc = 1.0 * (max - hits_all[0][0]) / max if len(hits_all) > 0 else 0\n    return len(hits_k), auc\n```\n\nThen, the overall Hit rate and AUC are calculated as follows."
    },
    {
      "chunk_id": "8d16a36b81b4_1",
      "chapter": "neumf",
      "heading": "Evaluator",
      "text": "```{.python .input  n=5}\n#@tab mxnet\n#@save\ndef evaluate_ranking(net, test_input, seq, candidates, num_users, num_items,\n                     devices):\n    ranked_list, ranked_items, hit_rate, auc = {}, {}, [], []\n    all_items = set([i for i in range(num_users)])\n    for u in range(num_users):\n        neg_items = list(all_items - set(candidates[int(u)]))\n        user_ids, item_ids, x, scores = [], [], [], []\n        [item_ids.append(i) for i in neg_items]\n        [user_ids.append(u) for _ in neg_items]\n        x.extend([np.array(user_ids)])\n        if seq is not None:\n            x.append(seq[user_ids, :])\n        x.extend([np.array(item_ids)])\n        test_data_iter = gluon.data.DataLoader(\n            gluon.data.ArrayDataset(*x), shuffle=False, last_batch=\"keep\",\n            batch_size=1024)\n        for index, values in enumerate(test_data_iter):\n            x = [gluon.utils.split_and_load(v, devices, even_split=False)\n                 for v in values]\n            scores.extend([list(net(*t).asnumpy()) for t in zip(*x)])\n        scores = [item for sublist in scores for item in sublist]\n        item_scores = list(zip(item_ids, scores))\n        ranked_list[u] = sorted(item_scores, key=lambda t: t[1], reverse=True)\n        ranked_items[u] = [r[0] for r in ranked_list[u]]\n        temp = hit_and_auc(ranked_items[u], test_input[u], 50)\n        hit_rate.append(temp[0])\n        auc.append(temp[1])\n    return np.mean(np.array(hit_rate)), np.mean(np.array(auc))\n```"
    },
    {
      "chunk_id": "61fc5f04d9ad_0",
      "chapter": "neumf",
      "heading": "Training and Evaluating the Model",
      "text": "The training function is defined below. We train the model in the pairwise manner. ```{.python .input  n=6}\n#@tab mxnet\n#@save\ndef train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter,\n                  num_users, num_items, num_epochs, devices, evaluator,\n                  candidates, eval_step=1):\n    timer, hit_rate, auc = d2l.Timer(), 0, 0\n    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n                            legend=['test hit rate', 'test AUC'])\n    for epoch in range(num_epochs):\n        metric, l = d2l.Accumulator(3), 0. for i, values in enumerate(train_iter):\n            input_data = []\n            for v in values:\n                input_data.append(gluon.utils.split_and_load(v, devices))\n            with autograd.record():\n                p_pos = [net(*t) for t in zip(*input_data[:-1])]\n                p_neg = [net(*t) for t in zip(*input_data[:-2],\n                                              input_data[-1])]\n                ls = [loss(p, n) for p, n in zip(p_pos, p_neg)]\n            [l.backward(retain_graph=False) for l in ls]\n            l += sum([l.asnumpy() for l in ls]).mean()/len(devices)\n            trainer.step(values[0].shape[0])\n            metric.add(l, values[0].shape[0], values[0].size)\n            timer.stop()\n        with autograd.predict_mode():\n            if (epoch + 1) % eval_step == 0:\n                hit_rate, auc = evaluator(net, test_iter, test_seq_iter,\n                                          candidates, num_users, num_items,\n                                          devices)\n                animator.add(epoch + 1, (hit_rate, auc))\n    print(f'train loss {metric[0] / metric[1]:.3f}, '\n          f'test hit rate {float(hit_rate):.3f}, test AUC {float(auc):.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n          f'on {str(devices)}')\n```\n\nNow, we can load the MovieLens 100k dataset and train the model."
    },
    {
      "chunk_id": "61fc5f04d9ad_1",
      "chapter": "neumf",
      "heading": "Training and Evaluating the Model",
      "text": "Since there are only ratings in the MovieLens dataset, with some losses of accuracy, we binarize these ratings to zeros and ones. If a user rated an item, we consider the implicit feedback as one, otherwise as zero. The action of rating an item can be treated as a form of providing implicit feedback. Here, we split the dataset in the `seq-aware` mode where users' latest interacted items are left out for test. ```{.python .input  n=11}\n#@tab mxnet\nbatch_size = 1024\ndf, num_users, num_items = d2l.read_data_ml100k()\ntrain_data, test_data = d2l.split_data_ml100k(df, num_users, num_items,\n                                              'seq-aware')\nusers_train, items_train, ratings_train, candidates = d2l.load_data_ml100k(\n    train_data, num_users, num_items, feedback=\"implicit\")\nusers_test, items_test, ratings_test, test_iter = d2l.load_data_ml100k(\n    test_data, num_users, num_items, feedback=\"implicit\")\ntrain_iter = gluon.data.DataLoader(\n    PRDataset(users_train, items_train, candidates, num_items ), batch_size,\n    True, last_batch=\"rollover\", num_workers=d2l.get_dataloader_workers())\n```\n\nWe then create and initialize the model. We use a three-layer MLP with constant hidden size 10. ```{.python .input  n=8}\n#@tab mxnet\ndevices = d2l.try_all_gpus()\nnet = NeuMF(10, num_users, num_items, nums_hiddens=[10, 10, 10])\nnet.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))\n```\n\nThe following code trains the model. ```{.python .input  n=12}\n#@tab mxnet\nlr, num_epochs, wd, optimizer = 0.01, 10, 1e-5, 'adam'\nloss = d2l.BPRLoss()\ntrainer = gluon.Trainer(net.collect_params(), optimizer,\n                        {\"learning_rate\": lr, 'wd': wd})\ntrain_ranking(net, train_iter, test_iter, loss, trainer, None, num_users,\n              num_items, num_epochs, devices, evaluate_ranking, candidates)\n```"
    },
    {
      "chunk_id": "abb766dba8b9_0",
      "chapter": "neumf",
      "heading": "Summary",
      "text": "* Adding nonlinearity to matrix factorization model is beneficial for improving the model capability and effectiveness.\n* NeuMF is a combination of matrix factorization and an MLP. The MLP takes the concatenation of user and item embeddings as input."
    },
    {
      "chunk_id": "6bb035d48fd8_0",
      "chapter": "neumf",
      "heading": "Exercises",
      "text": "* Vary the size of latent factors. How the size of latent factors impact the model performance?\n* Vary the architectures (e.g., number of layers, number of neurons of each layer) of the MLP to check the its impact on the performance.\n* Try different optimizers, learning rate and weight decay rate.\n* Try to use hinge loss defined in the last section to optimize this model.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/403)\n:end_tab:"
    },
    {
      "chunk_id": "8ea5230a946b_0",
      "chapter": "ranking",
      "heading": "ranking",
      "text": "# Personalized Ranking for Recommender Systems\n\nIn the former sections, only explicit feedback was considered and models were trained and tested on observed ratings. There are two demerits of such methods: First, most feedback is not explicit but implicit in real-world scenarios, and explicit feedback can be more expensive to collect. Second, non-observed user-item pairs which may be predictive for users' interests are totally ignored, making these methods unsuitable for cases where ratings are not missing at random but because of users' preferences. Non-observed user-item pairs are a  mixture of real negative feedback (users are not interested in the items) and missing values (the user might interact with the items in the future). We simply ignore the non-observed pairs in matrix factorization and AutoRec. Clearly, these models are incapable of distinguishing between observed and non-observed pairs and are usually not suitable for personalized ranking tasks. To this end, a class of recommendation models targeting at generating ranked recommendation lists from implicit feedback have gained popularity. In general, personalized ranking models can be optimized with pointwise, pairwise or listwise approaches. Pointwise approaches considers a single interaction at a time and train a classifier or a regressor to predict individual preferences. Matrix factorization and AutoRec are optimized with pointwise objectives. Pairwise approaches consider a pair of items for each user and aim to approximate the optimal ordering for that pair. Usually, pairwise approaches are more suitable for the ranking task because predicting relative order is reminiscent to the nature of ranking. Listwise approaches approximate the ordering of the entire list of items, for example, direct optimizing the ranking measures such as Normalized Discounted Cumulative Gain ([NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)). However, listwise approaches are more complex and compute-intensive than pointwise or pairwise approaches."
    },
    {
      "chunk_id": "8ea5230a946b_1",
      "chapter": "ranking",
      "heading": "ranking",
      "text": "However, listwise approaches are more complex and compute-intensive than pointwise or pairwise approaches. In this section, we will introduce two pairwise objectives/losses, Bayesian Personalized Ranking loss and Hinge loss, and their respective implementations."
    },
    {
      "chunk_id": "2f31c4b3c6eb_0",
      "chapter": "ranking",
      "heading": "Bayesian Personalized Ranking Loss and its Implementation",
      "text": "Bayesian personalized ranking (BPR) :cite:`Rendle.Freudenthaler.Gantner.ea.2009` is a pairwise personalized ranking loss that is derived from the maximum posterior estimator. It has been widely used in many existing recommendation models. The training data of BPR consists of both positive and negative pairs (missing values). It assumes that the user prefers the positive item over all other non-observed items. In formal, the training data is constructed by tuples in the form of $(u, i, j)$, which represents that the user $u$ prefers the item $i$ over the item $j$. The Bayesian formulation of BPR which aims to maximize the posterior probability is given below:\n\n$$\np(\\Theta \\mid >_u )  \\propto  p(>_u \\mid \\Theta) p(\\Theta)\n$$\n\nWhere $\\Theta$ represents the parameters of an arbitrary recommendation model, $>_u$ represents the desired personalized total ranking of all items for user $u$. We can formulate the maximum posterior estimator to derive the generic optimization criterion for the personalized ranking task. $$\n\\begin{aligned}\n\\textrm{BPR-OPT} : &= \\ln p(\\Theta \\mid >_u) \\\\\n         & \\propto \\ln p(>_u \\mid \\Theta) p(\\Theta) \\\\\n         &= \\ln \\prod_{(u, i, j \\in D)} \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj}) p(\\Theta) \\\\\n         &= \\sum_{(u, i, j \\in D)} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj}) + \\ln p(\\Theta) \\\\\n         &= \\sum_{(u, i, j \\in D)} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj}) - \\lambda_\\Theta \\|\\Theta \\|^2\n\\end{aligned}\n$$\n\n\nwhere $D \\stackrel{\\textrm{def}}{=} \\{(u, i, j) \\mid i \\in I^+_u \\wedge j \\in I \\backslash I^+_u \\}$ is the training set, with $I^+_u$ denoting the items the user $u$ liked, $I$ denoting all items, and $I \\backslash I^+_u$ indicating all other items excluding items the user liked. $\\hat{y}_{ui}$ and $\\hat{y}_{uj}$ are the predicted scores of the user $u$ to item $i$ and $j$, respectively. The prior $p(\\Theta)$ is a normal distribution with zero mean and variance-covariance matrix $\\Sigma_\\Theta$. Here, we let $\\Sigma_\\Theta = \\lambda_\\Theta I$."
    },
    {
      "chunk_id": "2f31c4b3c6eb_1",
      "chapter": "ranking",
      "heading": "Bayesian Personalized Ranking Loss and its Implementation",
      "text": "The prior $p(\\Theta)$ is a normal distribution with zero mean and variance-covariance matrix $\\Sigma_\\Theta$. Here, we let $\\Sigma_\\Theta = \\lambda_\\Theta I$. ![Illustration of Bayesian Personalized Ranking](../img/rec-ranking.svg)\nWe will implement the base class  `mxnet.gluon.loss.Loss` and override the `forward` method to construct the Bayesian personalized ranking loss. We begin by importing the Loss class and the np module. ```{.python .input  n=5}\n#@tab mxnet\nfrom mxnet import gluon, np, npx\nnpx.set_np()\n```\n\nThe implementation of BPR loss is as follows. ```{.python .input  n=2}\n#@tab mxnet\n#@save\nclass BPRLoss(gluon.loss.Loss):\n    def __init__(self, weight=None, batch_axis=0, **kwargs):\n        super(BPRLoss, self).__init__(weight=None, batch_axis=0, **kwargs)\n\n    def forward(self, positive, negative):\n        distances = positive - negative\n        loss = - np.sum(np.log(npx.sigmoid(distances)), 0, keepdims=True)\n        return loss\n```"
    },
    {
      "chunk_id": "93ef5fac5b2b_0",
      "chapter": "ranking",
      "heading": "Hinge Loss and its Implementation",
      "text": "The Hinge loss for ranking has different form to the [hinge loss](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HingeLoss) provided within the gluon library that is often used in classifiers such as SVMs.  The loss used for ranking in recommender systems has the following form.\n\n$$\n \\sum_{(u, i, j \\in D)} \\max( m - \\hat{y}_{ui} + \\hat{y}_{uj}, 0)\n$$\n\nwhere $m$ is the safety margin size. It aims to push negative items away from positive items. Similar to BPR, it aims to optimize for relevant distance between positive and negative samples instead of absolute outputs, making it well suited to recommender systems.\n\n```{.python .input  n=3}\n#@tab mxnet\n#@save\nclass HingeLossbRec(gluon.loss.Loss):\n    def __init__(self, weight=None, batch_axis=0, **kwargs):\n        super(HingeLossbRec, self).__init__(weight=None, batch_axis=0,\n                                            **kwargs)\n\n    def forward(self, positive, negative, margin=1):\n        distances = positive - negative\n        loss = np.sum(np.maximum(- distances + margin, 0))\n        return loss\n```\n\nThese two losses are interchangeable for personalized ranking in recommendation."
    },
    {
      "chunk_id": "a23acaacaf25_0",
      "chapter": "ranking",
      "heading": "Summary",
      "text": "- There are three types of ranking losses available for the personalized ranking task in recommender systems, namely, pointwise, pairwise and listwise methods.\n- The two pairwise loses, Bayesian personalized ranking loss and hinge loss, can be used interchangeably."
    },
    {
      "chunk_id": "bc6294cbd3fc_0",
      "chapter": "ranking",
      "heading": "Exercises",
      "text": "- Are there any variants of BPR and hinge loss available?\n- Can you find any recommendation models that use BPR or hinge loss?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/402)\n:end_tab:"
    },
    {
      "chunk_id": "253790c6b0d7_0",
      "chapter": "recsys-intro",
      "heading": "recsys-intro",
      "text": "# Overview of Recommender Systems\n\n\n\nIn the last decade, the Internet has evolved into a platform for large-scale online services, which profoundly changed the way we communicate, read news, buy products, and watch movies.  In the meanwhile, the unprecedented number of items (we use the term *item* to refer to movies, news, books, and products.) offered online requires a system that can help us discover items that we preferred. Recommender systems are therefore powerful information filtering tools that can facilitate personalized services and provide tailored experience to individual users. In short, recommender systems play a pivotal role in utilizing the wealth of data available to make choices manageable. Nowadays, recommender systems are at the core of a number of online services providers such as Amazon, Netflix, and YouTube. Recall the example of Deep learning books recommended by Amazon in :numref:`subsec_recommender_systems`. The benefits of employing recommender systems are two-folds: On the one hand, it can largely reduce users' effort in finding items and alleviate the issue of information overload. On the other hand, it can add business value to  online\nservice providers and is an important source of revenue.  This chapter will introduce the fundamental concepts, classic models and recent advances with deep learning in the field of recommender systems, together with implemented examples.\n\n![Illustration of the Recommendation Process](../img/rec-intro.svg)"
    },
    {
      "chunk_id": "3cab8e340b37_0",
      "chapter": "recsys-intro",
      "heading": "Collaborative Filtering",
      "text": "We start the journey with the important concept in recommender systems---collaborative filtering\n(CF), which was first coined by the Tapestry system :cite:`Goldberg.Nichols.Oki.ea.1992`, referring to \"people collaborate to help one another perform the filtering process  in order to handle the large amounts of email and messages posted to newsgroups\". This term has been enriched with more senses. In a broad sense, it is the process of\nfiltering for information or patterns using techniques involving collaboration among multiple users, agents, and data sources. CF has many forms and numerous CF methods proposed since its advent.\n\nOverall, CF techniques can be categorized into: memory-based CF, model-based CF, and their hybrid :cite:`Su.Khoshgoftaar.2009`. Representative memory-based CF techniques are nearest neighbor-based CF such as user-based CF and item-based CF :cite:`Sarwar.Karypis.Konstan.ea.2001`.  Latent factor models such as matrix factorization are examples of model-based CF.  Memory-based CF has limitations in dealing with sparse and large-scale data since it computes the similarity values based on common items.  Model-based methods become more popular with its\nbetter capability in dealing with sparsity and scalability.  Many model-based CF approaches can be extended with neural networks, leading to more flexible and scalable models with the computation acceleration in deep learning :cite:`Zhang.Yao.Sun.ea.2019`.  In general, CF only uses the user-item interaction data to make predictions and recommendations. Besides CF, content-based and context-based recommender systems are also useful in incorporating the content descriptions of items/users and contextual signals such as timestamps and locations.  Obviously, we may need to adjust the model types/structures when different input data is available."
    },
    {
      "chunk_id": "bba0fa06c21f_0",
      "chapter": "recsys-intro",
      "heading": "Explicit Feedback and Implicit Feedback",
      "text": "To learn the preference of users, the system shall collect feedback from them.  The feedback can be either explicit or implicit :cite:`Hu.Koren.Volinsky.2008`. For example, [IMDb](https://www.imdb.com/) collects star ratings ranging from one to ten stars for movies. YouTube provides the thumbs-up and thumbs-down buttons for users to show their preferences.  It is apparent that gathering explicit feedback requires users to indicate their interests proactively.  Nonetheless, explicit feedback is not always readily available as many users may be reluctant to rate products. Relatively speaking, implicit feedback is often readily available since it is mainly concerned with modeling implicit behavior such as user clicks. As such, many recommender systems are centered on implicit feedback which indirectly reflects user's opinion through observing user behavior.  There are diverse forms of implicit feedback including purchase history, browsing history, watches and even mouse movements. For example, a user that purchased many books by the same author probably likes that author.   Note that implicit feedback is inherently noisy.  We can only *guess* their preferences and true motives. A user watched a movie does not necessarily indicate a positive view of that movie."
    },
    {
      "chunk_id": "fb90bafbb245_0",
      "chapter": "recsys-intro",
      "heading": "Recommendation Tasks",
      "text": "A number of recommendation tasks have been investigated in the past decades.  Based on the domain of applications, there are movies recommendation, news recommendations, point-of-interest recommendation :cite:`Ye.Yin.Lee.ea.2011` and so forth.  It is also possible to differentiate the tasks based on the types of feedback and input data, for example, the rating prediction task aims to predict the explicit ratings. Top-$n$ recommendation (item ranking) ranks all items for each user personally based on the implicit feedback. If time-stamp information is also included, we can build sequence-aware recommendation :cite:`Quadrana.Cremonesi.Jannach.2018`.  Another popular task is called click-through rate prediction, which is also based on implicit feedback, but various categorical features can be utilized. Recommending for new users and recommending new items to existing users are called cold-start recommendation :cite:`Schein.Popescul.Ungar.ea.2002`."
    },
    {
      "chunk_id": "9e3edd58a088_0",
      "chapter": "recsys-intro",
      "heading": "Summary",
      "text": "* Recommender systems are important for individual users and industries. Collaborative filtering is a key concept in recommendation.\n* There are two types of feedbacks: implicit feedback and explicit feedback.  A number of recommendation tasks have been explored during the last decade."
    },
    {
      "chunk_id": "dfccf17a29c4_0",
      "chapter": "recsys-intro",
      "heading": "Exercises",
      "text": "1. Can you explain how recommender systems influence your daily life?\n2. What interesting recommendation tasks do you think can be investigated?\n\n[Discussions](https://discuss.d2l.ai/t/398)"
    },
    {
      "chunk_id": "3b3806c5770c_0",
      "chapter": "seqrec",
      "heading": "seqrec",
      "text": "# Sequence-Aware Recommender Systems\n\nIn previous sections, we abstract the recommendation task as a matrix completion problem without considering users' short-term behaviors. In this section, we will introduce a recommendation model that takes  the sequentially-ordered user interaction logs into account.  It is a sequence-aware recommender :cite:`Quadrana.Cremonesi.Jannach.2018` where the input is an ordered and often timestamped list of past user actions.  A number of recent literatures have demonstrated the usefulness of incorporating such information in modeling users' temporal behavioral patterns and discovering their interest drift.\n\nThe model we will introduce, Caser :cite:`Tang.Wang.2018`, short for convolutional sequence embedding recommendation model, adopts convolutional neural networks capture the dynamic pattern influences of users' recent activities. The main component of Caser consists of a horizontal convolutional network and a vertical convolutional network, aiming to uncover the union-level and point-level sequence patterns, respectively.  Point-level pattern indicates the impact of single item in the historical sequence on the target item, while union level pattern implies the influences of several previous actions on the subsequent target. For example, buying both milk and butter together leads to higher probability of buying flour than just buying one of them. Moreover, users' general interests, or long term preferences are also modeled in the last fully connected layers, resulting in a more comprehensive modeling of user interests. Details of the model are described as follows."
    },
    {
      "chunk_id": "c943b04775f3_0",
      "chapter": "seqrec",
      "heading": "Model Architectures",
      "text": "In sequence-aware recommendation system, each user is associated with a sequence of some items from the item set. Let $S^u = (S_1^u, ... S_{|S_u|}^u)$ denotes the ordered sequence. The goal of Caser is to recommend item by considering user general tastes as well as short-term intention. Suppose we take the previous $L$ items into consideration, an embedding matrix that represents the former interactions for time step $t$ can be constructed:\n\n$$\n\\mathbf{E}^{(u, t)} = [ \\mathbf{q}_{S_{t-L}^u} , ..., \\mathbf{q}_{S_{t-2}^u}, \\mathbf{q}_{S_{t-1}^u} ]^\\top,\n$$\n\nwhere $\\mathbf{Q} \\in \\mathbb{R}^{n \\times k}$ represents item embeddings and $\\mathbf{q}_i$ denotes the $i^\\textrm{th}$ row. $\\mathbf{E}^{(u, t)} \\in \\mathbb{R}^{L \\times k}$ can be used to infer the transient interest of user $u$ at time-step $t$. We can view the input matrix $\\mathbf{E}^{(u, t)}$ as an image which is the input of the subsequent two convolutional components. The horizontal convolutional layer has $d$ horizontal filters $\\mathbf{F}^j \\in \\mathbb{R}^{h \\times k}, 1 \\leq j \\leq d, h = \\{1, ..., L\\}$, and the vertical convolutional layer has $d'$ vertical filters $\\mathbf{G}^j \\in \\mathbb{R}^{ L \\times 1}, 1 \\leq j \\leq d'$. After a series of convolutional and pool operations, we get the two outputs:\n\n$$\n\\mathbf{o} = \\textrm{HConv}(\\mathbf{E}^{(u, t)}, \\mathbf{F}) \\\\\n\\mathbf{o}'= \\textrm{VConv}(\\mathbf{E}^{(u, t)}, \\mathbf{G}) ,\n$$\n\nwhere $\\mathbf{o} \\in \\mathbb{R}^d$ is the output of horizontal convolutional network and $\\mathbf{o}' \\in \\mathbb{R}^{kd'}$ is the output of vertical convolutional network. For simplicity, we omit the details of convolution and pool operations. They are concatenated and fed into a fully connected neural network layer to get more high-level representations. $$\n\\mathbf{z} = \\phi(\\mathbf{W}[\\mathbf{o}, \\mathbf{o}']^\\top + \\mathbf{b}),\n$$\n\nwhere $\\mathbf{W} \\in \\mathbb{R}^{k \\times (d + kd')}$ is the weight matrix and $\\mathbf{b} \\in \\mathbb{R}^k$ is the bias."
    },
    {
      "chunk_id": "c943b04775f3_1",
      "chapter": "seqrec",
      "heading": "Model Architectures",
      "text": "$$\n\\mathbf{z} = \\phi(\\mathbf{W}[\\mathbf{o}, \\mathbf{o}']^\\top + \\mathbf{b}),\n$$\n\nwhere $\\mathbf{W} \\in \\mathbb{R}^{k \\times (d + kd')}$ is the weight matrix and $\\mathbf{b} \\in \\mathbb{R}^k$ is the bias. The learned vector $\\mathbf{z} \\in \\mathbb{R}^k$ is the representation of user's short-term intent. At last, the prediction function combines users' short-term and general taste together, which is defined as:\n\n$$\n\\hat{y}_{uit} = \\mathbf{v}_i \\cdot [\\mathbf{z}, \\mathbf{p}_u]^\\top + \\mathbf{b}'_i,\n$$\n\nwhere $\\mathbf{V} \\in \\mathbb{R}^{n \\times 2k}$ is another item embedding matrix. $\\mathbf{b}' \\in \\mathbb{R}^n$ is the item specific bias. $\\mathbf{P} \\in \\mathbb{R}^{m \\times k}$ is the user embedding matrix for users' general tastes. $\\mathbf{p}_u \\in \\mathbb{R}^{ k}$ is the $u^\\textrm{th}$ row of $P$ and $\\mathbf{v}_i \\in \\mathbb{R}^{2k}$ is the $i^\\textrm{th}$ row of $\\mathbf{V}$. The model can be learned with BPR or Hinge loss. The architecture of Caser is shown below:\n\n![Illustration of the Caser Model](../img/rec-caser.svg)\n\nWe first import the required libraries. ```{.python .input  n=3}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, np, npx\nfrom mxnet.gluon import nn\nimport mxnet as mx\nimport random\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "75bc2cfb74d1_0",
      "chapter": "seqrec",
      "heading": "Model Implementation",
      "text": "The following code implements the Caser model. It consists of a vertical convolutional layer, a horizontal convolutional layer, and a full-connected layer."
    },
    {
      "chunk_id": "75bc2cfb74d1_1",
      "chapter": "seqrec",
      "heading": "Model Implementation",
      "text": "The following code implements the Caser model. It consists of a vertical convolutional layer, a horizontal convolutional layer, and a full-connected layer. ```{.python .input  n=4}\n#@tab mxnet\nclass Caser(nn.Block):\n    def __init__(self, num_factors, num_users, num_items, L=5, d=16,\n                 d_prime=4, drop_ratio=0.05, **kwargs):\n        super(Caser, self).__init__(**kwargs)\n        self.P = nn.Embedding(num_users, num_factors)\n        self.Q = nn.Embedding(num_items, num_factors)\n        self.d_prime, self.d = d_prime, d\n        # Vertical convolution layer\n        self.conv_v = nn.Conv2D(d_prime, (L, 1), in_channels=1)\n        # Horizontal convolution layer\n        h = [i + 1 for i in range(L)]\n        self.conv_h, self.max_pool = nn.Sequential(), nn.Sequential()\n        for i in h:\n            self.conv_h.add(nn.Conv2D(d, (i, num_factors), in_channels=1))\n            self.max_pool.add(nn.MaxPool1D(L - i + 1))\n        # Fully connected layer\n        self.fc1_dim_v, self.fc1_dim_h = d_prime * num_factors, d * len(h)\n        self.fc = nn.Dense(in_units=d_prime * num_factors + d * L,\n                           activation='relu', units=num_factors)\n        self.Q_prime = nn.Embedding(num_items, num_factors * 2)\n        self.b = nn.Embedding(num_items, 1)\n        self.dropout = nn.Dropout(drop_ratio)\n\n    def forward(self, user_id, seq, item_id):\n        item_embs = np.expand_dims(self.Q(seq), 1)\n        user_emb = self.P(user_id)\n        out, out_h, out_v, out_hs = None, None, None, []\n        if self.d_prime:\n            out_v = self.conv_v(item_embs)\n            out_v = out_v.reshape(out_v.shape[0], self.fc1_dim_v)\n        if self.d:\n            for conv, maxp in zip(self.conv_h, self.max_pool):\n                conv_out = np.squeeze(npx.relu(conv(item_embs)), axis=3)\n                t = maxp(conv_out)\n                pool_out = np.squeeze(t, axis=2)\n                out_hs.append(pool_out)\n            out_h = np.concatenate(out_hs, axis=1)\n        out = np.concatenate([out_v, out_h], axis=1)\n        z = self.fc(self.dropout(out))\n        x = np.concatenate([z, user_emb], axis=1)\n        q_prime_i = np.squeeze(self.Q_prime(item_id))\n        b = np.squeeze(self.b(item_id))\n        res = (x * q_prime_i).sum(1) + b\n        return res\n```"
    },
    {
      "chunk_id": "07c3af721a9a_0",
      "chapter": "seqrec",
      "heading": "Sequential Dataset with Negative Sampling",
      "text": "To process the sequential interaction data, we need to reimplement the `Dataset` class. The following code creates a new dataset class named `SeqDataset`. In each sample, it outputs the user identity, his previous $L$ interacted items as a sequence and the next item he interacts as the target. The following figure demonstrates the data loading process for one user. Suppose that this user liked 9 movies, we organize these nine movies in chronological order. The latest movie is left out as the test item. For the remaining eight movies, we can get three training samples, with each sample containing a sequence of five ($L=5$) movies and its subsequent item as the target item. Negative samples are also included in the customized dataset."
    },
    {
      "chunk_id": "07c3af721a9a_1",
      "chapter": "seqrec",
      "heading": "Sequential Dataset with Negative Sampling",
      "text": "For the remaining eight movies, we can get three training samples, with each sample containing a sequence of five ($L=5$) movies and its subsequent item as the target item. Negative samples are also included in the customized dataset. ![Illustration of the data generation process](../img/rec-seq-data.svg)\n\n```{.python .input  n=5}\n#@tab mxnet\nclass SeqDataset(gluon.data.Dataset):\n    def __init__(self, user_ids, item_ids, L, num_users, num_items,\n                 candidates):\n        user_ids, item_ids = np.array(user_ids), np.array(item_ids)\n        sort_idx = np.array(sorted(range(len(user_ids)),\n                                   key=lambda k: user_ids[k]))\n        u_ids, i_ids = user_ids[sort_idx], item_ids[sort_idx]\n        temp, u_ids, self.cand = {}, u_ids.asnumpy(), candidates\n        self.all_items = set([i for i in range(num_items)])\n        [temp.setdefault(u_ids[i], []).append(i) for i, _ in enumerate(u_ids)]\n        temp = sorted(temp.items(), key=lambda x: x[0])\n        u_ids = np.array([i[0] for i in temp])\n        idx = np.array([i[1][0] for i in temp])\n        self.ns = ns = int(sum([c - L if c >= L + 1 else 1 for c\n                                in np.array([len(i[1]) for i in temp])]))\n        self.seq_items = np.zeros((ns, L))\n        self.seq_users = np.zeros(ns, dtype='int32')\n        self.seq_tgt = np.zeros((ns, 1))\n        self.test_seq = np.zeros((num_users, L))\n        test_users, _uid = np.empty(num_users), None\n        for i, (uid, i_seq) in enumerate(self._seq(u_ids, i_ids, idx, L + 1)):\n            if uid != _uid:\n                self.test_seq[uid][:] = i_seq[-L:]\n                test_users[uid], _uid = uid, uid\n            self.seq_tgt[i][:] = i_seq[-1:]\n            self.seq_items[i][:], self.seq_users[i] = i_seq[:L], uid\n\n    def _win(self, tensor, window_size, step_size=1):\n        if len(tensor) - window_size >= 0:\n            for i in range(len(tensor), 0, - step_size):\n                if i - window_size >= 0:\n                    yield tensor[i - window_size:i]\n                else:\n                    break\n        else:\n            yield tensor\n\n    def _seq(self, u_ids, i_ids, idx, max_len):\n        for i in range(len(idx)):\n            stop_idx = None if i >= len(idx) - 1 else int(idx[i + 1])\n            for s in self._win(i_ids[int(idx[i]):stop_idx], max_len):\n                yield (int(u_ids[i]), s)\n\n    def __len__(self):\n        return self.ns\n\n    def __getitem__(self, idx):\n        neg = list(self.all_items - set(self.cand[int(self.seq_users[idx])]))\n        i = random.randint(0, len(neg) - 1)\n        return (self.seq_users[idx], self.seq_items[idx], self.seq_tgt[idx],\n                neg[i])\n```"
    },
    {
      "chunk_id": "b58508951cc0_0",
      "chapter": "seqrec",
      "heading": "Load the MovieLens 100K dataset",
      "text": "Afterwards, we read and split the MovieLens 100K dataset in sequence-aware mode and load the training data with sequential dataloader implemented above.\n\n```{.python .input  n=6}\n#@tab mxnet\nTARGET_NUM, L, batch_size = 1, 5, 4096\ndf, num_users, num_items = d2l.read_data_ml100k()\ntrain_data, test_data = d2l.split_data_ml100k(df, num_users, num_items,\n                                              'seq-aware')\nusers_train, items_train, ratings_train, candidates = d2l.load_data_ml100k(\n    train_data, num_users, num_items, feedback=\"implicit\")\nusers_test, items_test, ratings_test, test_iter = d2l.load_data_ml100k(\n    test_data, num_users, num_items, feedback=\"implicit\")\ntrain_seq_data = SeqDataset(users_train, items_train, L, num_users,\n                            num_items, candidates)\ntrain_iter = gluon.data.DataLoader(train_seq_data, batch_size, True,\n                                   last_batch=\"rollover\",\n                                   num_workers=d2l.get_dataloader_workers())\ntest_seq_iter = train_seq_data.test_seq\ntrain_seq_data[0]\n```\n\nThe training data structure is shown above. The first element is the user identity, the next list indicates the last five items this user liked, and the last element is the item this user liked after the five items."
    },
    {
      "chunk_id": "5b859bbca31c_0",
      "chapter": "seqrec",
      "heading": "Train the Model",
      "text": "Now, let's train the model. We use the same setting as NeuMF, including learning rate, optimizer, and $k$, in the last section so that the results are comparable.\n\n```{.python .input  n=7}\n#@tab mxnet\ndevices = d2l.try_all_gpus()\nnet = Caser(10, num_users, num_items, L)\nnet.initialize(ctx=devices, force_reinit=True, init=mx.init.Normal(0.01))\nlr, num_epochs, wd, optimizer = 0.04, 8, 1e-5, 'adam'\nloss = d2l.BPRLoss()\ntrainer = gluon.Trainer(net.collect_params(), optimizer,\n                        {\"learning_rate\": lr, 'wd': wd})\n\n# Running takes > 1h (pending fix from MXNet)\n# d2l.train_ranking(net, train_iter, test_iter, loss, trainer, test_seq_iter, num_users, num_items, num_epochs, devices, d2l.evaluate_ranking, candidates, eval_step=1)\n```"
    },
    {
      "chunk_id": "dc7ff823f944_0",
      "chapter": "seqrec",
      "heading": "Summary",
      "text": "* Inferring a user's short-term and long-term interests can make prediction of the next item that he preferred more effectively.\n* Convolutional neural networks can be utilized to capture users' short-term interests from sequential interactions."
    },
    {
      "chunk_id": "bcee6ce73f41_0",
      "chapter": "seqrec",
      "heading": "Exercises",
      "text": "* Conduct an ablation study by removing one of the horizontal and vertical convolutional networks, which component is the more important ?\n* Vary the hyperparameter $L$. Does longer historical interactions bring higher accuracy?\n* Apart from the sequence-aware recommendation task we introduced above, there is another type of sequence-aware recommendation task called session-based recommendation :cite:`Hidasi.Karatzoglou.Baltrunas.ea.2015`. Can you explain the differences between these two tasks?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/404)\n:end_tab:"
    },
    {
      "chunk_id": "2a094131955f_0",
      "chapter": "beam-search",
      "heading": "beam-search",
      "text": "# Beam Search\n:label:`sec_beam-search`\n\nIn :numref:`sec_seq2seq`, \nwe introduced the encoder--decoder architecture,\nand the standard techniques for training them end-to-end. However, when it came to test-time prediction,\nwe mentioned only the *greedy* strategy,\nwhere we select at each time step \nthe token given the highest \npredicted probability of coming next, \nuntil, at some time step, \nwe find that we have predicted\nthe special end-of-sequence \"&lt;eos&gt;\" token.\nIn this section, we will begin \nby formalizing this *greedy search* strategy\nand identifying some problems \nthat practitioners tend to run into.\nSubsequently, we compare this strategy\nwith two alternatives:\n*exhaustive search* (illustrative but not practical)\nand *beam search* (the standard method in practice).\n\nLet's begin by setting up our mathematical notation,\nborrowing conventions from :numref:`sec_seq2seq`.\nAt any time step $t'$, the decoder outputs \npredictions representing the probability \nof each token in the vocabulary \ncoming next in the sequence \n(the likely value of $y_{t'+1}$), \nconditioned on the previous tokens\n$y_1, \\ldots, y_{t'}$ and \nthe context variable $\\mathbf{c}$,\nproduced by the encoder \nto represent the input sequence.\nTo quantify computational cost,\ndenote by $\\mathcal{Y}$\nthe output vocabulary \n(including the special end-of-sequence token \"&lt;eos&gt;\").\nLet's also specify the maximum number of tokens\nof an output sequence as $T'$.\nOur goal is to search for an ideal output from all \n$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$\npossible output sequences.\nNote that this slightly overestimates \nthe number of distinct outputs \nbecause there are no subsequent tokens\nonce the  \"&lt;eos&gt;\" token occurs.\nHowever, for our purposes, \nthis number roughly captures \nthe size of the search space."
    },
    {
      "chunk_id": "abf12afbc732_0",
      "chapter": "beam-search",
      "heading": "Greedy Search",
      "text": "Consider the simple *greedy search* strategy from :numref:`sec_seq2seq`. Here, at any time step $t'$, \nwe simply select the token \nwith the highest conditional probability\nfrom $\\mathcal{Y}$, i.e., \n\n$$y_{t'} = \\operatorname*{argmax}_{y \\in \\mathcal{Y}} P(y \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c}).$$\n\nOnce our model outputs \"&lt;eos&gt;\" \n(or we reach the maximum length $T'$)\nthe output sequence is completed. This strategy might look reasonable, \nand in fact it is not so bad! Considering how computationally undemanding it is,\nyou'd be hard pressed to get more bang for your buck. However, if we put aside efficiency for a minute,\nit might seem more reasonable to search \nfor the *most likely sequence*, \nnot the sequence of (greedily selected) *most likely tokens*. It turns out that these two objects can be quite different. The most likely sequence is the one that maximizes the expression\n$\\prod_{t'=1}^{T'} P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$. In our machine translation example,\nif the decoder truly recovered the probabilities\nof the underlying generative process, \nthen this would give us the most likely translation. Unfortunately, there is no guarantee \nthat greedy search will give us this sequence. Let's illustrate it with an example. Suppose that there are four tokens \n\"A\", \"B\", \"C\", and \"&lt;eos&gt;\" in the output dictionary. In :numref:`fig_s2s-prob1`,\nthe four numbers under each time step represent\nthe conditional probabilities of generating \"A\", \"B\", \"C\", \nand \"&lt;eos&gt;\" respectively, at that time step. ![At each time step, greedy search selects the token with the highest conditional probability.](../img/s2s-prob1.svg)\n:label:`fig_s2s-prob1`\n\nAt each time step, greedy search selects \nthe token with the highest conditional probability. Therefore, the output sequence \"A\", \"B\", \"C\", and \"&lt;eos&gt;\" \nwill be predicted (:numref:`fig_s2s-prob1`). The conditional probability of this output sequence\nis $0.5\\times0.4\\times0.4\\times0.6 = 0.048$."
    },
    {
      "chunk_id": "abf12afbc732_1",
      "chapter": "beam-search",
      "heading": "Greedy Search",
      "text": "Therefore, the output sequence \"A\", \"B\", \"C\", and \"&lt;eos&gt;\" \nwill be predicted (:numref:`fig_s2s-prob1`). The conditional probability of this output sequence\nis $0.5\\times0.4\\times0.4\\times0.6 = 0.048$. Next, let's look at another example in :numref:`fig_s2s-prob2`. Unlike in :numref:`fig_s2s-prob1`, \nat time step 2 we select the token \"C\", \nwhich has the *second* highest conditional probability. ![The four numbers under each time step represent \nthe conditional probabilities of generating \"A\", \"B\", \"C\", and \"&lt;eos&gt;\" at that time step. At time step 2, the token \"C\", which has the second highest conditional probability, \nis selected.](../img/s2s-prob2.svg)\n:label:`fig_s2s-prob2`\n\nSince the output subsequences at time steps 1 and 2, \non which time step 3 is based, \nhave changed from \"A\" and \"B\" in :numref:`fig_s2s-prob1` \nto \"A\" and \"C\" in :numref:`fig_s2s-prob2`, \nthe conditional probability of each token \nat time step 3 has also changed in :numref:`fig_s2s-prob2`. Suppose that we choose the token \"B\" at time step 3. Now time step 4 is conditional on\nthe output subsequence at the first three time steps\n\"A\", \"C\", and \"B\", \nwhich has changed from \"A\", \"B\", and \"C\" in :numref:`fig_s2s-prob1`. Therefore, the conditional probability of generating \neach token at time step 4 in :numref:`fig_s2s-prob2` \nis also different from that in :numref:`fig_s2s-prob1`. As a result, the conditional probability of the output sequence \n\"A\", \"C\", \"B\", and \"&lt;eos&gt;\" in :numref:`fig_s2s-prob2`\nis $0.5\\times0.3 \\times0.6\\times0.6=0.054$, \nwhich is greater than that of greedy search in :numref:`fig_s2s-prob1`. In this example, the output sequence \"A\", \"B\", \"C\", and \"&lt;eos&gt;\" \nobtained by the greedy search is not optimal."
    },
    {
      "chunk_id": "8f7f8a13e841_0",
      "chapter": "beam-search",
      "heading": "Exhaustive Search",
      "text": "If the goal is to obtain the most likely sequence, \nwe may consider using *exhaustive search*: \nenumerate all the possible output sequences \nwith their conditional probabilities,\nand then output the one that scores \nthe highest predicted probability.\n\n\nWhile this would certainly give us what we desire,\nit would come at a prohibitive computational cost \nof $\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$,\nexponential in the sequence length and with an enormous\nbase given by the vocabulary size.\nFor example, when $|\\mathcal{Y}|=10000$ and $T'=10$, \nboth small numbers when compared with ones in real applications, we will need to evaluate $10000^{10} = 10^{40}$ sequences, which is already beyond the capabilities of any foreseeable computers.\nOn the other hand, the computational cost of greedy search is \n$\\mathcal{O}(\\left|\\mathcal{Y}\\right|T')$: \nmiraculously cheap but far from optimal.\nFor example, when $|\\mathcal{Y}|=10000$ and $T'=10$, \nwe only need to evaluate $10000\\times10=10^5$ sequences."
    },
    {
      "chunk_id": "1eae59d96c1f_0",
      "chapter": "beam-search",
      "heading": "Beam Search",
      "text": "You could view sequence decoding strategies as lying on a spectrum,\nwith *beam search* striking a compromise \nbetween the efficiency of greedy search\nand the optimality of exhaustive search. The most straightforward version of beam search \nis characterized by a single hyperparameter,\nthe *beam size*, $k$. Let's explain this terminology. At time step 1, we select the $k$ tokens \nwith the highest predicted probabilities. Each of them will be the first token of \n$k$ candidate output sequences, respectively. At each subsequent time step, \nbased on the $k$ candidate output sequences\nat the previous time step,\nwe continue to select $k$ candidate output sequences \nwith the highest predicted probabilities \nfrom $k\\left|\\mathcal{Y}\\right|$ possible choices. ![The process of beam search (beam size $=2$; maximum length of an output sequence $=3$). The candidate output sequences are $\\mathit{A}$, $\\mathit{C}$, $\\mathit{AB}$, $\\mathit{CE}$, $\\mathit{ABD}$, and $\\mathit{CED}$.](../img/beam-search.svg)\n:label:`fig_beam-search`\n\n\n:numref:`fig_beam-search` demonstrates the \nprocess of beam search with an example. Suppose that the output vocabulary\ncontains only five elements: \n$\\mathcal{Y} = \\{A, B, C, D, E\\}$, \nwhere one of them is \u201c&lt;eos&gt;\u201d. Let the beam size be two and \nthe maximum length of an output sequence be three. At time step 1, \nsuppose that the tokens with the highest conditional probabilities \n$P(y_1 \\mid \\mathbf{c})$ are $A$ and $C$. At time step 2, for all $y_2 \\in \\mathcal{Y},$ \nwe compute \n\n$$\\begin{aligned}P(A, y_2 \\mid \\mathbf{c}) = P(A \\mid \\mathbf{c})P(y_2 \\mid A, \\mathbf{c}),\\\\ P(C, y_2 \\mid \\mathbf{c}) = P(C \\mid \\mathbf{c})P(y_2 \\mid C, \\mathbf{c}),\\end{aligned}$$  \n\nand pick the largest two among these ten values, say\n$P(A, B \\mid \\mathbf{c})$ and $P(C, E \\mid \\mathbf{c})$."
    },
    {
      "chunk_id": "1eae59d96c1f_1",
      "chapter": "beam-search",
      "heading": "Beam Search",
      "text": "Then at time step 3, for all $y_3 \\in \\mathcal{Y}$, we compute \n\n$$\\begin{aligned}P(A, B, y_3 \\mid \\mathbf{c}) = P(A, B \\mid \\mathbf{c})P(y_3 \\mid A, B, \\mathbf{c}),\\\\P(C, E, y_3 \\mid \\mathbf{c}) = P(C, E \\mid \\mathbf{c})P(y_3 \\mid C, E, \\mathbf{c}),\\end{aligned}$$ \n\nand pick the largest two among these ten values, say \n$P(A, B, D \\mid \\mathbf{c})$   and  $P(C, E, D \\mid  \\mathbf{c}).$\nAs a result, we get six candidates output sequences: \n(i) $A$; (ii) $C$; (iii) $A$, $B$; (iv) $C$, $E$; (v) $A$, $B$, $D$; and (vi) $C$, $E$, $D$. In the end, we obtain the set of final candidate output sequences \nbased on these six sequences (e.g., discard portions including and after \u201c&lt;eos&gt;\u201d). Then we choose the output sequence which maximizes the following score:\n\n$$ \\frac{1}{L^\\alpha} \\log P(y_1, \\ldots, y_{L}\\mid \\mathbf{c}) = \\frac{1}{L^\\alpha} \\sum_{t'=1}^L \\log P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c});$$\n:eqlabel:`eq_beam-search-score`\n\nhere $L$ is the length of the final candidate sequence \nand $\\alpha$ is usually set to 0.75. Since a longer sequence has more logarithmic terms \nin the summation of :eqref:`eq_beam-search-score`,\nthe term $L^\\alpha$ in the denominator penalizes\nlong sequences. The computational cost of beam search is $\\mathcal{O}(k\\left|\\mathcal{Y}\\right|T')$. This result is in between that of greedy search and that of exhaustive search. Greedy search can be treated as a special case of beam search \narising when the beam size is set to 1."
    },
    {
      "chunk_id": "3c0bf6cef93f_0",
      "chapter": "beam-search",
      "heading": "Summary",
      "text": "Sequence searching strategies include \ngreedy search, exhaustive search, and beam search.\nBeam search provides a trade-off between accuracy and \ncomputational cost via the flexible choice of the beam size."
    },
    {
      "chunk_id": "7e23f749c90f_0",
      "chapter": "beam-search",
      "heading": "Exercises",
      "text": "1. Can we treat exhaustive search as a special type of beam search? Why or why not?\n1. Apply beam search in the machine translation problem in :numref:`sec_seq2seq`. How does the beam size affect the translation results and the prediction speed?\n1. We used language modeling for generating text following  user-provided prefixes in :numref:`sec_rnn-scratch`. Which kind of search strategy does it use? Can you improve it?\n\n[Discussions](https://discuss.d2l.ai/t/338)"
    },
    {
      "chunk_id": "8c6158e8c5fb_0",
      "chapter": "bi-rnn",
      "heading": "bi-rnn",
      "text": "# Bidirectional Recurrent Neural Networks\n:label:`sec_bi_rnn`\n\nSo far, our working example of a sequence learning task has been language modeling,\nwhere we aim to predict the next token given all previous tokens in a sequence. In this scenario, we wish only to condition upon the leftward context,\nand thus the unidirectional chaining of a standard RNN seems appropriate. However, there are many other sequence learning tasks contexts\nwhere it is perfectly fine to condition the prediction at every time step\non both the leftward and the rightward context. Consider, for example, part of speech detection. Why shouldn't we take the context in both directions into account\nwhen assessing the part of speech associated with a given word? Another common task---often useful as a pretraining exercise\nprior to fine-tuning a model on an actual task of interest---is\nto mask out random tokens in a text document and then to train\na sequence model to predict the values of the missing tokens. Note that depending on what comes after the blank,\nthe likely value of the missing token changes dramatically:\n\n* I am `___`. * I am `___` hungry. * I am `___` hungry, and I can eat half a pig. In the first sentence \"happy\" seems to be a likely candidate. The words \"not\" and \"very\" seem plausible in the second sentence,\nbut \"not\" seems incompatible with the third sentences. Fortunately, a simple technique transforms any unidirectional RNN\ninto a bidirectional RNN :cite:`Schuster.Paliwal.1997`. We simply implement two unidirectional RNN layers\nchained together in opposite directions\nand acting on the same input (:numref:`fig_birnn`). For the first RNN layer,\nthe first input is $\\mathbf{x}_1$\nand the last input is $\\mathbf{x}_T$,\nbut for the second RNN layer,\nthe first input is $\\mathbf{x}_T$\nand the last input is $\\mathbf{x}_1$. To produce the output of this bidirectional RNN layer,\nwe simply concatenate together the corresponding outputs\nof the two underlying unidirectional RNN layers."
    },
    {
      "chunk_id": "8c6158e8c5fb_1",
      "chapter": "bi-rnn",
      "heading": "bi-rnn",
      "text": "To produce the output of this bidirectional RNN layer,\nwe simply concatenate together the corresponding outputs\nof the two underlying unidirectional RNN layers. ![Architecture of a bidirectional RNN.](../img/birnn.svg)\n:label:`fig_birnn`\n\n\nFormally for any time step $t$,\nwe consider a minibatch input $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n(number of examples $=n$; number of inputs in each example $=d$)\nand let the hidden layer activation function be $\\phi$. In the bidirectional architecture,\nthe forward and backward hidden states for this time step\nare $\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$\nand $\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$, respectively,\nwhere $h$ is the number of hidden units. The forward and backward hidden state updates are as follows:\n\n\n$$\n\\begin{aligned}\n\\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{\\textrm{hh}}^{(f)}  + \\mathbf{b}_\\textrm{h}^{(f)}),\\\\\n\\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{\\textrm{hh}}^{(b)}  + \\mathbf{b}_\\textrm{h}^{(b)}),\n\\end{aligned}\n$$\n\nwhere the weights $\\mathbf{W}_{\\textrm{xh}}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{\\textrm{hh}}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{\\textrm{xh}}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\textrm{ and } \\mathbf{W}_{\\textrm{hh}}^{(b)} \\in \\mathbb{R}^{h \\times h}$, and the biases $\\mathbf{b}_\\textrm{h}^{(f)} \\in \\mathbb{R}^{1 \\times h}$ and $\\mathbf{b}_\\textrm{h}^{(b)} \\in \\mathbb{R}^{1 \\times h}$ are all the model parameters. Next, we concatenate the forward and backward hidden states\n$\\overrightarrow{\\mathbf{H}}_t$ and $\\overleftarrow{\\mathbf{H}}_t$\nto obtain the hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$ for feeding into the output layer. In deep bidirectional RNNs with multiple hidden layers,\nsuch information is passed on as *input* to the next bidirectional layer."
    },
    {
      "chunk_id": "8c6158e8c5fb_2",
      "chapter": "bi-rnn",
      "heading": "bi-rnn",
      "text": "In deep bidirectional RNNs with multiple hidden layers,\nsuch information is passed on as *input* to the next bidirectional layer. Last, the output layer computes the output\n$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$ (number of outputs $=q$):\n\n$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q}.$$\n\nHere, the weight matrix $\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{2h \\times q}$\nand the bias $\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}$\nare the model parameters of the output layer. While technically, the two directions can have different numbers of hidden units,\nthis design choice is seldom made in practice. We now demonstrate a simple implementation of a bidirectional RNN. ```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import npx, np\nfrom mxnet.gluon import rnn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "2d5d17181d01_0",
      "chapter": "bi-rnn",
      "heading": "Implementation from Scratch",
      "text": "If we want to implement a bidirectional RNN from scratch, we can\ninclude two unidirectional `RNNScratch` instances\nwith separate learnable parameters.\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass BiRNNScratch(d2l.Module):\n    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n        self.num_hiddens *= 2  # The output dimension will be doubled\n```\n\n```{.python .input}\n%%tab jax\nclass BiRNNScratch(d2l.Module):\n    num_inputs: int\n    num_hiddens: int\n    sigma: float = 0.01\n\n    def setup(self):\n        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n        self.num_hiddens *= 2  # The output dimension will be doubled\n```\n\nStates of forward and backward RNNs\nare updated separately,\nwhile outputs of these two RNNs are concatenated.\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(BiRNNScratch)\ndef forward(self, inputs, Hs=None):\n    f_H, b_H = Hs if Hs is not None else (None, None)\n    f_outputs, f_H = self.f_rnn(inputs, f_H)\n    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n    outputs = [d2l.concat((f, b), -1) for f, b in zip(\n        f_outputs, reversed(b_outputs))]\n    return outputs, (f_H, b_H)\n```"
    },
    {
      "chunk_id": "badf122f8ad4_0",
      "chapter": "bi-rnn",
      "heading": "Concise Implementation",
      "text": ":begin_tab:`pytorch, mxnet, tensorflow`\nUsing the high-level APIs,\nwe can implement bidirectional RNNs more concisely.\nHere we take a GRU model as an example.\n:end_tab:\n\n:begin_tab:`jax`\nFlax API does not offer RNN layers and hence there is no\nnotion of any `bidirectional` argument. One needs to manually\nreverse the inputs as shown in the scratch implementation,\nif a bidirectional layer is needed.\n:end_tab:\n\n```{.python .input}\n%%tab mxnet, pytorch\nclass BiGRU(d2l.RNN):\n    def __init__(self, num_inputs, num_hiddens):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.rnn = rnn.GRU(num_hiddens, bidirectional=True)\n        if tab.selected('pytorch'):\n            self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n        self.num_hiddens *= 2\n```"
    },
    {
      "chunk_id": "96f53c682a99_0",
      "chapter": "bi-rnn",
      "heading": "Summary",
      "text": "In bidirectional RNNs, the hidden state for each time step is simultaneously determined by the data prior to and after the current time step. Bidirectional RNNs are mostly useful for sequence encoding and the estimation of observations given bidirectional context. Bidirectional RNNs are very costly to train due to long gradient chains."
    },
    {
      "chunk_id": "a69d2fb4b9d2_0",
      "chapter": "bi-rnn",
      "heading": "Exercises",
      "text": "1. If the different directions use a different number of hidden units, how will the shape of $\\mathbf{H}_t$ change?\n1. Design a bidirectional RNN with multiple hidden layers.\n1. Polysemy is common in natural languages. For example, the word \"bank\" has different meanings in contexts \u201ci went to the bank to deposit cash\u201d and \u201ci went to the bank to sit down\u201d. How can we design a neural network model such that given a context sequence and a word, a vector representation of the word in the correct context will be returned? What type of neural architectures is preferred for handling polysemy?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/339)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1059)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18019)\n:end_tab:"
    },
    {
      "chunk_id": "e93f6f498269_0",
      "chapter": "deep-rnn",
      "heading": "deep-rnn",
      "text": "# Deep Recurrent Neural Networks\n\n:label:`sec_deep_rnn`\n\nUp until now, we have focused on defining networks \nconsisting of a sequence input, \na single hidden RNN layer,\nand an output layer. Despite having just one hidden layer \nbetween the input at any time step\nand the corresponding output,\nthere is a sense in which these networks are deep. Inputs from the first time step can influence\nthe outputs at the final time step $T$ \n(often 100s or 1000s of steps later). These inputs pass through $T$ applications\nof the recurrent layer before reaching \nthe final output. However, we often also wish to retain the ability\nto express complex relationships \nbetween the inputs at a given time step\nand the outputs at that same time step. Thus we often construct RNNs that are deep\nnot only in the time direction \nbut also in the input-to-output direction. This is precisely the notion of depth\nthat we have already encountered \nin our development of MLPs\nand deep CNNs. The standard method for building this sort of deep RNN \nis strikingly simple: we stack the RNNs on top of each other. Given a sequence of length $T$, the first RNN produces \na sequence of outputs, also of length $T$. These, in turn, constitute the inputs to the next RNN layer. In this short section, we illustrate this design pattern\nand present a simple example for how to code up such stacked RNNs. Below, in :numref:`fig_deep_rnn`, we illustrate\na deep RNN with $L$ hidden layers. Each hidden state operates on a sequential input\nand produces a sequential output. Moreover, any RNN cell (white box in :numref:`fig_deep_rnn`) at each time step\ndepends on both the same layer's \nvalue at the previous time step\nand the previous layer's value \nat the same time step. ![Architecture of a deep RNN.](../img/deep-rnn.svg)\n:label:`fig_deep_rnn`\n\nFormally, suppose that we have a minibatch input\n$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n(number of examples $=n$; number of inputs in each example $=d$) at time step $t$."
    },
    {
      "chunk_id": "e93f6f498269_1",
      "chapter": "deep-rnn",
      "heading": "deep-rnn",
      "text": "![Architecture of a deep RNN.](../img/deep-rnn.svg)\n:label:`fig_deep_rnn`\n\nFormally, suppose that we have a minibatch input\n$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n(number of examples $=n$; number of inputs in each example $=d$) at time step $t$. At the same time step, \nlet the hidden state of the $l^\\textrm{th}$ hidden layer ($l=1,\\ldots,L$) be $\\mathbf{H}_t^{(l)} \\in \\mathbb{R}^{n \\times h}$ \n(number of hidden units $=h$)\nand the output layer variable be \n$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$ \n(number of outputs: $q$). Setting $\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$,\nthe hidden state of\nthe $l^\\textrm{th}$ hidden layer\nthat uses the activation function $\\phi_l$\nis calculated as follows:\n\n$$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{\\textrm{xh}}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{\\textrm{hh}}^{(l)}  + \\mathbf{b}_\\textrm{h}^{(l)}),$$\n:eqlabel:`eq_deep_rnn_H`\n\nwhere the weights $\\mathbf{W}_{\\textrm{xh}}^{(l)} \\in \\mathbb{R}^{h \\times h}$ and $\\mathbf{W}_{\\textrm{hh}}^{(l)} \\in \\mathbb{R}^{h \\times h}$, together with\nthe bias $\\mathbf{b}_\\textrm{h}^{(l)} \\in \\mathbb{R}^{1 \\times h}$, \nare the model parameters of the $l^\\textrm{th}$ hidden layer. At the end, the calculation of the output layer \nis only based on the hidden state \nof the final $L^\\textrm{th}$ hidden layer:\n\n$$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q},$$\n\nwhere the weight $\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{h \\times q}$ \nand the bias $\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}$ \nare the model parameters of the output layer. Just as with MLPs, the number of hidden layers $L$ \nand the number of hidden units $h$ are hyperparameters\nthat we can tune. Common RNN layer widths ($h$) are in the range $(64, 2056)$,\nand common depths ($L$) are in the range $(1, 8)$. In addition, we can easily get a deep-gated RNN\nby replacing the hidden state computation in :eqref:`eq_deep_rnn_H`\nwith that from an LSTM or a GRU."
    },
    {
      "chunk_id": "e93f6f498269_2",
      "chapter": "deep-rnn",
      "heading": "deep-rnn",
      "text": "In addition, we can easily get a deep-gated RNN\nby replacing the hidden state computation in :eqref:`eq_deep_rnn_H`\nwith that from an LSTM or a GRU. ```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import rnn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "3a771eb09405_0",
      "chapter": "deep-rnn",
      "heading": "Implementation from Scratch",
      "text": "To implement a multilayer RNN from scratch,\nwe can treat each layer as an `RNNScratch` instance\nwith its own learnable parameters. ```{.python .input}\n%%tab mxnet, tensorflow\nclass StackedRNNScratch(d2l.Module):\n    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.rnns = [d2l.RNNScratch(num_inputs if i==0 else num_hiddens,\n                                    num_hiddens, sigma)\n                     for i in range(num_layers)]\n```\n\n```{.python .input}\n%%tab pytorch\nclass StackedRNNScratch(d2l.Module):\n    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.rnns = nn.Sequential(*[d2l.RNNScratch(\n            num_inputs if i==0 else num_hiddens, num_hiddens, sigma)\n                                    for i in range(num_layers)])\n```\n\n```{.python .input}\n%%tab jax\nclass StackedRNNScratch(d2l.Module):\n    num_inputs: int\n    num_hiddens: int\n    num_layers: int\n    sigma: float = 0.01\n\n    def setup(self):\n        self.rnns = [d2l.RNNScratch(self.num_inputs if i==0 else self.num_hiddens,\n                                    self.num_hiddens, self.sigma)\n                     for i in range(self.num_layers)]\n```\n\nThe multilayer forward computation\nsimply performs forward computation\nlayer by layer. ```{.python .input}\n%%tab all\n@d2l.add_to_class(StackedRNNScratch)\ndef forward(self, inputs, Hs=None):\n    outputs = inputs\n    if Hs is None: Hs = [None] * self.num_layers\n    for i in range(self.num_layers):\n        outputs, Hs[i] = self.rnns[i](outputs, Hs[i])\n        outputs = d2l.stack(outputs, 0)\n    return outputs, Hs\n```\n\nAs an example, we train a deep GRU model on\n*The Time Machine* dataset (same as in :numref:`sec_rnn-scratch`). To keep things simple we set the number of layers to 2."
    },
    {
      "chunk_id": "3a771eb09405_1",
      "chapter": "deep-rnn",
      "heading": "Implementation from Scratch",
      "text": "To keep things simple we set the number of layers to 2. ```{.python .input}\n%%tab all\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n                                  num_hiddens=32, num_layers=2)\n    model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n                                  num_hiddens=32, num_layers=2)\n        model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "23f25bcef596_0",
      "chapter": "deep-rnn",
      "heading": "Concise Implementation",
      "text": ":begin_tab:`pytorch, mxnet, tensorflow`\nFortunately many of the logistical details required\nto implement multiple layers of an RNN \nare readily available in high-level APIs. Our concise implementation will use such built-in functionalities. The code generalizes the one we used previously in :numref:`sec_gru`,\nletting us specify the number of layers explicitly \nrather than picking the default of only one layer. :end_tab:\n\n:begin_tab:`jax`\nFlax takes a minimalistic approach while implementing\nRNNs. Defining the number of layers in an RNN or combining it with dropout\nis not available out of the box. Our concise implementation will use all built-in functionalities and\nadd `num_layers` and `dropout` features on top. The code generalizes the one we used previously in :numref:`sec_gru`,\nallowing specification of the number of layers explicitly\nrather than picking the default of a single layer."
    },
    {
      "chunk_id": "23f25bcef596_1",
      "chapter": "deep-rnn",
      "heading": "Concise Implementation",
      "text": "The code generalizes the one we used previously in :numref:`sec_gru`,\nallowing specification of the number of layers explicitly\nrather than picking the default of a single layer. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nclass GRU(d2l.RNN):  #@save\n    \"\"\"The multilayer GRU model.\"\"\"\n    def __init__(self, num_hiddens, num_layers, dropout=0):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n```\n\n```{.python .input}\n%%tab pytorch\nclass GRU(d2l.RNN):  #@save\n    \"\"\"The multilayer GRU model.\"\"\"\n    def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,\n                          dropout=dropout)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass GRU(d2l.RNN):  #@save\n    \"\"\"The multilayer GRU model.\"\"\"\n    def __init__(self, num_hiddens, num_layers, dropout=0):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        gru_cells = [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)\n                     for _ in range(num_layers)]\n        self.rnn = tf.keras.layers.RNN(gru_cells, return_sequences=True,\n                                       return_state=True, time_major=True)\n\n    def forward(self, X, state=None):\n        outputs, *state = self.rnn(X, state)\n        return outputs, state\n```\n\n```{.python .input}\n%%tab jax\nclass GRU(d2l.RNN):  #@save\n    \"\"\"The multilayer GRU model.\"\"\"\n    num_hiddens: int\n    num_layers: int\n    dropout: float = 0\n\n    @nn.compact\n    def __call__(self, X, state=None, training=False):\n        outputs = X\n        new_state = []\n        if state is None:\n            batch_size = X.shape[1]\n            state = [nn.GRUCell.initialize_carry(jax.random.PRNGKey(0),\n                    (batch_size,), self.num_hiddens)] * self.num_layers\n\n        GRU = nn.scan(nn.GRUCell, variable_broadcast=\"params\",\n                      in_axes=0, out_axes=0, split_rngs={\"params\": False})\n\n        # Introduce a dropout layer after every GRU layer except last\n        for i in range(self.num_layers - 1):\n            layer_i_state, X = GRU()(state[i], outputs)\n            new_state.append(layer_i_state)\n            X = nn.Dropout(self.dropout, deterministic=not training)(X)\n\n        # Final GRU layer without dropout\n        out_state, X = GRU()(state[-1], X)\n        new_state.append(out_state)\n        return X, jnp.array(new_state)\n```\n\nThe architectural decisions such as choosing hyperparameters \nare very similar to those of :numref:`sec_gru`."
    },
    {
      "chunk_id": "23f25bcef596_2",
      "chapter": "deep-rnn",
      "heading": "Concise Implementation",
      "text": "We pick the same number of inputs and outputs \nas we have distinct tokens, i.e., `vocab_size`. The number of hidden units is still 32. The only difference is that we now \n(**select a nontrivial number of hidden layers \nby specifying the value of `num_layers`.**)\n\n```{.python .input}\n%%tab mxnet\ngru = GRU(num_hiddens=32, num_layers=2)\nmodel = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n\n# Running takes > 1h (pending fix from MXNet)\n# trainer.fit(model, data)\n# model.predict('it has', 20, data.vocab, d2l.try_gpu())\n```\n\n```{.python .input}\n%%tab pytorch, tensorflow, jax\nif tab.selected('tensorflow', 'jax'):\n    gru = GRU(num_hiddens=32, num_layers=2)\nif tab.selected('pytorch'):\n    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)\nif tab.selected('pytorch', 'jax'):\n    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab pytorch\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n```\n\n```{.python .input}\n%%tab tensorflow\nmodel.predict('it has', 20, data.vocab)\n```\n\n```{.python .input}\n%%tab jax\nmodel.predict('it has', 20, data.vocab, trainer.state.params)\n```"
    },
    {
      "chunk_id": "9e1f6cf32748_0",
      "chapter": "deep-rnn",
      "heading": "Summary",
      "text": "In deep RNNs, the hidden state information is passed \nto the next time step of the current layer \nand the current time step of the next layer.\nThere exist many different flavors of deep RNNs, such as LSTMs, GRUs, or vanilla RNNs. \nConveniently, these models are all available \nas parts of the high-level APIs of deep learning frameworks.\nInitialization of models requires care. \nOverall, deep RNNs require considerable amount of work \n(such as learning rate and clipping) \nto ensure proper convergence."
    },
    {
      "chunk_id": "3520cdc3773d_0",
      "chapter": "deep-rnn",
      "heading": "Exercises",
      "text": "1. Replace the GRU by an LSTM and compare the accuracy and training speed.\n1. Increase the training data to include multiple books. How low can you go on the perplexity scale?\n1. Would you want to combine sources of different authors when modeling text? Why is this a good idea? What could go wrong?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/340)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1058)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3862)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18018)\n:end_tab:"
    },
    {
      "chunk_id": "4d2e1293481a_0",
      "chapter": "encoder-decoder",
      "heading": "encoder-decoder",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n# The Encoder--Decoder Architecture\n:label:`sec_encoder-decoder`\n\nIn general sequence-to-sequence problems\nlike machine translation\n(:numref:`sec_machine_translation`),\ninputs and outputs are of varying lengths\nthat are unaligned.\nThe standard approach to handling this sort of data\nis to design an *encoder--decoder* architecture (:numref:`fig_encoder_decoder`)\nconsisting of two major components:\nan *encoder* that takes a variable-length sequence as input,\nand a *decoder* that acts as a conditional language model,\ntaking in the encoded input\nand the leftwards context of the target sequence\nand predicting the subsequent token in the target sequence.\n\n\n![The encoder--decoder architecture.](../img/encoder-decoder.svg)\n:label:`fig_encoder_decoder`\n\nLet's take machine translation from English to French as an example.\nGiven an input sequence in English:\n\"They\", \"are\", \"watching\", \".\",\nthis encoder--decoder architecture\nfirst encodes the variable-length input into a state,\nthen decodes the state\nto generate the translated sequence,\ntoken by token, as output:\n\"Ils\", \"regardent\", \".\".\nSince the encoder--decoder architecture\nforms the basis of different sequence-to-sequence models\nin subsequent sections,\nthis section will convert this architecture\ninto an interface that will be implemented later.\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet.gluon import nn\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\n```"
    },
    {
      "chunk_id": "c810ac2ae48b_0",
      "chapter": "encoder-decoder",
      "heading": "(**Encoder**)",
      "text": "In the encoder interface,\nwe just specify that\nthe encoder takes variable-length sequences as input `X`.\nThe implementation will be provided\nby any model that inherits this base `Encoder` class.\n\n```{.python .input}\n%%tab mxnet\nclass Encoder(nn.Block):  #@save\n    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def forward(self, X, *args):\n        raise NotImplementedError\n```\n\n```{.python .input}\n%%tab pytorch\nclass Encoder(nn.Module):  #@save\n    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def forward(self, X, *args):\n        raise NotImplementedError\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Encoder(tf.keras.layers.Layer):  #@save\n    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def call(self, X, *args):\n        raise NotImplementedError\n```\n\n```{.python .input}\n%%tab jax\nclass Encoder(nn.Module):  #@save\n    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n    def setup(self):\n        raise NotImplementedError\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def __call__(self, X, *args):\n        raise NotImplementedError\n```"
    },
    {
      "chunk_id": "f4aabc02f1bf_0",
      "chapter": "encoder-decoder",
      "heading": "[**Decoder**]",
      "text": "In the following decoder interface,\nwe add an additional `init_state` method\nto convert the encoder output (`enc_all_outputs`)\ninto the encoded state. Note that this step\nmay require extra inputs,\nsuch as the valid length of the input,\nwhich was explained\nin :numref:`sec_machine_translation`. To generate a variable-length sequence token by token,\nevery time the decoder may map an input\n(e.g., the generated token at the previous time step)\nand the encoded state\ninto an output token at the current time step."
    },
    {
      "chunk_id": "f4aabc02f1bf_1",
      "chapter": "encoder-decoder",
      "heading": "[**Decoder**]",
      "text": "To generate a variable-length sequence token by token,\nevery time the decoder may map an input\n(e.g., the generated token at the previous time step)\nand the encoded state\ninto an output token at the current time step. ```{.python .input}\n%%tab mxnet\nclass Decoder(nn.Block):  #@save\n    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def init_state(self, enc_all_outputs, *args):\n        raise NotImplementedError\n\n    def forward(self, X, state):\n        raise NotImplementedError\n```\n\n```{.python .input}\n%%tab pytorch\nclass Decoder(nn.Module):  #@save\n    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def init_state(self, enc_all_outputs, *args):\n        raise NotImplementedError\n\n    def forward(self, X, state):\n        raise NotImplementedError\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Decoder(tf.keras.layers.Layer):  #@save\n    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def init_state(self, enc_all_outputs, *args):\n        raise NotImplementedError\n\n    def call(self, X, state):\n        raise NotImplementedError\n```\n\n```{.python .input}\n%%tab jax\nclass Decoder(nn.Module):  #@save\n    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n    def setup(self):\n        raise NotImplementedError\n\n    # Later there can be additional arguments (e.g., length excluding padding)\n    def init_state(self, enc_all_outputs, *args):\n        raise NotImplementedError\n\n    def __call__(self, X, state):\n        raise NotImplementedError\n```"
    },
    {
      "chunk_id": "bbf5f3b8ff66_0",
      "chapter": "encoder-decoder",
      "heading": "[**Putting the Encoder and Decoder Together**]",
      "text": "In the forward propagation,\nthe output of the encoder\nis used to produce the encoded state,\nand this state will be further used\nby the decoder as one of its input.\n\n```{.python .input}\n%%tab mxnet, pytorch\nclass EncoderDecoder(d2l.Classifier):  #@save\n    \"\"\"The base class for the encoder--decoder architecture.\"\"\"\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, enc_X, dec_X, *args):\n        enc_all_outputs = self.encoder(enc_X, *args)\n        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n        # Return decoder output only\n        return self.decoder(dec_X, dec_state)[0]\n```\n\n```{.python .input}\n%%tab tensorflow\nclass EncoderDecoder(d2l.Classifier):  #@save\n    \"\"\"The base class for the encoder--decoder architecture.\"\"\"\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def call(self, enc_X, dec_X, *args):\n        enc_all_outputs = self.encoder(enc_X, *args, training=True)\n        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n        # Return decoder output only\n        return self.decoder(dec_X, dec_state, training=True)[0]\n```\n\n```{.python .input}\n%%tab jax\nclass EncoderDecoder(d2l.Classifier):  #@save\n    \"\"\"The base class for the encoder--decoder architecture.\"\"\"\n    encoder: nn.Module\n    decoder: nn.Module\n    training: bool\n\n    def __call__(self, enc_X, dec_X, *args):\n        enc_all_outputs = self.encoder(enc_X, *args, training=self.training)\n        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n        # Return decoder output only\n        return self.decoder(dec_X, dec_state, training=self.training)[0]\n```\n\nIn the next section,\nwe will see how to apply RNNs to design\nsequence-to-sequence models based on\nthis encoder--decoder architecture."
    },
    {
      "chunk_id": "1626d480bf40_0",
      "chapter": "encoder-decoder",
      "heading": "Summary",
      "text": "Encoder-decoder architectures\ncan handle inputs and outputs\nthat both consist of variable-length sequences\nand thus are suitable for sequence-to-sequence problems\nsuch as machine translation.\nThe encoder takes a variable-length sequence as input\nand transforms it into a state with a fixed shape.\nThe decoder maps the encoded state of a fixed shape\nto a variable-length sequence."
    },
    {
      "chunk_id": "d49425dae482_0",
      "chapter": "encoder-decoder",
      "heading": "Exercises",
      "text": "1. Suppose that we use neural networks to implement the encoder--decoder architecture. Do the encoder and the decoder have to be the same type of neural network?\n1. Besides machine translation, can you think of another application where the encoder--decoder architecture can be applied?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/341)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1061)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3864)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18021)\n:end_tab:"
    },
    {
      "chunk_id": "cb410df0fe94_0",
      "chapter": "gru",
      "heading": "gru",
      "text": "# Gated Recurrent Units (GRU)\n:label:`sec_gru`\n\n\nAs RNNs and particularly the LSTM architecture (:numref:`sec_lstm`)\nrapidly gained popularity during the 2010s,\na number of researchers began to experiment \nwith simplified architectures in hopes \nof retaining the key idea of incorporating\nan internal state and multiplicative gating mechanisms\nbut with the aim of speeding up computation.\nThe gated recurrent unit (GRU) :cite:`Cho.Van-Merrienboer.Bahdanau.ea.2014` \noffered a streamlined version of the LSTM memory cell\nthat often achieves comparable performance\nbut with the advantage of being faster \nto compute :cite:`Chung.Gulcehre.Cho.ea.2014`.\n\n```{.python .input  n=5}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n```{.python .input  n=6}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import rnn\nnpx.set_np()\n```\n\n```{.python .input  n=7}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input  n=8}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "9f16ecd19bf2_0",
      "chapter": "gru",
      "heading": "Reset Gate and Update Gate",
      "text": "Here, the LSTM's three gates are replaced by two:\nthe *reset gate* and the *update gate*.\nAs with LSTMs, these gates are given sigmoid activations,\nforcing their values to lie in the interval $(0, 1)$.\nIntuitively, the reset gate controls how much of the previous state \nwe might still want to remember.\nLikewise, an update gate would allow us to control \nhow much of the new state is just a copy of the old one.\n:numref:`fig_gru_1` illustrates the inputs for both\nthe reset and update gates in a GRU, \ngiven the input of the current time step\nand the hidden state of the previous time step.\nThe outputs of the gates are given \nby two fully connected layers\nwith a sigmoid activation function.\n\n![Computing the reset gate and the update gate in a GRU model.](../img/gru-1.svg)\n:label:`fig_gru_1`\n\nMathematically, for a given time step $t$,\nsuppose that the input is a minibatch\n$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n(number of examples $=n$; number of inputs $=d$)\nand the hidden state of the previous time step \nis $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$ \n(number of hidden units $=h$). \nThen the reset gate $\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$ \nand update gate $\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$ are computed as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xr}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hr}} + \\mathbf{b}_\\textrm{r}),\\\\\n\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xz}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hz}} + \\mathbf{b}_\\textrm{z}),\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}_{\\textrm{xr}}, \\mathbf{W}_{\\textrm{xz}} \\in \\mathbb{R}^{d \\times h}$ \nand $\\mathbf{W}_{\\textrm{hr}}, \\mathbf{W}_{\\textrm{hz}} \\in \\mathbb{R}^{h \\times h}$ \nare weight parameters and $\\mathbf{b}_\\textrm{r}, \\mathbf{b}_\\textrm{z} \\in \\mathbb{R}^{1 \\times h}$ \nare bias parameters."
    },
    {
      "chunk_id": "d41a3cb6fc5e_0",
      "chapter": "gru",
      "heading": "Candidate Hidden State",
      "text": "Next, we integrate the reset gate $\\mathbf{R}_t$ \nwith the regular updating mechanism\nin :eqref:`rnn_h_with_state`,\nleading to the following\n*candidate hidden state*\n$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$ at time step $t$:\n\n$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{\\textrm{hh}} + \\mathbf{b}_\\textrm{h}),$$\n:eqlabel:`gru_tilde_H`\n\nwhere $\\mathbf{W}_{\\textrm{xh}} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{\\textrm{hh}} \\in \\mathbb{R}^{h \\times h}$\nare weight parameters,\n$\\mathbf{b}_\\textrm{h} \\in \\mathbb{R}^{1 \\times h}$\nis the bias,\nand the symbol $\\odot$ is the Hadamard (elementwise) product operator.\nHere we use a tanh activation function.\n\nThe result is a *candidate*, since we still need \nto incorporate the action of the update gate.\nComparing with :eqref:`rnn_h_with_state`,\nthe influence of the previous states\ncan now be reduced with the\nelementwise multiplication of\n$\\mathbf{R}_t$ and $\\mathbf{H}_{t-1}$\nin :eqref:`gru_tilde_H`.\nWhenever the entries in the reset gate $\\mathbf{R}_t$ are close to 1, \nwe recover a vanilla RNN such as that in :eqref:`rnn_h_with_state`.\nFor all entries of the reset gate $\\mathbf{R}_t$ that are close to 0, \nthe candidate hidden state is the result of an MLP with $\\mathbf{X}_t$ as input. \nAny pre-existing hidden state is thus *reset* to defaults.\n\n:numref:`fig_gru_2` illustrates the computational flow after applying the reset gate.\n\n![Computing the candidate hidden state in a GRU model.](../img/gru-2.svg)\n:label:`fig_gru_2`"
    },
    {
      "chunk_id": "77227a6ed182_0",
      "chapter": "gru",
      "heading": "Hidden State",
      "text": "Finally, we need to incorporate the effect of the update gate $\\mathbf{Z}_t$.\nThis determines the extent to which the new hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$ \nmatches the old state $\\mathbf{H}_{t-1}$ compared with how much \nit resembles the new candidate state $\\tilde{\\mathbf{H}}_t$.\nThe update gate $\\mathbf{Z}_t$ can be used for this purpose, \nsimply by taking elementwise convex combinations \nof $\\mathbf{H}_{t-1}$ and $\\tilde{\\mathbf{H}}_t$.\nThis leads to the final update equation for the GRU:\n\n$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n\n\nWhenever the update gate $\\mathbf{Z}_t$ is close to 1,\nwe simply retain the old state. \nIn this case the information from $\\mathbf{X}_t$ is ignored, \neffectively skipping time step $t$ in the dependency chain. \nBy contrast, whenever $\\mathbf{Z}_t$ is close to 0,\nthe new latent state $\\mathbf{H}_t$ approaches the candidate latent state $\\tilde{\\mathbf{H}}_t$. \n:numref:`fig_gru_3` shows the computational flow after the update gate is in action.\n\n![Computing the hidden state in a GRU model.](../img/gru-3.svg)\n:label:`fig_gru_3`\n\n\nIn summary, GRUs have the following two distinguishing features:\n\n* Reset gates help capture short-term dependencies in sequences.\n* Update gates help capture long-term dependencies in sequences."
    },
    {
      "chunk_id": "494ef7a34ec9_0",
      "chapter": "gru",
      "heading": "Implementation from Scratch",
      "text": "To gain a better understanding of the GRU model, let's implement it from scratch."
    },
    {
      "chunk_id": "38adc08c1c99_0",
      "chapter": "gru",
      "heading": "(**Initializing Model Parameters**)",
      "text": "The first step is to initialize the model parameters. We draw the weights from a Gaussian distribution\nwith standard deviation to be `sigma` and set the bias to 0. The hyperparameter `num_hiddens` defines the number of hidden units. We instantiate all weights and biases relating to the update gate, \nthe reset gate, and the candidate hidden state."
    },
    {
      "chunk_id": "38adc08c1c99_1",
      "chapter": "gru",
      "heading": "(**Initializing Model Parameters**)",
      "text": "The hyperparameter `num_hiddens` defines the number of hidden units. We instantiate all weights and biases relating to the update gate, \nthe reset gate, and the candidate hidden state. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass GRUScratch(d2l.Module):\n    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        if tab.selected('mxnet'):\n            init_weight = lambda *shape: d2l.randn(*shape) * sigma\n            triple = lambda: (init_weight(num_inputs, num_hiddens),\n                              init_weight(num_hiddens, num_hiddens),\n                              d2l.zeros(num_hiddens))            \n        if tab.selected('pytorch'):\n            init_weight = lambda *shape: nn.Parameter(d2l.randn(*shape) * sigma)\n            triple = lambda: (init_weight(num_inputs, num_hiddens),\n                              init_weight(num_hiddens, num_hiddens),\n                              nn.Parameter(d2l.zeros(num_hiddens)))\n        if tab.selected('tensorflow'):\n            init_weight = lambda *shape: tf.Variable(d2l.normal(shape) * sigma)\n            triple = lambda: (init_weight(num_inputs, num_hiddens),\n                              init_weight(num_hiddens, num_hiddens),\n                              tf.Variable(d2l.zeros(num_hiddens)))            \n            \n        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state        \n```\n\n```{.python .input}\n%%tab jax\nclass GRUScratch(d2l.Module):\n    num_inputs: int\n    num_hiddens: int\n    sigma: float = 0.01\n\n    def setup(self):\n        init_weight = lambda name, shape: self.param(name,\n                                                     nn.initializers.normal(self.sigma),\n                                                     shape)\n        triple = lambda name : (\n            init_weight(f'W_x{name}', (self.num_inputs, self.num_hiddens)),\n            init_weight(f'W_h{name}', (self.num_hiddens, self.num_hiddens)),\n            self.param(f'b_{name}', nn.initializers.zeros, (self.num_hiddens)))\n\n        self.W_xz, self.W_hz, self.b_z = triple('z')  # Update gate\n        self.W_xr, self.W_hr, self.b_r = triple('r')  # Reset gate\n        self.W_xh, self.W_hh, self.b_h = triple('h')  # Candidate hidden state\n```"
    },
    {
      "chunk_id": "2b1ee8057766_0",
      "chapter": "gru",
      "heading": "Defining the Model",
      "text": "Now we are ready to [**define the GRU forward computation**]. Its structure is the same as that of the basic RNN cell, \nexcept that the update equations are more complex."
    },
    {
      "chunk_id": "2b1ee8057766_1",
      "chapter": "gru",
      "heading": "Defining the Model",
      "text": "Now we are ready to [**define the GRU forward computation**]. Its structure is the same as that of the basic RNN cell, \nexcept that the update equations are more complex. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(GRUScratch)\ndef forward(self, inputs, H=None):\n    if H is None:\n        # Initial state with shape: (batch_size, num_hiddens)\n        if tab.selected('mxnet'):\n            H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                          ctx=inputs.ctx)\n        if tab.selected('pytorch'):\n            H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                          device=inputs.device)\n        if tab.selected('tensorflow'):\n            H = d2l.zeros((inputs.shape[1], self.num_hiddens))\n    outputs = []\n    for X in inputs:\n        Z = d2l.sigmoid(d2l.matmul(X, self.W_xz) +\n                        d2l.matmul(H, self.W_hz) + self.b_z)\n        R = d2l.sigmoid(d2l.matmul(X, self.W_xr) + \n                        d2l.matmul(H, self.W_hr) + self.b_r)\n        H_tilde = d2l.tanh(d2l.matmul(X, self.W_xh) + \n                           d2l.matmul(R * H, self.W_hh) + self.b_h)\n        H = Z * H + (1 - Z) * H_tilde\n        outputs.append(H)\n    return outputs, H\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(GRUScratch)\ndef forward(self, inputs, H=None):\n    # Use lax.scan primitive instead of looping over the\n    # inputs, since scan saves time in jit compilation\n    def scan_fn(H, X):\n        Z = d2l.sigmoid(d2l.matmul(X, self.W_xz) + d2l.matmul(H, self.W_hz) +\n                        self.b_z)\n        R = d2l.sigmoid(d2l.matmul(X, self.W_xr) +\n                        d2l.matmul(H, self.W_hr) + self.b_r)\n        H_tilde = d2l.tanh(d2l.matmul(X, self.W_xh) +\n                           d2l.matmul(R * H, self.W_hh) + self.b_h)\n        H = Z * H + (1 - Z) * H_tilde\n        return H, H  # return carry, y\n\n    if H is None:\n        batch_size = inputs.shape[1]\n        carry = jnp.zeros((batch_size, self.num_hiddens))\n    else:\n        carry = H\n\n    # scan takes the scan_fn, initial carry state, xs with leading axis to be scanned\n    carry, outputs = jax.lax.scan(scan_fn, carry, inputs)\n    return outputs, carry\n```"
    },
    {
      "chunk_id": "b2f2b839423a_0",
      "chapter": "gru",
      "heading": "Training",
      "text": "[**Training**] a language model on *The Time Machine* dataset\nworks in exactly the same manner as in :numref:`sec_rnn-scratch`.\n\n```{.python .input}\n%%tab all\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n    model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)\n    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n        model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)\n    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "a3815e397d8e_0",
      "chapter": "gru",
      "heading": "[**Concise Implementation**]",
      "text": "In high-level APIs, we can directly instantiate a GRU model. This encapsulates all the configuration detail that we made explicit above. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass GRU(d2l.RNN):\n    def __init__(self, num_inputs, num_hiddens):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.rnn = rnn.GRU(num_hiddens)\n        if tab.selected('pytorch'):\n            self.rnn = nn.GRU(num_inputs, num_hiddens)\n        if tab.selected('tensorflow'):\n            self.rnn = tf.keras.layers.GRU(num_hiddens, return_sequences=True, \n                                           return_state=True)\n```\n\n```{.python .input}\n%%tab jax\nclass GRU(d2l.RNN):\n    num_hiddens: int\n\n    @nn.compact\n    def __call__(self, inputs, H=None, training=False):\n        if H is None:\n            batch_size = inputs.shape[1]\n            H = nn.GRUCell.initialize_carry(jax.random.PRNGKey(0),\n                                            (batch_size,), self.num_hiddens)\n\n        GRU = nn.scan(nn.GRUCell, variable_broadcast=\"params\",\n                      in_axes=0, out_axes=0, split_rngs={\"params\": False})\n\n        H, outputs = GRU()(H, inputs)\n        return outputs, H\n```\n\nThe code is significantly faster in training as it uses compiled operators \nrather than Python. ```{.python .input}\n%%tab all\nif tab.selected('mxnet', 'pytorch', 'tensorflow'):\n    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\nif tab.selected('jax'):\n    gru = GRU(num_hiddens=32)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)\ntrainer.fit(model, data)\n```\n\nAfter training, we print out the perplexity on the training set\nand the predicted sequence following the provided prefix."
    },
    {
      "chunk_id": "a3815e397d8e_1",
      "chapter": "gru",
      "heading": "[**Concise Implementation**]",
      "text": "```{.python .input}\n%%tab mxnet, pytorch\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n```\n\n```{.python .input}\n%%tab tensorflow\nmodel.predict('it has', 20, data.vocab)\n```\n\n```{.python .input}\n%%tab jax\nmodel.predict('it has', 20, data.vocab, trainer.state.params)\n```"
    },
    {
      "chunk_id": "31211664a881_0",
      "chapter": "gru",
      "heading": "Summary",
      "text": "Compared with LSTMs, GRUs achieve similar performance but tend to be lighter computationally.\nGenerally, compared with simple RNNs, gated RNNS, just like LSTMs and GRUs,\ncan better capture dependencies for sequences with large time step distances.\nGRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. \nThey can also skip subsequences by turning on the update gate."
    },
    {
      "chunk_id": "07dc46a1da14_0",
      "chapter": "gru",
      "heading": "Exercises",
      "text": "1. Assume that we only want to use the input at time step $t'$ to predict the output at time step $t > t'$. What are the best values for the reset and update gates for each time step?\n1. Adjust the hyperparameters and analyze their influence on running time, perplexity, and the output sequence.\n1. Compare runtime, perplexity, and the output strings for `rnn.RNN` and `rnn.GRU` implementations with each other.\n1. What happens if you implement only parts of a GRU, e.g., with only a reset gate or only an update gate?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/342)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1056)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3860)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18017)\n:end_tab:"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Modern Recurrent Neural Networks\n:label:`chap_modern_rnn`\n\nThe previous chapter introduced the key ideas \nbehind recurrent neural networks (RNNs). However, just as with convolutional neural networks,\nthere has been a tremendous amount of innovation\nin RNN architectures, culminating in several complex\ndesigns that have proven successful in practice. In particular, the most popular designs \nfeature mechanisms for mitigating the notorious\nnumerical instability faced by RNNs,\nas typified by vanishing and exploding gradients. Recall that in :numref:`chap_rnn` we dealt \nwith exploding gradients by applying a blunt\ngradient clipping heuristic. Despite the efficacy of this hack,\nit leaves open the problem of vanishing gradients. In this chapter, we introduce the key ideas behind \nthe most successful RNN architectures for sequences,\nwhich stem from two papers. The first, *Long Short-Term Memory* :cite:`Hochreiter.Schmidhuber.1997`,\nintroduces the *memory cell*, a unit of computation that replaces \ntraditional nodes in the hidden layer of a network. With these memory cells, networks are able \nto overcome difficulties with training \nencountered by earlier recurrent networks. Intuitively, the memory cell avoids \nthe vanishing gradient problem\nby keeping values in each memory cell's internal state\ncascading along a recurrent edge with weight 1 \nacross many successive time steps. A set of multiplicative gates help the network\nto determine not only the inputs to allow \ninto the memory state, \nbut when the content of the memory state \nshould influence the model's output. The second paper, *Bidirectional Recurrent Neural Networks* :cite:`Schuster.Paliwal.1997`,\nintroduces an architecture in which information \nfrom both the future (subsequent time steps) \nand the past (preceding time steps)\nare used to determine the output \nat any point in the sequence. This is in contrast to previous networks, \nin which only past input can affect the output."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "This is in contrast to previous networks, \nin which only past input can affect the output. Bidirectional RNNs have become a mainstay \nfor sequence labeling tasks in natural language processing,\namong a myriad of other tasks. Fortunately, the two innovations are not mutually exclusive, \nand have been successfully combined for phoneme classification\n:cite:`Graves.Schmidhuber.2005` and handwriting recognition :cite:`graves2008novel`. The first sections in this chapter will explain the LSTM architecture,\na lighter-weight version called the gated recurrent unit (GRU),\nthe key ideas behind bidirectional RNNs \nand a brief explanation of how RNN layers \nare stacked together to form deep RNNs. Subsequently, we will explore the application of RNNs\nin sequence-to-sequence tasks, \nintroducing machine translation\nalong with key ideas such as *encoder--decoder* architectures and *beam search*. ```toc\n:maxdepth: 2\n\nlstm\ngru\ndeep-rnn\nbi-rnn\nmachine-translation-and-dataset\nencoder-decoder\nseq2seq\nbeam-search\n```"
    },
    {
      "chunk_id": "f54bcbae7402_0",
      "chapter": "lstm",
      "heading": "lstm",
      "text": "# Long Short-Term Memory (LSTM)\n:label:`sec_lstm`\n\n\nShortly after the first Elman-style RNNs were trained using backpropagation \n:cite:`elman1990finding`, the problems of learning long-term dependencies\n(owing to vanishing and exploding gradients)\nbecame salient, with Bengio and Hochreiter \ndiscussing the problem\n:cite:`bengio1994learning,Hochreiter.Bengio.Frasconi.ea.2001`. Hochreiter had articulated this problem as early \nas 1991 in his Master's thesis, although the results \nwere not widely known because the thesis was written in German. While gradient clipping helps with exploding gradients, \nhandling vanishing gradients appears \nto require a more elaborate solution. One of the first and most successful techniques \nfor addressing vanishing gradients \ncame in the form of the long short-term memory (LSTM) model \ndue to :citet:`Hochreiter.Schmidhuber.1997`. LSTMs resemble standard recurrent neural networks \nbut here each ordinary recurrent node\nis replaced by a *memory cell*. Each memory cell contains an *internal state*,\ni.e., a node with a self-connected recurrent edge of fixed weight 1,\nensuring that the gradient can pass across many time steps \nwithout vanishing or exploding. The term \"long short-term memory\" comes from the following intuition. Simple recurrent neural networks \nhave *long-term memory* in the form of weights. The weights change slowly during training, \nencoding general knowledge about the data. They also have *short-term memory*\nin the form of ephemeral activations,\nwhich pass from each node to successive nodes. The LSTM model introduces an intermediate type of storage via the memory cell. A memory cell is a composite unit, \nbuilt from simpler nodes \nin a specific connectivity pattern,\nwith the novel inclusion of multiplicative nodes."
    },
    {
      "chunk_id": "f54bcbae7402_1",
      "chapter": "lstm",
      "heading": "lstm",
      "text": "The LSTM model introduces an intermediate type of storage via the memory cell. A memory cell is a composite unit, \nbuilt from simpler nodes \nin a specific connectivity pattern,\nwith the novel inclusion of multiplicative nodes. ```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import rnn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "43ed4d1c07f0_0",
      "chapter": "lstm",
      "heading": "Gated Memory Cell",
      "text": "Each memory cell is equipped with an *internal state*\nand a number of multiplicative gates that determine whether\n(i) a given input should impact the internal state (the *input gate*),\n(ii) the internal state should be flushed to $0$ (the *forget gate*),\nand (iii) the internal state of a given neuron \nshould be allowed to impact the cell's output (the *output* gate)."
    },
    {
      "chunk_id": "73b0d8d5add5_0",
      "chapter": "lstm",
      "heading": "Gated Hidden State",
      "text": "The key distinction between vanilla RNNs and LSTMs\nis that the latter support gating of the hidden state.\nThis means that we have dedicated mechanisms for\nwhen a hidden state should be *updated* and\nalso for when it should be *reset*.\nThese mechanisms are learned and they address the concerns listed above.\nFor instance, if the first token is of great importance\nwe will learn not to update the hidden state after the first observation.\nLikewise, we will learn to skip irrelevant temporary observations.\nLast, we will learn to reset the latent state whenever needed.\nWe discuss this in detail below."
    },
    {
      "chunk_id": "743dfd6f6acd_0",
      "chapter": "lstm",
      "heading": "Input Gate, Forget Gate, and Output Gate",
      "text": "The data feeding into the LSTM gates are\nthe input at the current time step and\nthe hidden state of the previous time step,\nas illustrated in :numref:`fig_lstm_0`. Three fully connected layers with sigmoid activation functions\ncompute the values of the input, forget, and output gates. As a result of the sigmoid activation,\nall values of the three gates\nare in the range of $(0, 1)$. Additionally, we require an *input node*,\ntypically computed with a *tanh* activation function. Intuitively, the *input gate* determines how much\nof the input node's value should be added \nto the current memory cell internal state. The *forget gate* determines whether to keep\nthe current value of the memory or flush it. And the *output gate* determines whether \nthe memory cell should influence the output\nat the current time step. ![Computing the input gate, the forget gate, and the output gate in an LSTM model.](../img/lstm-0.svg)\n:label:`fig_lstm_0`\n\nMathematically, suppose that there are $h$ hidden units, \nthe batch size is $n$, and the number of inputs is $d$. Thus, the input is $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \nand the hidden state of the previous time step \nis $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$. Correspondingly, the gates at time step $t$\nare defined as follows: the input gate is $\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$, \nthe forget gate is $\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$, \nand the output gate is $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$."
    },
    {
      "chunk_id": "743dfd6f6acd_1",
      "chapter": "lstm",
      "heading": "Input Gate, Forget Gate, and Output Gate",
      "text": "They are calculated as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xi}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hi}} + \\mathbf{b}_\\textrm{i}),\\\\\n\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xf}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hf}} + \\mathbf{b}_\\textrm{f}),\\\\\n\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xo}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{ho}} + \\mathbf{b}_\\textrm{o}),\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}_{\\textrm{xi}}, \\mathbf{W}_{\\textrm{xf}}, \\mathbf{W}_{\\textrm{xo}} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{\\textrm{hi}}, \\mathbf{W}_{\\textrm{hf}}, \\mathbf{W}_{\\textrm{ho}} \\in \\mathbb{R}^{h \\times h}$ are weight parameters \nand $\\mathbf{b}_\\textrm{i}, \\mathbf{b}_\\textrm{f}, \\mathbf{b}_\\textrm{o} \\in \\mathbb{R}^{1 \\times h}$ are bias parameters. Note that broadcasting \n(see :numref:`subsec_broadcasting`)\nis triggered during the summation. We use sigmoid functions \n(as introduced in :numref:`sec_mlp`) \nto map the input values to the interval $(0, 1)$."
    },
    {
      "chunk_id": "1d609153ad76_0",
      "chapter": "lstm",
      "heading": "Input Node",
      "text": "Next we design the memory cell. \nSince we have not specified the action of the various gates yet, \nwe first introduce the *input node* \n$\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$.\nIts computation is similar to that of the three gates described above, \nbut uses a $\\tanh$ function with a value range for $(-1, 1)$ as the activation function. \nThis leads to the following equation at time step $t$:\n\n$$\\tilde{\\mathbf{C}}_t = \\textrm{tanh}(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xc}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hc}} + \\mathbf{b}_\\textrm{c}),$$\n\nwhere $\\mathbf{W}_{\\textrm{xc}} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{\\textrm{hc}} \\in \\mathbb{R}^{h \\times h}$ are weight parameters and $\\mathbf{b}_\\textrm{c} \\in \\mathbb{R}^{1 \\times h}$ is a bias parameter.\n\nA quick illustration of the input node is shown in :numref:`fig_lstm_1`.\n\n![Computing the input node in an LSTM model.](../img/lstm-1.svg)\n:label:`fig_lstm_1`"
    },
    {
      "chunk_id": "3b091d7a6fec_0",
      "chapter": "lstm",
      "heading": "Memory Cell Internal State",
      "text": "In LSTMs, the input gate $\\mathbf{I}_t$ governs \nhow much we take new data into account via $\\tilde{\\mathbf{C}}_t$ \nand the forget gate $\\mathbf{F}_t$ addresses \nhow much of the old cell internal state $\\mathbf{C}_{t-1} \\in \\mathbb{R}^{n \\times h}$ we retain. \nUsing the Hadamard (elementwise) product operator $\\odot$\nwe arrive at the following update equation:\n\n$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$\n\nIf the forget gate is always 1 and the input gate is always 0, \nthe memory cell internal state $\\mathbf{C}_{t-1}$\nwill remain constant forever, \npassing unchanged to each subsequent time step.\nHowever, input gates and forget gates\ngive the model the flexibility of being able to learn \nwhen to keep this value unchanged\nand when to perturb it in response \nto subsequent inputs. \nIn practice, this design alleviates the vanishing gradient problem,\nresulting in models that are much easier to train,\nespecially when facing datasets with long sequence lengths. \n\nWe thus arrive at the flow diagram in :numref:`fig_lstm_2`.\n\n![Computing the memory cell internal state in an LSTM model.](../img/lstm-2.svg)\n\n:label:`fig_lstm_2`"
    },
    {
      "chunk_id": "e0aabc65c839_0",
      "chapter": "lstm",
      "heading": "Hidden State",
      "text": "Last, we need to define how to compute the output\nof the memory cell, i.e., the hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$, as seen by other layers. \nThis is where the output gate comes into play.\nIn LSTMs, we first apply $\\tanh$ to the memory cell internal state\nand then apply another point-wise multiplication,\nthis time with the output gate.\nThis ensures that the values of $\\mathbf{H}_t$ \nare always in the interval $(-1, 1)$:\n\n$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$\n\n\nWhenever the output gate is close to 1, \nwe allow the memory cell internal state to impact the subsequent layers uninhibited,\nwhereas for output gate values close to 0,\nwe prevent the current memory from impacting other layers of the network\nat the current time step. \nNote that a memory cell can accrue information \nacross many time steps without impacting the rest of the network\n(as long as the output gate takes values close to 0),\nand then suddenly impact the network at a subsequent time step\nas soon as the output gate flips from values close to 0\nto values close to 1. :numref:`fig_lstm_3` has a graphical illustration of the data flow.\n\n![Computing the hidden state in an LSTM model.](../img/lstm-3.svg)\n:label:`fig_lstm_3`"
    },
    {
      "chunk_id": "5b68e5c78d9f_0",
      "chapter": "lstm",
      "heading": "Implementation from Scratch",
      "text": "Now let's implement an LSTM from scratch.\nAs same as the experiments in :numref:`sec_rnn-scratch`,\nwe first load *The Time Machine* dataset."
    },
    {
      "chunk_id": "3495c902637f_0",
      "chapter": "lstm",
      "heading": "[**Initializing Model Parameters**]",
      "text": "Next, we need to define and initialize the model parameters. As previously, the hyperparameter `num_hiddens` \ndictates the number of hidden units. We initialize weights following a Gaussian distribution\nwith 0.01 standard deviation, \nand we set the biases to 0."
    },
    {
      "chunk_id": "3495c902637f_1",
      "chapter": "lstm",
      "heading": "[**Initializing Model Parameters**]",
      "text": "As previously, the hyperparameter `num_hiddens` \ndictates the number of hidden units. We initialize weights following a Gaussian distribution\nwith 0.01 standard deviation, \nand we set the biases to 0. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass LSTMScratch(d2l.Module):\n    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n\n        if tab.selected('mxnet'):\n            init_weight = lambda *shape: d2l.randn(*shape) * sigma\n            triple = lambda: (init_weight(num_inputs, num_hiddens),\n                              init_weight(num_hiddens, num_hiddens),\n                              d2l.zeros(num_hiddens))\n        if tab.selected('pytorch'):\n            init_weight = lambda *shape: nn.Parameter(d2l.randn(*shape) * sigma)\n            triple = lambda: (init_weight(num_inputs, num_hiddens),\n                              init_weight(num_hiddens, num_hiddens),\n                              nn.Parameter(d2l.zeros(num_hiddens)))\n        if tab.selected('tensorflow'):\n            init_weight = lambda *shape: tf.Variable(d2l.normal(shape) * sigma)\n            triple = lambda: (init_weight(num_inputs, num_hiddens),\n                              init_weight(num_hiddens, num_hiddens),\n                              tf.Variable(d2l.zeros(num_hiddens)))\n\n        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n```\n\n```{.python .input}\n%%tab jax\nclass LSTMScratch(d2l.Module):\n    num_inputs: int\n    num_hiddens: int\n    sigma: float = 0.01\n\n    def setup(self):\n        init_weight = lambda name, shape: self.param(name,\n                                                     nn.initializers.normal(self.sigma),\n                                                     shape)\n        triple = lambda name : (\n            init_weight(f'W_x{name}', (self.num_inputs, self.num_hiddens)),\n            init_weight(f'W_h{name}', (self.num_hiddens, self.num_hiddens)),\n            self.param(f'b_{name}', nn.initializers.zeros, (self.num_hiddens)))\n\n        self.W_xi, self.W_hi, self.b_i = triple('i')  # Input gate\n        self.W_xf, self.W_hf, self.b_f = triple('f')  # Forget gate\n        self.W_xo, self.W_ho, self.b_o = triple('o')  # Output gate\n        self.W_xc, self.W_hc, self.b_c = triple('c')  # Input node\n```\n\n:begin_tab:`pytorch, mxnet, tensorflow`\n[**The actual model**] is defined as described above,\nconsisting of three gates and an input node."
    },
    {
      "chunk_id": "3495c902637f_2",
      "chapter": "lstm",
      "heading": "[**Initializing Model Parameters**]",
      "text": "Note that only the hidden state is passed to the output layer. :end_tab:\n\n:begin_tab:`jax`\n[**The actual model**] is defined as described above,\nconsisting of three gates and an input node. Note that only the hidden state is passed to the output layer. A long for-loop in the `forward` method will result in an extremely long\nJIT compilation time for the first run. As a solution to this, instead\nof using a for-loop to update the state with every time step,\nJAX has `jax.lax.scan` utility transformation to achieve the same behavior. It takes in an initial state called `carry` and an `inputs` array which\nis scanned on its leading axis. The `scan` transformation ultimately\nreturns the final state and the stacked outputs as expected."
    },
    {
      "chunk_id": "3495c902637f_3",
      "chapter": "lstm",
      "heading": "[**Initializing Model Parameters**]",
      "text": "It takes in an initial state called `carry` and an `inputs` array which\nis scanned on its leading axis. The `scan` transformation ultimately\nreturns the final state and the stacked outputs as expected. :end_tab:\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(LSTMScratch)\ndef forward(self, inputs, H_C=None):\n    if H_C is None:\n        # Initial state with shape: (batch_size, num_hiddens)\n        if tab.selected('mxnet'):\n            H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                          ctx=inputs.ctx)\n            C = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                          ctx=inputs.ctx)\n        if tab.selected('pytorch'):\n            H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                          device=inputs.device)\n            C = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                          device=inputs.device)\n        if tab.selected('tensorflow'):\n            H = d2l.zeros((inputs.shape[1], self.num_hiddens))\n            C = d2l.zeros((inputs.shape[1], self.num_hiddens))\n    else:\n        H, C = H_C\n    outputs = []\n    for X in inputs:\n        I = d2l.sigmoid(d2l.matmul(X, self.W_xi) +\n                        d2l.matmul(H, self.W_hi) + self.b_i)\n        F = d2l.sigmoid(d2l.matmul(X, self.W_xf) +\n                        d2l.matmul(H, self.W_hf) + self.b_f)\n        O = d2l.sigmoid(d2l.matmul(X, self.W_xo) +\n                        d2l.matmul(H, self.W_ho) + self.b_o)\n        C_tilde = d2l.tanh(d2l.matmul(X, self.W_xc) +\n                           d2l.matmul(H, self.W_hc) + self.b_c)\n        C = F * C + I * C_tilde\n        H = O * d2l.tanh(C)\n        outputs.append(H)\n    return outputs, (H, C)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(LSTMScratch)\ndef forward(self, inputs, H_C=None):\n    # Use lax.scan primitive instead of looping over the\n    # inputs, since scan saves time in jit compilation."
    },
    {
      "chunk_id": "3495c902637f_4",
      "chapter": "lstm",
      "heading": "[**Initializing Model Parameters**]",
      "text": "def scan_fn(carry, X):\n        H, C = carry\n        I = d2l.sigmoid(d2l.matmul(X, self.W_xi) + (\n            d2l.matmul(H, self.W_hi)) + self.b_i)\n        F = d2l.sigmoid(d2l.matmul(X, self.W_xf) +\n                        d2l.matmul(H, self.W_hf) + self.b_f)\n        O = d2l.sigmoid(d2l.matmul(X, self.W_xo) +\n                        d2l.matmul(H, self.W_ho) + self.b_o)\n        C_tilde = d2l.tanh(d2l.matmul(X, self.W_xc) +\n                           d2l.matmul(H, self.W_hc) + self.b_c)\n        C = F * C + I * C_tilde\n        H = O * d2l.tanh(C)\n        return (H, C), H  # return carry, y\n\n    if H_C is None:\n        batch_size = inputs.shape[1]\n        carry = jnp.zeros((batch_size, self.num_hiddens)), \\\n                jnp.zeros((batch_size, self.num_hiddens))\n    else:\n        carry = H_C\n\n    # scan takes the scan_fn, initial carry state, xs with leading axis to be scanned\n    carry, outputs = jax.lax.scan(scan_fn, carry, inputs)\n    return outputs, carry\n```"
    },
    {
      "chunk_id": "8fafeed18445_0",
      "chapter": "lstm",
      "heading": "[**Training**] and Prediction",
      "text": "Let's train an LSTM model by instantiating the `RNNLMScratch` class from :numref:`sec_rnn-scratch`.\n\n```{.python .input}\n%%tab all\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n    model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n        model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "b597adb753ce_0",
      "chapter": "lstm",
      "heading": "[**Concise Implementation**]",
      "text": "Using high-level APIs,\nwe can directly instantiate an LSTM model. This encapsulates all the configuration details \nthat we made explicit above. The code is significantly faster as it uses \ncompiled operators rather than Python\nfor many details that we spelled out before."
    },
    {
      "chunk_id": "b597adb753ce_1",
      "chapter": "lstm",
      "heading": "[**Concise Implementation**]",
      "text": "This encapsulates all the configuration details \nthat we made explicit above. The code is significantly faster as it uses \ncompiled operators rather than Python\nfor many details that we spelled out before. ```{.python .input}\n%%tab mxnet\nclass LSTM(d2l.RNN):\n    def __init__(self, num_hiddens):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        self.rnn = rnn.LSTM(num_hiddens)\n\n    def forward(self, inputs, H_C=None):\n        if H_C is None: H_C = self.rnn.begin_state(\n            inputs.shape[1], ctx=inputs.ctx)\n        return self.rnn(inputs, H_C)\n```\n\n```{.python .input}\n%%tab pytorch\nclass LSTM(d2l.RNN):\n    def __init__(self, num_inputs, num_hiddens):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        self.rnn = nn.LSTM(num_inputs, num_hiddens)\n\n    def forward(self, inputs, H_C=None):\n        return self.rnn(inputs, H_C)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass LSTM(d2l.RNN):\n    def __init__(self, num_hiddens):\n        d2l.Module.__init__(self)\n        self.save_hyperparameters()\n        self.rnn = tf.keras.layers.LSTM(\n                num_hiddens, return_sequences=True,\n                return_state=True, time_major=True)\n\n    def forward(self, inputs, H_C=None):\n        outputs, *H_C = self.rnn(inputs, H_C)\n        return outputs, H_C\n```\n\n```{.python .input}\n%%tab jax\nclass LSTM(d2l.RNN):\n    num_hiddens: int\n\n    @nn.compact\n    def __call__(self, inputs, H_C=None, training=False):\n        if H_C is None:\n            batch_size = inputs.shape[1]\n            H_C = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),\n                                                        (batch_size,),\n                                                        self.num_hiddens)\n\n        LSTM = nn.scan(nn.OptimizedLSTMCell, variable_broadcast=\"params\",\n                       in_axes=0, out_axes=0, split_rngs={\"params\": False})\n\n        H_C, outputs = LSTM()(H_C, inputs)\n        return outputs, H_C\n```\n\n```{.python .input}\n%%tab all\nif tab.selected('pytorch'):\n    lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)\nif tab.selected('mxnet', 'tensorflow', 'jax'):\n    lstm = LSTM(num_hiddens=32)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\ntrainer.fit(model, data)\n```\n\n```{.python .input}\n%%tab mxnet, pytorch\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n```\n\n```{.python .input}\n%%tab tensorflow\nmodel.predict('it has', 20, data.vocab)\n```\n\n```{.python .input}\n%%tab jax\nmodel.predict('it has', 20, data.vocab, trainer.state.params)\n```\n\nLSTMs are the prototypical latent variable autoregressive model with nontrivial state control."
    },
    {
      "chunk_id": "b597adb753ce_2",
      "chapter": "lstm",
      "heading": "[**Concise Implementation**]",
      "text": "Many variants thereof have been proposed over the years, e.g., multiple layers, residual connections, different types of regularization. However, training LSTMs and other sequence models (such as GRUs) is quite costly because of the long range dependency of the sequence. Later we will encounter alternative models such as Transformers that can be used in some cases."
    },
    {
      "chunk_id": "c0282fb46b79_0",
      "chapter": "lstm",
      "heading": "Summary",
      "text": "While LSTMs were published in 1997, \nthey rose to great prominence \nwith some victories in prediction competitions in the mid-2000s,\nand became the dominant models for sequence learning from 2011 \nuntil the rise of Transformer models, starting in 2017.\nEven Tranformers owe some of their key ideas \nto architecture design innovations introduced by the LSTM.\n\n\nLSTMs have three types of gates: \ninput gates, forget gates, and output gates \nthat control the flow of information.\nThe hidden layer output of LSTM includes the hidden state and the memory cell internal state. \nOnly the hidden state is passed into the output layer while \nthe memory cell internal state remains entirely internal.\nLSTMs can alleviate vanishing and exploding gradients."
    },
    {
      "chunk_id": "b94bacdd97c7_0",
      "chapter": "lstm",
      "heading": "Exercises",
      "text": "1. Adjust the hyperparameters and analyze their influence on running time, perplexity, and the output sequence.\n1. How would you need to change the model to generate proper words rather than just sequences of characters?\n1. Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension. Pay special attention to the training and inference cost.\n1. Since the candidate memory cell ensures that the value range is between $-1$ and $1$ by  using the $\\tanh$ function, why does the hidden state need to use the $\\tanh$ function again to ensure that the output value range is between $-1$ and $1$?\n1. Implement an LSTM model for time series prediction rather than character sequence prediction.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/343)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1057)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3861)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18016)\n:end_tab:"
    },
    {
      "chunk_id": "b798740223ea_0",
      "chapter": "machine-translation-and-dataset",
      "heading": "machine-translation-and-dataset",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n# Machine Translation and the Dataset\n:label:`sec_machine_translation`\n\nAmong the major breakthroughs that prompted \nwidespread interest in modern RNNs\nwas a major advance in the applied field of \nstatistical  *machine translation*. Here, the model is presented with a sentence in one language\nand must predict the corresponding sentence in another. Note that here the sentences may be of different lengths,\nand that corresponding words in the two sentences \nmay not occur in the same order, \nowing to differences \nin the two language's grammatical structure. Many problems have this flavor of mapping \nbetween two such \"unaligned\" sequences. Examples include mapping \nfrom dialog prompts to replies\nor from questions to answers. Broadly, such problems are called \n*sequence-to-sequence* (seq2seq) problems \nand they are our focus for \nboth the remainder of this chapter\nand much of :numref:`chap_attention-and-transformers`. In this section, we introduce the machine translation problem\nand an example dataset that we will use in the subsequent examples. For decades, statistical formulations of translation between languages\nhad been popular :cite:`Brown.Cocke.Della-Pietra.ea.1988,Brown.Cocke.Della-Pietra.ea.1990`,\neven before researchers got neural network approaches working\n(methods were often lumped together under the term *neural machine translation*). First we will need some new code to process our data. Unlike the language modeling that we saw in :numref:`sec_language-model`,\nhere each example consists of two separate text sequences,\none in the source language and another (the translation) in the target language. The following code snippets will show how \nto load the preprocessed data into minibatches for training."
    },
    {
      "chunk_id": "b798740223ea_1",
      "chapter": "machine-translation-and-dataset",
      "heading": "machine-translation-and-dataset",
      "text": "The following code snippets will show how \nto load the preprocessed data into minibatches for training. ```{.python .input  n=2}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nimport os\nnpx.set_np()\n```\n\n```{.python .input  n=3}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nimport os\n```\n\n```{.python .input  n=4}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport os\n```\n\n```{.python .input  n=4}\n%%tab jax\nfrom d2l import jax as d2l\nfrom jax import numpy as jnp\nimport os\n```"
    },
    {
      "chunk_id": "0c680d1ae392_0",
      "chapter": "machine-translation-and-dataset",
      "heading": "[**Downloading and Preprocessing the Dataset**]",
      "text": "To begin, we download an English--French dataset\nthat consists of [bilingual sentence pairs from the Tatoeba Project](http://www.manythings.org/anki/).\nEach line in the dataset is a tab-delimited pair \nconsisting of an English text sequence (the *source*) \nand the translated French text sequence (the *target*).\nNote that each text sequence\ncan be just one sentence,\nor a paragraph of multiple sentences.\n\n```{.python .input  n=5}\n%%tab all\nclass MTFraEng(d2l.DataModule):  #@save\n    \"\"\"The English-French dataset.\"\"\"\n    def _download(self):\n        d2l.extract(d2l.download(\n            d2l.DATA_URL+'fra-eng.zip', self.root, \n            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n            return f.read()\n```\n\n```{.python .input}\n%%tab all\ndata = MTFraEng() \nraw_text = data._download()\nprint(raw_text[:75])\n```\n\nAfter downloading the dataset,\nwe [**proceed with several preprocessing steps**]\nfor the raw text data.\nFor instance, we replace non-breaking space with space,\nconvert uppercase letters to lowercase ones,\nand insert space between words and punctuation marks.\n\n```{.python .input  n=6}\n%%tab all\n@d2l.add_to_class(MTFraEng)  #@save\ndef _preprocess(self, text):\n    # Replace non-breaking space with space\n    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n    # Insert space between words and punctuation marks\n    no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n           for i, char in enumerate(text.lower())]\n    return ''.join(out)\n```\n\n```{.python .input}\n%%tab all\ntext = data._preprocess(raw_text)\nprint(text[:80])\n```"
    },
    {
      "chunk_id": "ccd5865340e0_0",
      "chapter": "machine-translation-and-dataset",
      "heading": "[**Tokenization**]",
      "text": "Unlike the character-level tokenization\nin :numref:`sec_language-model`,\nfor machine translation\nwe prefer word-level tokenization here\n(today's state-of-the-art models use \nmore complex tokenization techniques). The following `_tokenize` method\ntokenizes the first `max_examples` text sequence pairs,\nwhere each token is either a word or a punctuation mark. We append the special \u201c&lt;eos&gt;\u201d token\nto the end of every sequence to indicate the\nend of the sequence. When a model is predicting\nby generating a sequence token after token,\nthe generation of the \u201c&lt;eos&gt;\u201d token\ncan suggest that the output sequence is complete. In the end, the method below returns\ntwo lists of token lists: `src` and `tgt`. Specifically, `src[i]` is a list of tokens from the\n$i^\\textrm{th}$ text sequence in the source language (English here) \nand `tgt[i]` is that in the target language (French here). ```{.python .input  n=7}\n%%tab all\n@d2l.add_to_class(MTFraEng)  #@save\ndef _tokenize(self, text, max_examples=None):\n    src, tgt = [], []\n    for i, line in enumerate(text.split('\\n')):\n        if max_examples and i > max_examples: break\n        parts = line.split('\\t')\n        if len(parts) == 2:\n            # Skip empty tokens\n            src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n            tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n    return src, tgt\n```\n\n```{.python .input}\n%%tab all\nsrc, tgt = data._tokenize(text)\nsrc[:6], tgt[:6]\n```\n\nLet's [**plot the histogram of the number of tokens per text sequence.**]\nIn this simple English--French dataset,\nmost of the text sequences have fewer than 20 tokens."
    },
    {
      "chunk_id": "ccd5865340e0_1",
      "chapter": "machine-translation-and-dataset",
      "heading": "[**Tokenization**]",
      "text": "```{.python .input  n=8}\n%%tab all\n#@save\ndef show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n    \"\"\"Plot the histogram for list length pairs.\"\"\"\n    d2l.set_figsize()\n    _, _, patches = d2l.plt.hist(\n        [[len(l) for l in xlist], [len(l) for l in ylist]])\n    d2l.plt.xlabel(xlabel)\n    d2l.plt.ylabel(ylabel)\n    for patch in patches[1].patches:\n        patch.set_hatch('/')\n    d2l.plt.legend(legend)\n```\n\n```{.python .input}\n%%tab all\nshow_list_len_pair_hist(['source', 'target'], '# tokens per sequence',\n                        'count', src, tgt);\n```"
    },
    {
      "chunk_id": "900bb0dc0a38_0",
      "chapter": "machine-translation-and-dataset",
      "heading": "Loading Sequences of Fixed Length",
      "text": ":label:`subsec_loading-seq-fixed-len`\n\nRecall that in language modeling\n[**each example sequence**],\neither a segment of one sentence\nor a span over multiple sentences,\n(**had a fixed length.**)\nThis was specified by the `num_steps`\n(number of time steps or tokens) argument from :numref:`sec_language-model`. In machine translation, each example is\na pair of source and target text sequences,\nwhere the two text sequences may have different lengths. For computational efficiency,\nwe can still process a minibatch of text sequences\nat one time by *truncation* and *padding*. Suppose that every sequence in the same minibatch\nshould have the same length `num_steps`. If a text sequence has fewer than `num_steps` tokens,\nwe will keep appending the special \"&lt;pad&gt;\" token\nto its end until its length reaches `num_steps`. Otherwise, we will truncate the text sequence\nby only taking its first `num_steps` tokens\nand discarding the remaining. In this way, every text sequence\nwill have the same length\nto be loaded in minibatches of the same shape. Furthermore, we also record length of the source sequence excluding padding tokens. This information will be needed by some models that we will cover later. Since the machine translation dataset\nconsists of pairs of languages,\nwe can build two vocabularies for\nboth the source language and\nthe target language separately. With word-level tokenization,\nthe vocabulary size will be significantly larger\nthan that using character-level tokenization. To alleviate this,\nhere we treat infrequent tokens\nthat appear less than twice\nas the same unknown (\"&lt;unk&gt;\") token. As we will explain later (:numref:`fig_seq2seq`),\nwhen training with target sequences,\nthe decoder output (label tokens)\ncan be the same decoder input (target tokens),\nshifted by one token;\nand the special beginning-of-sequence \"&lt;bos&gt;\" token\nwill be used as the first input token\nfor predicting the target sequence (:numref:`fig_seq2seq_predict`)."
    },
    {
      "chunk_id": "900bb0dc0a38_1",
      "chapter": "machine-translation-and-dataset",
      "heading": "Loading Sequences of Fixed Length",
      "text": "```{.python .input  n=9}\n%%tab all\n@d2l.add_to_class(MTFraEng)  #@save\ndef __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n    super(MTFraEng, self).__init__()\n    self.save_hyperparameters()\n    self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n        self._download())\n```\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(MTFraEng)  #@save\ndef _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n    def _build_array(sentences, vocab, is_tgt=False):\n        pad_or_trim = lambda seq, t: (\n            seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n        sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n        if is_tgt:\n            sentences = [['<bos>'] + s for s in sentences]\n        if vocab is None:\n            vocab = d2l.Vocab(sentences, min_freq=2)\n        array = d2l.tensor([vocab[s] for s in sentences])\n        valid_len = d2l.reduce_sum(\n            d2l.astype(array != vocab['<pad>'], d2l.int32), 1)\n        return array, vocab, valid_len\n    src, tgt = self._tokenize(self._preprocess(raw_text), \n                              self.num_train + self.num_val)\n    src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n    tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n    return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n            src_vocab, tgt_vocab)\n```"
    },
    {
      "chunk_id": "34742d712487_0",
      "chapter": "machine-translation-and-dataset",
      "heading": "[**Reading the Dataset**]",
      "text": "Finally, we define the `get_dataloader` method\nto return the data iterator.\n\n```{.python .input  n=10}\n%%tab all\n@d2l.add_to_class(MTFraEng)  #@save\ndef get_dataloader(self, train):\n    idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n    return self.get_tensorloader(self.arrays, train, idx)\n```\n\nLet's [**read the first minibatch from the English--French dataset.**]\n\n```{.python .input  n=11}\n%%tab all\ndata = MTFraEng(batch_size=3)\nsrc, tgt, src_valid_len, label = next(iter(data.train_dataloader()))\nprint('source:', d2l.astype(src, d2l.int32))\nprint('decoder input:', d2l.astype(tgt, d2l.int32))\nprint('source len excluding pad:', d2l.astype(src_valid_len, d2l.int32))\nprint('label:', d2l.astype(label, d2l.int32))\n```\n\nWe show a pair of source and target sequences\nprocessed by the above `_build_arrays` method\n(in the string format).\n\n```{.python .input  n=12}\n%%tab all\n@d2l.add_to_class(MTFraEng)  #@save\ndef build(self, src_sentences, tgt_sentences):\n    raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n        src_sentences, tgt_sentences)])\n    arrays, _, _ = self._build_arrays(\n        raw_text, self.src_vocab, self.tgt_vocab)\n    return arrays\n```\n\n```{.python .input  n=13}\n%%tab all\nsrc, tgt, _,  _ = data.build(['hi .'], ['salut .'])\nprint('source:', data.src_vocab.to_tokens(d2l.astype(src[0], d2l.int32)))\nprint('target:', data.tgt_vocab.to_tokens(d2l.astype(tgt[0], d2l.int32)))\n```"
    },
    {
      "chunk_id": "a2740eb74e99_0",
      "chapter": "machine-translation-and-dataset",
      "heading": "Summary",
      "text": "In natural language processing, *machine translation* refers to the task of automatically mapping from a sequence representing a string of text in a *source* language to a string representing a plausible translation in a *target* language. Using word-level tokenization, the vocabulary size will be significantly larger than that using character-level tokenization, but the sequence lengths will be much shorter. To mitigate the large vocabulary size, we can treat infrequent tokens as some \"unknown\" token. We can truncate and pad text sequences so that all of them will have the same length to be loaded in minibatches. Modern implementations often bucket sequences with similar lengths to avoid wasting excessive computation on padding."
    },
    {
      "chunk_id": "4ed166791ebf_0",
      "chapter": "machine-translation-and-dataset",
      "heading": "Exercises",
      "text": "1. Try different values of the `max_examples` argument in the `_tokenize` method. How does this affect the vocabulary sizes of the source language and the target language?\n1. Text in some languages such as Chinese and Japanese does not have word boundary indicators (e.g., space). Is word-level tokenization still a good idea for such cases? Why or why not?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/344)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1060)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3863)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18020)\n:end_tab:"
    },
    {
      "chunk_id": "78acf2eb134e_0",
      "chapter": "seq2seq",
      "heading": "seq2seq",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n#  Sequence-to-Sequence Learning for Machine Translation\n:label:`sec_seq2seq`\n\nIn so-called sequence-to-sequence problems such as machine translation\n(as discussed in :numref:`sec_machine_translation`),\nwhere inputs and outputs each consist \nof variable-length unaligned sequences,\nwe generally rely on encoder--decoder architectures\n(:numref:`sec_encoder-decoder`). In this section,\nwe will demonstrate the application \nof an encoder--decoder architecture,\nwhere both the encoder and decoder \nare implemented as RNNs,\nto the task of machine translation\n:cite:`Sutskever.Vinyals.Le.2014,Cho.Van-Merrienboer.Gulcehre.ea.2014`. Here, the encoder RNN will take a variable-length sequence as input \nand transform it into a fixed-shape hidden state. Later, in :numref:`chap_attention-and-transformers`,\nwe will introduce attention mechanisms, \nwhich allow us to access encoded inputs\nwithout having to compress the entire input\ninto a single fixed-length representation. Then to generate the output sequence, \none token at a time,\nthe decoder model, \nconsisting of a separate RNN,\nwill predict each successive target token\ngiven both the input sequence\nand the preceding tokens in the output. During training, the decoder will typically\nbe conditioned upon the preceding tokens\nin the official \"ground truth\" label. However, at test time, we will want to condition\neach output of the decoder on the tokens already predicted. Note that if we ignore the encoder,\nthe decoder in a sequence-to-sequence architecture \nbehaves just like a normal language model. :numref:`fig_seq2seq` illustrates\nhow to use two RNNs\nfor sequence-to-sequence learning\nin machine translation. ![Sequence-to-sequence learning with an RNN encoder and an RNN decoder.](../img/seq2seq.svg)\n:label:`fig_seq2seq`\n\nIn :numref:`fig_seq2seq`,\nthe special \"&lt;eos&gt;\" token\nmarks the end of the sequence. Our model can stop making predictions\nonce this token is generated."
    },
    {
      "chunk_id": "78acf2eb134e_1",
      "chapter": "seq2seq",
      "heading": "seq2seq",
      "text": "Our model can stop making predictions\nonce this token is generated. At the initial time step of the RNN decoder,\nthere are two special design decisions to be aware of:\nFirst, we begin every input with a special \nbeginning-of-sequence \"&lt;bos&gt;\" token. Second, we may feed\nthe final hidden state of the encoder\ninto the decoder\nat every single decoding time step :cite:`Cho.Van-Merrienboer.Gulcehre.ea.2014`. In some other designs,\nsuch as that of :citet:`Sutskever.Vinyals.Le.2014`,\nthe final hidden state of the RNN encoder\nis used\nto initiate the hidden state of the decoder\nonly at the first decoding step. ```{.python .input}\n%%tab mxnet\nimport collections\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import np, npx, init, gluon, autograd\nfrom mxnet.gluon import nn, rnn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nimport collections\nfrom d2l import torch as d2l\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nimport collections\nfrom d2l import tensorflow as d2l\nimport math\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nimport collections\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom functools import partial\nimport jax\nfrom jax import numpy as jnp\nimport math\nimport optax\n```"
    },
    {
      "chunk_id": "16f4347cd46a_0",
      "chapter": "seq2seq",
      "heading": "Teacher Forcing",
      "text": "While running the encoder on the input sequence\nis relatively straightforward,\nhandling the input and output \nof the decoder requires more care. \nThe most common approach is sometimes called *teacher forcing*.\nHere, the original target sequence (token labels)\nis fed into the decoder as input.\nMore concretely,\nthe special beginning-of-sequence token\nand the original target sequence,\nexcluding the final token,\nare concatenated as input to the decoder,\nwhile the decoder output (labels for training) is\nthe original target sequence,\nshifted by one token:\n\"&lt;bos&gt;\", \"Ils\", \"regardent\", \".\" $\\rightarrow$\n\"Ils\", \"regardent\", \".\", \"&lt;eos&gt;\" (:numref:`fig_seq2seq`).\n\nOur implementation in\n:numref:`subsec_loading-seq-fixed-len`\nprepared training data for teacher forcing,\nwhere shifting tokens for self-supervised learning\nis similar to the training of language models in\n:numref:`sec_language-model`.\nAn alternative approach is\nto feed the *predicted* token\nfrom the previous time step\nas the current input to the decoder.\n\n\nIn the following, we explain the design \ndepicted in :numref:`fig_seq2seq`\nin greater detail.\nWe will train this model for machine translation\non the English--French dataset as introduced in\n:numref:`sec_machine_translation`."
    },
    {
      "chunk_id": "8e624e2d4a2d_0",
      "chapter": "seq2seq",
      "heading": "Encoder",
      "text": "Recall that the encoder transforms an input sequence of variable length\ninto a fixed-shape *context variable* $\\mathbf{c}$ (see :numref:`fig_seq2seq`). Consider a single sequence example (batch size 1). Suppose the input sequence is $x_1, \\ldots, x_T$, \nsuch that $x_t$ is the $t^{\\textrm{th}}$ token. At time step $t$, the RNN transforms\nthe input feature vector $\\mathbf{x}_t$ for $x_t$\nand the hidden state $\\mathbf{h} _{t-1}$ \nfrom the previous time step \ninto the current hidden state $\\mathbf{h}_t$. We can use a function $f$ to express \nthe transformation of the RNN's recurrent layer:\n\n$$\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}). $$\n\nIn general, the encoder transforms \nthe hidden states at all time steps\ninto a context variable through a customized function $q$:\n\n$$\\mathbf{c} =  q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T).$$\n\nFor example, in :numref:`fig_seq2seq`,\nthe context variable is just the hidden state $\\mathbf{h}_T$\ncorresponding to the encoder RNN's representation\nafter processing the final token of the input sequence. In this example, we have used a unidirectional RNN\nto design the encoder,\nwhere the hidden state only depends on the input subsequence \nat and before the time step of the hidden state. We can also construct encoders using bidirectional RNNs. In this case, a hidden state depends on the subsequence before and after the time step \n(including the input at the current time step), \nwhich encodes the information of the entire sequence. Now let's [**implement the RNN encoder**]. Note that we use an *embedding layer*\nto obtain the feature vector for each token in the input sequence. The weight of an embedding layer is a matrix,\nwhere the number of rows corresponds to \nthe size of the input vocabulary (`vocab_size`)\nand number of columns corresponds to \nthe feature vector's dimension (`embed_size`). For any input token index $i$,\nthe embedding layer fetches the $i^{\\textrm{th}}$ row \n(starting from 0) of the weight matrix\nto return its feature vector."
    },
    {
      "chunk_id": "8e624e2d4a2d_1",
      "chapter": "seq2seq",
      "heading": "Encoder",
      "text": "For any input token index $i$,\nthe embedding layer fetches the $i^{\\textrm{th}}$ row \n(starting from 0) of the weight matrix\nto return its feature vector. Here we implement the encoder with a multilayer GRU."
    },
    {
      "chunk_id": "8e624e2d4a2d_2",
      "chapter": "seq2seq",
      "heading": "Encoder",
      "text": "For any input token index $i$,\nthe embedding layer fetches the $i^{\\textrm{th}}$ row \n(starting from 0) of the weight matrix\nto return its feature vector. Here we implement the encoder with a multilayer GRU. ```{.python .input}\n%%tab mxnet\nclass Seq2SeqEncoder(d2l.Encoder):  #@save\n    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n        self.initialize(init.Xavier())\n            \n    def forward(self, X, *args):\n        # X shape: (batch_size, num_steps)\n        embs = self.embedding(d2l.transpose(X))\n        # embs shape: (num_steps, batch_size, embed_size)    \n        outputs, state = self.rnn(embs)\n        # outputs shape: (num_steps, batch_size, num_hiddens)\n        # state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, state\n```\n\n```{.python .input}\n%%tab pytorch\ndef init_seq2seq(module):  #@save\n    \"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\n    if type(module) == nn.Linear:\n         nn.init.xavier_uniform_(module.weight)\n    if type(module) == nn.GRU:\n        for param in module._flat_weights_names:\n            if \"weight\" in param:\n                nn.init.xavier_uniform_(module._parameters[param])\n```\n\n```{.python .input}\n%%tab pytorch\nclass Seq2SeqEncoder(d2l.Encoder):  #@save\n    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\n        self.apply(init_seq2seq)\n            \n    def forward(self, X, *args):\n        # X shape: (batch_size, num_steps)\n        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int64))\n        # embs shape: (num_steps, batch_size, embed_size)\n        outputs, state = self.rnn(embs)\n        # outputs shape: (num_steps, batch_size, num_hiddens)\n        # state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, state\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Seq2SeqEncoder(d2l.Encoder):  #@save\n    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n            \n    def call(self, X, *args):\n        # X shape: (batch_size, num_steps)\n        embs = self.embedding(d2l.transpose(X))\n        # embs shape: (num_steps, batch_size, embed_size)    \n        outputs, state = self.rnn(embs)\n        # outputs shape: (num_steps, batch_size, num_hiddens)\n        # state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, state\n```\n\n```{.python .input}\n%%tab jax\nclass Seq2SeqEncoder(d2l.Encoder):  #@save\n    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n    vocab_size: int\n    embed_size: int\n    num_hiddens: int\n    num_layers: int\n    dropout: float = 0\n\n    def setup(self):\n        self.embedding = nn.Embed(self.vocab_size, self.embed_size)\n        self.rnn = d2l.GRU(self.num_hiddens, self.num_layers, self.dropout)\n\n    def __call__(self, X, *args, training=False):\n        # X shape: (batch_size, num_steps)\n        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int32))\n        # embs shape: (num_steps, batch_size, embed_size)\n        outputs, state = self.rnn(embs, training=training)\n        # outputs shape: (num_steps, batch_size, num_hiddens)\n        # state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, state\n```\n\nLet's use a concrete example\nto [**illustrate the above encoder implementation.**]\nBelow, we instantiate a two-layer GRU encoder\nwhose number of hidden units is 16."
    },
    {
      "chunk_id": "8e624e2d4a2d_3",
      "chapter": "seq2seq",
      "heading": "Encoder",
      "text": "Given a minibatch of sequence inputs `X`\n(batch size $=4$; number of time steps $=9$),\nthe hidden states of the final layer\nat all the time steps\n(`enc_outputs` returned by the encoder's recurrent layers)\nare a tensor of shape\n(number of time steps, batch size, number of hidden units). ```{.python .input}\n%%tab all\nvocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\nbatch_size, num_steps = 4, 9\nencoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\nX = d2l.zeros((batch_size, num_steps))\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    enc_outputs, enc_state = encoder(X)\nif tab.selected('jax'):\n    (enc_outputs, enc_state), _ = encoder.init_with_output(d2l.get_key(), X)\n\nd2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))\n```\n\nSince we are using a GRU here,\nthe shape of the multilayer hidden states\nat the final time step is\n(number of hidden layers, batch size, number of hidden units). ```{.python .input}\n%%tab all\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    d2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))\nif tab.selected('tensorflow'):\n    d2l.check_len(enc_state, num_layers)\n    d2l.check_shape(enc_state[0], (batch_size, num_hiddens))\n```"
    },
    {
      "chunk_id": "ca145c3c48c5_0",
      "chapter": "seq2seq",
      "heading": "[**Decoder**]",
      "text": ":label:`sec_seq2seq_decoder`\n\nGiven a target output sequence $y_1, y_2, \\ldots, y_{T'}$\nfor each time step $t'$\n(we use $t^\\prime$ to differentiate from the input sequence time steps),\nthe decoder assigns a predicted probability\nto each possible token occurring at step $y_{t'+1}$\nconditioned upon the previous tokens in the target\n$y_1, \\ldots, y_{t'}$ \nand the context variable \n$\\mathbf{c}$, i.e., $P(y_{t'+1} \\mid y_1, \\ldots, y_{t'}, \\mathbf{c})$. To predict the subsequent token $t^\\prime+1$ in the target sequence,\nthe RNN decoder takes the previous step's target token $y_{t^\\prime}$,\nthe hidden RNN state from the previous time step $\\mathbf{s}_{t^\\prime-1}$,\nand the context variable $\\mathbf{c}$ as its input,\nand transforms them into the hidden state \n$\\mathbf{s}_{t^\\prime}$ at the current time step. We can use a function $g$ to express \nthe transformation of the decoder's hidden layer:\n\n$$\\mathbf{s}_{t^\\prime} = g(y_{t^\\prime-1}, \\mathbf{c}, \\mathbf{s}_{t^\\prime-1}).$$\n:eqlabel:`eq_seq2seq_s_t`\n\nAfter obtaining the hidden state of the decoder,\nwe can use an output layer and the softmax operation \nto compute the predictive distribution\n$p(y_{t^{\\prime}+1} \\mid y_1, \\ldots, y_{t^\\prime}, \\mathbf{c})$ \nover the subsequent output token ${t^\\prime+1}$. Following :numref:`fig_seq2seq`,\nwhen implementing the decoder as follows,\nwe directly use the hidden state at the final time step\nof the encoder\nto initialize the hidden state of the decoder. This requires that the RNN encoder and the RNN decoder \nhave the same number of layers and hidden units. To further incorporate the encoded input sequence information,\nthe context variable is concatenated\nwith the decoder input at all the time steps. To predict the probability distribution of the output token,\nwe use a fully connected layer\nto transform the hidden state \nat the final layer of the RNN decoder."
    },
    {
      "chunk_id": "ca145c3c48c5_1",
      "chapter": "seq2seq",
      "heading": "[**Decoder**]",
      "text": "To predict the probability distribution of the output token,\nwe use a fully connected layer\nto transform the hidden state \nat the final layer of the RNN decoder. ```{.python .input}\n%%tab mxnet\nclass Seq2SeqDecoder(d2l.Decoder):\n    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n        self.dense = nn.Dense(vocab_size, flatten=False)\n        self.initialize(init.Xavier())\n            \n    def init_state(self, enc_all_outputs, *args):\n        return enc_all_outputs \n\n    def forward(self, X, state):\n        # X shape: (batch_size, num_steps)\n        # embs shape: (num_steps, batch_size, embed_size)\n        embs = self.embedding(d2l.transpose(X))\n        enc_output, hidden_state = state\n        # context shape: (batch_size, num_hiddens)\n        context = enc_output[-1]\n        # Broadcast context to (num_steps, batch_size, num_hiddens)\n        context = np.tile(context, (embs.shape[0], 1, 1))\n        # Concat at the feature dimension\n        embs_and_context = d2l.concat((embs, context), -1)\n        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n        outputs = d2l.swapaxes(self.dense(outputs), 0, 1)\n        # outputs shape: (batch_size, num_steps, vocab_size)\n        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, [enc_output, hidden_state]\n```\n\n```{.python .input}\n%%tab pytorch\nclass Seq2SeqDecoder(d2l.Decoder):\n    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,\n                           num_layers, dropout)\n        self.dense = nn.LazyLinear(vocab_size)\n        self.apply(init_seq2seq)\n            \n    def init_state(self, enc_all_outputs, *args):\n        return enc_all_outputs\n\n    def forward(self, X, state):\n        # X shape: (batch_size, num_steps)\n        # embs shape: (num_steps, batch_size, embed_size)\n        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int32))\n        enc_output, hidden_state = state\n        # context shape: (batch_size, num_hiddens)\n        context = enc_output[-1]\n        # Broadcast context to (num_steps, batch_size, num_hiddens)\n        context = context.repeat(embs.shape[0], 1, 1)\n        # Concat at the feature dimension\n        embs_and_context = d2l.concat((embs, context), -1)\n        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n        outputs = d2l.swapaxes(self.dense(outputs), 0, 1)\n        # outputs shape: (batch_size, num_steps, vocab_size)\n        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, [enc_output, hidden_state]\n```\n\n```{.python .input}\n%%tab tensorflow\nclass Seq2SeqDecoder(d2l.Decoder):\n    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0):\n        super().__init__()\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n        self.rnn = d2l.GRU(num_hiddens, num_layers, dropout)\n        self.dense = tf.keras.layers.Dense(vocab_size)\n            \n    def init_state(self, enc_all_outputs, *args):\n        return enc_all_outputs\n\n    def call(self, X, state):\n        # X shape: (batch_size, num_steps)\n        # embs shape: (num_steps, batch_size, embed_size)\n        embs = self.embedding(d2l.transpose(X))\n        enc_output, hidden_state = state\n        # context shape: (batch_size, num_hiddens)\n        context = enc_output[-1]\n        # Broadcast context to (num_steps, batch_size, num_hiddens)\n        context = tf.tile(tf.expand_dims(context, 0), (embs.shape[0], 1, 1))\n        # Concat at the feature dimension\n        embs_and_context = d2l.concat((embs, context), -1)\n        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n        outputs = d2l.transpose(self.dense(outputs), (1, 0, 2))\n        # outputs shape: (batch_size, num_steps, vocab_size)\n        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, [enc_output, hidden_state]\n```\n\n```{.python .input}\n%%tab jax\nclass Seq2SeqDecoder(d2l.Decoder):\n    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n    vocab_size: int\n    embed_size: int\n    num_hiddens: int\n    num_layers: int\n    dropout: float = 0\n\n    def setup(self):\n        self.embedding = nn.Embed(self.vocab_size, self.embed_size)\n        self.rnn = d2l.GRU(self.num_hiddens, self.num_layers, self.dropout)\n        self.dense = nn.Dense(self.vocab_size)\n\n    def init_state(self, enc_all_outputs, *args):\n        return enc_all_outputs\n\n    def __call__(self, X, state, training=False):\n        # X shape: (batch_size, num_steps)\n        # embs shape: (num_steps, batch_size, embed_size)\n        embs = self.embedding(d2l.astype(d2l.transpose(X), d2l.int32))\n        enc_output, hidden_state = state\n        # context shape: (batch_size, num_hiddens)\n        context = enc_output[-1]\n        # Broadcast context to (num_steps, batch_size, num_hiddens)\n        context = jnp.tile(context, (embs.shape[0], 1, 1))\n        # Concat at the feature dimension\n        embs_and_context = d2l.concat((embs, context), -1)\n        outputs, hidden_state = self.rnn(embs_and_context, hidden_state,\n                                         training=training)\n        outputs = d2l.swapaxes(self.dense(outputs), 0, 1)\n        # outputs shape: (batch_size, num_steps, vocab_size)\n        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n        return outputs, [enc_output, hidden_state]\n```\n\nTo [**illustrate the implemented decoder**],\nbelow we instantiate it with the same hyperparameters from the aforementioned encoder."
    },
    {
      "chunk_id": "ca145c3c48c5_2",
      "chapter": "seq2seq",
      "heading": "[**Decoder**]",
      "text": "As we can see, the output shape of the decoder becomes (batch size, number of time steps, vocabulary size),\nwhere the final dimension of the tensor stores the predicted token distribution. ```{.python .input}\n%%tab all\ndecoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\nif tab.selected('mxnet', 'pytorch', 'tensorflow'):\n    state = decoder.init_state(encoder(X))\n    dec_outputs, state = decoder(X, state)\nif tab.selected('jax'):\n    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])\n    (dec_outputs, state), _ = decoder.init_with_output(d2l.get_key(), X,\n                                                       state)\n\n\nd2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))\nif tab.selected('tensorflow'):\n    d2l.check_len(state[1], num_layers)\n    d2l.check_shape(state[1][0], (batch_size, num_hiddens))\n```\n\nThe layers in the above RNN encoder--decoder model \nare summarized in :numref:`fig_seq2seq_details`. ![Layers in an RNN encoder--decoder model.](../img/seq2seq-details.svg)\n:label:`fig_seq2seq_details`"
    },
    {
      "chunk_id": "d5b1e79070e6_0",
      "chapter": "seq2seq",
      "heading": "Encoder--Decoder for Sequence-to-Sequence Learning",
      "text": "Putting it all together in code yields the following:\n\n```{.python .input}\n%%tab pytorch, tensorflow, mxnet\nclass Seq2Seq(d2l.EncoderDecoder):  #@save\n    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n    def __init__(self, encoder, decoder, tgt_pad, lr):\n        super().__init__(encoder, decoder)\n        self.save_hyperparameters()\n        \n    def validation_step(self, batch):\n        Y_hat = self(*batch[:-1])\n        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n        \n    def configure_optimizers(self):\n        # Adam optimizer is used here\n        if tab.selected('mxnet'):\n            return gluon.Trainer(self.parameters(), 'adam',\n                                 {'learning_rate': self.lr})\n        if tab.selected('pytorch'):\n            return torch.optim.Adam(self.parameters(), lr=self.lr)\n        if tab.selected('tensorflow'):\n            return tf.keras.optimizers.Adam(learning_rate=self.lr)\n```\n\n```{.python .input}\n%%tab jax\nclass Seq2Seq(d2l.EncoderDecoder):  #@save\n    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n    encoder: nn.Module\n    decoder: nn.Module\n    tgt_pad: int\n    lr: float\n\n    def validation_step(self, params, batch, state):\n        l, _ = self.loss(params, batch[:-1], batch[-1], state)\n        self.plot('loss', l, train=False)\n\n    def configure_optimizers(self):\n        # Adam optimizer is used here\n        return optax.adam(learning_rate=self.lr)\n```"
    },
    {
      "chunk_id": "8a3e869dac7c_0",
      "chapter": "seq2seq",
      "heading": "Loss Function with Masking",
      "text": "At each time step, the decoder predicts \na probability distribution for the output tokens.\nAs with language modeling, \nwe can apply softmax \nto obtain the distribution\nand calculate the cross-entropy loss for optimization.\nRecall from :numref:`sec_machine_translation`\nthat the special padding tokens\nare appended to the end of sequences\nand so sequences of varying lengths\ncan be efficiently loaded\nin minibatches of the same shape.\nHowever, prediction of padding tokens\nshould be excluded from loss calculations.\nTo this end, we can \n[**mask irrelevant entries with zero values**]\nso that multiplication \nof any irrelevant prediction\nwith zero equates to zero.\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(Seq2Seq)\ndef loss(self, Y_hat, Y):\n    l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\n    mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)\n    return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask)\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(Seq2Seq)\n@partial(jax.jit, static_argnums=(0, 5))\ndef loss(self, params, X, Y, state, averaged=False):\n    Y_hat = state.apply_fn({'params': params}, *X,\n                           rngs={'dropout': state.dropout_rng})\n    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n    Y = d2l.reshape(Y, (-1,))\n    fn = optax.softmax_cross_entropy_with_integer_labels\n    l = fn(Y_hat, Y)\n    mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)\n    return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask), {}\n```"
    },
    {
      "chunk_id": "c118aa67ada5_0",
      "chapter": "seq2seq",
      "heading": "[**Training**]",
      "text": ":label:`sec_seq2seq_training`\n\nNow we can [**create and train an RNN encoder--decoder model**]\nfor sequence-to-sequence learning on the machine translation dataset.\n\n```{.python .input}\n%%tab all\ndata = d2l.MTFraEng(batch_size=128) \nembed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    encoder = Seq2SeqEncoder(\n        len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n    decoder = Seq2SeqDecoder(\n        len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nif tab.selected('mxnet', 'pytorch'):\n    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                    lr=0.005)\nif tab.selected('jax'):\n    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                    lr=0.005, training=True)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        encoder = Seq2SeqEncoder(\n            len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n        decoder = Seq2SeqDecoder(\n            len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n        model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                        lr=0.005)\n    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "d253741a9d64_0",
      "chapter": "seq2seq",
      "heading": "[**Prediction**]",
      "text": "To predict the output sequence\nat each step, \nthe predicted token from the previous\ntime step is fed into the decoder as an input. One simple strategy is to sample whichever token\nthat has been assigned by the decoder the highest probability\nwhen predicting at each step. As in training, at the initial time step\nthe beginning-of-sequence (\"&lt;bos&gt;\") token\nis fed into the decoder. This prediction process\nis illustrated in :numref:`fig_seq2seq_predict`. When the end-of-sequence (\"&lt;eos&gt;\") token is predicted,\nthe prediction of the output sequence is complete. ![Predicting the output sequence token by token using an RNN encoder--decoder.](../img/seq2seq-predict.svg)\n:label:`fig_seq2seq_predict`\n\nIn the next section, we will introduce \nmore sophisticated strategies \nbased on beam search (:numref:`sec_beam-search`)."
    },
    {
      "chunk_id": "d253741a9d64_1",
      "chapter": "seq2seq",
      "heading": "[**Prediction**]",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(d2l.EncoderDecoder)  #@save\ndef predict_step(self, batch, device, num_steps,\n                 save_attention_weights=False):\n    if tab.selected('mxnet', 'pytorch'):\n        batch = [d2l.to(a, device) for a in batch]\n    src, tgt, src_valid_len, _ = batch\n    if tab.selected('mxnet', 'pytorch'):\n        enc_all_outputs = self.encoder(src, src_valid_len)\n    if tab.selected('tensorflow'):\n        enc_all_outputs = self.encoder(src, src_valid_len, training=False)\n    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n    outputs, attention_weights = [d2l.expand_dims(tgt[:, 0], 1), ], []\n    for _ in range(num_steps):\n        if tab.selected('mxnet', 'pytorch'):\n            Y, dec_state = self.decoder(outputs[-1], dec_state)\n        if tab.selected('tensorflow'):\n            Y, dec_state = self.decoder(outputs[-1], dec_state, training=False)\n        outputs.append(d2l.argmax(Y, 2))\n        # Save attention weights (to be covered later)\n        if save_attention_weights:\n            attention_weights.append(self.decoder.attention_weights)\n    return d2l.concat(outputs[1:], 1), attention_weights\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(d2l.EncoderDecoder)  #@save\ndef predict_step(self, params, batch, num_steps,\n                 save_attention_weights=False):\n    src, tgt, src_valid_len, _ = batch\n    enc_all_outputs, inter_enc_vars = self.encoder.apply(\n        {'params': params['encoder']}, src, src_valid_len, training=False,\n        mutable='intermediates')\n    # Save encoder attention weights if inter_enc_vars containing encoder\n    # attention weights is not empty."
    },
    {
      "chunk_id": "d253741a9d64_2",
      "chapter": "seq2seq",
      "heading": "[**Prediction**]",
      "text": "(to be covered later)\n    enc_attention_weights = []\n    if bool(inter_enc_vars) and save_attention_weights:\n        # Encoder Attention Weights saved in the intermediates collection\n        enc_attention_weights = inter_enc_vars[\n            'intermediates']['enc_attention_weights'][0]\n\n    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n    outputs, attention_weights = [d2l.expand_dims(tgt[:,0], 1), ], []\n    for _ in range(num_steps):\n        (Y, dec_state), inter_dec_vars = self.decoder.apply(\n            {'params': params['decoder']}, outputs[-1], dec_state,\n            training=False, mutable='intermediates')\n        outputs.append(d2l.argmax(Y, 2))\n        # Save attention weights (to be covered later)\n        if save_attention_weights:\n            # Decoder Attention Weights saved in the intermediates collection\n            dec_attention_weights = inter_dec_vars[\n                'intermediates']['dec_attention_weights'][0]\n            attention_weights.append(dec_attention_weights)\n    return d2l.concat(outputs[1:], 1), (attention_weights,\n                                        enc_attention_weights)\n```"
    },
    {
      "chunk_id": "ed2cfe17fb72_0",
      "chapter": "seq2seq",
      "heading": "Evaluation of Predicted Sequences",
      "text": "We can evaluate a predicted sequence\nby comparing it with the\ntarget sequence (the ground truth). But what precisely is the appropriate measure \nfor comparing similarity between two sequences? Bilingual Evaluation Understudy (BLEU),\nthough originally proposed for evaluating\nmachine translation results :cite:`Papineni.Roukos.Ward.ea.2002`,\nhas been extensively used in measuring\nthe quality of output sequences for different applications. In principle, for any $n$-gram (:numref:`subsec_markov-models-and-n-grams`) in the predicted sequence,\nBLEU evaluates whether this $n$-gram appears\nin the target sequence. Denote by $p_n$ the precision of an $n$-gram,\ndefined as the ratio \nof the number of matched $n$-grams in\nthe predicted and target sequences\nto the number of $n$-grams in the predicted sequence. To explain, given a target sequence $A$, $B$, $C$, $D$, $E$, $F$,\nand a predicted sequence $A$, $B$, $B$, $C$, $D$,\nwe have $p_1 = 4/5$,  $p_2 = 3/4$, $p_3 = 1/3$, and $p_4 = 0$. Now let $\\textrm{len}_{\\textrm{label}}$ and $\\textrm{len}_{\\textrm{pred}}$\nbe the numbers of tokens in the target sequence \nand the predicted sequence, respectively. Then, BLEU is defined as\n\n$$ \\exp\\left(\\min\\left(0, 1 - \\frac{\\textrm{len}_{\\textrm{label}}}{\\textrm{len}_{\\textrm{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},$$\n:eqlabel:`eq_bleu`\n\nwhere $k$ is the longest $n$-gram for matching. Based on the definition of BLEU in :eqref:`eq_bleu`,\nwhenever the predicted sequence is the same as the target sequence, BLEU is 1. Moreover,\nsince matching longer $n$-grams is more difficult,\nBLEU assigns a greater weight\nwhen a longer $n$-gram has high precision. Specifically, when $p_n$ is fixed,\n$p_n^{1/2^n}$ increases as $n$ grows (the original paper uses $p_n^{1/n}$). Furthermore,\nsince\npredicting shorter sequences\ntends to yield a higher $p_n$ value,\nthe coefficient before the multiplication term in :eqref:`eq_bleu`\npenalizes shorter predicted sequences."
    },
    {
      "chunk_id": "ed2cfe17fb72_1",
      "chapter": "seq2seq",
      "heading": "Evaluation of Predicted Sequences",
      "text": "Furthermore,\nsince\npredicting shorter sequences\ntends to yield a higher $p_n$ value,\nthe coefficient before the multiplication term in :eqref:`eq_bleu`\npenalizes shorter predicted sequences. For example, when $k=2$,\ngiven the target sequence $A$, $B$, $C$, $D$, $E$, $F$ and the predicted sequence $A$, $B$,\nalthough $p_1 = p_2 = 1$, the penalty factor $\\exp(1-6/2) \\approx 0.14$ lowers the BLEU. We [**implement the BLEU measure**] as follows. ```{.python .input}\n%%tab all\ndef bleu(pred_seq, label_seq, k):  #@save\n    \"\"\"Compute the BLEU.\"\"\"\n    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n    len_pred, len_label = len(pred_tokens), len(label_tokens)\n    score = math.exp(min(0, 1 - len_label / len_pred))\n    for n in range(1, min(k, len_pred) + 1):\n        num_matches, label_subs = 0, collections.defaultdict(int)\n        for i in range(len_label - n + 1):\n            label_subs[' '.join(label_tokens[i: i + n])] += 1\n        for i in range(len_pred - n + 1):\n            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n                num_matches += 1\n                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n    return score\n```\n\nIn the end,\nwe use the trained RNN encoder--decoder\nto [**translate a few English sentences into French**]\nand compute the BLEU of the results."
    },
    {
      "chunk_id": "ed2cfe17fb72_2",
      "chapter": "seq2seq",
      "heading": "Evaluation of Predicted Sequences",
      "text": "```{.python .input}\n%%tab all\nengs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\nfras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\nif tab.selected('pytorch', 'mxnet', 'tensorflow'):\n    preds, _ = model.predict_step(\n        data.build(engs, fras), d2l.try_gpu(), data.num_steps)\nif tab.selected('jax'):\n    preds, _ = model.predict_step(trainer.state.params, data.build(engs, fras),\n                                  data.num_steps)\nfor en, fr, p in zip(engs, fras, preds):\n    translation = []\n    for token in data.tgt_vocab.to_tokens(p):\n        if token == '<eos>':\n            break\n        translation.append(token)        \n    print(f'{en} => {translation}, bleu,'\n          f'{bleu(\" \".join(translation), fr, k=2):.3f}')\n```"
    },
    {
      "chunk_id": "09e56f8a84b2_0",
      "chapter": "seq2seq",
      "heading": "Summary",
      "text": "Following the design of the encoder--decoder architecture, we can use two RNNs to design a model for sequence-to-sequence learning.\nIn encoder--decoder training, the teacher forcing approach feeds original output sequences (in contrast to predictions) into the decoder.\nWhen implementing the encoder and the decoder, we can use multilayer RNNs.\nWe can use masks to filter out irrelevant computations, such as when calculating the loss.\nFor evaluating output sequences,\nBLEU is a popular measure that matches $n$-grams between the predicted sequence and the target sequence."
    },
    {
      "chunk_id": "3ce322d3a228_0",
      "chapter": "seq2seq",
      "heading": "Exercises",
      "text": "1. Can you adjust the hyperparameters to improve the translation results?\n1. Rerun the experiment without using masks in the loss calculation. What results do you observe? Why?\n1. If the encoder and the decoder differ in the number of layers or the number of hidden units, how can we initialize the hidden state of the decoder?\n1. In training, replace teacher forcing with feeding the prediction at the previous time step into the decoder. How does this influence the performance?\n1. Rerun the experiment by replacing GRU with LSTM.\n1. Are there any other ways to design the output layer of the decoder?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/345)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1062)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/3865)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18022)\n:end_tab:"
    },
    {
      "chunk_id": "127aca0232f1_0",
      "chapter": "bptt",
      "heading": "bptt",
      "text": "# Backpropagation Through Time\n:label:`sec_bptt`\n\nIf you completed the exercises in :numref:`sec_rnn-scratch`,\nyou would have seen that gradient clipping is vital \nfor preventing the occasional massive gradients\nfrom destabilizing training. We hinted that the exploding gradients\nstem from backpropagating across long sequences. Before introducing a slew of modern RNN architectures,\nlet's take a closer look at how *backpropagation*\nworks in sequence models in mathematical detail. Hopefully, this discussion will bring some precision \nto the notion of *vanishing* and *exploding* gradients. If you recall our discussion of forward and backward \npropagation through computational graphs\nwhen we introduced MLPs in :numref:`sec_backprop`,\nthen forward propagation in RNNs\nshould be relatively straightforward. Applying backpropagation in RNNs \nis called *backpropagation through time* :cite:`Werbos.1990`. This procedure requires us to expand (or unroll) \nthe computational graph of an RNN\none time step at a time. The unrolled RNN is essentially \na feedforward neural network \nwith the special property \nthat the same parameters \nare repeated throughout the unrolled network,\nappearing at each time step. Then, just as in any feedforward neural network,\nwe can apply the chain rule, \nbackpropagating gradients through the unrolled net. The gradient with respect to each parameter\nmust be summed across all places \nthat the parameter occurs in the unrolled net. Handling such weight tying should be familiar \nfrom our chapters on convolutional neural networks. Complications arise because sequences\ncan be rather long. It is not unusual to work with text sequences\nconsisting of over a thousand tokens. Note that this poses problems both from \na computational (too much memory)\nand optimization (numerical instability)\nstandpoint. Input from the first step passes through\nover 1000 matrix products before arriving at the output, \nand another 1000 matrix products \nare required to compute the gradient."
    },
    {
      "chunk_id": "127aca0232f1_1",
      "chapter": "bptt",
      "heading": "bptt",
      "text": "Input from the first step passes through\nover 1000 matrix products before arriving at the output, \nand another 1000 matrix products \nare required to compute the gradient. We now analyze what can go wrong and \nhow to address it in practice."
    },
    {
      "chunk_id": "06f287ed78ed_0",
      "chapter": "bptt",
      "heading": "Analysis of Gradients in RNNs",
      "text": ":label:`subsec_bptt_analysis`\n\nWe start with a simplified model of how an RNN works. This model ignores details about the specifics \nof the hidden state and how it is updated. The mathematical notation here\ndoes not explicitly distinguish\nscalars, vectors, and matrices. We are just trying to develop some intuition. In this simplified model,\nwe denote $h_t$ as the hidden state,\n$x_t$ as input, and $o_t$ as output\nat time step $t$. Recall our discussions in\n:numref:`subsec_rnn_w_hidden_states`\nthat the input and the hidden state\ncan be concatenated before being multiplied \nby one weight variable in the hidden layer. Thus, we use $w_\\textrm{h}$ and $w_\\textrm{o}$ to indicate the weights \nof the hidden layer and the output layer, respectively. As a result, the hidden states and outputs \nat each time step are\n\n$$\\begin{aligned}h_t &= f(x_t, h_{t-1}, w_\\textrm{h}),\\\\o_t &= g(h_t, w_\\textrm{o}),\\end{aligned}$$\n:eqlabel:`eq_bptt_ht_ot`\n\nwhere $f$ and $g$ are transformations\nof the hidden layer and the output layer, respectively. Hence, we have a chain of values \n$\\{\\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \\ldots\\}$ \nthat depend on each other via recurrent computation. The forward propagation is fairly straightforward. All we need is to loop through the $(x_t, h_t, o_t)$ triples one time step at a time. The discrepancy between output $o_t$ and the desired target $y_t$ \nis then evaluated by an objective function \nacross all the $T$ time steps as\n\n$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_\\textrm{h}, w_\\textrm{o}) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t).$$\n\n\n\nFor backpropagation, matters are a bit trickier, \nespecially when we compute the gradients \nwith regard to the parameters $w_\\textrm{h}$ of the objective function $L$."
    },
    {
      "chunk_id": "06f287ed78ed_1",
      "chapter": "bptt",
      "heading": "Analysis of Gradients in RNNs",
      "text": "To be specific, by the chain rule,\n\n$$\\begin{aligned}\\frac{\\partial L}{\\partial w_\\textrm{h}}  & = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_\\textrm{h}}  \\\\& = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_\\textrm{o})}{\\partial h_t}  \\frac{\\partial h_t}{\\partial w_\\textrm{h}}.\\end{aligned}$$\n:eqlabel:`eq_bptt_partial_L_wh`\n\nThe first and the second factors of the\nproduct in :eqref:`eq_bptt_partial_L_wh`\nare easy to compute. The third factor $\\partial h_t/\\partial w_\\textrm{h}$ is where things get tricky, \nsince we need to recurrently compute the effect of the parameter $w_\\textrm{h}$ on $h_t$. According to the recurrent computation\nin :eqref:`eq_bptt_ht_ot`,\n$h_t$ depends on both $h_{t-1}$ and $w_\\textrm{h}$,\nwhere computation of $h_{t-1}$\nalso depends on $w_\\textrm{h}$. Thus, evaluating the total derivate of $h_t$ \nwith respect to $w_\\textrm{h}$ using the chain rule yields\n\n$$\\frac{\\partial h_t}{\\partial w_\\textrm{h}}= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}} +\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}.$$\n:eqlabel:`eq_bptt_partial_ht_wh_recur`\n\n\nTo derive the above gradient, assume that we have \nthree sequences $\\{a_{t}\\},\\{b_{t}\\},\\{c_{t}\\}$ \nsatisfying $a_{0}=0$ and $a_{t}=b_{t}+c_{t}a_{t-1}$ for $t=1, 2,\\ldots$. Then for $t\\geq 1$, it is easy to show\n\n$$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}.$$\n:eqlabel:`eq_bptt_at`\n\nBy substituting $a_t$, $b_t$, and $c_t$ according to\n\n$$\\begin{aligned}a_t &= \\frac{\\partial h_t}{\\partial w_\\textrm{h}},\\\\\nb_t &= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}, \\\\\nc_t &= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}},\\end{aligned}$$\n\nthe gradient computation in :eqref:`eq_bptt_partial_ht_wh_recur` satisfies\n$a_{t}=b_{t}+c_{t}a_{t-1}$."
    },
    {
      "chunk_id": "06f287ed78ed_2",
      "chapter": "bptt",
      "heading": "Analysis of Gradients in RNNs",
      "text": "Thus, per :eqref:`eq_bptt_at`, \nwe can remove the recurrent computation \nin :eqref:`eq_bptt_partial_ht_wh_recur` with\n\n$$\\frac{\\partial h_t}{\\partial w_\\textrm{h}}=\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_\\textrm{h})}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_{i},h_{i-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}.$$\n:eqlabel:`eq_bptt_partial_ht_wh_gen`\n\nWhile we can use the chain rule to compute $\\partial h_t/\\partial w_\\textrm{h}$ recursively, \nthis chain can get very long whenever $t$ is large. Let's discuss a number of strategies for dealing with this problem."
    },
    {
      "chunk_id": "775cdabd4869_0",
      "chapter": "bptt",
      "heading": "Full Computation",
      "text": "One idea might be to compute the full sum in :eqref:`eq_bptt_partial_ht_wh_gen`.\nHowever, this is very slow and gradients can blow up,\nsince subtle changes in the initial conditions\ncan potentially affect the outcome a lot.\nThat is, we could see things similar to the butterfly effect,\nwhere minimal changes in the initial conditions \nlead to disproportionate changes in the outcome.\nThis is generally undesirable.\nAfter all, we are looking for robust estimators that generalize well. \nHence this strategy is almost never used in practice."
    },
    {
      "chunk_id": "27041ed3997b_0",
      "chapter": "bptt",
      "heading": "Truncating Time Steps",
      "text": "Alternatively,\nwe can truncate the sum in\n:eqref:`eq_bptt_partial_ht_wh_gen`\nafter $\\tau$ steps. \nThis is what we have been discussing so far. \nThis leads to an *approximation* of the true gradient,\nsimply by terminating the sum at $\\partial h_{t-\\tau}/\\partial w_\\textrm{h}$. \nIn practice this works quite well. \nIt is what is commonly referred to as truncated \nbackpropgation through time :cite:`Jaeger.2002`.\nOne of the consequences of this is that the model \nfocuses primarily on short-term influence \nrather than long-term consequences. \nThis is actually *desirable*, since it biases the estimate \ntowards simpler and more stable models."
    },
    {
      "chunk_id": "27cf03c93cfa_0",
      "chapter": "bptt",
      "heading": "Randomized Truncation",
      "text": "Last, we can replace $\\partial h_t/\\partial w_\\textrm{h}$\nby a random variable which is correct in expectation \nbut truncates the sequence.\nThis is achieved by using a sequence of $\\xi_t$\nwith predefined $0 \\leq \\pi_t \\leq 1$,\nwhere $P(\\xi_t = 0) = 1-\\pi_t$ and \n$P(\\xi_t = \\pi_t^{-1}) = \\pi_t$, thus $E[\\xi_t] = 1$.\nWe use this to replace the gradient\n$\\partial h_t/\\partial w_\\textrm{h}$\nin :eqref:`eq_bptt_partial_ht_wh_recur`\nwith\n\n$$z_t= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}.$$\n\n\nIt follows from the definition of $\\xi_t$ \nthat $E[z_t] = \\partial h_t/\\partial w_\\textrm{h}$.\nWhenever $\\xi_t = 0$ the recurrent computation\nterminates at that time step $t$.\nThis leads to a weighted sum of sequences of varying lengths,\nwhere long sequences are rare but appropriately overweighted. \nThis idea was proposed by \n:citet:`Tallec.Ollivier.2017`."
    },
    {
      "chunk_id": "8b380eb91d27_0",
      "chapter": "bptt",
      "heading": "Comparing Strategies",
      "text": "![Comparing strategies for computing gradients in RNNs. From top to bottom: randomized truncation, regular truncation, and full computation.](../img/truncated-bptt.svg)\n:label:`fig_truncated_bptt`\n\n\n:numref:`fig_truncated_bptt` illustrates the three strategies \nwhen analyzing the first few characters of *The Time Machine* \nusing backpropagation through time for RNNs:\n\n* The first row is the randomized truncation that partitions the text into segments of varying lengths.\n* The second row is the regular truncation that breaks the text into subsequences of the same length. This is what we have been doing in RNN experiments.\n* The third row is the full backpropagation through time that leads to a computationally infeasible expression.\n\n\nUnfortunately, while appealing in theory, \nrandomized truncation does not work \nmuch better than regular truncation, \nmost likely due to a number of factors.\nFirst, the effect of an observation\nafter a number of backpropagation steps \ninto the past is quite sufficient \nto capture dependencies in practice. \nSecond, the increased variance counteracts the fact \nthat the gradient is more accurate with more steps. \nThird, we actually *want* models that have only \na short range of interactions. \nHence, regularly truncated backpropagation through time \nhas a slight regularizing effect that can be desirable."
    },
    {
      "chunk_id": "c3f4497cabd5_0",
      "chapter": "bptt",
      "heading": "Backpropagation Through Time in Detail",
      "text": "After discussing the general principle,\nlet's discuss backpropagation through time in detail. In contrast to the analysis in :numref:`subsec_bptt_analysis`,\nin the following we will show how to compute\nthe gradients of the objective function\nwith respect to all the decomposed model parameters. To keep things simple, we consider \nan RNN without bias parameters,\nwhose activation function in the hidden layer\nuses the identity mapping ($\\phi(x)=x$). For time step $t$, let the single example input \nand the target be $\\mathbf{x}_t \\in \\mathbb{R}^d$ and $y_t$, respectively. The hidden state $\\mathbf{h}_t \\in \\mathbb{R}^h$ \nand the output $\\mathbf{o}_t \\in \\mathbb{R}^q$\nare computed as\n\n$$\\begin{aligned}\\mathbf{h}_t &= \\mathbf{W}_\\textrm{hx} \\mathbf{x}_t + \\mathbf{W}_\\textrm{hh} \\mathbf{h}_{t-1},\\\\\n\\mathbf{o}_t &= \\mathbf{W}_\\textrm{qh} \\mathbf{h}_{t},\\end{aligned}$$\n\nwhere $\\mathbf{W}_\\textrm{hx} \\in \\mathbb{R}^{h \\times d}$, $\\mathbf{W}_\\textrm{hh} \\in \\mathbb{R}^{h \\times h}$, and\n$\\mathbf{W}_\\textrm{qh} \\in \\mathbb{R}^{q \\times h}$\nare the weight parameters. Denote by $l(\\mathbf{o}_t, y_t)$\nthe loss at time step $t$. Our objective function,\nthe loss over $T$ time steps\nfrom the beginning of the sequence is thus\n\n$$L = \\frac{1}{T} \\sum_{t=1}^T l(\\mathbf{o}_t, y_t).$$\n\n\nIn order to visualize the dependencies among\nmodel variables and parameters during computation\nof the RNN,\nwe can draw a computational graph for the model,\nas shown in :numref:`fig_rnn_bptt`. For example, the computation of the hidden states of time step 3,\n$\\mathbf{h}_3$, depends on the model parameters\n$\\mathbf{W}_\\textrm{hx}$ and $\\mathbf{W}_\\textrm{hh}$,\nthe hidden state of the previous time step $\\mathbf{h}_2$,\nand the input of the current time step $\\mathbf{x}_3$. ![Computational graph showing dependencies for an RNN model with three time steps."
    },
    {
      "chunk_id": "c3f4497cabd5_1",
      "chapter": "bptt",
      "heading": "Backpropagation Through Time in Detail",
      "text": "![Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators.](../img/rnn-bptt.svg)\n:label:`fig_rnn_bptt`\n\nAs just mentioned, the model parameters in :numref:`fig_rnn_bptt` \nare $\\mathbf{W}_\\textrm{hx}$, $\\mathbf{W}_\\textrm{hh}$, and $\\mathbf{W}_\\textrm{qh}$. Generally, training this model requires \ngradient computation with respect to these parameters\n$\\partial L/\\partial \\mathbf{W}_\\textrm{hx}$, $\\partial L/\\partial \\mathbf{W}_\\textrm{hh}$, and $\\partial L/\\partial \\mathbf{W}_\\textrm{qh}$. According to the dependencies in :numref:`fig_rnn_bptt`,\nwe can traverse in the opposite direction of the arrows\nto calculate and store the gradients in turn. To flexibly express the multiplication of \nmatrices, vectors, and scalars of different shapes\nin the chain rule,\nwe continue to use the $\\textrm{prod}$ operator \nas described in :numref:`sec_backprop`. First of all, differentiating the objective function\nwith respect to the model output at any time step $t$\nis fairly straightforward:\n\n$$\\frac{\\partial L}{\\partial \\mathbf{o}_t} =  \\frac{\\partial l (\\mathbf{o}_t, y_t)}{T \\cdot \\partial \\mathbf{o}_t} \\in \\mathbb{R}^q.$$\n:eqlabel:`eq_bptt_partial_L_ot`\n\nNow we can calculate the gradient of the objective \nwith respect to the parameter $\\mathbf{W}_\\textrm{qh}$\nin the output layer:\n$\\partial L/\\partial \\mathbf{W}_\\textrm{qh} \\in \\mathbb{R}^{q \\times h}$. Based on :numref:`fig_rnn_bptt`, \nthe objective $L$ depends on $\\mathbf{W}_\\textrm{qh}$ \nvia $\\mathbf{o}_1, \\ldots, \\mathbf{o}_T$. Using the chain rule yields\n\n$$\n\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{qh}}\n= \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{W}_\\textrm{qh}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{o}_t} \\mathbf{h}_t^\\top,\n$$\n\nwhere $\\partial L/\\partial \\mathbf{o}_t$\nis given by :eqref:`eq_bptt_partial_L_ot`."
    },
    {
      "chunk_id": "c3f4497cabd5_2",
      "chapter": "bptt",
      "heading": "Backpropagation Through Time in Detail",
      "text": "Next, as shown in :numref:`fig_rnn_bptt`,\nat the final time step $T$,\nthe objective function\n$L$ depends on the hidden state $\\mathbf{h}_T$ \nonly via $\\mathbf{o}_T$. Therefore, we can easily find the gradient \n$\\partial L/\\partial \\mathbf{h}_T \\in \\mathbb{R}^h$\nusing the chain rule:\n\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_T} = \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_T}, \\frac{\\partial \\mathbf{o}_T}{\\partial \\mathbf{h}_T} \\right) = \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_T}.$$\n:eqlabel:`eq_bptt_partial_L_hT_final_step`\n\nIt gets trickier for any time step $t < T$,\nwhere the objective function $L$ depends on \n$\\mathbf{h}_t$ via $\\mathbf{h}_{t+1}$ and $\\mathbf{o}_t$. According to the chain rule,\nthe gradient of the hidden state\n$\\partial L/\\partial \\mathbf{h}_t \\in \\mathbb{R}^h$\nat any time step $t < T$ can be recurrently computed as:\n\n\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}, \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} \\right) + \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} \\right) = \\mathbf{W}_\\textrm{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} + \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}.$$\n:eqlabel:`eq_bptt_partial_L_ht_recur`\n\nFor analysis, expanding the recurrent computation\nfor any time step $1 \\leq t \\leq T$ gives\n\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_\\textrm{hh}^\\top\\right)}^{T-i} \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_{T+t-i}}.$$\n:eqlabel:`eq_bptt_partial_L_ht`\n\nWe can see from :eqref:`eq_bptt_partial_L_ht` \nthat this simple linear example already\nexhibits some key problems of long sequence models:\nit involves potentially very large powers of $\\mathbf{W}_\\textrm{hh}^\\top$. In it, eigenvalues smaller than 1 vanish\nand eigenvalues larger than 1 diverge."
    },
    {
      "chunk_id": "c3f4497cabd5_3",
      "chapter": "bptt",
      "heading": "Backpropagation Through Time in Detail",
      "text": "In it, eigenvalues smaller than 1 vanish\nand eigenvalues larger than 1 diverge. This is numerically unstable,\nwhich manifests itself in the form of vanishing \nand exploding gradients. One way to address this is to truncate the time steps\nat a computationally convenient size \nas discussed in :numref:`subsec_bptt_analysis`. In practice, this truncation can also be effected \nby detaching the gradient after a given number of time steps. Later on, we will see how more sophisticated sequence models \nsuch as long short-term memory can alleviate this further. Finally, :numref:`fig_rnn_bptt` shows \nthat the objective function $L$ \ndepends on model parameters $\\mathbf{W}_\\textrm{hx}$ and $\\mathbf{W}_\\textrm{hh}$\nin the hidden layer via hidden states\n$\\mathbf{h}_1, \\ldots, \\mathbf{h}_T$. To compute gradients with respect to such parameters\n$\\partial L / \\partial \\mathbf{W}_\\textrm{hx} \\in \\mathbb{R}^{h \\times d}$ and $\\partial L / \\partial \\mathbf{W}_\\textrm{hh} \\in \\mathbb{R}^{h \\times h}$,\nwe apply the chain rule giving\n\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{hx}}\n&= \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_\\textrm{hx}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{x}_t^\\top,\\\\\n\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{hh}}\n&= \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_\\textrm{hh}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{h}_{t-1}^\\top,\n\\end{aligned}\n$$\n\nwhere $\\partial L/\\partial \\mathbf{h}_t$\nwhich is recurrently computed by\n:eqref:`eq_bptt_partial_L_hT_final_step`\nand :eqref:`eq_bptt_partial_L_ht_recur`\nis the key quantity that affects the numerical stability. Since backpropagation through time is the application of backpropagation in RNNs,\nas we have explained in :numref:`sec_backprop`,\ntraining RNNs alternates forward propagation with\nbackpropagation through time."
    },
    {
      "chunk_id": "c3f4497cabd5_4",
      "chapter": "bptt",
      "heading": "Backpropagation Through Time in Detail",
      "text": "Since backpropagation through time is the application of backpropagation in RNNs,\nas we have explained in :numref:`sec_backprop`,\ntraining RNNs alternates forward propagation with\nbackpropagation through time. Moreover, backpropagation through time\ncomputes and stores the above gradients in turn. Specifically, stored intermediate values\nare reused to avoid duplicate calculations,\nsuch as storing $\\partial L/\\partial \\mathbf{h}_t$\nto be used in computation of both $\\partial L / \\partial \\mathbf{W}_\\textrm{hx}$ \nand $\\partial L / \\partial \\mathbf{W}_\\textrm{hh}$."
    },
    {
      "chunk_id": "63c9ac44e697_0",
      "chapter": "bptt",
      "heading": "Summary",
      "text": "Backpropagation through time is merely an application of backpropagation to sequence models with a hidden state.\nTruncation, such as regular or randomized, is needed for computational convenience and numerical stability.\nHigh powers of matrices can lead to divergent or vanishing eigenvalues. This manifests itself in the form of exploding or vanishing gradients.\nFor efficient computation, intermediate values are cached during backpropagation through time."
    },
    {
      "chunk_id": "879d692d33e5_0",
      "chapter": "bptt",
      "heading": "Exercises",
      "text": "1. Assume that we have a symmetric matrix $\\mathbf{M} \\in \\mathbb{R}^{n \\times n}$ with eigenvalues $\\lambda_i$ whose corresponding eigenvectors are $\\mathbf{v}_i$ ($i = 1, \\ldots, n$). Without loss of generality, assume that they are ordered in the order $|\\lambda_i| \\geq |\\lambda_{i+1}|$. \n   1. Show that $\\mathbf{M}^k$ has eigenvalues $\\lambda_i^k$.\n   1. Prove that for a random vector $\\mathbf{x} \\in \\mathbb{R}^n$, with high probability $\\mathbf{M}^k \\mathbf{x}$ will be very much aligned with the eigenvector $\\mathbf{v}_1$ \nof $\\mathbf{M}$. Formalize this statement.\n   1. What does the above result mean for gradients in RNNs?\n1. Besides gradient clipping, can you think of any other methods to cope with gradient explosion in recurrent neural networks?\n\n[Discussions](https://discuss.d2l.ai/t/334)"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Recurrent Neural Networks\n:label:`chap_rnn`\n\nUp until now, we have focused primarily on fixed-length data. When introducing linear and logistic regression\nin :numref:`chap_regression` and :numref:`chap_classification`\nand multilayer perceptrons in :numref:`chap_perceptrons`,\nwe were happy to assume that each feature vector $\\mathbf{x}_i$\nconsisted of a fixed number of components $x_1, \\dots, x_d$,\nwhere each numerical feature $x_j$\ncorresponded to a particular attribute. These datasets are sometimes called *tabular*,\nbecause they can be arranged in tables,\nwhere each example $i$ gets its own row,\nand each attribute gets its own column. Crucially, with tabular data, we seldom\nassume any particular structure over the columns. Subsequently, in :numref:`chap_cnn`,\nwe moved on to image data, where inputs consist\nof the raw pixel values at each coordinate in an image. Image data hardly fitted the bill\nof a protypical tabular dataset. There, we needed to call upon convolutional neural networks (CNNs)\nto handle the hierarchical structure and invariances. However, our data were still of fixed length. Every Fashion-MNIST image is represented\nas a $28 \\times 28$ grid of pixel values. Moreover, our goal was to develop a model\nthat looked at just one image and then\noutputted a single prediction. But what should we do when faced with a\nsequence of images, as in a video,\nor when tasked with producing\na sequentially structured prediction,\nas in the case of image captioning? A great many learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation\nall require that models produce outputs consisting of sequences. In other domains, such as time series prediction,\nvideo analysis, and musical information retrieval,\na model must learn from inputs that are sequences."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "In other domains, such as time series prediction,\nvideo analysis, and musical information retrieval,\na model must learn from inputs that are sequences. These demands often arise simultaneously:\ntasks such as translating passages of text\nfrom one natural language to another,\nengaging in dialogue, or controlling a robot,\ndemand that models both ingest and output\nsequentially structured data. Recurrent neural networks (RNNs) are deep learning models\nthat capture the dynamics of sequences via\n*recurrent* connections, which can be thought of\nas cycles in the network of nodes. This might seem counterintuitive at first. After all, it is the feedforward nature of neural networks\nthat makes the order of computation unambiguous. However, recurrent edges are defined in a precise way\nthat ensures that no such ambiguity can arise. Recurrent neural networks are *unrolled* across time steps (or sequence steps),\nwith the *same* underlying parameters applied at each step. While the standard connections are applied *synchronously*\nto propagate each layer's activations\nto the subsequent layer *at the same time step*,\nthe recurrent connections are *dynamic*,\npassing information across adjacent time steps. As the unfolded view in :numref:`fig_unfolded-rnn` reveals,\nRNNs can be thought of as feedforward neural networks\nwhere each layer's parameters (both conventional and recurrent)\nare shared across time steps. ![On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously.](../img/unfolded-rnn.svg)\n:label:`fig_unfolded-rnn`\n\n\nLike neural networks more broadly,\nRNNs have a long discipline-spanning history,\noriginating as models of the brain popularized\nby cognitive scientists and subsequently adopted\nas practical modeling tools employed\nby the machine learning community."
    },
    {
      "chunk_id": "01f4e33118cb_2",
      "chapter": "index",
      "heading": "index",
      "text": "As we do for deep learning more broadly,\nin this book we adopt the machine learning perspective,\nfocusing on RNNs as practical tools that rose\nto popularity in the 2010s owing to\nbreakthrough results on such diverse tasks\nas handwriting recognition :cite:`graves2008novel`,\nmachine translation :cite:`Sutskever.Vinyals.Le.2014`,\nand recognizing medical diagnoses :cite:`Lipton.Kale.2016`. We point the reader interested in more\nbackground material to a publicly available\ncomprehensive review :cite:`Lipton.Berkowitz.Elkan.2015`. We also note that sequentiality is not unique to RNNs. For example, the CNNs that we already introduced\ncan be adapted to handle data of varying length,\ne.g., images of varying resolution. Moreover, RNNs have recently ceded considerable\nmarket share to Transformer models,\nwhich will be covered in :numref:`chap_attention-and-transformers`. However, RNNs rose to prominence as the default models\nfor handling complex sequential structure in deep learning,\nand remain staple models for sequential modeling to this day. The stories of RNNs and of sequence modeling\nare inextricably linked, and this is as much\na chapter about the ABCs of sequence modeling problems\nas it is a chapter about RNNs. One key insight paved the way for a revolution in sequence modeling. While the inputs and targets for many fundamental tasks in machine learning\ncannot easily be represented as fixed-length vectors,\nthey can often nevertheless be represented as\nvarying-length sequences of fixed-length vectors. For example, documents can be represented as sequences of words;\nmedical records can often be represented as sequences of events\n(encounters, medications, procedures, lab tests, diagnoses);\nvideos can be represented as varying-length sequences of still images. While sequence models have popped up in numerous application areas,\nbasic research in the area has been driven predominantly\nby advances on core tasks in natural language processing."
    },
    {
      "chunk_id": "01f4e33118cb_3",
      "chapter": "index",
      "heading": "index",
      "text": "While sequence models have popped up in numerous application areas,\nbasic research in the area has been driven predominantly\nby advances on core tasks in natural language processing. Thus, throughout this chapter, we will focus\nour exposition and examples on text data. If you get the hang of these examples,\nthen applying the models to other data modalities\nshould be relatively straightforward. In the next few sections, we introduce basic\nnotation for sequences and some evaluation measures\nfor assessing the quality of sequentially structured model outputs. After that, we discuss basic concepts of a language model\nand use this discussion to motivate our first RNN models. Finally, we describe the method for calculating gradients\nwhen backpropagating through RNNs and explore some challenges\nthat are often encountered when training such networks,\nmotivating the modern RNN architectures that will follow\nin :numref:`chap_modern_rnn`. ```toc\n:maxdepth: 2\n\nsequence\ntext-sequence\nlanguage-model\nrnn\nrnn-scratch\nrnn-concise\nbptt\n```"
    },
    {
      "chunk_id": "8f6b4b4290f6_0",
      "chapter": "language-model",
      "heading": "language-model",
      "text": "# Language Models\n:label:`sec_language-model`\n\nIn :numref:`sec_text-sequence`, we saw how to map text sequences into tokens, where these tokens can be viewed as a sequence of discrete observations such as words or characters. Assume that the tokens in a text sequence of length $T$ are in turn $x_1, x_2, \\ldots, x_T$. The goal of *language models*\nis to estimate the joint probability of the whole sequence:\n\n$$P(x_1, x_2, \\ldots, x_T),$$\n\nwhere statistical tools\nin :numref:`sec_sequence`\ncan be applied. Language models are incredibly useful. For instance, an ideal language model should generate natural text on its own, simply by drawing one token at a time $x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1)$. Quite unlike the monkey using a typewriter, all text emerging from such a model would pass as natural language, e.g., English text. Furthermore, it would be sufficient for generating a meaningful dialog, simply by conditioning the text on previous dialog fragments. Clearly we are still very far from designing such a system, since it would need to *understand* the text rather than just generate grammatically sensible content. Nonetheless, language models are of great service even in their limited form. For instance, the phrases \"to recognize speech\" and \"to wreck a nice beach\" sound very similar. This can cause ambiguity in speech recognition,\nwhich is easily resolved through a language model that rejects the second translation as outlandish. Likewise, in a document summarization algorithm\nit is worthwhile knowing that \"dog bites man\" is much more frequent than \"man bites dog\", or that \"I want to eat grandma\" is a rather disturbing statement, whereas \"I want to eat, grandma\" is much more benign."
    },
    {
      "chunk_id": "8f6b4b4290f6_1",
      "chapter": "language-model",
      "heading": "language-model",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n```{.python .input  n=2}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input  n=3}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input  n=4}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "f56ea4fc3e63_0",
      "chapter": "language-model",
      "heading": "Learning Language Models",
      "text": "The obvious question is how we should model a document, or even a sequence of tokens. \nSuppose that we tokenize text data at the word level.\nLet's start by applying basic probability rules:\n\n$$P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^T P(x_t  \\mid  x_1, \\ldots, x_{t-1}).$$\n\nFor example, \nthe probability of a text sequence containing four words would be given as:\n\n$$\\begin{aligned}&P(\\textrm{deep}, \\textrm{learning}, \\textrm{is}, \\textrm{fun}) \\\\\n=&P(\\textrm{deep}) P(\\textrm{learning}  \\mid  \\textrm{deep}) P(\\textrm{is}  \\mid  \\textrm{deep}, \\textrm{learning}) P(\\textrm{fun}  \\mid  \\textrm{deep}, \\textrm{learning}, \\textrm{is}).\\end{aligned}$$"
    },
    {
      "chunk_id": "cb3c4c448a6d_0",
      "chapter": "language-model",
      "heading": "Markov Models and $n$-grams",
      "text": ":label:`subsec_markov-models-and-n-grams`\n\nAmong those sequence model analyses in :numref:`sec_sequence`,\nlet's apply Markov models to language modeling.\nA distribution over sequences satisfies the Markov property of first order if $P(x_{t+1} \\mid x_t, \\ldots, x_1) = P(x_{t+1} \\mid x_t)$. Higher orders correspond to longer dependencies. This leads to a number of approximations that we could apply to model a sequence:\n\n$$\n\\begin{aligned}\nP(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\\\\nP(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_2) P(x_4  \\mid  x_3),\\\\\nP(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_1, x_2) P(x_4  \\mid  x_2, x_3).\n\\end{aligned}\n$$\n\nThe probability formulae that involve one, two, and three variables are typically referred to as *unigram*, *bigram*, and *trigram* models, respectively. \nIn order to compute the language model, we need to calculate the\nprobability of words and the conditional probability of a word given\nthe previous few words.\nNote that\nsuch probabilities are\nlanguage model parameters."
    },
    {
      "chunk_id": "81863acc723f_0",
      "chapter": "language-model",
      "heading": "Word Frequency",
      "text": "Here, we\nassume that the training dataset is a large text corpus, such as all\nWikipedia entries, [Project Gutenberg](https://en.wikipedia.org/wiki/Project_Gutenberg),\nand all text posted on the\nweb.\nThe probability of words can be calculated from the relative word\nfrequency of a given word in the training dataset.\nFor example, the estimate $\\hat{P}(\\textrm{deep})$ can be calculated as the\nprobability of any sentence starting with the word \"deep\". A\nslightly less accurate approach would be to count all occurrences of\nthe word \"deep\" and divide it by the total number of words in\nthe corpus.\nThis works fairly well, particularly for frequent\nwords. Moving on, we could attempt to estimate\n\n$$\\hat{P}(\\textrm{learning} \\mid \\textrm{deep}) = \\frac{n(\\textrm{deep, learning})}{n(\\textrm{deep})},$$\n\nwhere $n(x)$ and $n(x, x')$ are the number of occurrences of singletons\nand consecutive word pairs, respectively.\nUnfortunately, \nestimating the\nprobability of a word pair is somewhat more difficult, since the\noccurrences of \"deep learning\" are a lot less frequent. \nIn particular, for some unusual word combinations it may be tricky to\nfind enough occurrences to get accurate estimates.\nAs suggested by the empirical results in :numref:`subsec_natural-lang-stat`,\nthings take a turn for the worse for three-word combinations and beyond.\nThere will be many plausible three-word combinations that we likely will not see in our dataset.\nUnless we provide some solution to assign such word combinations a nonzero count, we will not be able to use them in a language model. If the dataset is small or if the words are very rare, we might not find even a single one of them."
    },
    {
      "chunk_id": "6e6ebb7f9406_0",
      "chapter": "language-model",
      "heading": "Laplace Smoothing",
      "text": "A common strategy is to perform some form of *Laplace smoothing*.\nThe solution is to\nadd a small constant to all counts. \nDenote by $n$ the total number of words in\nthe training set\nand $m$ the number of unique words.\nThis solution helps with singletons, e.g., via\n\n$$\\begin{aligned}\n\t\\hat{P}(x) & = \\frac{n(x) + \\epsilon_1/m}{n + \\epsilon_1}, \\\\\n\t\\hat{P}(x' \\mid x) & = \\frac{n(x, x') + \\epsilon_2 \\hat{P}(x')}{n(x) + \\epsilon_2}, \\\\\n\t\\hat{P}(x'' \\mid x,x') & = \\frac{n(x, x',x'') + \\epsilon_3 \\hat{P}(x'')}{n(x, x') + \\epsilon_3}.\n\\end{aligned}$$\n\nHere $\\epsilon_1,\\epsilon_2$, and $\\epsilon_3$ are hyperparameters.\nTake $\\epsilon_1$ as an example:\nwhen $\\epsilon_1 = 0$, no smoothing is applied;\nwhen $\\epsilon_1$ approaches positive infinity,\n$\\hat{P}(x)$ approaches the uniform probability $1/m$. \nThe above is a rather primitive variant of what\nother techniques can accomplish :cite:`Wood.Gasthaus.Archambeau.ea.2011`.\n\n\nUnfortunately, models like this get unwieldy rather quickly\nfor the following reasons. \nFirst, \nas discussed in :numref:`subsec_natural-lang-stat`,\nmany $n$-grams occur very rarely, \nmaking Laplace smoothing rather unsuitable for language modeling.\nSecond, we need to store all counts.\nThird, this entirely ignores the meaning of the words. For\ninstance, \"cat\" and \"feline\" should occur in related contexts.\nIt is quite difficult to adjust such models to additional contexts,\nwhereas, deep learning based language models are well suited to\ntake this into account.\nLast, long word\nsequences are almost certain to be novel, hence a model that simply\ncounts the frequency of previously seen word sequences is bound to perform poorly there.\nTherefore, we focus on using neural networks for language modeling\nin the rest of the chapter."
    },
    {
      "chunk_id": "b8c0b72f688e_0",
      "chapter": "language-model",
      "heading": "Perplexity",
      "text": ":label:`subsec_perplexity`\n\nNext, let's discuss about how to measure the quality of the language model, which we will then use to evaluate our models in the subsequent sections. One way is to check how surprising the text is. A good language model is able to predict, with high accuracy, the tokens that come next. Consider the following continuations of the phrase \"It is raining\", as proposed by different language models:\n\n1. \"It is raining outside\"\n1. \"It is raining banana tree\"\n1. \"It is raining piouw;kcj pwepoiut\"\n\nIn terms of quality, Example 1 is clearly the best. The words are sensible and logically coherent. While it might not quite accurately reflect which word follows semantically (\"in San Francisco\" and \"in winter\" would have been perfectly reasonable extensions), the model is able to capture which kind of word follows. Example 2 is considerably worse by producing a nonsensical extension. Nonetheless, at least the model has learned how to spell words and some degree of correlation between words. Last, Example 3 indicates a poorly trained model that does not fit data properly. We might measure the quality of the model by computing  the likelihood of the sequence. Unfortunately this is a number that is hard to understand and difficult to compare. After all, shorter sequences are much more likely to occur than the longer ones,\nhence evaluating the model on Tolstoy's magnum opus\n*War and Peace* will inevitably produce a much smaller likelihood than, say, on Saint-Exupery's novella *The Little Prince*. What is missing is the equivalent of an average. Information theory comes handy here. We defined entropy, surprisal, and cross-entropy\nwhen we introduced the softmax regression\n(:numref:`subsec_info_theory_basics`). If we want to compress text, we can ask about\npredicting the next token given the current set of tokens. A better language model should allow us to predict the next token more accurately. Thus, it should allow us to spend fewer bits in compressing the sequence."
    },
    {
      "chunk_id": "b8c0b72f688e_1",
      "chapter": "language-model",
      "heading": "Perplexity",
      "text": "A better language model should allow us to predict the next token more accurately. Thus, it should allow us to spend fewer bits in compressing the sequence. So we can measure it by the cross-entropy loss averaged\nover all the $n$ tokens of a sequence:\n\n$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n:eqlabel:`eq_avg_ce_for_lm`\n\nwhere $P$ is given by a language model and $x_t$ is the actual token observed at time step $t$ from the sequence. This makes the performance on documents of different lengths comparable. For historical reasons, scientists in natural language processing prefer to use a quantity called *perplexity*. In a nutshell, it is the exponential of :eqref:`eq_avg_ce_for_lm`:\n\n$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right).$$\n\nPerplexity can be best understood as the reciprocal of the geometric mean of the number of real choices that we have when deciding which token to pick next. Let's look at a number of cases:\n\n* In the best case scenario, the model always perfectly estimates the probability of the target token as 1. In this case the perplexity of the model is 1. * In the worst case scenario, the model always predicts the probability of the target token as 0. In this situation, the perplexity is positive infinity. * At the baseline, the model predicts a uniform distribution over all the available tokens of the vocabulary. In this case, the perplexity equals the number of unique tokens of the vocabulary. In fact, if we were to store the sequence without any compression, this would be the best we could do for encoding it. Hence, this provides a nontrivial upper bound that any useful model must beat."
    },
    {
      "chunk_id": "e5a3a9a3ee33_0",
      "chapter": "language-model",
      "heading": "Partitioning Sequences",
      "text": ":label:`subsec_partitioning-seqs`\n\nWe will design language models using neural networks\nand use perplexity to evaluate \nhow good the model is at \npredicting the next token given the current set of tokens\nin text sequences. Before introducing the model,\nlet's assume that it\nprocesses a minibatch of sequences with predefined length\nat a time. Now the question is how to [**read minibatches of input sequences and target sequences at random**]. Suppose that the dataset takes the form of a sequence of $T$ token indices in `corpus`. We will\npartition it\ninto subsequences, where each subsequence has $n$ tokens (time steps). To iterate over \n(almost) all the tokens of the entire dataset \nfor each epoch\nand obtain all possible length-$n$ subsequences,\nwe can introduce randomness. More concretely,\nat the beginning of each epoch,\ndiscard the first $d$ tokens,\nwhere $d\\in [0,n)$ is uniformly sampled at random. The rest of the sequence\nis then partitioned\ninto $m=\\lfloor (T-d)/n \\rfloor$ subsequences. Denote by $\\mathbf x_t = [x_t, \\ldots, x_{t+n-1}]$ the length-$n$ subsequence starting from token $x_t$ at time step $t$. The resulting $m$ partitioned subsequences\nare \n$\\mathbf x_d, \\mathbf x_{d+n}, \\ldots, \\mathbf x_{d+n(m-1)}.$\nEach subsequence will be used as an input sequence into the language model. For language modeling,\nthe goal is to predict the next token based on the tokens we have seen so far; hence the targets (labels) are the original sequence, shifted by one token. The target sequence for any input sequence $\\mathbf x_t$\nis $\\mathbf x_{t+1}$ with length $n$. ![Obtaining five pairs of input sequences and target sequences from partitioned length-5 subsequences.](../img/lang-model-data.svg) \n:label:`fig_lang_model_data`\n\n:numref:`fig_lang_model_data` shows an example of obtaining five pairs of input sequences and target sequences with $n=5$ and $d=2$."
    },
    {
      "chunk_id": "e5a3a9a3ee33_1",
      "chapter": "language-model",
      "heading": "Partitioning Sequences",
      "text": "```{.python .input  n=5}\n%%tab all\n@d2l.add_to_class(d2l.TimeMachine)  #@save\ndef __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n    super(d2l.TimeMachine, self).__init__()\n    self.save_hyperparameters()\n    corpus, self.vocab = self.build(self._download())\n    array = d2l.tensor([corpus[i:i+num_steps+1] \n                        for i in range(len(corpus)-num_steps)])\n    self.X, self.Y = array[:,:-1], array[:,1:]\n```\n\nTo train language models,\nwe will randomly sample \npairs of input sequences and target sequences\nin minibatches. The following data loader randomly generates a minibatch from the dataset each time. The argument `batch_size` specifies the number of subsequence examples in each minibatch\nand `num_steps` is the subsequence length in tokens. ```{.python .input  n=6}\n%%tab all\n@d2l.add_to_class(d2l.TimeMachine)  #@save\ndef get_dataloader(self, train):\n    idx = slice(0, self.num_train) if train else slice(\n        self.num_train, self.num_train + self.num_val)\n    return self.get_tensorloader([self.X, self.Y], train, idx)\n```\n\nAs we can see in the following, \na minibatch of target sequences\ncan be obtained \nby shifting the input sequences\nby one token. ```{.python .input  n=7}\n%%tab all\ndata = d2l.TimeMachine(batch_size=2, num_steps=10)\nfor X, Y in data.train_dataloader():\n    print('X:', X, '\\nY:', Y)\n    break\n```"
    },
    {
      "chunk_id": "d7d6878ab948_0",
      "chapter": "language-model",
      "heading": "Summary and Discussion",
      "text": "Language models estimate the joint probability of a text sequence. For long sequences, $n$-grams provide a convenient model by truncating the dependence. However, there is a lot of structure but not enough frequency to deal efficiently with infrequent word combinations via Laplace smoothing. Thus, we will focus on neural language modeling in subsequent sections.\nTo train language models, we can randomly sample pairs of input sequences and target sequences in minibatches. After training, we will use perplexity to measure the language model quality.\n\nLanguage models can be scaled up with increased data size, model size, and amount in training compute. Large language models can perform desired tasks by predicting output text given input text instructions. As we will discuss later (e.g., :numref:`sec_large-pretraining-transformers`),\nat the present moment\nlarge language models form the basis of state-of-the-art systems across diverse tasks."
    },
    {
      "chunk_id": "e8d31955a1db_0",
      "chapter": "language-model",
      "heading": "Exercises",
      "text": "1. Suppose there are 100,000 words in the training dataset. How much word frequency and multi-word adjacent frequency does a four-gram need to store?\n1. How would you model a dialogue?\n1. What other methods can you think of for reading long sequence data?\n1. Consider our method for discarding a uniformly random number of the first few tokens at the beginning of each epoch.\n    1. Does it really lead to a perfectly uniform distribution over the sequences on the document?\n    1. What would you have to do to make things even more uniform? \n1. If we want a sequence example to be a complete sentence, what kind of problem does this introduce in minibatch sampling? How can we fix it?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/117)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/118)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1049)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18012)\n:end_tab:"
    },
    {
      "chunk_id": "3f7abfa99f3b_0",
      "chapter": "rnn-concise",
      "heading": "rnn-concise",
      "text": "# Concise Implementation of Recurrent Neural Networks\n:label:`sec_rnn-concise`\n\nLike most of our from-scratch implementations,\n:numref:`sec_rnn-scratch` was designed \nto provide insight into how each component works.\nBut when you are using RNNs every day \nor writing production code,\nyou will want to rely more on libraries\nthat cut down on both implementation time \n(by supplying library code for common models and functions)\nand computation time \n(by optimizing the heck out of these library implementations).\nThis section will show you how to implement \nthe same language model more efficiently\nusing the high-level API provided \nby your deep learning framework.\nWe begin, as before, by loading \n*The Time Machine* dataset.\n\n```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nfrom mxnet.gluon import nn, rnn\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "d4f136234f33_0",
      "chapter": "rnn-concise",
      "heading": "[**Defining the Model**]",
      "text": "We define the following class\nusing the RNN implemented\nby high-level APIs. :begin_tab:`mxnet`\nSpecifically, to initialize the hidden state,\nwe invoke the member method `begin_state`. This returns a list that contains\nan initial hidden state\nfor each example in the minibatch,\nwhose shape is\n(number of hidden layers, batch size, number of hidden units). For some models to be introduced later\n(e.g., long short-term memory),\nthis list will also contain other information. :end_tab:\n\n:begin_tab:`jax`\nFlax does not provide an RNNCell for concise implementation of Vanilla RNNs\nas of today. There are more advanced variants of RNNs like LSTMs and GRUs\nwhich are available in the Flax `linen` API."
    },
    {
      "chunk_id": "d4f136234f33_1",
      "chapter": "rnn-concise",
      "heading": "[**Defining the Model**]",
      "text": ":end_tab:\n\n:begin_tab:`jax`\nFlax does not provide an RNNCell for concise implementation of Vanilla RNNs\nas of today. There are more advanced variants of RNNs like LSTMs and GRUs\nwhich are available in the Flax `linen` API. :end_tab:\n\n```{.python .input}\n%%tab mxnet\nclass RNN(d2l.Module):  #@save\n    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n    def __init__(self, num_hiddens):\n        super().__init__()\n        self.save_hyperparameters()        \n        self.rnn = rnn.RNN(num_hiddens)\n        \n    def forward(self, inputs, H=None):\n        if H is None:\n            H, = self.rnn.begin_state(inputs.shape[1], ctx=inputs.ctx)\n        outputs, (H, ) = self.rnn(inputs, (H, ))\n        return outputs, H\n```\n\n```{.python .input}\n%%tab pytorch\nclass RNN(d2l.Module):  #@save\n    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n    def __init__(self, num_inputs, num_hiddens):\n        super().__init__()\n        self.save_hyperparameters()\n        self.rnn = nn.RNN(num_inputs, num_hiddens)\n        \n    def forward(self, inputs, H=None):\n        return self.rnn(inputs, H)\n```\n\n```{.python .input}\n%%tab tensorflow\nclass RNN(d2l.Module):  #@save\n    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n    def __init__(self, num_hiddens):\n        super().__init__()\n        self.save_hyperparameters()            \n        self.rnn = tf.keras.layers.SimpleRNN(\n            num_hiddens, return_sequences=True, return_state=True,\n            time_major=True)\n        \n    def forward(self, inputs, H=None):\n        outputs, H = self.rnn(inputs, H)\n        return outputs, H\n```\n\n```{.python .input}\n%%tab jax\nclass RNN(nn.Module):  #@save\n    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n    num_hiddens: int\n\n    @nn.compact\n    def __call__(self, inputs, H=None):\n        raise NotImplementedError\n```\n\nInheriting from the `RNNLMScratch` class in :numref:`sec_rnn-scratch`, \nthe following `RNNLM` class defines a complete RNN-based language model. Note that we need to create a separate fully connected output layer."
    },
    {
      "chunk_id": "d4f136234f33_2",
      "chapter": "rnn-concise",
      "heading": "[**Defining the Model**]",
      "text": "Note that we need to create a separate fully connected output layer. ```{.python .input}\n%%tab pytorch\nclass RNNLM(d2l.RNNLMScratch):  #@save\n    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n    def init_params(self):\n        self.linear = nn.LazyLinear(self.vocab_size)\n        \n    def output_layer(self, hiddens):\n        return d2l.swapaxes(self.linear(hiddens), 0, 1)\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow\nclass RNNLM(d2l.RNNLMScratch):  #@save\n    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n    def init_params(self):\n        if tab.selected('mxnet'):\n            self.linear = nn.Dense(self.vocab_size, flatten=False)\n            self.initialize()\n        if tab.selected('tensorflow'):\n            self.linear = tf.keras.layers.Dense(self.vocab_size)\n        \n    def output_layer(self, hiddens):\n        if tab.selected('mxnet'):\n            return d2l.swapaxes(self.linear(hiddens), 0, 1)        \n        if tab.selected('tensorflow'):\n            return d2l.transpose(self.linear(hiddens), (1, 0, 2))\n```\n\n```{.python .input}\n%%tab jax\nclass RNNLM(d2l.RNNLMScratch):  #@save\n    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n    training: bool = True\n\n    def setup(self):\n        self.linear = nn.Dense(self.vocab_size)\n\n    def output_layer(self, hiddens):\n        return d2l.swapaxes(self.linear(hiddens), 0, 1)\n\n    def forward(self, X, state=None):\n        embs = self.one_hot(X)\n        rnn_outputs, _ = self.rnn(embs, state, self.training)\n        return self.output_layer(rnn_outputs)\n```"
    },
    {
      "chunk_id": "524435cafd0a_0",
      "chapter": "rnn-concise",
      "heading": "Training and Predicting",
      "text": "Before training the model, let's [**make a prediction \nwith a model initialized with random weights.**]\nGiven that we have not trained the network, \nit will generate nonsensical predictions.\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nif tab.selected('mxnet', 'tensorflow'):\n    rnn = RNN(num_hiddens=32)\nif tab.selected('pytorch'):\n    rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)\nmodel = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)\nmodel.predict('it has', 20, data.vocab)\n```\n\nNext, we [**train our model, leveraging the high-level API**].\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nif tab.selected('mxnet', 'pytorch'):\n    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)\ntrainer.fit(model, data)\n```\n\nCompared with :numref:`sec_rnn-scratch`,\nthis model achieves comparable perplexity,\nbut runs faster due to the optimized implementations.\nAs before, we can generate predicted tokens \nfollowing the specified prefix string.\n\n```{.python .input}\n%%tab mxnet, pytorch\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n```\n\n```{.python .input}\n%%tab tensorflow\nmodel.predict('it has', 20, data.vocab)\n```"
    },
    {
      "chunk_id": "a9dfb492c2fc_0",
      "chapter": "rnn-concise",
      "heading": "Summary",
      "text": "High-level APIs in deep learning frameworks provide implementations of standard RNNs.\nThese libraries help you to avoid wasting time reimplementing standard models.\nMoreover,\nframework implementations are often highly optimized, \n  leading to significant (computational) performance gains \n  when compared with implementations from scratch."
    },
    {
      "chunk_id": "9dce234a717c_0",
      "chapter": "rnn-concise",
      "heading": "Exercises",
      "text": "1. Can you make the RNN model overfit using the high-level APIs?\n1. Implement the autoregressive model of :numref:`sec_sequence` using an RNN.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/335)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1053)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/2211)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18015)\n:end_tab:"
    },
    {
      "chunk_id": "7e9d3280d782_0",
      "chapter": "rnn-scratch",
      "heading": "rnn-scratch",
      "text": "# Recurrent Neural Network Implementation from Scratch\n:label:`sec_rnn-scratch`\n\nWe are now ready to implement an RNN from scratch.\nIn particular, we will train this RNN to function\nas a character-level language model\n(see :numref:`sec_rnn`)\nand train it on a corpus consisting of \nthe entire text of H. G. Wells' *The Time Machine*,\nfollowing the data processing steps \noutlined in :numref:`sec_text-sequence`.\nWe start by loading the dataset.\n\n```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n```{.python .input  n=2}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nimport math\nfrom mxnet import autograd, gluon, np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n```\n\n```{.python .input}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport math\nimport tensorflow as tf\n```\n\n```{.python .input  n=5}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\nimport math\n```"
    },
    {
      "chunk_id": "0e8f6e8b4db5_0",
      "chapter": "rnn-scratch",
      "heading": "RNN Model",
      "text": "We begin by defining a class \nto implement the RNN model\n(:numref:`subsec_rnn_w_hidden_states`). Note that the number of hidden units `num_hiddens` \nis a tunable hyperparameter."
    },
    {
      "chunk_id": "0e8f6e8b4db5_1",
      "chapter": "rnn-scratch",
      "heading": "RNN Model",
      "text": "We begin by defining a class \nto implement the RNN model\n(:numref:`subsec_rnn_w_hidden_states`). Note that the number of hidden units `num_hiddens` \nis a tunable hyperparameter. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nclass RNNScratch(d2l.Module):  #@save\n    \"\"\"The RNN model implemented from scratch.\"\"\"\n    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        if tab.selected('mxnet'):\n            self.W_xh = d2l.randn(num_inputs, num_hiddens) * sigma\n            self.W_hh = d2l.randn(\n                num_hiddens, num_hiddens) * sigma\n            self.b_h = d2l.zeros(num_hiddens)\n        if tab.selected('pytorch'):\n            self.W_xh = nn.Parameter(\n                d2l.randn(num_inputs, num_hiddens) * sigma)\n            self.W_hh = nn.Parameter(\n                d2l.randn(num_hiddens, num_hiddens) * sigma)\n            self.b_h = nn.Parameter(d2l.zeros(num_hiddens))\n        if tab.selected('tensorflow'):\n            self.W_xh = tf.Variable(d2l.normal(\n                (num_inputs, num_hiddens)) * sigma)\n            self.W_hh = tf.Variable(d2l.normal(\n                (num_hiddens, num_hiddens)) * sigma)\n            self.b_h = tf.Variable(d2l.zeros(num_hiddens))\n```\n\n```{.python .input  n=7}\n%%tab jax\nclass RNNScratch(nn.Module):  #@save\n    \"\"\"The RNN model implemented from scratch.\"\"\"\n    num_inputs: int\n    num_hiddens: int\n    sigma: float = 0.01\n\n    def setup(self):\n        self.W_xh = self.param('W_xh', nn.initializers.normal(self.sigma),\n                               (self.num_inputs, self.num_hiddens))\n        self.W_hh = self.param('W_hh', nn.initializers.normal(self.sigma),\n                               (self.num_hiddens, self.num_hiddens))\n        self.b_h = self.param('b_h', nn.initializers.zeros, (self.num_hiddens))\n```\n\n[**The `forward` method below defines how to compute \nthe output and hidden state at any time step,\ngiven the current input and the state of the model\nat the previous time step.**]\nNote that the RNN model loops through \nthe outermost dimension of `inputs`,\nupdating the hidden state \none time step at a time."
    },
    {
      "chunk_id": "0e8f6e8b4db5_2",
      "chapter": "rnn-scratch",
      "heading": "RNN Model",
      "text": "The model here uses a $\\tanh$ activation function (:numref:`subsec_tanh`). ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(RNNScratch)  #@save\ndef forward(self, inputs, state=None):\n    if state is None:\n        # Initial state with shape: (batch_size, num_hiddens)\n        if tab.selected('mxnet'):\n            state = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                              ctx=inputs.ctx)\n        if tab.selected('pytorch'):\n            state = d2l.zeros((inputs.shape[1], self.num_hiddens),\n                              device=inputs.device)\n        if tab.selected('tensorflow'):\n            state = d2l.zeros((inputs.shape[1], self.num_hiddens))\n    else:\n        state, = state\n        if tab.selected('tensorflow'):\n            state = d2l.reshape(state, (-1, self.num_hiddens))\n    outputs = []\n    for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs) \n        state = d2l.tanh(d2l.matmul(X, self.W_xh) +\n                         d2l.matmul(state, self.W_hh) + self.b_h)\n        outputs.append(state)\n    return outputs, state\n```\n\n```{.python .input  n=9}\n%%tab jax\n@d2l.add_to_class(RNNScratch)  #@save\ndef __call__(self, inputs, state=None):\n    if state is not None:\n        state, = state\n    outputs = []\n    for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs) \n        state = d2l.tanh(d2l.matmul(X, self.W_xh) + (\n            d2l.matmul(state, self.W_hh) if state is not None else 0)\n                         + self.b_h)\n        outputs.append(state)\n    return outputs, state\n```\n\nWe can feed a minibatch of input sequences into an RNN model as follows."
    },
    {
      "chunk_id": "0e8f6e8b4db5_3",
      "chapter": "rnn-scratch",
      "heading": "RNN Model",
      "text": "```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nbatch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\nrnn = RNNScratch(num_inputs, num_hiddens)\nX = d2l.ones((num_steps, batch_size, num_inputs))\noutputs, state = rnn(X)\n```\n\n```{.python .input  n=11}\n%%tab jax\nbatch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\nrnn = RNNScratch(num_inputs, num_hiddens)\nX = d2l.ones((num_steps, batch_size, num_inputs))\n(outputs, state), _ = rnn.init_with_output(d2l.get_key(), X)\n```\n\nLet's check whether the RNN model\nproduces results of the correct shapes\nto ensure that the dimensionality \nof the hidden state remains unchanged. ```{.python .input}\n%%tab all\ndef check_len(a, n):  #@save\n    \"\"\"Check the length of a list.\"\"\"\n    assert len(a) == n, f'list\\'s length {len(a)} != expected length {n}'\n    \ndef check_shape(a, shape):  #@save\n    \"\"\"Check the shape of a tensor.\"\"\"\n    assert a.shape == shape, \\\n            f'tensor\\'s shape {a.shape} != expected shape {shape}'\n\ncheck_len(outputs, num_steps)\ncheck_shape(outputs[0], (batch_size, num_hiddens))\ncheck_shape(state, (batch_size, num_hiddens))\n```"
    },
    {
      "chunk_id": "61d5cc1e9436_0",
      "chapter": "rnn-scratch",
      "heading": "RNN-Based Language Model",
      "text": "The following `RNNLMScratch` class defines \nan RNN-based language model,\nwhere we pass in our RNN \nvia the `rnn` argument\nof the `__init__` method. When training language models, \nthe inputs and outputs are \nfrom the same vocabulary. Hence, they have the same dimension,\nwhich is equal to the vocabulary size. Note that we use perplexity to evaluate the model. As discussed in :numref:`subsec_perplexity`, this ensures \nthat sequences of different length are comparable."
    },
    {
      "chunk_id": "61d5cc1e9436_1",
      "chapter": "rnn-scratch",
      "heading": "RNN-Based Language Model",
      "text": "Hence, they have the same dimension,\nwhich is equal to the vocabulary size. Note that we use perplexity to evaluate the model. As discussed in :numref:`subsec_perplexity`, this ensures \nthat sequences of different length are comparable. ```{.python .input}\n%%tab pytorch\nclass RNNLMScratch(d2l.Classifier):  #@save\n    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n    def __init__(self, rnn, vocab_size, lr=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.init_params()\n        \n    def init_params(self):\n        self.W_hq = nn.Parameter(\n            d2l.randn(\n                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n        self.b_q = nn.Parameter(d2l.zeros(self.vocab_size)) \n\n    def training_step(self, batch):\n        l = self.loss(self(*batch[:-1]), batch[-1])\n        self.plot('ppl', d2l.exp(l), train=True)\n        return l\n        \n    def validation_step(self, batch):\n        l = self.loss(self(*batch[:-1]), batch[-1])\n        self.plot('ppl', d2l.exp(l), train=False)\n```\n\n```{.python .input}\n%%tab mxnet, tensorflow\nclass RNNLMScratch(d2l.Classifier):  #@save\n    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n    def __init__(self, rnn, vocab_size, lr=0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.init_params()\n        \n    def init_params(self):\n        if tab.selected('mxnet'):\n            self.W_hq = d2l.randn(\n                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma\n            self.b_q = d2l.zeros(self.vocab_size)        \n            for param in self.get_scratch_params():\n                param.attach_grad()\n        if tab.selected('tensorflow'):\n            self.W_hq = tf.Variable(d2l.normal(\n                (self.rnn.num_hiddens, self.vocab_size)) * self.rnn.sigma)\n            self.b_q = tf.Variable(d2l.zeros(self.vocab_size))\n        \n    def training_step(self, batch):\n        l = self.loss(self(*batch[:-1]), batch[-1])\n        self.plot('ppl', d2l.exp(l), train=True)\n        return l\n        \n    def validation_step(self, batch):\n        l = self.loss(self(*batch[:-1]), batch[-1])\n        self.plot('ppl', d2l.exp(l), train=False)\n```\n\n```{.python .input  n=14}\n%%tab jax\nclass RNNLMScratch(d2l.Classifier):  #@save\n    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n    rnn: nn.Module\n    vocab_size: int\n    lr: float = 0.01\n\n    def setup(self):\n        self.W_hq = self.param('W_hq', nn.initializers.normal(self.rnn.sigma),\n                               (self.rnn.num_hiddens, self.vocab_size))\n        self.b_q = self.param('b_q', nn.initializers.zeros, (self.vocab_size))\n\n    def training_step(self, params, batch, state):\n        value, grads = jax.value_and_grad(\n            self.loss, has_aux=True)(params, batch[:-1], batch[-1], state)\n        l, _ = value\n        self.plot('ppl', d2l.exp(l), train=True)\n        return value, grads\n\n    def validation_step(self, params, batch, state):\n        l, _ = self.loss(params, batch[:-1], batch[-1], state)\n        self.plot('ppl', d2l.exp(l), train=False)\n```"
    },
    {
      "chunk_id": "e0d0fa6c2a81_0",
      "chapter": "rnn-scratch",
      "heading": "[**One-Hot Encoding**]",
      "text": "Recall that each token is represented \nby a numerical index indicating the\nposition in the vocabulary of the \ncorresponding word/character/word piece. You might be tempted to build a neural network\nwith a single input node (at each time step),\nwhere the index could be fed in as a scalar value. This works when we are dealing with numerical inputs \nlike price or temperature, where any two values\nsufficiently close together\nshould be treated similarly. But this does not quite make sense. The $45^{\\textrm{th}}$ and $46^{\\textrm{th}}$ words \nin our vocabulary happen to be \"their\" and \"said\",\nwhose meanings are not remotely similar. When dealing with such categorical data,\nthe most common strategy is to represent\neach item by a *one-hot encoding*\n(recall from :numref:`subsec_classification-problem`). A one-hot encoding is a vector whose length\nis given by the size of the vocabulary $N$,\nwhere all entries are set to $0$,\nexcept for the entry corresponding \nto our token, which is set to $1$. For example, if the vocabulary had five elements,\nthen the one-hot vectors corresponding \nto indices 0 and 2 would be the following. ```{.python .input}\n%%tab mxnet\nnpx.one_hot(np.array([0, 2]), 5)\n```\n\n```{.python .input}\n%%tab pytorch\nF.one_hot(torch.tensor([0, 2]), 5)\n```\n\n```{.python .input}\n%%tab tensorflow\ntf.one_hot(tf.constant([0, 2]), 5)\n```\n\n```{.python .input  n=18}\n%%tab jax\njax.nn.one_hot(jnp.array([0, 2]), 5)\n```\n\n(**The minibatches that we sample at each iteration\nwill take the shape (batch size, number of time steps). Once representing each input as a one-hot vector,\nwe can think of each minibatch as a three-dimensional tensor, \nwhere the length along the third axis \nis given by the vocabulary size (`len(vocab)`).**)\nWe often transpose the input so that we will obtain an output \nof shape (number of time steps, batch size, vocabulary size). This will allow us to loop more conveniently through the outermost dimension\nfor updating hidden states of a minibatch,\ntime step by time step\n(e.g., in the above `forward` method)."
    },
    {
      "chunk_id": "e0d0fa6c2a81_1",
      "chapter": "rnn-scratch",
      "heading": "[**One-Hot Encoding**]",
      "text": "This will allow us to loop more conveniently through the outermost dimension\nfor updating hidden states of a minibatch,\ntime step by time step\n(e.g., in the above `forward` method). ```{.python .input}\n%%tab all\n@d2l.add_to_class(RNNLMScratch)  #@save\ndef one_hot(self, X):    \n    # Output shape: (num_steps, batch_size, vocab_size)    \n    if tab.selected('mxnet'):\n        return npx.one_hot(X.T, self.vocab_size)\n    if tab.selected('pytorch'):\n        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n    if tab.selected('tensorflow'):\n        return tf.one_hot(tf.transpose(X), self.vocab_size)\n    if tab.selected('jax'):\n        return jax.nn.one_hot(X.T, self.vocab_size)\n```"
    },
    {
      "chunk_id": "c48516404f00_0",
      "chapter": "rnn-scratch",
      "heading": "Transforming RNN Outputs",
      "text": "The language model uses a fully connected output layer\nto transform RNN outputs into token predictions at each time step.\n\n```{.python .input}\n%%tab all\n@d2l.add_to_class(RNNLMScratch)  #@save\ndef output_layer(self, rnn_outputs):\n    outputs = [d2l.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n    return d2l.stack(outputs, 1)\n\n@d2l.add_to_class(RNNLMScratch)  #@save\ndef forward(self, X, state=None):\n    embs = self.one_hot(X)\n    rnn_outputs, _ = self.rnn(embs, state)\n    return self.output_layer(rnn_outputs)\n```\n\nLet's [**check whether the forward computation\nproduces outputs with the correct shape.**]\n\n```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nmodel = RNNLMScratch(rnn, num_inputs)\noutputs = model(d2l.ones((batch_size, num_steps), dtype=d2l.int64))\ncheck_shape(outputs, (batch_size, num_steps, num_inputs))\n```\n\n```{.python .input  n=23}\n%%tab jax\nmodel = RNNLMScratch(rnn, num_inputs)\noutputs, _ = model.init_with_output(d2l.get_key(),\n                                    d2l.ones((batch_size, num_steps),\n                                             dtype=d2l.int32))\ncheck_shape(outputs, (batch_size, num_steps, num_inputs))\n```"
    },
    {
      "chunk_id": "1bdb3ec914c4_0",
      "chapter": "rnn-scratch",
      "heading": "[**Gradient Clipping**]",
      "text": "While you are already used to thinking of neural networks\nas \"deep\" in the sense that many layers\nseparate the input and output \neven within a single time step,\nthe length of the sequence introduces\na new notion of depth. In addition to the passing through the network\nin the input-to-output direction,\ninputs at the first time step\nmust pass through a chain of $T$ layers\nalong the time steps in order \nto influence the output of the model\nat the final time step. Taking the backwards view, in each iteration,\nwe backpropagate gradients through time,\nresulting in a chain of matrix-products \nof length  $\\mathcal{O}(T)$. As mentioned in :numref:`sec_numerical_stability`, \nthis can result in numerical instability, \ncausing the gradients either to explode or vanish,\ndepending on the properties of the weight matrices. Dealing with vanishing and exploding gradients \nis a fundamental problem when designing RNNs\nand has inspired some of the biggest advances\nin modern neural network architectures. In the next chapter, we will talk about\nspecialized architectures that were designed\nin hopes of mitigating the vanishing gradient problem. However, even modern RNNs often suffer\nfrom exploding gradients. One inelegant but ubiquitous solution\nis to simply clip the gradients \nforcing the resulting \"clipped\" gradients\nto take smaller values. Generally speaking, when optimizing some objective\nby gradient descent, we iteratively update\nthe parameter of interest, say a vector $\\mathbf{x}$,\nbut pushing it in the direction of the \nnegative gradient $\\mathbf{g}$\n(in stochastic gradient descent, \nwe calculate this gradient\non a randomly sampled minibatch). For example, with learning rate $\\eta > 0$,\neach update takes the form \n$\\mathbf{x} \\gets \\mathbf{x} - \\eta \\mathbf{g}$. Let's further assume that the objective function $f$\nis sufficiently smooth."
    },
    {
      "chunk_id": "1bdb3ec914c4_1",
      "chapter": "rnn-scratch",
      "heading": "[**Gradient Clipping**]",
      "text": "For example, with learning rate $\\eta > 0$,\neach update takes the form \n$\\mathbf{x} \\gets \\mathbf{x} - \\eta \\mathbf{g}$. Let's further assume that the objective function $f$\nis sufficiently smooth. Formally, we say that the objective \nis *Lipschitz continuous* with constant $L$,\nmeaning that for any $\\mathbf{x}$ and $\\mathbf{y}$, we have\n\n$$|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|.$$\n\nAs you can see, when we update the parameter vector by subtracting $\\eta \\mathbf{g}$,\nthe change in the value of the objective\ndepends on the learning rate,\nthe norm of the gradient and $L$ as follows:\n\n$$|f(\\mathbf{x}) - f(\\mathbf{x} - \\eta\\mathbf{g})| \\leq L \\eta\\|\\mathbf{g}\\|.$$\n\nIn other words, the objective cannot\nchange by more than $L \\eta \\|\\mathbf{g}\\|$. Having a small value for this upper bound \nmight be viewed as good or bad. On the downside, we are limiting the speed\nat which we can reduce the value of the objective. On the bright side, this limits by just how much\nwe can go wrong in any one gradient step. When we say that gradients explode, \nwe mean that $\\|\\mathbf{g}\\|$ \nbecomes excessively large. In this worst case, we might do so much\ndamage in a single gradient step that we\ncould undo all of the progress made over\nthe course of thousands of training iterations. When gradients can be so large,\nneural network training often diverges,\nfailing to reduce the value of the objective. At other times, training eventually converges\nbut is unstable owing to massive spikes in the loss. One way to limit the size of $L \\eta \\|\\mathbf{g}\\|$ \nis to shrink the learning rate $\\eta$ to tiny values. This has the advantage that we do not bias the updates. But what if we only *rarely* get large gradients? This drastic move slows down our progress at all steps,\njust to deal with the rare exploding gradient events."
    },
    {
      "chunk_id": "1bdb3ec914c4_2",
      "chapter": "rnn-scratch",
      "heading": "[**Gradient Clipping**]",
      "text": "This has the advantage that we do not bias the updates. But what if we only *rarely* get large gradients? This drastic move slows down our progress at all steps,\njust to deal with the rare exploding gradient events. A popular alternative is to adopt a *gradient clipping* heuristic\nprojecting the gradients $\\mathbf{g}$ onto a ball \nof some given radius $\\theta$ as follows:\n\n(**$$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}.$$**)\n\nThis ensures that the gradient norm never exceeds $\\theta$ \nand that the updated gradient is entirely aligned \nwith the original direction of $\\mathbf{g}$. It also has the desirable side-effect \nof limiting the influence any given minibatch \n(and within it any given sample) \ncan exert on the parameter vector. This bestows a certain degree of robustness to the model. To be clear, it is a hack. Gradient clipping means that we are not always\nfollowing the true gradient and it is hard \nto reason analytically about the possible side effects. However, it is a very useful hack,\nand is widely adopted in RNN implementations\nin most deep learning frameworks. Below we define a method to clip gradients,\nwhich is invoked by the `fit_epoch` method of\nthe `d2l.Trainer` class (see :numref:`sec_linear_scratch`). Note that when computing the gradient norm,\nwe are concatenating all model parameters,\ntreating them as a single giant parameter vector."
    },
    {
      "chunk_id": "1bdb3ec914c4_3",
      "chapter": "rnn-scratch",
      "heading": "[**Gradient Clipping**]",
      "text": "Note that when computing the gradient norm,\nwe are concatenating all model parameters,\ntreating them as a single giant parameter vector. ```{.python .input}\n%%tab mxnet\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef clip_gradients(self, grad_clip_val, model):\n    params = model.parameters()\n    if not isinstance(params, list):\n        params = [p.data() for p in params.values()]    \n    norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))\n    if norm > grad_clip_val:\n        for param in params:\n            param.grad[:] *= grad_clip_val / norm\n```\n\n```{.python .input}\n%%tab pytorch\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef clip_gradients(self, grad_clip_val, model):\n    params = [p for p in model.parameters() if p.requires_grad]\n    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n    if norm > grad_clip_val:\n        for param in params:\n            param.grad[:] *= grad_clip_val / norm\n```\n\n```{.python .input}\n%%tab tensorflow\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef clip_gradients(self, grad_clip_val, grads):\n    grad_clip_val = tf.constant(grad_clip_val, dtype=tf.float32)\n    new_grads = [tf.convert_to_tensor(grad) if isinstance(\n        grad, tf.IndexedSlices) else grad for grad in grads]    \n    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)) for grad in new_grads))\n    if tf.greater(norm, grad_clip_val):\n        for i, grad in enumerate(new_grads):\n            new_grads[i] = grad * grad_clip_val / norm\n        return new_grads\n    return grads\n```\n\n```{.python .input  n=27}\n%%tab jax\n@d2l.add_to_class(d2l.Trainer)  #@save\ndef clip_gradients(self, grad_clip_val, grads):\n    grad_leaves, _ = jax.tree_util.tree_flatten(grads)\n    norm = jnp.sqrt(sum(jnp.vdot(x, x) for x in grad_leaves))\n    clip = lambda grad: jnp.where(norm < grad_clip_val,\n                                  grad, grad * (grad_clip_val / norm))\n    return jax.tree_util.tree_map(clip, grads)\n```"
    },
    {
      "chunk_id": "5fcb4f38e915_0",
      "chapter": "rnn-scratch",
      "heading": "Training",
      "text": "Using *The Time Machine* dataset (`data`),\nwe train a character-level language model (`model`)\nbased on the RNN (`rnn`) implemented from scratch.\nNote that we first calculate the gradients,\nthen clip them, and finally \nupdate the model parameters\nusing the clipped gradients.\n\n```{.python .input}\n%%tab all\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nif tab.selected('mxnet', 'pytorch', 'jax'):\n    rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n    model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\nif tab.selected('tensorflow'):\n    with d2l.try_gpu():\n        rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n        model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "80bdf367b8c0_0",
      "chapter": "rnn-scratch",
      "heading": "Decoding",
      "text": "Once a language model has been learned,\nwe can use it not only to predict the next token\nbut to continue predicting each subsequent one,\ntreating the previously predicted token as though\nit were the next in the input. Sometimes we will just want to generate text\nas though we were starting at the beginning \nof a document. However, it is often useful to condition\nthe language model on a user-supplied prefix. For example, if we were developing an\nautocomplete feature for a search engine\nor to assist users in writing emails,\nwe would want to feed in what they \nhad written so far (the prefix), \nand then generate a likely continuation. [**The following `predict` method\ngenerates a continuation, one character at a time,\nafter ingesting a user-provided `prefix`**]. When looping through the characters in `prefix`,\nwe keep passing the hidden state\nto the next time step \nbut do not generate any output. This is called the *warm-up* period. After ingesting the prefix, we are now\nready to begin emitting the subsequent characters,\neach of which will be fed back into the model \nas the input at the next time step."
    },
    {
      "chunk_id": "80bdf367b8c0_1",
      "chapter": "rnn-scratch",
      "heading": "Decoding",
      "text": "This is called the *warm-up* period. After ingesting the prefix, we are now\nready to begin emitting the subsequent characters,\neach of which will be fed back into the model \nas the input at the next time step. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\n@d2l.add_to_class(RNNLMScratch)  #@save\ndef predict(self, prefix, num_preds, vocab, device=None):\n    state, outputs = None, [vocab[prefix[0]]]\n    for i in range(len(prefix) + num_preds - 1):\n        if tab.selected('mxnet'):\n            X = d2l.tensor([[outputs[-1]]], ctx=device)\n        if tab.selected('pytorch'):\n            X = d2l.tensor([[outputs[-1]]], device=device)\n        if tab.selected('tensorflow'):\n            X = d2l.tensor([[outputs[-1]]])\n        embs = self.one_hot(X)\n        rnn_outputs, state = self.rnn(embs, state)\n        if i < len(prefix) - 1:  # Warm-up period\n            outputs.append(vocab[prefix[i + 1]])\n        else:  # Predict num_preds steps\n            Y = self.output_layer(rnn_outputs)\n            outputs.append(int(d2l.reshape(d2l.argmax(Y, axis=2), 1)))\n    return ''.join([vocab.idx_to_token[i] for i in outputs])\n```\n\n```{.python .input}\n%%tab jax\n@d2l.add_to_class(RNNLMScratch)  #@save\ndef predict(self, prefix, num_preds, vocab, params):\n    state, outputs = None, [vocab[prefix[0]]]\n    for i in range(len(prefix) + num_preds - 1):\n        X = d2l.tensor([[outputs[-1]]])\n        embs = self.one_hot(X)\n        rnn_outputs, state = self.rnn.apply({'params': params['rnn']},\n                                            embs, state)\n        if i < len(prefix) - 1:  # Warm-up period\n            outputs.append(vocab[prefix[i + 1]])\n        else:  # Predict num_preds steps\n            Y = self.apply({'params': params}, rnn_outputs,\n                           method=self.output_layer)\n            outputs.append(int(d2l.reshape(d2l.argmax(Y, axis=2), 1)))\n    return ''.join([vocab.idx_to_token[i] for i in outputs])\n```\n\nIn the following, we specify the prefix \nand have it generate 20 additional characters."
    },
    {
      "chunk_id": "80bdf367b8c0_2",
      "chapter": "rnn-scratch",
      "heading": "Decoding",
      "text": "```{.python .input}\n%%tab mxnet, pytorch\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n```\n\n```{.python .input}\n%%tab tensorflow\nmodel.predict('it has', 20, data.vocab)\n```\n\n```{.python .input}\n%%tab jax\nmodel.predict('it has', 20, data.vocab, trainer.state.params)\n```\n\nWhile implementing the above RNN model from scratch is instructive, it is not convenient. In the next section, we will see how to leverage deep learning frameworks to whip up RNNs\nusing standard architectures, and to reap performance gains \nby relying on highly optimized library functions."
    },
    {
      "chunk_id": "18ab60d734d5_0",
      "chapter": "rnn-scratch",
      "heading": "Summary",
      "text": "We can train RNN-based language models to generate text following the user-provided text prefix. \nA simple RNN language model consists of input encoding, RNN modeling, and output generation.\nDuring training, gradient clipping can mitigate the problem of exploding gradients but does not address the problem of vanishing gradients. In the experiment, we implemented a simple RNN language model and trained it with gradient clipping on sequences of text, tokenized at the character level. By conditioning on a prefix, we can use a language model to generate likely continuations, which proves useful in many applications, e.g., autocomplete features."
    },
    {
      "chunk_id": "38ab287220f4_0",
      "chapter": "rnn-scratch",
      "heading": "Exercises",
      "text": "1. Does the implemented language model predict the next token based on all the past tokens up to the very first token in *The Time Machine*? \n1. Which hyperparameter controls the length of history used for prediction?\n1. Show that one-hot encoding is equivalent to picking a different embedding for each object.\n1. Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number of time steps in a minibatch, and learning rate) to improve the perplexity. How low can you go while sticking with this simple architecture?\n1. Replace one-hot encoding with learnable embeddings. Does this lead to better performance?\n1. Conduct an experiment to determine how well this language model \n   trained on *The Time Machine* works on other books by H. G. Wells,\n   e.g., *The War of the Worlds*.\n1. Conduct another experiment to evaluate the perplexity of this model\n   on books written by other authors. \n1. Modify the prediction method so as to use sampling \n   rather than picking the most likely next character.\n    * What happens?\n    * Bias the model towards more likely outputs, e.g., \n    by sampling from $q(x_t \\mid x_{t-1}, \\ldots, x_1) \\propto P(x_t \\mid x_{t-1}, \\ldots, x_1)^\\alpha$ for $\\alpha > 1$.\n1. Run the code in this section without clipping the gradient. What happens?\n1. Replace the activation function used in this section with ReLU \n   and repeat the experiments in this section. Do we still need gradient clipping? Why?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/336)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/486)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1052)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18014)\n:end_tab:"
    },
    {
      "chunk_id": "3f5d0b101f15_0",
      "chapter": "rnn",
      "heading": "rnn",
      "text": "# Recurrent Neural Networks\n:label:`sec_rnn`\n\n\nIn :numref:`sec_language-model` we described Markov models and $n$-grams for language modeling, where the conditional probability of token $x_t$ at time step $t$ only depends on the $n-1$ previous tokens. If we want to incorporate the possible effect of tokens earlier than time step $t-(n-1)$ on $x_t$,\nwe need to increase $n$. However, the number of model parameters would also increase exponentially with it, as we need to store $|\\mathcal{V}|^n$ numbers for a vocabulary set $\\mathcal{V}$. Hence, rather than modeling $P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$ it is preferable to use a latent variable model,\n\n$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),$$\n\nwhere $h_{t-1}$ is a *hidden state*  that stores the sequence information up to time step $t-1$. In general,\nthe hidden state at any time step $t$ could be computed based on both the current input $x_{t}$ and the previous hidden state $h_{t-1}$:\n\n$$h_t = f(x_{t}, h_{t-1}).$$\n:eqlabel:`eq_ht_xt`\n\nFor a sufficiently powerful function $f$ in :eqref:`eq_ht_xt`, the latent variable model is not an approximation. After all, $h_t$ may simply store all the data it has observed so far. However, it could potentially make both computation and storage expensive. Recall that we have discussed hidden layers with hidden units in :numref:`chap_perceptrons`. It is noteworthy that\nhidden layers and hidden states refer to two very different concepts. Hidden layers are, as explained, layers that are hidden from view on the path from input to output. Hidden states are technically speaking *inputs* to whatever we do at a given step,\nand they can only be computed by looking at data at previous time steps. *Recurrent neural networks* (RNNs) are neural networks with hidden states. Before introducing the RNN model, we first revisit the MLP model introduced in :numref:`sec_mlp`."
    },
    {
      "chunk_id": "3f5d0b101f15_1",
      "chapter": "rnn",
      "heading": "rnn",
      "text": "*Recurrent neural networks* (RNNs) are neural networks with hidden states. Before introducing the RNN model, we first revisit the MLP model introduced in :numref:`sec_mlp`. ```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n```{.python .input}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nnpx.set_np()\n```\n\n```{.python .input}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\n```\n\n```{.python .input}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input}\n%%tab jax\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\n```"
    },
    {
      "chunk_id": "830045afeec9_0",
      "chapter": "rnn",
      "heading": "Neural Networks without Hidden States",
      "text": "Let's take a look at an MLP with a single hidden layer.\nLet the hidden layer's activation function be $\\phi$.\nGiven a minibatch of examples $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with batch size $n$ and $d$ inputs, the hidden layer output $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$ is calculated as\n\n$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{\\textrm{xh}} + \\mathbf{b}_\\textrm{h}).$$\n:eqlabel:`rnn_h_without_state`\n\nIn :eqref:`rnn_h_without_state`, we have the weight parameter $\\mathbf{W}_{\\textrm{xh}} \\in \\mathbb{R}^{d \\times h}$, the bias parameter $\\mathbf{b}_\\textrm{h} \\in \\mathbb{R}^{1 \\times h}$, and the number of hidden units $h$, for the hidden layer.\nSo armed, we apply broadcasting (see :numref:`subsec_broadcasting`) during the summation.\nNext, the hidden layer output $\\mathbf{H}$ is used as input of the output layer, which is given by\n\n$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q},$$\n\nwhere $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$ is the output variable, $\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{h \\times q}$ is the weight parameter, and $\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}$ is the bias parameter of the output layer.  If it is a classification problem, we can use $\\mathrm{softmax}(\\mathbf{O})$ to compute the probability distribution of the output categories.\n\nThis is entirely analogous to the regression problem we solved previously in :numref:`sec_sequence`, hence we omit details.\nSuffice it to say that we can pick feature-label pairs at random and learn the parameters of our network via automatic differentiation and stochastic gradient descent."
    },
    {
      "chunk_id": "3b4355c6947c_0",
      "chapter": "rnn",
      "heading": "Recurrent Neural Networks with Hidden States",
      "text": ":label:`subsec_rnn_w_hidden_states`\n\nMatters are entirely different when we have hidden states. Let's look at the structure in some more detail. Assume that we have\na minibatch of inputs\n$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\nat time step $t$. In other words,\nfor a minibatch of $n$ sequence examples,\neach row of $\\mathbf{X}_t$ corresponds to one example at time step $t$ from the sequence. Next,\ndenote by $\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$ the hidden layer output of time step $t$. Unlike with MLP, here we save the hidden layer output $\\mathbf{H}_{t-1}$ from the previous time step and introduce a new weight parameter $\\mathbf{W}_{\\textrm{hh}} \\in \\mathbb{R}^{h \\times h}$ to describe how to use the hidden layer output of the previous time step in the current time step. Specifically, the calculation of the hidden layer output of the current time step is determined by the input of the current time step together with the hidden layer output of the previous time step:\n\n$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hh}}  + \\mathbf{b}_\\textrm{h}).$$\n:eqlabel:`rnn_h_with_state`\n\nCompared with :eqref:`rnn_h_without_state`, :eqref:`rnn_h_with_state` adds one more term $\\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hh}}$ and thus\ninstantiates :eqref:`eq_ht_xt`. From the relationship between hidden layer outputs $\\mathbf{H}_t$ and $\\mathbf{H}_{t-1}$ of adjacent time steps,\nwe know that these variables captured and retained the sequence's historical information up to their current time step, just like the state or memory of the neural network's current time step. Therefore, such a hidden layer output is called a *hidden state*. Since the hidden state uses the same definition of the previous time step in the current time step, the computation of :eqref:`rnn_h_with_state` is *recurrent*. Hence, as we said, neural networks with hidden states\nbased on recurrent computation are named\n*recurrent neural networks*."
    },
    {
      "chunk_id": "3b4355c6947c_1",
      "chapter": "rnn",
      "heading": "Recurrent Neural Networks with Hidden States",
      "text": "Hence, as we said, neural networks with hidden states\nbased on recurrent computation are named\n*recurrent neural networks*. Layers that perform\nthe computation of :eqref:`rnn_h_with_state`\nin RNNs\nare called *recurrent layers*. There are many different ways for constructing RNNs. Those with a hidden state defined by :eqref:`rnn_h_with_state` are very common. For time step $t$,\nthe output of the output layer is similar to the computation in the MLP:\n\n$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q}.$$\n\nParameters of the RNN\ninclude the weights $\\mathbf{W}_{\\textrm{xh}} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{\\textrm{hh}} \\in \\mathbb{R}^{h \\times h}$,\nand the bias $\\mathbf{b}_\\textrm{h} \\in \\mathbb{R}^{1 \\times h}$\nof the hidden layer,\ntogether with the weights $\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{h \\times q}$\nand the bias $\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}$\nof the output layer. It is worth mentioning that\neven at different time steps,\nRNNs always use these model parameters. Therefore, the parametrization cost of an RNN\ndoes not grow as the number of time steps increases. :numref:`fig_rnn` illustrates the computational logic of an RNN at three adjacent time steps. At any time step $t$,\nthe computation of the hidden state can be treated as:\n(i) concatenating the input $\\mathbf{X}_t$ at the current time step $t$ and the hidden state $\\mathbf{H}_{t-1}$ at the previous time step $t-1$;\n(ii) feeding the concatenation result into a fully connected layer with the activation function $\\phi$. The output of such a fully connected layer is the hidden state $\\mathbf{H}_t$ of the current time step $t$. In this case,\nthe model parameters are the concatenation of $\\mathbf{W}_{\\textrm{xh}}$ and $\\mathbf{W}_{\\textrm{hh}}$, and a bias of $\\mathbf{b}_\\textrm{h}$, all from :eqref:`rnn_h_with_state`. The hidden state of the current time step $t$, $\\mathbf{H}_t$, will participate in computing the hidden state $\\mathbf{H}_{t+1}$ of the next time step $t+1$."
    },
    {
      "chunk_id": "3b4355c6947c_2",
      "chapter": "rnn",
      "heading": "Recurrent Neural Networks with Hidden States",
      "text": "The hidden state of the current time step $t$, $\\mathbf{H}_t$, will participate in computing the hidden state $\\mathbf{H}_{t+1}$ of the next time step $t+1$. What is more, $\\mathbf{H}_t$ will also be\nfed into the fully connected output layer\nto compute the output\n$\\mathbf{O}_t$ of the current time step $t$. ![An RNN with a hidden state.](../img/rnn.svg)\n:label:`fig_rnn`\n\nWe just mentioned that the calculation of $\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hh}}$ for the hidden state is equivalent to\nmatrix multiplication of the\nconcatenation of $\\mathbf{X}_t$ and $\\mathbf{H}_{t-1}$\nand the\nconcatenation of $\\mathbf{W}_{\\textrm{xh}}$ and $\\mathbf{W}_{\\textrm{hh}}$. Though this can be proven mathematically,\nin the following we just use a simple code snippet as a demonstration. To begin with,\nwe define matrices `X`, `W_xh`, `H`, and `W_hh`, whose shapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively. Multiplying `X` by `W_xh`, and `H` by `W_hh`, and then adding these two products,\nwe obtain a matrix of shape (3, 4). ```{.python .input}\n%%tab mxnet, pytorch\nX, W_xh = d2l.randn(3, 1), d2l.randn(1, 4)\nH, W_hh = d2l.randn(3, 4), d2l.randn(4, 4)\nd2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)\n```\n\n```{.python .input}\n%%tab tensorflow\nX, W_xh = d2l.normal((3, 1)), d2l.normal((1, 4))\nH, W_hh = d2l.normal((3, 4)), d2l.normal((4, 4))\nd2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)\n```\n\n```{.python .input}\n%%tab jax\nX, W_xh = jax.random.normal(d2l.get_key(), (3, 1)), jax.random.normal(\n                                                        d2l.get_key(), (1, 4))\nH, W_hh = jax.random.normal(d2l.get_key(), (3, 4)), jax.random.normal(\n                                                        d2l.get_key(), (4, 4))\nd2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)\n```\n\nNow we concatenate the matrices `X` and `H`\nalong columns (axis 1),\nand the matrices\n`W_xh` and `W_hh` along rows (axis 0). These two concatenations\nresult in\nmatrices of shape (3, 5)\nand of shape (5, 4), respectively."
    },
    {
      "chunk_id": "3b4355c6947c_3",
      "chapter": "rnn",
      "heading": "Recurrent Neural Networks with Hidden States",
      "text": "These two concatenations\nresult in\nmatrices of shape (3, 5)\nand of shape (5, 4), respectively. Multiplying these two concatenated matrices,\nwe obtain the same output matrix of shape (3, 4)\nas above. ```{.python .input}\n%%tab all\nd2l.matmul(d2l.concat((X, H), 1), d2l.concat((W_xh, W_hh), 0))\n```"
    },
    {
      "chunk_id": "309b6eee8af4_0",
      "chapter": "rnn",
      "heading": "RNN-Based Character-Level Language Models",
      "text": "Recall that for language modeling in :numref:`sec_language-model`,\nwe aim to predict the next token based on\nthe current and past tokens;\nthus we shift the original sequence by one token\nas the targets (labels).\n:citet:`Bengio.Ducharme.Vincent.ea.2003` first proposed\nto use a neural network for language modeling.\nIn the following we illustrate how RNNs can be used to build a language model.\nLet the minibatch size be one, and the sequence of the text be \"machine\".\nTo simplify training in subsequent sections,\nwe tokenize text into characters rather than words\nand consider a *character-level language model*.\n:numref:`fig_rnn_train` demonstrates how to predict the next character based on the current and previous characters via an RNN for character-level language modeling.\n\n![A character-level language model based on the RNN. The input and target sequences are \"machin\" and \"achine\", respectively.](../img/rnn-train.svg)\n:label:`fig_rnn_train`\n\nDuring the training process,\nwe run a softmax operation on the output from the output layer for each time step, and then use the cross-entropy loss to compute the error between the model output and the target.\nBecause of the recurrent computation of the hidden state in the hidden layer, the output, $\\mathbf{O}_3$,  of time step 3 in :numref:`fig_rnn_train` is determined by the text sequence \"m\", \"a\", and \"c\". Since the next character of the sequence in the training data is \"h\", the loss of time step 3 will depend on the probability distribution of the next character generated based on the feature sequence \"m\", \"a\", \"c\" and the target \"h\" of this time step.\n\nIn practice, each token is represented by a $d$-dimensional vector, and we use a batch size $n>1$. Therefore, the input $\\mathbf X_t$ at time step $t$ will be an $n\\times d$ matrix, which is identical to what we discussed in :numref:`subsec_rnn_w_hidden_states`.\n\nIn the following sections, we will implement RNNs\nfor character-level language models."
    },
    {
      "chunk_id": "ca14b19b369b_0",
      "chapter": "rnn",
      "heading": "Summary",
      "text": "A neural network that uses recurrent computation for hidden states is called a recurrent neural network (RNN).\nThe hidden state of an RNN can capture historical information of the sequence up to the current time step. With recurrent computation, the number of RNN model parameters does not grow as the number of time steps increases. As for applications, an RNN can be used to create character-level language models."
    },
    {
      "chunk_id": "eeb56f77a9cf_0",
      "chapter": "rnn",
      "heading": "Exercises",
      "text": "1. If we use an RNN to predict the next character in a text sequence, what is the required dimension for any output?\n1. Why can RNNs express the conditional probability of a token at some time step based on all the previous tokens in the text sequence?\n1. What happens to the gradient if you backpropagate through a long sequence?\n1. What are some of the problems associated with the language model described in this section?\n\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/337)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/1050)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1051)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/180013)\n:end_tab:"
    },
    {
      "chunk_id": "d010f713cb6a_0",
      "chapter": "sequence",
      "heading": "sequence",
      "text": "# Working with Sequences\n:label:`sec_sequence`\n\nUp until now, we have focused on models whose inputs\nconsisted of a single feature vector $\\mathbf{x} \\in \\mathbb{R}^d$. The main change of perspective when developing models\ncapable of processing sequences is that we now\nfocus on inputs that consist of an ordered list\nof feature vectors $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$,\nwhere each feature vector $\\mathbf{x}_t$ is\nindexed by a time step $t \\in \\mathbb{Z}^+$\nlying in $\\mathbb{R}^d$. Some datasets consist of a single massive sequence. Consider, for example, the extremely long streams\nof sensor readings that might be available to climate scientists. In such cases, we might create training datasets\nby randomly sampling subsequences of some predetermined length. More often, our data arrives as a collection of sequences. Consider the following examples:\n(i) a collection of documents,\neach represented as its own sequence of words,\nand each having its own length $T_i$;\n(ii) sequence representation of\npatient stays in the hospital,\nwhere each stay consists of a number of events\nand the sequence length depends roughly\non the length of the stay. Previously, when dealing with individual inputs,\nwe assumed that they were sampled independently\nfrom the same underlying distribution $P(X)$. While we still assume that entire sequences\n(e.g., entire documents or patient trajectories)\nare sampled independently,\nwe cannot assume that the data arriving\nat each time step are independent of each other. For example, the words that likely to appear later in a document\ndepend heavily on words occurring earlier in the document. The medicine a patient is likely to receive\non the 10th day of a hospital visit\ndepends heavily on what transpired\nin the previous nine days. This should come as no surprise. If we did not believe that the elements in a sequence were related,\nwe would not have bothered to model them as a sequence in the first place."
    },
    {
      "chunk_id": "d010f713cb6a_1",
      "chapter": "sequence",
      "heading": "sequence",
      "text": "This should come as no surprise. If we did not believe that the elements in a sequence were related,\nwe would not have bothered to model them as a sequence in the first place. Consider the usefulness of the auto-fill features\nthat are popular on search tools and modern email clients. They are useful precisely because it is often possible\nto predict (imperfectly, but better than random guessing)\nwhat the likely continuations of a sequence might be,\ngiven some initial prefix. For most sequence models,\nwe do not require independence,\nor even stationarity, of our sequences. Instead, we require only that\nthe sequences themselves are sampled\nfrom some fixed underlying distribution\nover entire sequences. This flexible approach allows for such phenomena\nas (i) documents looking significantly different\nat the beginning than at the end;\nor (ii) patient status evolving either\ntowards recovery or towards death\nover the course of a hospital stay;\nor (iii) customer taste evolving in predictable ways\nover the course of continued interaction with a recommender system. We sometimes wish to predict a fixed target $y$\ngiven sequentially structured input\n(e.g., sentiment classification based on a movie review). At other times, we wish to predict a sequentially structured target\n($y_1, \\ldots, y_T$)\ngiven a fixed input (e.g., image captioning). Still other times, our goal is to predict sequentially structured targets\nbased on sequentially structured inputs\n(e.g., machine translation or video captioning). Such sequence-to-sequence tasks take two forms:\n(i) *aligned*: where the input at each time step\naligns with a corresponding target (e.g., part of speech tagging);\n(ii) *unaligned*: where the input and target\ndo not necessarily exhibit a step-for-step correspondence\n(e.g., machine translation). Before we worry about handling targets of any kind,\nwe can tackle the most straightforward problem:\nunsupervised density modeling (also called *sequence modeling*)."
    },
    {
      "chunk_id": "d010f713cb6a_2",
      "chapter": "sequence",
      "heading": "sequence",
      "text": "Before we worry about handling targets of any kind,\nwe can tackle the most straightforward problem:\nunsupervised density modeling (also called *sequence modeling*). Here, given a collection of sequences,\nour goal is to estimate the probability mass function\nthat tells us how likely we are to see any given sequence,\ni.e., $p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T)$. ```{.python .input  n=6}\n%load_ext d2lbook.tab\ntab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')\n```\n\n```{.python .input  n=7}\n%%tab mxnet\n%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, np, npx, gluon, init\nfrom mxnet.gluon import nn\nnpx.set_np()\n```\n\n```{.python .input  n=8}\n%%tab pytorch\n%matplotlib inline\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\n```\n\n```{.python .input  n=9}\n%%tab tensorflow\n%matplotlib inline\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\n```\n\n```{.python .input  n=9}\n%%tab jax\n%matplotlib inline\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\n```"
    },
    {
      "chunk_id": "e16a7dd307ae_0",
      "chapter": "sequence",
      "heading": "Autoregressive Models",
      "text": "Before introducing specialized neural networks\ndesigned to handle sequentially structured data,\nlet's take a look at some actual sequence data\nand build up some basic intuitions and statistical tools. In particular, we will focus on stock price data\nfrom the FTSE 100 index (:numref:`fig_ftse100`). At each *time step* $t \\in \\mathbb{Z}^+$, we observe\nthe price, $x_t$, of the index at that time. ![FTSE 100 index over about 30 years.](../img/ftse100.png)\n:width:`400px`\n:label:`fig_ftse100`\n\n\nNow suppose that a trader would like to make short-term trades,\nstrategically getting into or out of the index,\ndepending on whether they believe\nthat it will rise or decline\nin the subsequent time step. Absent any other features\n(news, financial reporting data, etc.),\nthe only available signal for predicting\nthe subsequent value is the history of prices to date. The trader is thus interested in knowing\nthe probability distribution\n\n$$P(x_t \\mid x_{t-1}, \\ldots, x_1)$$\n\nover prices that the index might take\nin the subsequent time step. While estimating the entire distribution\nover a continuously valued random variable\ncan be difficult, the trader would be happy\nto focus on a few key statistics of the distribution,\nparticularly the expected value and the variance. One simple strategy for estimating the conditional expectation\n\n$$\\mathbb{E}[(x_t \\mid x_{t-1}, \\ldots, x_1)],$$\n\nwould be to apply a linear regression model\n(recall :numref:`sec_linear_regression`). Such models that regress the value of a signal\non the previous values of that same signal\nare naturally called *autoregressive models*. There is just one major problem: the number of inputs,\n$x_{t-1}, \\ldots, x_1$ varies, depending on $t$. In other words, the number of inputs increases\nwith the amount of data that we encounter. Thus if we want to treat our historical data\nas a training set, we are left with the problem\nthat each example has a different number of features."
    },
    {
      "chunk_id": "e16a7dd307ae_1",
      "chapter": "sequence",
      "heading": "Autoregressive Models",
      "text": "In other words, the number of inputs increases\nwith the amount of data that we encounter. Thus if we want to treat our historical data\nas a training set, we are left with the problem\nthat each example has a different number of features. Much of what follows in this chapter\nwill revolve around techniques\nfor overcoming these challenges\nwhen engaging in such *autoregressive* modeling problems\nwhere the object of interest is\n$P(x_t \\mid x_{t-1}, \\ldots, x_1)$\nor some statistic(s) of this distribution. A few strategies recur frequently. First of all,\nwe might believe that although long sequences\n$x_{t-1}, \\ldots, x_1$ are available,\nit may not be necessary\nto look back so far in the history\nwhen predicting the near future. In this case we might content ourselves\nto condition on some window of length $\\tau$\nand only use $x_{t-1}, \\ldots, x_{t-\\tau}$ observations. The immediate benefit is that now the number of arguments\nis always the same, at least for $t > \\tau$. This allows us to train any linear model or deep network\nthat requires fixed-length vectors as inputs. Second, we might develop models that maintain\nsome summary $h_t$ of the past observations\n(see :numref:`fig_sequence-model`)\nand at the same time update $h_t$\nin addition to the prediction $\\hat{x}_t$. This leads to models that estimate not only $x_t$\nwith $\\hat{x}_t = P(x_t \\mid h_{t})$\nbut also updates of the form\n$h_t = g(h_{t-1}, x_{t-1})$. Since $h_t$ is never observed,\nthese models are also called\n*latent autoregressive models*. ![A latent autoregressive model.](../img/sequence-model.svg)\n:label:`fig_sequence-model`\n\nTo construct training data from historical data, one\ntypically creates examples by sampling windows randomly. In general, we do not expect time to stand still. However, we often assume that while\nthe specific values of $x_t$ might change,\nthe dynamics according to which each subsequent\nobservation is generated given the previous observations do not. Statisticians call dynamics that do not change *stationary*."
    },
    {
      "chunk_id": "2c48f0200af9_0",
      "chapter": "sequence",
      "heading": "Sequence Models",
      "text": "Sometimes, especially when working with language,\nwe wish to estimate the joint probability\nof an entire sequence.\nThis is a common task when working with sequences\ncomposed of discrete *tokens*, such as words.\nGenerally, these estimated functions are called *sequence models*\nand for natural language data, they are called *language models*.\nThe field of sequence modeling has been driven so much by natural language processing,\nthat we often describe sequence models as \"language models\",\neven when dealing with non-language data.\nLanguage models prove useful for all sorts of reasons.\nSometimes we want to evaluate the likelihood of sentences.\nFor example, we might wish to compare\nthe naturalness of two candidate outputs\ngenerated by a machine translation system\nor by a speech recognition system.\nBut language modeling gives us not only\nthe capacity to *evaluate* likelihood,\nbut the ability to *sample* sequences,\nand even to optimize for the most likely sequences.\n\nWhile language modeling might not, at first glance, look\nlike an autoregressive problem,\nwe can reduce language modeling to autoregressive prediction\nby decomposing the joint density  of a sequence $p(x_1, \\ldots, x_T)$\ninto the product of conditional densities\nin a left-to-right fashion\nby applying the chain rule of probability:\n\n$$P(x_1, \\ldots, x_T) = P(x_1) \\prod_{t=2}^T P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n\nNote that if we are working with discrete signals such as words,\nthen the autoregressive model must be a probabilistic classifier,\noutputting a full probability distribution\nover the vocabulary for whatever word will come next,\ngiven the leftwards context."
    },
    {
      "chunk_id": "c42e41d9bff6_0",
      "chapter": "sequence",
      "heading": "Markov Models",
      "text": ":label:`subsec_markov-models`\n\n\nNow suppose that we wish to employ the strategy mentioned above,\nwhere we condition only on the $\\tau$ previous time steps,\ni.e., $x_{t-1}, \\ldots, x_{t-\\tau}$, rather than\nthe entire sequence history $x_{t-1}, \\ldots, x_1$.\nWhenever we can throw away the history\nbeyond the previous $\\tau$ steps\nwithout any loss in predictive power,\nwe say that the sequence satisfies a *Markov condition*,\ni.e., *that the future is conditionally independent of the past,\ngiven the recent history*.\nWhen $\\tau = 1$, we say that the data is characterized\nby a *first-order Markov model*,\nand when $\\tau = k$, we say that the data is characterized\nby a $k^{\\textrm{th}}$-order Markov model.\nFor when the first-order Markov condition holds ($\\tau = 1$)\nthe factorization of our joint probability becomes a product\nof probabilities of each word given the previous *word*:\n\n$$P(x_1, \\ldots, x_T) = P(x_1) \\prod_{t=2}^T P(x_t \\mid x_{t-1}).$$\n\nWe often find it useful to work with models that proceed\nas though a Markov condition were satisfied,\neven when we know that this is only *approximately* true.\nWith real text documents we continue to gain information\nas we include more and more leftwards context.\nBut these gains diminish rapidly.\nThus, sometimes we compromise, obviating computational and statistical difficulties\nby training models whose validity depends\non a $k^{\\textrm{th}}$-order Markov condition.\nEven today's massive RNN- and Transformer-based language models\nseldom incorporate more than thousands of words of context.\n\n\nWith discrete data, a true Markov model\nsimply counts the number of times\nthat each word has occurred in each context, producing\nthe relative frequency estimate of $P(x_t \\mid x_{t-1})$.\nWhenever the data assumes only discrete values\n(as in language),\nthe most likely sequence of words can be computed efficiently\nusing dynamic programming."
    },
    {
      "chunk_id": "7dcac7292c56_0",
      "chapter": "sequence",
      "heading": "The Order of Decoding",
      "text": "You may be wondering why we represented\nthe factorization of a text sequence $P(x_1, \\ldots, x_T)$\nas a left-to-right chain of conditional probabilities. Why not right-to-left or some other, seemingly random order? In principle, there is nothing wrong with unfolding\n$P(x_1, \\ldots, x_T)$ in reverse order. The result is a valid factorization:\n\n$$P(x_1, \\ldots, x_T) = P(x_T) \\prod_{t=T-1}^1 P(x_t \\mid x_{t+1}, \\ldots, x_T).$$\n\n\nHowever, there are many reasons why factorizing text\nin the same direction in which we read it\n(left-to-right for most languages,\nbut right-to-left for Arabic and Hebrew)\nis preferred for the task of language modeling. First, this is just a more natural direction for us to think about. After all we all read text every day,\nand this process is guided by our ability\nto anticipate which words and phrases\nare likely to come next. Just think of how many times you have completed\nsomeone else's sentence. Thus, even if we had no other reason to prefer such in-order decodings,\nthey would be useful if only because we have better intuitions\nfor what should be likely when predicting in this order. Second, by factorizing in order,\nwe can assign probabilities to arbitrarily long sequences\nusing the same language model. To convert a probability over steps $1$ through $t$\ninto one that extends to word $t+1$ we simply\nmultiply by the conditional probability\nof the additional token given the previous ones:\n$P(x_{t+1}, \\ldots, x_1) = P(x_{t}, \\ldots, x_1) \\cdot P(x_{t+1} \\mid x_{t}, \\ldots, x_1)$. Third, we have stronger predictive models\nfor predicting adjacent words than\nwords at arbitrary other locations. While all orders of factorization are valid,\nthey do not necessarily all represent equally easy\npredictive modeling problems. This is true not only for language,\nbut for other kinds of data as well,\ne.g., when the data is causally structured. For example, we believe that future events cannot influence the past."
    },
    {
      "chunk_id": "7dcac7292c56_1",
      "chapter": "sequence",
      "heading": "The Order of Decoding",
      "text": "This is true not only for language,\nbut for other kinds of data as well,\ne.g., when the data is causally structured. For example, we believe that future events cannot influence the past. Hence, if we change $x_t$, we may be able to influence\nwhat happens for $x_{t+1}$ going forward but not the converse. That is, if we change $x_t$, the distribution over past events will not change. In some contexts, this makes it easier to predict $P(x_{t+1} \\mid x_t)$\nthan to predict $P(x_t \\mid x_{t+1})$. For instance, in some cases, we can find $x_{t+1} = f(x_t) + \\epsilon$\nfor some additive noise $\\epsilon$,\nwhereas the converse is not true :cite:`Hoyer.Janzing.Mooij.ea.2009`. This is great news, since it is typically the forward direction\nthat we are interested in estimating. The book by :citet:`Peters.Janzing.Scholkopf.2017` contains more on this topic. We barely scratch the surface of it."
    },
    {
      "chunk_id": "f3858312f2eb_0",
      "chapter": "sequence",
      "heading": "Training",
      "text": "Before we focus our attention on text data,\nlet's first try this out with some\ncontinuous-valued synthetic data. (**Here, our 1000 synthetic data will follow\nthe trigonometric `sin` function,\napplied to 0.01 times the time step. To make the problem a little more interesting,\nwe corrupt each sample with additive noise.**)\nFrom this sequence we extract training examples,\neach consisting of features and a label. ```{.python .input  n=10}\n%%tab all\nclass Data(d2l.DataModule):\n    def __init__(self, batch_size=16, T=1000, num_train=600, tau=4):\n        self.save_hyperparameters()\n        self.time = d2l.arange(1, T + 1, dtype=d2l.float32)\n        if tab.selected('mxnet', 'pytorch'):\n            self.x = d2l.sin(0.01 * self.time) + d2l.randn(T) * 0.2\n        if tab.selected('tensorflow'):\n            self.x = d2l.sin(0.01 * self.time) + d2l.normal([T]) * 0.2\n        if tab.selected('jax'):\n            key = d2l.get_key()\n            self.x = d2l.sin(0.01 * self.time) + jax.random.normal(key,\n                                                                   [T]) * 0.2\n```\n\n```{.python .input}\n%%tab all\ndata = Data()\nd2l.plot(data.time, data.x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))\n```\n\nTo begin, we try a model that acts as if\nthe data satisfied a $\\tau^{\\textrm{th}}$-order Markov condition,\nand thus predicts $x_t$ using only the past $\\tau$ observations. [**Thus for each time step we have an example\nwith label $y  = x_t$ and features\n$\\mathbf{x}_t = [x_{t-\\tau}, \\ldots, x_{t-1}]$.**]\nThe astute reader might have noticed that\nthis results in $1000-\\tau$ examples,\nsince we lack sufficient history for $y_1, \\ldots, y_\\tau$. While we could pad the first $\\tau$ sequences with zeros,\nto keep things simple, we drop them for now. The resulting dataset contains $T - \\tau$ examples,\nwhere each input to the model has sequence length $\\tau$. We (**create a data iterator on the first 600 examples**),\ncovering a period of the sin function."
    },
    {
      "chunk_id": "f3858312f2eb_1",
      "chapter": "sequence",
      "heading": "Training",
      "text": "The resulting dataset contains $T - \\tau$ examples,\nwhere each input to the model has sequence length $\\tau$. We (**create a data iterator on the first 600 examples**),\ncovering a period of the sin function. ```{.python .input}\n%%tab all\n@d2l.add_to_class(Data)\ndef get_dataloader(self, train):\n    features = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]\n    self.features = d2l.stack(features, 1)\n    self.labels = d2l.reshape(self.x[self.tau:], (-1, 1))\n    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n    return self.get_tensorloader([self.features, self.labels], train, i)\n```\n\nIn this example our model will be a standard linear regression. ```{.python .input}\n%%tab all\nmodel = d2l.LinearRegression(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=5)\ntrainer.fit(model, data)\n```"
    },
    {
      "chunk_id": "963ace7fec08_0",
      "chapter": "sequence",
      "heading": "Prediction",
      "text": "[**To evaluate our model, we first check\nhow well it performs at one-step-ahead prediction**]. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\nonestep_preds = d2l.numpy(model(data.features))\nd2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',\n         legend=['labels', '1-step preds'], figsize=(6, 3))\n```\n\n```{.python .input}\n%%tab jax\nonestep_preds = model.apply({'params': trainer.state.params}, data.features)\nd2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',\n         legend=['labels', '1-step preds'], figsize=(6, 3))\n```\n\nThese predictions look good,\neven near the end at $t=1000$. But what if we only observed sequence data\nup until time step 604 (`n_train + tau`)\nand wished to make predictions several steps\ninto the future? Unfortunately, we cannot directly compute\nthe one-step-ahead prediction for time step 609,\nbecause we do not know the corresponding inputs,\nhaving seen only up to $x_{604}$. We can address this problem by plugging in\nour earlier predictions as inputs to our model\nfor making subsequent predictions,\nprojecting forward, one step at a time,\nuntil reaching the desired time step:\n\n$$\\begin{aligned}\n\\hat{x}_{605} &= f(x_{601}, x_{602}, x_{603}, x_{604}), \\\\\n\\hat{x}_{606} &= f(x_{602}, x_{603}, x_{604}, \\hat{x}_{605}), \\\\\n\\hat{x}_{607} &= f(x_{603}, x_{604}, \\hat{x}_{605}, \\hat{x}_{606}),\\\\\n\\hat{x}_{608} &= f(x_{604}, \\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}),\\\\\n\\hat{x}_{609} &= f(\\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}, \\hat{x}_{608}),\\\\\n&\\vdots\\end{aligned}$$\n\nGenerally, for an observed sequence $x_1, \\ldots, x_t$,\nits predicted output $\\hat{x}_{t+k}$ at time step $t+k$\nis called the $k$*-step-ahead prediction*. Since we have observed up to $x_{604}$,\nits $k$-step-ahead prediction is $\\hat{x}_{604+k}$. In other words, we will have to\nkeep on using our own predictions\nto make multistep-ahead predictions. Let's see how well this goes."
    },
    {
      "chunk_id": "963ace7fec08_1",
      "chapter": "sequence",
      "heading": "Prediction",
      "text": "Since we have observed up to $x_{604}$,\nits $k$-step-ahead prediction is $\\hat{x}_{604+k}$. In other words, we will have to\nkeep on using our own predictions\nto make multistep-ahead predictions. Let's see how well this goes. ```{.python .input}\n%%tab mxnet, pytorch\nmultistep_preds = d2l.zeros(data.T)\nmultistep_preds[:] = data.x\nfor i in range(data.num_train + data.tau, data.T):\n    multistep_preds[i] = model(\n        d2l.reshape(multistep_preds[i-data.tau : i], (1, -1)))\nmultistep_preds = d2l.numpy(multistep_preds)\n```\n\n```{.python .input}\n%%tab tensorflow\nmultistep_preds = tf.Variable(d2l.zeros(data.T))\nmultistep_preds[:].assign(data.x)\nfor i in range(data.num_train + data.tau, data.T):\n    multistep_preds[i].assign(d2l.reshape(model(\n        d2l.reshape(multistep_preds[i-data.tau : i], (1, -1))), ()))\n```\n\n```{.python .input}\n%%tab jax\nmultistep_preds = d2l.zeros(data.T)\nmultistep_preds = multistep_preds.at[:].set(data.x)\nfor i in range(data.num_train + data.tau, data.T):\n    pred = model.apply({'params': trainer.state.params},\n                       d2l.reshape(multistep_preds[i-data.tau : i], (1, -1)))\n    multistep_preds = multistep_preds.at[i].set(pred.item())\n```\n\n```{.python .input}\n%%tab all\nd2l.plot([data.time[data.tau:], data.time[data.num_train+data.tau:]],\n         [onestep_preds, multistep_preds[data.num_train+data.tau:]], 'time',\n         'x', legend=['1-step preds', 'multistep preds'], figsize=(6, 3))\n```\n\nUnfortunately, in this case we fail spectacularly. The predictions decay to a constant\npretty quickly after a few steps. Why did the algorithm perform so much worse\nwhen predicting further into the future? Ultimately, this is down to the fact\nthat errors build up. Let's say that after step 1 we have some error $\\epsilon_1 = \\bar\\epsilon$. Now the *input* for step 2 is perturbed by $\\epsilon_1$,\nhence we suffer some error in the order of\n$\\epsilon_2 = \\bar\\epsilon + c \\epsilon_1$\nfor some constant $c$, and so on. The predictions can diverge rapidly\nfrom the true observations."
    },
    {
      "chunk_id": "963ace7fec08_2",
      "chapter": "sequence",
      "heading": "Prediction",
      "text": "Now the *input* for step 2 is perturbed by $\\epsilon_1$,\nhence we suffer some error in the order of\n$\\epsilon_2 = \\bar\\epsilon + c \\epsilon_1$\nfor some constant $c$, and so on. The predictions can diverge rapidly\nfrom the true observations. You may already be familiar\nwith this common phenomenon. For instance, weather forecasts for the next 24 hours\ntend to be pretty accurate but beyond that,\naccuracy declines rapidly. We will discuss methods for improving this\nthroughout this chapter and beyond. Let's [**take a closer look at the difficulties in $k$-step-ahead predictions**]\nby computing predictions on the entire sequence for $k = 1, 4, 16, 64$. ```{.python .input}\n%%tab pytorch, mxnet, tensorflow\ndef k_step_pred(k):\n    features = []\n    for i in range(data.tau):\n        features.append(data.x[i : i+data.T-data.tau-k+1])\n    # The (i+tau)-th element stores the (i+1)-step-ahead predictions\n    for i in range(k):\n        preds = model(d2l.stack(features[i : i+data.tau], 1))\n        features.append(d2l.reshape(preds, -1))\n    return features[data.tau:]\n```\n\n```{.python .input}\n%%tab jax\ndef k_step_pred(k):\n    features = []\n    for i in range(data.tau):\n        features.append(data.x[i : i+data.T-data.tau-k+1])\n    # The (i+tau)-th element stores the (i+1)-step-ahead predictions\n    for i in range(k):\n        preds = model.apply({'params': trainer.state.params},\n                            d2l.stack(features[i : i+data.tau], 1))\n        features.append(d2l.reshape(preds, -1))\n    return features[data.tau:]\n```\n\n```{.python .input}\n%%tab all\nsteps = (1, 4, 16, 64)\npreds = k_step_pred(steps[-1])\nd2l.plot(data.time[data.tau+steps[-1]-1:],\n         [d2l.numpy(preds[k-1]) for k in steps], 'time', 'x',\n         legend=[f'{k}-step preds' for k in steps], figsize=(6, 3))\n```\n\nThis clearly illustrates how the quality of the prediction changes\nas we try to predict further into the future. While the 4-step-ahead predictions still look good,\nanything beyond that is almost useless."
    },
    {
      "chunk_id": "be1803e570d1_0",
      "chapter": "sequence",
      "heading": "Summary",
      "text": "There is quite a difference in difficulty\nbetween interpolation and extrapolation.\nConsequently, if you have a sequence, always respect\nthe temporal order of the data when training,\ni.e., never train on future data.\nGiven this kind of data,\nsequence models require specialized statistical tools for estimation.\nTwo popular choices are autoregressive models\nand latent-variable autoregressive models.\nFor causal models (e.g., time going forward),\nestimating the forward direction is typically\na lot easier than the reverse direction.\nFor an observed sequence up to time step $t$,\nits predicted output at time step $t+k$\nis the $k$*-step-ahead prediction*.\nAs we predict further in time by increasing $k$,\nthe errors accumulate and the quality of the prediction degrades,\noften dramatically."
    },
    {
      "chunk_id": "abfcaa160af5_0",
      "chapter": "sequence",
      "heading": "Exercises",
      "text": "1. Improve the model in the experiment of this section.\n    1. Incorporate more than the past four observations? How many do you really need?\n    1. How many past observations would you need if there was no noise? Hint: you can write $\\sin$ and $\\cos$ as a differential equation.\n    1. Can you incorporate older observations while keeping the total number of features constant? Does this improve accuracy? Why?\n    1. Change the neural network architecture and evaluate the performance. You may train the new model with more epochs. What do you observe?\n1. An investor wants to find a good security to buy.\n   They look at past returns to decide which one is likely to do well.\n   What could possibly go wrong with this strategy?\n1. Does causality also apply to text? To which extent?\n1. Give an example for when a latent autoregressive model\n   might be needed to capture the dynamic of the data.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/113)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/114)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1048)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18010)\n:end_tab:"
    },
    {
      "chunk_id": "72ed2e92df53_0",
      "chapter": "text-sequence",
      "heading": "text-sequence",
      "text": "# Converting Raw Text into Sequence Data\n:label:`sec_text-sequence`\n\nThroughout this book,\nwe will often work with text data\nrepresented as sequences\nof words, characters, or word pieces.\nTo get going, we will need some basic\ntools for converting raw text\ninto sequences of the appropriate form.\nTypical preprocessing pipelines\nexecute the following steps:\n\n1. Load text as strings into memory.\n1. Split the strings into tokens (e.g., words or characters).\n1. Build a vocabulary dictionary to associate each vocabulary element with a numerical index.\n1. Convert the text into sequences of numerical indices.\n\n```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n```\n\n```{.python .input  n=2}\n%%tab mxnet\nimport collections\nimport re\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx\nimport random\nnpx.set_np()\n```\n\n```{.python .input  n=3}\n%%tab pytorch\nimport collections\nimport re\nfrom d2l import torch as d2l\nimport torch\nimport random\n```\n\n```{.python .input  n=4}\n%%tab tensorflow\nimport collections\nimport re\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport random\n```\n\n```{.python .input}\n%%tab jax\nimport collections\nfrom d2l import jax as d2l\nimport jax\nfrom jax import numpy as jnp\nimport random\nimport re\n```"
    },
    {
      "chunk_id": "bef9088939c7_0",
      "chapter": "text-sequence",
      "heading": "Reading the Dataset",
      "text": "Here, we will work with H. G. Wells'\n[The Time Machine](http://www.gutenberg.org/ebooks/35),\na book containing just over 30,000 words.\nWhile real applications will typically\ninvolve significantly larger datasets,\nthis is sufficient to demonstrate\nthe preprocessing pipeline.\nThe following `_download` method\n(**reads the raw text into a string**).\n\n```{.python .input  n=5}\n%%tab all\nclass TimeMachine(d2l.DataModule): #@save\n    \"\"\"The Time Machine dataset.\"\"\"\n    def _download(self):\n        fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n                             '090b5e7e70c295757f55df93cb0a180b9691891a')\n        with open(fname) as f:\n            return f.read()\n\ndata = TimeMachine()\nraw_text = data._download()\nraw_text[:60]\n```\n\nFor simplicity, we ignore punctuation and capitalization when preprocessing the raw text.\n\n```{.python .input  n=6}\n%%tab all\n@d2l.add_to_class(TimeMachine)  #@save\ndef _preprocess(self, text):\n    return re.sub('[^A-Za-z]+', ' ', text).lower()\n\ntext = data._preprocess(raw_text)\ntext[:60]\n```"
    },
    {
      "chunk_id": "2a471ba1e2fd_0",
      "chapter": "text-sequence",
      "heading": "Tokenization",
      "text": "*Tokens* are the atomic (indivisible) units of text.\nEach time step corresponds to 1 token,\nbut what precisely constitutes a token is a design choice.\nFor example, we could represent the sentence\n\"Baby needs a new pair of shoes\"\nas a sequence of 7 words,\nwhere the set of all words comprise\na large vocabulary (typically tens\nor hundreds of thousands of words).\nOr we would represent the same sentence\nas a much longer sequence of 30 characters,\nusing a much smaller vocabulary\n(there are only 256 distinct ASCII characters).\nBelow, we tokenize our preprocessed text\ninto a sequence of characters.\n\n```{.python .input  n=7}\n%%tab all\n@d2l.add_to_class(TimeMachine)  #@save\ndef _tokenize(self, text):\n    return list(text)\n\ntokens = data._tokenize(text)\n','.join(tokens[:30])\n```"
    },
    {
      "chunk_id": "b4b219f1747e_0",
      "chapter": "text-sequence",
      "heading": "Vocabulary",
      "text": "These tokens are still strings. However, the inputs to our models\nmust ultimately consist\nof numerical inputs. [**Next, we introduce a class\nfor constructing *vocabularies*,\ni.e., objects that associate\neach distinct token value\nwith a unique index.**]\nFirst, we determine the set of unique tokens in our training *corpus*. We then assign a numerical index to each unique token. Rare vocabulary elements are often dropped for convenience. Whenever we encounter a token at training or test time\nthat had not been previously seen or was dropped from the vocabulary,\nwe represent it by a special \"&lt;unk&gt;\" token,\nsignifying that this is an *unknown* value."
    },
    {
      "chunk_id": "b4b219f1747e_1",
      "chapter": "text-sequence",
      "heading": "Vocabulary",
      "text": "Whenever we encounter a token at training or test time\nthat had not been previously seen or was dropped from the vocabulary,\nwe represent it by a special \"&lt;unk&gt;\" token,\nsignifying that this is an *unknown* value. ```{.python .input  n=8}\n%%tab all\nclass Vocab:  #@save\n    \"\"\"Vocabulary for text.\"\"\"\n    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n        # Flatten a 2D list if needed\n        if tokens and isinstance(tokens[0], list):\n            tokens = [token for line in tokens for token in line]\n        # Count token frequencies\n        counter = collections.Counter(tokens)\n        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n                                  reverse=True)\n        # The list of unique tokens\n        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n            token for token, freq in self.token_freqs if freq >= min_freq])))\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if hasattr(indices, '__len__') and len(indices) > 1:\n            return [self.idx_to_token[int(index)] for index in indices]\n        return self.idx_to_token[indices]\n\n    @property\n    def unk(self):  # Index for the unknown token\n        return self.token_to_idx['<unk>']\n```\n\nWe now [**construct a vocabulary**] for our dataset,\nconverting the sequence of strings\ninto a list of numerical indices. Note that we have not lost any information\nand can easily convert our dataset\nback to its original (string) representation. ```{.python .input  n=9}\n%%tab all\nvocab = Vocab(tokens)\nindices = vocab[tokens[:10]]\nprint('indices:', indices)\nprint('words:', vocab.to_tokens(indices))\n```"
    },
    {
      "chunk_id": "22da8f3e0c7c_0",
      "chapter": "text-sequence",
      "heading": "Putting It All Together",
      "text": "Using the above classes and methods,\nwe [**package everything into the following\n`build` method of the `TimeMachine` class**],\nwhich returns `corpus`, a list of token indices, and `vocab`,\nthe vocabulary of *The Time Machine* corpus.\nThe modifications we did here are:\n(i) we tokenize text into characters, not words,\nto simplify the training in later sections;\n(ii) `corpus` is a single list, not a list of token lists,\nsince each text line in *The Time Machine* dataset\nis not necessarily a sentence or paragraph.\n\n```{.python .input  n=10}\n%%tab all\n@d2l.add_to_class(TimeMachine)  #@save\ndef build(self, raw_text, vocab=None):\n    tokens = self._tokenize(self._preprocess(raw_text))\n    if vocab is None: vocab = Vocab(tokens)\n    corpus = [vocab[token] for token in tokens]\n    return corpus, vocab\n\ncorpus, vocab = data.build(raw_text)\nlen(corpus), len(vocab)\n```"
    },
    {
      "chunk_id": "b6fa0d87fd20_0",
      "chapter": "text-sequence",
      "heading": "Exploratory Language Statistics",
      "text": ":label:`subsec_natural-lang-stat`\n\nUsing the real corpus and the `Vocab` class defined over words,\nwe can inspect basic statistics concerning word use in our corpus. Below, we construct a vocabulary from words used in *The Time Machine*\nand print the ten most frequently occurring of them. ```{.python .input  n=11}\n%%tab all\nwords = text.split()\nvocab = Vocab(words)\nvocab.token_freqs[:10]\n```\n\nNote that (**the ten most frequent words**)\nare not all that descriptive. You might even imagine that\nwe might see a very similar list\nif we had chosen any book at random. Articles like \"the\" and \"a\",\npronouns like \"i\" and \"my\",\nand prepositions like \"of\", \"to\", and \"in\"\noccur often because they serve common syntactic roles. Such words that are common but not particularly descriptive\nare often called (***stop words***) and,\nin previous generations of text classifiers\nbased on so-called bag-of-words representations,\nthey were most often filtered out. However, they carry meaning and\nit is not necessary to filter them out\nwhen working with modern RNN- and\nTransformer-based neural models. If you look further down the list,\nyou will notice\nthat word frequency decays quickly. The $10^{\\textrm{th}}$ most frequent word\nis less than $1/5$ as common as the most popular. Word frequency tends to follow a power law distribution\n(specifically the Zipfian) as we go down the ranks. To get a better idea, we [**plot the figure of the word frequency**]. ```{.python .input  n=12}\n%%tab all\nfreqs = [freq for token, freq in vocab.token_freqs]\nd2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',\n         xscale='log', yscale='log')\n```\n\nAfter dealing with the first few words as exceptions,\nall the remaining words roughly follow a straight line on a log--log plot."
    },
    {
      "chunk_id": "b6fa0d87fd20_1",
      "chapter": "text-sequence",
      "heading": "Exploratory Language Statistics",
      "text": "This phenomenon is captured by *Zipf's law*,\nwhich states that the frequency $n_i$\nof the $i^\\textrm{th}$ most frequent word is:\n\n$$n_i \\propto \\frac{1}{i^\\alpha},$$\n:eqlabel:`eq_zipf_law`\n\nwhich is equivalent to\n\n$$\\log n_i = -\\alpha \\log i + c,$$\n\nwhere $\\alpha$ is the exponent that characterizes\nthe distribution and $c$ is a constant. This should already give us pause for thought if we want\nto model words by counting statistics. After all, we will significantly overestimate the frequency of the tail, also known as the infrequent words. But [**what about the other word combinations, such as two consecutive words (bigrams), three consecutive words (trigrams)**], and beyond? Let's see whether the bigram frequency behaves in the same manner as the single word (unigram) frequency. ```{.python .input  n=13}\n%%tab all\nbigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]\nbigram_vocab = Vocab(bigram_tokens)\nbigram_vocab.token_freqs[:10]\n```\n\nOne thing is notable here. Out of the ten most frequent word pairs, nine are composed of both stop words and only one is relevant to the actual book---\"the time\". Furthermore, let's see whether the trigram frequency behaves in the same manner. ```{.python .input  n=14}\n%%tab all\ntrigram_tokens = ['--'.join(triple) for triple in zip(\n    words[:-2], words[1:-1], words[2:])]\ntrigram_vocab = Vocab(trigram_tokens)\ntrigram_vocab.token_freqs[:10]\n```\n\nNow, let's [**visualize the token frequency**] among these three models: unigrams, bigrams, and trigrams. ```{.python .input  n=15}\n%%tab all\nbigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\ntrigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\nd2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n         ylabel='frequency: n(x)', xscale='log', yscale='log',\n         legend=['unigram', 'bigram', 'trigram'])\n```\n\nThis figure is quite exciting."
    },
    {
      "chunk_id": "b6fa0d87fd20_2",
      "chapter": "text-sequence",
      "heading": "Exploratory Language Statistics",
      "text": "First, beyond unigram words, sequences of words\nalso appear to be following Zipf's law,\nalbeit with a smaller exponent\n$\\alpha$ in :eqref:`eq_zipf_law`,\ndepending on the sequence length. Second, the number of distinct $n$-grams is not that large. This gives us hope that there is quite a lot of structure in language. Third, many $n$-grams occur very rarely. This makes certain methods unsuitable for language modeling\nand motivates the use of deep learning models. We will discuss this in the next section."
    },
    {
      "chunk_id": "24df4e119ec4_0",
      "chapter": "text-sequence",
      "heading": "Summary",
      "text": "Text is among the most common forms of sequence data encountered in deep learning.\nCommon choices for what constitutes a token are characters, words, and word pieces.\nTo preprocess text, we usually (i) split text into tokens; (ii) build a vocabulary to map token strings to numerical indices; and (iii) convert text data into token indices for models to manipulate.\nIn practice, the frequency of words tends to follow Zipf's law. This is true not just for individual words (unigrams), but also for $n$-grams."
    },
    {
      "chunk_id": "3bc4d8304bfe_0",
      "chapter": "text-sequence",
      "heading": "Exercises",
      "text": "1. In the experiment of this section, tokenize text into words and vary the `min_freq` argument value of the `Vocab` instance. Qualitatively characterize how changes in `min_freq` impact the size of the resulting vocabulary.\n1. Estimate the exponent of Zipfian distribution for unigrams, bigrams, and trigrams in this corpus.\n1. Find some other sources of data (download a standard machine learning dataset, pick another public domain book,\n   scrape a website, etc). For each, tokenize the data at both the word and character levels. How do the vocabulary sizes compare with *The Time Machine* corpus at equivalent values of `min_freq`. Estimate the exponent of the Zipfian distribution corresponding to the unigram and bigram distributions for these corpora. How do they compare with the values that you observed for *The Time Machine* corpus?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/117)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/118)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/1049)\n:end_tab:\n\n:begin_tab:`jax`\n[Discussions](https://discuss.d2l.ai/t/18011)\n:end_tab:"
    },
    {
      "chunk_id": "2c9948e31e4c_0",
      "chapter": "zreferences",
      "heading": "zreferences",
      "text": "```eval_rst\n\n.. only:: html\n\n   References\n   ==========\n\n```\n\n:bibliography:`../d2l.bib`"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Reinforcement Learning\n:label:`chap_reinforcement_learning`\n\n\n**Pratik Chaudhari** (*University of Pennsylvania and Amazon*), **Rasool Fakoor** (*Amazon*), and **Kavosh Asadi** (*Amazon*)\n\nReinforcement Learning (RL) is a suite of techniques that allows us to build machine learning systems that take decisions sequentially. For example, a package containing new clothes that you purchased from an online retailer arrives at your doorstep after a sequence of decisions, e.g., the retailer finding the clothes in the warehouse closest to your house, putting the clothes in a box, transporting the box via land or by air, and delivering it to your house within the city. There are many variables that affect the delivery of the package along the way, e.g., whether or not the clothes were available in the warehouse, how long it took to transport the box, whether it arrived in your city before the daily delivery truck left, etc. The key idea is that at each stage these variables that we do not often control affect the entire sequence of events in the future, e.g., if there were delays in packing the box in the warehouse the retailer may need to send the package via air instead of ground to ensure a timely delivery. Reinforcement Learning methods allow us to take the appropriate action at each stage of a sequential decision making problem in order to maximize some utility eventually, e.g., the timely delivery of the package to you. Such sequential decision making problems are seen in numerous other places, e.g., while playing [Go](https://en.wikipedia.org/wiki/Go_(game)) your current move determines the next moves and the opponent's moves are the variables that you cannot control... a sequence of moves eventually determines whether or not you win; the movies that Netflix recommends to you now determine what you watch, whether you like the movie or not is unknown to Netflix, eventually a sequence of movie recommendations determines how satisfied you are with Netflix."
    },
    {
      "chunk_id": "01f4e33118cb_1",
      "chapter": "index",
      "heading": "index",
      "text": "Reinforcement learning is being used today to develop effective solutions to these problems :cite:`mnih2013playing,Silver.Huang.Maddison.ea.2016`. The key distinction between reinforcement learning and standard deep learning is that in standard deep learning the prediction of a trained model on one test datum does not affect the predictions on a future test datum; in reinforcement learning decisions at future instants (in RL, decisions are also called actions) are affected by what decisions were made in the past. In this chapter, we will develop the fundamentals of reinforcement learning and obtain hands-on experience in implementing some popular reinforcement learning methods. We will first develop a concept called a Markov Decision Process (MDP) which allows us to think of such sequential decision making problems. An algorithm called Value Iteration will be our first insight into solving reinforcement learning problems under the assumption that we know how the uncontrolled variables in an MDP (in RL, these controlled variables are called the environment) typically behave. Using the more general version of Value Iteration, an algorithm called Q-Learning, we will be able to take appropriate actions even when we do not necessarily have full knowledge of the environment. We will then study how to use deep networks for reinforcement learning problems by imitating the actions of an expert. And finally, we will develop a reinforcement learning method that uses a deep network to take actions in unknown environments. These techniques form the basis of more advanced RL algorithms that are used today in a variety of real-world applications, some of which we will point to in the chapter. ![Reinforcement Learning Structure](../img/RL_main.png)\n:width:`400px`\n:label:`fig_rl_big`\n\n```toc\n:maxdepth: 2\n\nmdp\nvalue-iter\nqlearning\n```"
    },
    {
      "chunk_id": "ff06ed117595_0",
      "chapter": "mdp",
      "heading": "mdp",
      "text": "# Markov Decision Process (MDP)\n:label:`sec_mdp`\nIn this section, we will discuss how to formulate reinforcement learning problems using Markov decision processes (MDPs) and describe various components of MDPs in detail."
    },
    {
      "chunk_id": "1d711851678b_0",
      "chapter": "mdp",
      "heading": "Definition of an MDP",
      "text": "A Markov decision process (MDP) :cite:`BellmanMDP` is a model for how the state of a system evolves as different actions are applied to the system. A few different quantities come together to form an MDP. ![A simple gridworld navigation task where the robot not only has to find its way to the goal location (shown as a green house) but also has to avoid trap locations (shown as red cross signs).](../img/mdp.png)\n:width:`250px`\n:label:`fig_mdp`\n\n* Let $\\mathcal{S}$ be the set of states in the MDP. As a concrete example see :numref:`fig_mdp`, for a robot that is navigating a gridworld. In this case, $\\mathcal{S}$ corresponds to the set of locations that the robot can be at any given timestep. * Let $\\mathcal{A}$ be the set of actions that the robot can take at each state, e.g., \"go forward\", \"turn right\", \"turn left\", \"stay at the same location\", etc. Actions can change the current state of the robot to some other state within the set $\\mathcal{S}$. * It may happen that we do not know how the robot moves *exactly* but only know it up to some approximation. We model this situation in reinforcement learning as follows: if the robot takes an action \"go forward\", there might be a small probability that it stays at the current state, another small probability that it \"turns left\", etc. Mathematically, this amounts to defining a \"transition function\" $T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to [0,1]$ such that $T(s, a, s') = P(s' \\mid s, a)$ using the conditional probability of reaching a state $s'$ given that the robot was at state $s$ and took an action $a$. The transition function is a probability distribution and we therefore have $\\sum_{s' \\in \\mathcal{S}} T(s, a, s') = 1$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$, i.e., the robot has to go to some state if it takes an action. * We now construct a notion of which actions are useful and which ones are not using the concept of a \"reward\" $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$."
    },
    {
      "chunk_id": "1d711851678b_1",
      "chapter": "mdp",
      "heading": "Definition of an MDP",
      "text": "* We now construct a notion of which actions are useful and which ones are not using the concept of a \"reward\" $r: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$. We say that the robot gets a reward $r(s,a)$ if the robot takes an action $a$ at state $s$. If the reward $r(s, a)$ is large, this indicates that taking the action $a$ at state $s$ is more useful to achieving the goal of the robot, i.e., going to the green house. If the reward $r(s, a)$ is small, then action $a$ is less useful to achieving this goal. It is important to note that the reward is designed by the user (the person who creates the reinforcement learning algorithm) with the goal in mind."
    },
    {
      "chunk_id": "20c5436dfb6e_0",
      "chapter": "mdp",
      "heading": "Return and Discount Factor",
      "text": "The different components above together form a Markov decision process (MDP)\n$$\\textrm{MDP}: (\\mathcal{S}, \\mathcal{A}, T, r).$$\n\nLet's now consider the situation when the robot starts at a particular state $s_0 \\in \\mathcal{S}$ and continues taking actions to result in a trajectory\n$$\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2, \\ldots).$$\n\nAt each time step $t$ the robot is at a state $s_t$ and takes an action $a_t$ which results in a reward $r_t = r(s_t, a_t)$. The *return* of a trajectory is the total reward obtained by the robot along such a trajectory\n$$R(\\tau) = r_0 + r_1 + r_2 + \\cdots.$$\n\nThe goal in reinforcement learning is to find a trajectory that has the largest *return*.\n\nThink of the situation when the robot continues to travel in the gridworld without ever reaching the goal location. The sequence of states and actions in a trajectory can be infinitely long in this case and the *return* of any such infinitely long trajectory will be infinite. In order to keep the reinforcement learning formulation meaningful even for such trajectories, we introduce the notion of a discount factor $\\gamma < 1$. We write the discounted *return* as\n$$R(\\tau) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\cdots = \\sum_{t=0}^\\infty \\gamma^t r_t.$$\n\nNote that if $\\gamma$ is very small, the rewards earned by the robot in the far future, say $t = 1000$, are heavily discounted by the factor $\\gamma^{1000}$. This encourages the robot to select short trajectories that achieve its goal, namely that of going to the green house in the gridwold example (see :numref:`fig_mdp`). For large values of the discount factor, say $\\gamma = 0.99$, the robot is encouraged to *explore* and then find the best trajectory to go to the goal location."
    },
    {
      "chunk_id": "2c3df20dfc14_0",
      "chapter": "mdp",
      "heading": "Discussion of the Markov Assumption",
      "text": "Let us think of a new robot where the state $s_t$ is the location as above but the action $a_t$ is the acceleration that the robot applies to its wheels instead of an abstract command like \"go forward\". If this robot has some non-zero velocity at state $s_t$, then the next location $s_{t+1}$ is a function of the past location $s_t$, the acceleration $a_t$, also the velocity of the robot at time $t$ which is proportional to $s_t - s_{t-1}$. This indicates that we should have\n\n$$s_{t+1} = \\textrm{some function}(s_t, a_t, s_{t-1});$$\n\nthe \"some function\" in our case would be Newton's law of motion. This is quite different from our transition function that simply depends upon $s_t$ and $a_t$.\n\nMarkov systems are all systems where the next state $s_{t+1}$ is only a function of the current state $s_t$ and the action $a_t$ taken at the current state. In Markov systems, the next state does not depend on which actions were taken in the past or the states that the robot was at in the past. For example, the new robot that has acceleration as the action above is not Markovian because the next location $s_{t+1}$ depends upon the previous state $s_{t-1}$ through the velocity. It may seem that Markovian nature of a system is a restrictive assumption, but it is not so. Markov Decision Processes are still capable of modeling a very large class of real systems. For example, for our new robot, if we chose our state $s_t$ to the tuple $(\\textrm{location}, \\textrm{velocity})$ then the system is Markovian because its next state $(\\textrm{location}_{t+1}, \\textrm{velocity}_{t+1})$ depends only upon the current state $(\\textrm{location}_t, \\textrm{velocity}_t)$ and the action at the current state $a_t$."
    },
    {
      "chunk_id": "2ee4fe1a4d38_0",
      "chapter": "mdp",
      "heading": "Summary",
      "text": "The reinforcement learning problem is typically modeled using Markov Decision Processes. A Markov decision process (MDP) is defined by a tuple of four entities $(\\mathcal{S}, \\mathcal{A}, T, r)$ where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $T$ is the transition function that encodes the transition probabilities of the MDP and $r$ is the immediate reward obtained by taking action at a particular state."
    },
    {
      "chunk_id": "f7d0e2d6332e_0",
      "chapter": "mdp",
      "heading": "Exercises",
      "text": "1. Suppose that we want to design an MDP to model [MountainCar](https://www.gymlibrary.dev/environments/classic_control/mountain_car/) problem.\n    1. What would be the set of states?\n    2. What would be the set of actions?\n    3. What would be the possible reward functions?\n2. How would you design an MDP for an Atari game like [Pong game](https://www.gymlibrary.dev/environments/atari/pong/)?\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12084)\n:end_tab:"
    },
    {
      "chunk_id": "d44c9b248c51_0",
      "chapter": "qlearning",
      "heading": "qlearning",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n#required_libs(\"setuptools==66\", \"wheel==0.38.4\", \"gym==0.21.0\")\n```\n\n# Q-Learning\n:label:`sec_qlearning`\n\nIn the previous section, we discussed the Value Iteration algorithm which requires accessing the complete Markov decision process (MDP), e.g., the transition and reward functions. In this section, we will look at Q-Learning :cite:`Watkins.Dayan.1992` which is an algorithm to learn the value function without necessarily knowing the MDP. This algorithm embodies the central idea behind reinforcement learning: it will enable the robot to obtain its own data.\n<!-- , instead of relying upon the expert. -->"
    },
    {
      "chunk_id": "947818c5fa2d_0",
      "chapter": "qlearning",
      "heading": "The Q-Learning Algorithm",
      "text": "Recall that value iteration for the action-value function in :ref:`sec_valueiter` corresponds to the update\n\n$$Q_{k+1}(s, a) = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) \\max_{a' \\in \\mathcal{A}} Q_k (s', a'); \\ \\textrm{for all } s \\in \\mathcal{S} \\textrm{ and } a \\in \\mathcal{A}.$$\n\nAs we discussed, implementing this algorithm requires knowing the MDP, specifically the transition function $P(s' \\mid s, a)$. The key idea behind Q-Learning is to replace the summation over all $s' \\in \\mathcal{S}$ in the above expression by a summation over the states visited by the robot. This allows us to subvert the need to know the transition function."
    },
    {
      "chunk_id": "7aff032083f4_0",
      "chapter": "qlearning",
      "heading": "An Optimization Problem Underlying Q-Learning",
      "text": "Let us imagine that the robot uses a policy $\\pi_e(a \\mid s)$ to take actions. Just like the previous chapter, it collects a dataset of $n$ trajectories of $T$ timesteps each $\\{ (s_t^i, a_t^i)_{t=0,\\ldots,T-1}\\}_{i=1,\\ldots, n}$. Recall that value iteration is really a set of constraints that ties together the action-value $Q^*(s, a)$ of different states and actions to each other. We can implement an approximate version of value iteration using the data that the robot has collected using $\\pi_e$ as\n\n$$\\hat{Q} = \\min_Q \\underbrace{\\frac{1}{nT} \\sum_{i=1}^n \\sum_{t=0}^{T-1} (Q(s_t^i, a_t^i) - r(s_t^i, a_t^i) - \\gamma \\max_{a'} Q(s_{t+1}^i, a'))^2}_{\\stackrel{\\textrm{def}}{=} \\ell(Q)}.$$\n:eqlabel:`q_learning_optimization_problem`\n\nLet us first observe the similarities and differences between this expression and value iteration above. If the robot's policy $\\pi_e$ were equal to the optimal policy $\\pi^*$, and if it collected an infinite amount of data, then this optimization problem would be identical to the optimization problem underlying value iteration. But while value iteration requires us to know $P(s' \\mid s, a)$, the optimization objective does not have this term. We have not cheated: as the robot uses the policy $\\pi_e$ to take an action $a_t^i$ at state $s_t^i$, the next state $s_{t+1}^i$ is a sample drawn from the transition function. So the optimization objective also has access to the transition function, but implicitly in terms of the data collected by the robot. The variables of our optimization problem are $Q(s, a)$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$. We can minimize the objective using gradient descent. For every pair $(s_t^i, a_t^i)$ in our dataset, we can write\n\n$$\\begin{aligned}Q(s_t^i, a_t^i) &\\leftarrow Q(s_t^i, a_t^i) - \\alpha \\nabla_{Q(s_t^i,a_t^i)} \\ell(Q) \\\\&=(1 - \\alpha) Q(s_t^i,a_t^i) - \\alpha \\Big( r(s_t^i, a_t^i) + \\gamma \\max_{a'} Q(s_{t+1}^i, a') \\Big),\\end{aligned}$$\n:eqlabel:`q_learning`\n\nwhere $\\alpha$ is the learning rate."
    },
    {
      "chunk_id": "7aff032083f4_1",
      "chapter": "qlearning",
      "heading": "An Optimization Problem Underlying Q-Learning",
      "text": "Typically in real problems, when the robot reaches the goal location, the trajectories end. The value of such a terminal state is zero because the robot does not take any further actions beyond this state. We should modify our update to handle such states as\n\n$$Q(s_t^i, a_t^i) =(1 - \\alpha) Q(s_t^i,a_t^i) - \\alpha \\Big( r(s_t^i, a_t^i) + \\gamma (1 - \\mathbb{1}_{s_{t+1}^i \\textrm{ is terminal}} )\\max_{a'} Q(s_{t+1}^i, a') \\Big).$$\n\nwhere $\\mathbb{1}_{s_{t+1}^i \\textrm{ is terminal}}$ is an indicator variable that is one if $s_{t+1}^i$ is a terminal state and zero otherwise. The value of state-action tuples $(s, a)$ that are not a part of the dataset is set to $-\\infty$. This algorithm is known as Q-Learning. Given the solution of these updates $\\hat{Q}$, which is an approximation of the optimal value function $Q^*$, we can obtain the optimal deterministic policy corresponding to this value function easily using\n\n$$\\hat{\\pi}(s) = \\mathrm{argmax}_{a} \\hat{Q}(s, a).$$\n\nThere can be situations when there are multiple deterministic policies that correspond to the same optimal value function; such ties can be broken arbitrarily because they have the same value function."
    },
    {
      "chunk_id": "38106c650330_0",
      "chapter": "qlearning",
      "heading": "Exploration in Q-Learning",
      "text": "The policy used by the robot to collect data $\\pi_e$ is critical to ensure that Q-Learning works well. Afterall, we have replaced the expectation over $s'$ using the transition function $P(s' \\mid s, a)$ using the data collected by the robot. If the policy $\\pi_e$ does not reach diverse parts of the state-action space, then it is easy to imagine our estimate $\\hat{Q}$ will be a poor approximation of the optimal $Q^*$. It is also important to note that in such a situation, the estimate of $Q^*$ at *all states* $s \\in \\mathcal{S}$ will be bad, not just the ones visited by $\\pi_e$. This is because the Q-Learning objective (or value iteration) is a constraint that ties together the value of all state-action pairs. It is therefore critical to pick the correct policy $\\pi_e$ to collect data. We can mitigate this concern by picking a completely random policy $\\pi_e$ that samples actions uniformly randomly from $\\mathcal{A}$. Such a policy would visit all states, but it will take a large number of trajectories before it does so. We thus arrive at the second key idea in Q-Learning, namely exploration. Typical implementations of Q-Learning tie together the current estimate of $Q$ and the policy $\\pi_e$ to set\n\n$$\\pi_e(a \\mid s) = \\begin{cases}\\mathrm{argmax}_{a'} \\hat{Q}(s, a') & \\textrm{with prob. } 1-\\epsilon \\\\ \\textrm{uniform}(\\mathcal{A}) & \\textrm{with prob. } \\epsilon,\\end{cases}$$\n:eqlabel:`epsilon_greedy`\n\nwhere $\\epsilon$ is called the \"exploration parameter\" and is chosen by the user. The policy $\\pi_e$ is called an exploration policy. This particular $\\pi_e$ is called an $\\epsilon$-greedy exploration policy because it chooses the optimal action (under the current estimate $\\hat{Q}$) with probability $1-\\epsilon$ but explores randomly with the remainder probability $\\epsilon$. We can also use the so-called softmax exploration policy\n\n$$\\pi_e(a \\mid s) = \\frac{e^{\\hat{Q}(s, a)/T}}{\\sum_{a'} e^{\\hat{Q}(s, a')/T}};$$\n\nwhere the hyper-parameter $T$ is called temperature."
    },
    {
      "chunk_id": "38106c650330_1",
      "chapter": "qlearning",
      "heading": "Exploration in Q-Learning",
      "text": "We can also use the so-called softmax exploration policy\n\n$$\\pi_e(a \\mid s) = \\frac{e^{\\hat{Q}(s, a)/T}}{\\sum_{a'} e^{\\hat{Q}(s, a')/T}};$$\n\nwhere the hyper-parameter $T$ is called temperature. A large value of $\\epsilon$ in $\\epsilon$-greedy policy functions similarly to a large value of temperature $T$ for the softmax policy. It is important to note that when we pick an exploration that depends upon the current estimate of the action-value function $\\hat{Q}$, we need to resolve the optimization problem periodically. Typical implementations of Q-Learning make one mini-batch update using a few state-action pairs in the collected dataset (typically the ones collected from the previous timestep of the robot) after taking every action using $\\pi_e$."
    },
    {
      "chunk_id": "b928c634916f_0",
      "chapter": "qlearning",
      "heading": "The \"Self-correcting\" Property of Q-Learning",
      "text": "The dataset collected by the robot during Q-Learning grows with time. Both the exploration policy $\\pi_e$ and the estimate $\\hat{Q}$ evolve as the robot collects more data. This gives us a key insight into why Q-Learning works well. Consider a state $s$: if a particular action $a$ has a large value under the current estimate $\\hat{Q}(s,a)$, then both the $\\epsilon$-greedy and the softmax exploration policies have a larger probability of picking this action. If this action actually is *not* the ideal action, then the future states that arise from this action will have poor rewards. The next update of the Q-Learning objective will therefore reduce the value $\\hat{Q}(s,a)$, which will reduce the probability of picking this action the next time the robot visits state $s$. Bad actions, e.g., ones whose value is overestimated in $\\hat{Q}(s,a)$, are explored by the robot but their value is correct in the next update of the Q-Learning objective. Good actions, e.g., whose value $\\hat{Q}(s, a)$ is large, are explored more often by the robot and thereby reinforced. This property can be used to show that Q-Learning can converge to the optimal policy even if it begins with a random policy $\\pi_e$ :cite:`Watkins.Dayan.1992`.\n\nThis ability to not only collect new data but also collect the right kind of data is the central feature of reinforcement learning algorithms, and this is what distinguishes them from supervised learning. Q-Learning, using deep neural networks (which we will see in the DQN chapeter later), is responsible for the resurgence of reinforcement learning :cite:`mnih2013playing`."
    },
    {
      "chunk_id": "a755ee2b12b3_0",
      "chapter": "qlearning",
      "heading": "Implementation of Q-Learning",
      "text": "We now show how to implement Q-Learning on FrozenLake from [Open AI Gym](https://gym.openai.com). Note this is the same setup as we consider in :ref:`sec_valueiter` experiment. ```{.python .input}\n%%tab all\n\n%matplotlib inline\nimport numpy as np\nimport random\nfrom d2l import torch as d2l\n\nseed = 0  # Random number generator seed\ngamma = 0.95  # Discount factor\nnum_iters = 256  # Number of iterations\nalpha   = 0.9  # Learing rate\nepsilon = 0.9  # Epsilon in epsilion gready algorithm\nrandom.seed(seed)  # Set the random seed\nnp.random.seed(seed)\n\n# Now set up the environment\nenv_info = d2l.make_env('FrozenLake-v1', seed=seed)\n```\n\nIn the FrozenLake environment, the robot moves on a $4 \\times 4$ grid (these are the states) with actions that are \"up\" ($\\uparrow$), \"down\" ($\\rightarrow$), \"left\" ($\\leftarrow$), and \"right\" ($\\rightarrow$). The environment contains a number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of which are unknown to the robot. To keep the problem simple, we assume the robot has reliable actions, i.e. $P(s' \\mid s, a) = 1$ for all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$. If the robot reaches the goal, the trial ends and the robot receives a reward of $1$ irrespective of the action; the reward at any other state is $0$ for all actions. The objective of the robot is to learn a policy that reaches the goal location (G) from a given start location (S) (this is $s_0$) to maximize the *return*."
    },
    {
      "chunk_id": "a755ee2b12b3_1",
      "chapter": "qlearning",
      "heading": "Implementation of Q-Learning",
      "text": "The objective of the robot is to learn a policy that reaches the goal location (G) from a given start location (S) (this is $s_0$) to maximize the *return*. We first implement $\\epsilon$-greedy method as follows:\n\n```{.python .input}\n%%tab all\n\ndef e_greedy(env, Q, s, epsilon):\n    if random.random() < epsilon:\n        return env.action_space.sample()\n\n    else:\n        return np.argmax(Q[s,:])\n\n```\n\nWe are now ready to implement Q-learning:\n\n```{.python .input}\n%%tab all\n\ndef q_learning(env_info, gamma, num_iters, alpha, epsilon):\n    env_desc = env_info['desc']  # 2D array specifying what each grid item means\n    env = env_info['env']  # 2D array specifying what each grid item means\n    num_states = env_info['num_states']\n    num_actions = env_info['num_actions']\n\n    Q  = np.zeros((num_states, num_actions))\n    V  = np.zeros((num_iters + 1, num_states))\n    pi = np.zeros((num_iters + 1, num_states))\n\n    for k in range(1, num_iters + 1):\n        # Reset environment\n        state, done = env.reset(), False\n        while not done:\n            # Select an action for a given state and acts in env based on selected action\n            action = e_greedy(env, Q, state, epsilon)\n            next_state, reward, done, _ = env.step(action)\n\n            # Q-update:\n            y = reward + gamma * np.max(Q[next_state,:])\n            Q[state, action] = Q[state, action] + alpha * (y - Q[state, action])\n\n            # Move to the next state\n            state = next_state\n        # Record max value and max action for visualization purpose only\n        for s in range(num_states):\n            V[k,s]  = np.max(Q[s,:])\n            pi[k,s] = np.argmax(Q[s,:])\n    d2l.show_Q_function_progress(env_desc, V[:-1], pi[:-1])\n\nq_learning(env_info=env_info, gamma=gamma, num_iters=num_iters, alpha=alpha, epsilon=epsilon)\n\n```\n\nThis result shows that Q-learning can find the optimal solution for this problem roughly after 250 iterations."
    },
    {
      "chunk_id": "a755ee2b12b3_2",
      "chapter": "qlearning",
      "heading": "Implementation of Q-Learning",
      "text": "However, when we compare this result with the Value Iteration algorithm's result (see :ref:`subsec_valueitercode`), we can see that the Value Iteration algorithm needs way fewer iterations to find the optimal solution for this problem. This happens because the Value Iteration algorithm has access to the full MDP whereas Q-learning does not."
    },
    {
      "chunk_id": "1751e98efffb_0",
      "chapter": "qlearning",
      "heading": "Summary",
      "text": "Q-learning is one of the most fundamental reinforcement-learning algorithms. It has been at the epicenter of the recent success of reinforcement learning, most notably in learning to play video games :cite:`mnih2013playing`. Implementing Q-learning does not require that we know the Markov decision process (MDP), e.g., the transition and reward functions, completely."
    },
    {
      "chunk_id": "66f64d13a202_0",
      "chapter": "qlearning",
      "heading": "Exercises",
      "text": "1. Try increasing the grid size to $8 \\times 8$. Compared with $4 \\times 4$ grid, how many iterations does it take to find the optimal value function?\n1. Run the Q-learning algorithm again with $\\gamma$ (i.e. \"gamma\" in the above code) when it equals to $0$, $0.5$, and $1$ and analyze its results.\n1. Run the Q-learning algorithm again with $\\epsilon$ (i.e. \"epsilon\" in the above code) when it equals to $0$, $0.5$, and $1$ and analyze its results.\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12103)\n:end_tab:"
    },
    {
      "chunk_id": "3e32af602b77_0",
      "chapter": "value-iter",
      "heading": "value-iter",
      "text": "```{.python .input}\n%load_ext d2lbook.tab\ntab.interact_select([\"pytorch\"])\n#required_libs(\"setuptools==66\", \"wheel==0.38.4\", \"gym==0.21.0\")\n```\n\n# Value Iteration\n:label:`sec_valueiter`\n\nIn this section we will discuss how to pick the best action for the robot at each state to maximize the *return* of the trajectory. We will describe an algorithm called Value Iteration and implement it for a simulated robot that travels over a frozen lake."
    },
    {
      "chunk_id": "df1f6ec209e6_0",
      "chapter": "value-iter",
      "heading": "Stochastic Policy",
      "text": "A stochastic policy denoted as $\\pi(a \\mid s)$ (policy for short) is a conditional distribution over the actions $a \\in \\mathcal{A}$ given the state $s \\in \\mathcal{S}$, $\\pi(a \\mid s) \\equiv P(a \\mid s)$. As an example, if the robot has four actions $\\mathcal{A}=$ {go left, go down, go right, go up}. The policy at a state $s \\in \\mathcal{S}$ for such a set of actions $\\mathcal{A}$ is a categorical distribution where the probabilities of the four actions could be $[0.4, 0.2, 0.1, 0.3]$; at some other state $s' \\in \\mathcal{S}$ the probabilities $\\pi(a \\mid s')$ of the same four actions could be $[0.1, 0.1, 0.2, 0.6]$. Note that we should have $\\sum_a \\pi(a \\mid s) = 1$ for any state $s$. A deterministic policy is a special case of a stochastic policy in that the distribution $\\pi(a \\mid s)$ only gives non-zero probability to one particular action, e.g., $[1, 0, 0, 0]$ for our example with four actions.\n\nTo make the notation less cumbersome, we will often write $\\pi(s)$ as the conditional distribution instead of $\\pi(a \\mid s)$."
    },
    {
      "chunk_id": "211a92ed7b6e_0",
      "chapter": "value-iter",
      "heading": "Value Function",
      "text": "Imagine now that the robot starts at a state $s_0$ and at each time instant, it first samples an action from the policy $a_t \\sim \\pi(s_t)$ and takes this action to result in the next state $s_{t+1}$. The trajectory $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots)$, can be different depending upon which particular action $a_t$ is sampled at intermediate instants. We define the average *return* $R(\\tau) = \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t)$ of all such trajectories\n$$V^\\pi(s_0) = E_{a_t \\sim \\pi(s_t)} \\Big[ R(\\tau) \\Big] = E_{a_t \\sim \\pi(s_t)} \\Big[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\Big],$$\n\nwhere $s_{t+1} \\sim P(s_{t+1} \\mid s_t, a_t)$ is the next state of the robot and $r(s_t, a_t)$ is the instantaneous reward obtained by taking action $a_t$ in state $s_t$ at time $t$. This is called the \"value function\" for the policy $\\pi$. In simple words, the value of a state $s_0$ for a policy $\\pi$, denoted by $V^\\pi(s_0)$, is the expected $\\gamma$-discounted *return* obtained by the robot if it begins at state $s_0$ and takes actions from the policy $\\pi$ at each time instant. We next break down the trajectory into two stages (i) the first stage which corresponds to $s_0 \\to s_1$ upon taking the action $a_0$, and (ii) a second stage which is the trajectory $\\tau' = (s_1, a_1, r_1, \\ldots)$ thereafter. The key idea behind all algorithms in reinforcement learning is that the value of state $s_0$ can be written as the average reward obtained in the first stage and the value function averaged over all possible next states $s_1$. This is quite intuitive and arises from our Markov assumption: the average return from the current state is the sum of the average return from the next state and the average reward of going to the next state."
    },
    {
      "chunk_id": "211a92ed7b6e_1",
      "chapter": "value-iter",
      "heading": "Value Function",
      "text": "This is quite intuitive and arises from our Markov assumption: the average return from the current state is the sum of the average return from the next state and the average reward of going to the next state. Mathematically, we write the two stages as\n\n$$V^\\pi(s_0) = r(s_0, a_0) + \\gamma\\ E_{a_0 \\sim \\pi(s_0)} \\Big[ E_{s_1 \\sim P(s_1 \\mid s_0, a_0)} \\Big[ V^\\pi(s_1) \\Big] \\Big].$$\n:eqlabel:`eq_dynamic_programming`\n\nThis decomposition is very powerful: it is the foundation of the principle of dynamic programming upon which all reinforcement learning algorithms are based. Notice that the second stage gets two expectations, one over the choices of the action $a_0$ taken in the first stage using the stochastic policy and another over the possible states $s_1$ obtained from the chosen action. We can write :eqref:`eq_dynamic_programming` using the transition probabilities in the Markov decision process (MDP) as\n\n$$V^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\Big[ r(s,  a) + \\gamma\\  \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V^\\pi(s') \\Big];\\ \\textrm{for all } s \\in \\mathcal{S}.$$\n:eqlabel:`eq_dynamic_programming_val`\n\nAn important thing to notice here is that the above identity holds for all states $s \\in \\mathcal{S}$ because we can think of any trajectory that begins at that state and break down the trajectory into two stages."
    },
    {
      "chunk_id": "9fcb2638bccd_0",
      "chapter": "value-iter",
      "heading": "Action-Value Function",
      "text": "In implementations, it is often useful to maintain a quantity called the \"action value\" function which is a closely related quantity to the value function. This is defined to be the average *return* of a trajectory that begins at $s_0$ but when the action of the first stage is fixed to be $a_0$\n\n$$Q^\\pi(s_0, a_0) = r(s_0, a_0) + E_{a_t \\sim \\pi(s_t)} \\Big[ \\sum_{t=1}^\\infty \\gamma^t r(s_t, a_t) \\Big],$$\n\nnote that the summation inside the expectation is from $t=1,\\ldots, \\infty$ because the reward of the first stage is fixed in this case. We can again break down the trajectory into two parts and write\n\n$$Q^\\pi(s, a) = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s')\\ Q^\\pi(s', a');\\ \\textrm{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}.$$\n:eqlabel:`eq_dynamic_programming_q`\n\nThis version is the analog of :eqref:`eq_dynamic_programming_val` for the action value function."
    },
    {
      "chunk_id": "bb430663772b_0",
      "chapter": "value-iter",
      "heading": "Optimal Stochastic Policy",
      "text": "Both the value function and the action-value function depend upon the policy that the robot chooses. We will next think of the \"optimal policy\" that achieves the maximal average *return*\n$$\\pi^* = \\underset{\\pi}{\\mathrm{argmax}} V^\\pi(s_0).$$\n\nOf all possible stochastic policies that the robot could have taken, the optimal policy $\\pi^*$  achieves the largest average discounted *return* for trajectories starting from state $s_0$. Let us denote the value function and the action-value function of the optimal policy as $V^* \\equiv V^{\\pi^*}$ and $Q^* \\equiv Q^{\\pi^*}$.\n\nLet us observe that for a deterministic policy where there is only one action that is possible under the policy at any given state. This gives us\n\n$$\\pi^*(s) = \\underset{a \\in \\mathcal{A}}{\\mathrm{argmax}} \\Big[ r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a)\\ V^*(s') \\Big].$$\n\nA good mnemonic to remember this is that the optimal action at state $s$ (for a deterministic policy) is the one that maximizes the sum of reward $r(s, a)$ from the first stage and the average *return* of the trajectories starting from the next sate $s'$, averaged over all possible next states $s'$ from the second stage."
    },
    {
      "chunk_id": "da5ec3907106_0",
      "chapter": "value-iter",
      "heading": "Principle of Dynamic Programming",
      "text": "Our developement in the previous section in :eqref:`eq_dynamic_programming` or :eqref:`eq_dynamic_programming_q` can be turned into an algorithm to compute the optimal value function $V^*$ or the action-value function $Q^*$, respectively. Observe that\n$$ V^*(s) = \\sum_{a \\in \\mathcal{A}} \\pi^*(a \\mid s) \\Big[ r(s,  a) + \\gamma\\  \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V^*(s') \\Big];\\ \\textrm{for all } s \\in \\mathcal{S}.$$\n\nFor a deterministic optimal policy $\\pi^*$, since there is only one action that can be taken at state $s$, we can also write \n\n$$V^*(s) = \\mathrm{argmax}_{a \\in \\mathcal{A}} \\Big\\{ r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V^*(s') \\Big\\}$$\n\nfor all states $s \\in \\mathcal{S}$. This identity is called the \"principle of dynamic programming\" :cite:`BellmanDPPaper,BellmanDPBook`. It was formulated by Richard Bellman in 1950s and we can remember it as \"the remainder of an optimal trajectory is also optimal\"."
    },
    {
      "chunk_id": "76e940658859_0",
      "chapter": "value-iter",
      "heading": "Value Iteration",
      "text": "We can turn the principle of dynamic programming into an algorithm for finding the optimal value function called value iteration. The key idea behind value iteration is to think of this identity as a set of constraints that tie together $V^*(s)$ at different states $s \\in \\mathcal{S}$. We initialize the value function to some arbitrary values $V_0(s)$ for all states $s \\in \\mathcal{S}$. At the $k^{\\textrm{th}}$ iteration, the Value Iteration algorithm updates the value function as\n\n$$V_{k+1}(s) = \\max_{a \\in \\mathcal{A}} \\Big\\{ r(s,  a) + \\gamma\\  \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V_k(s') \\Big\\};\\ \\textrm{for all } s \\in \\mathcal{S}.$$\n\nIt turns out that as $k \\to \\infty$ the value function estimated by the Value Iteration algorithm converges to the optimal value function irrespective of the initialization $V_0$,\n$$V^*(s) = \\lim_{k \\to \\infty} V_k(s);\\ \\textrm{for all states } s \\in \\mathcal{S}.$$\n\nThe same Value Iteration algorithm can be equivalently written using the action-value function as\n$$Q_{k+1}(s, a) = r(s, a) + \\gamma \\max_{a' \\in \\mathcal{A}} \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) Q_k (s', a');\\ \\textrm{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}.$$\n\nIn this case we initialize $Q_0(s, a)$ to some arbitrary values for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$. Again we have $Q^*(s, a) = \\lim_{k \\to \\infty} Q_k(s, a)$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$."
    },
    {
      "chunk_id": "77940acec282_0",
      "chapter": "value-iter",
      "heading": "Policy Evaluation",
      "text": "Value Iteration enables us to compute the optimal value function, i.e., $V^{\\pi^*}$ of the optimal deterministic policy $\\pi^*$. We can also use similar iterative updates to compute the value function associated with any other, potentially stochastic, policy $\\pi$. We again initialize $V^\\pi_0(s)$ to some arbitrary values for all states $s \\in \\mathcal{S}$ and at the $k^{\\textrm{th}}$ iteration, perform the updates\n\n$$    V^\\pi_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\Big[ r(s,  a) + \\gamma\\  \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, a) V^\\pi_k(s') \\Big];\\ \\textrm{for all } s \\in \\mathcal{S}.$$\n\nThis algorithm is known as policy evaluation and is useful to compute the value function given the policy. Again, it turns out that as $k \\to \\infty$ these updates converge to the correct value function irrespective of the initialization $V_0$,\n\n$$V^\\pi(s) = \\lim_{k \\to \\infty} V^\\pi_k(s);\\ \\textrm{for all states } s \\in \\mathcal{S}.$$\n\nThe algorithm for computing the action-value function $Q^\\pi(s, a)$ of a policy $\\pi$ is analogous."
    },
    {
      "chunk_id": "dc9ac321fd24_0",
      "chapter": "value-iter",
      "heading": "Implementation of Value Iteration",
      "text": ":label:`subsec_valueitercode`\nWe next show how to implement Value Iteration for a navigation problem called FrozenLake from [Open AI Gym](https://gym.openai.com). We first need to setup the enviroment as shown in the following code. ```{.python .input}\n%%tab all\n\n%matplotlib inline\nimport numpy as np\nimport random\nfrom d2l import torch as d2l\n\nseed = 0  # Random number generator seed\ngamma = 0.95  # Discount factor\nnum_iters = 10  # Number of iterations\nrandom.seed(seed)  # Set the random seed to ensure results can be reproduced\nnp.random.seed(seed)\n\n# Now set up the environment\nenv_info = d2l.make_env('FrozenLake-v1', seed=seed)\n```\n\nIn the FrozenLake environment, the robot moves on a $4 \\times 4$ grid (these are the states) with actions that are \"up\" ($\\uparrow$), \"down\" ($\\rightarrow$), \"left\" ($\\leftarrow$), and \"right\" ($\\rightarrow$). The environment contains a number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of which are unknown to the robot. To keep the problem simple, we assume the robot has reliable actions, i.e. $P(s' \\mid s, a) = 1$ for all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$. If the robot reaches the goal, the trial ends and the robot receives a reward of $1$ irrespective of the action; the reward at any other state is $0$ for all actions. The objective of the robot is to learn a policy that reaches the goal location (G) from a given start location (S) (this is $s_0$) to maximize the *return*."
    },
    {
      "chunk_id": "dc9ac321fd24_1",
      "chapter": "value-iter",
      "heading": "Implementation of Value Iteration",
      "text": "The objective of the robot is to learn a policy that reaches the goal location (G) from a given start location (S) (this is $s_0$) to maximize the *return*. The following function implements Value Iteration, where `env_info` contains MDP and environment related information and `gamma` is the discount factor:\n\n```{.python .input}\n%%tab all\n\ndef value_iteration(env_info, gamma, num_iters):\n    env_desc = env_info['desc']  # 2D array shows what each item means\n    prob_idx = env_info['trans_prob_idx']\n    nextstate_idx = env_info['nextstate_idx']\n    reward_idx = env_info['reward_idx']\n    num_states = env_info['num_states']\n    num_actions = env_info['num_actions']\n    mdp = env_info['mdp']\n\n    V  = np.zeros((num_iters + 1, num_states))\n    Q  = np.zeros((num_iters + 1, num_states, num_actions))\n    pi = np.zeros((num_iters + 1, num_states))\n\n    for k in range(1, num_iters + 1):\n        for s in range(num_states):\n            for a in range(num_actions):\n                # Calculate \\sum_{s'} p(s'\\mid s,a) [r + \\gamma v_k(s')]\n                for pxrds in mdp[(s,a)]:\n                    # mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]\n                    pr = pxrds[prob_idx]  # p(s'\\mid s,a)\n                    nextstate = pxrds[nextstate_idx]  # Next state\n                    reward = pxrds[reward_idx]  # Reward\n                    Q[k,s,a] += pr * (reward + gamma * V[k - 1, nextstate])\n            # Record max value and max action\n            V[k,s] = np.max(Q[k,s,:])\n            pi[k,s] = np.argmax(Q[k,s,:])\n    d2l.show_value_function_progress(env_desc, V[:-1], pi[:-1])\n\nvalue_iteration(env_info=env_info, gamma=gamma, num_iters=num_iters)\n```\n\nThe above pictures show the policy (the arrow indicates the action) and value function (the change in color shows how the value function changes over time from the initial value shown by dark color to the optimal value shown by light colors.)."
    },
    {
      "chunk_id": "dc9ac321fd24_2",
      "chapter": "value-iter",
      "heading": "Implementation of Value Iteration",
      "text": "As we see, Value Iteration finds the optimal value function after 10 iterations and the goal state (G) can be reached starting from any state as long as it is not an H cell. Another interesting aspect of the implementation is that in addition to finding the optimal value function, we also automatically found the optimal policy $\\pi^*$ corresponding to this value function."
    },
    {
      "chunk_id": "ca3fc2a7e741_0",
      "chapter": "value-iter",
      "heading": "Summary",
      "text": "The main idea behind the Value Iteration algorithm is to use the principle of dynamic programming to find the optimal average return obtained from a given state. Note that implementing the Value Iteration algorithm requires that we know the Markov decision process (MDP), e.g., the transition and reward functions, completely."
    },
    {
      "chunk_id": "4ab1ced428e3_0",
      "chapter": "value-iter",
      "heading": "Exercises",
      "text": "1. Try increasing the grid size to $8 \\times 8$. Compared with $4 \\times 4$ grid, how many iterations does it take to find the optimal value function?\n1. What is the computational complexity of the Value Iteration algorithm?\n1. Run the Value Iteration algorithm again with $\\gamma$ (i.e. \"gamma\" in the above code) when it equals to $0$, $0.5$, and $1$ and analyze its results. \n1. How does the value of $\\gamma$ affect the number of iterations taken by Value Iteration to converge? What happens when $\\gamma=1$?\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/12005)\n:end_tab:"
    },
    {
      "chunk_id": "2f4445a919ae_0",
      "chapter": "chapter-one-problem-set",
      "heading": "chapter-one-problem-set",
      "text": "# Problem Set \n\n\"For the things we have to learn before we can do them, we learn by doing them.\" - Aristotle\n\nThere's nothing quite like working with a new tool to really understand it, so we have put together some exercises through this book to give you a chance to put into practice what you learned in the previous lesson(s)."
    },
    {
      "chunk_id": "9dac203a54fe_0",
      "chapter": "chapter-one-problem-set",
      "heading": "Problems using NDarray [(Official Documentation)](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html)",
      "text": "Problem 1: Initialize an ndarray of dimension 1x256 on the GPU without overwriting its memory. Then, find the index corresponding to the maximum value in the array (argmax)\n\n\n```python\n# Problem 1 Work Area\n```"
    },
    {
      "chunk_id": "e887775627c3_0",
      "chapter": "chapter-one-problem-set",
      "heading": "Problems from Linear Algebra",
      "text": "Problem 2: Create a 4x4 matrix of random values (where values are uniformly random on the interval [0, 1]. Then create an 4x4 identity matrix (an identity of size n is the n \u00d7 n square matrix with ones on the main diagonal and zeros elsewhere). Multiply the two together and verify that you get the original matrix back.\n\n\n```python\n# Problem 2 Work Area\n```\n\nProblem 3: Create a 3x3x20 tensor such that at every x, y coordinate, moving through the z coordinate lists the [Fibonacci sequence](https://en.wikipedia.org/wiki/Fibonacci_number). So, at a z position of 0, the 3x3 matrix will be all 1s. At z-position 1, the 3x3 matrix will be all 1s. At z-position 2, the 3x3 matrix will be all 2s, at z-position 3, the 3x3 matrix will be all 3s and so forth.\n\nHint: Create the first 2 matrices by hand and then use elementwise operations in a loop to construct the rest of the tensor. \n\n\n```python\n# Problem 3 Work Area\n```\n\nProblem 4: What is the sum of the vector you created? What is the mean?\n\n\n```python\n# Problem 4 Work Area\n```\n\nProblem 5: Create a vector [0, 1], and another vector [1, 0], and use mxnet to calculate the angle between them. Remember that the dot product of two vectors is equal to the cosine of the angle between the vectors, and that the arccos function is the inverse of cosine.\n\n\n```python\n# Problem 5 Work Area\n```"
    },
    {
      "chunk_id": "f6f548854576_0",
      "chapter": "chapter-one-problem-set",
      "heading": "Problems from Probability",
      "text": "Problem 6: In the classic game of Risk, the attacker can roll a maximum of three dice, while the defender can roll a maximum of two dice. Simulate the attacking and defending dice using `sample_multinomial` to try to estimate the odds that an attacker will win against a defender when both are rolling the maximum number of dice.\n\n\n```python\n# Problem 6 Work Area\n```"
    },
    {
      "chunk_id": "6d0a4a01e336_0",
      "chapter": "chapter-one-problem-set",
      "heading": "Problems from Automatic differentiation with ``autograd``",
      "text": "Problem 7: The formula for a parabola is y=ax^2+bx+c. If a=5 and b = 13, what is the slope of y when x=0.  How about when x=7? \n\n\n```python\n# Problem 7 Work Area\n```\n\nProblem 8: Graph the parabola described in Problem 6 and inspect the slope of y when x = 0 and x = 7. Does it match up with your answer from Problem 6?\n\n\n\n```python\n# Problem 8 Work Area\n```"
    },
    {
      "chunk_id": "afada9ec084f_0",
      "chapter": "chapter-one-problem-set",
      "heading": "Next",
      "text": "[Chapter 2: Linear regression from scratch](../chapter02_supervised-learning/linear-regression-scratch.ipynb)\n\nFor whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)\n\n\n```python\n\n```"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "# Machine Learning Fundamentals\n:label:`chap_ml-fundamentals`\n\nAs illustrated in :numref:`chap_introduction`,\ndeep learning is just one among many popular methods for solving machine learning problems.\nAs we have encountered when training\nlinear regressions, softmax regressions,\nand multilayer perceptrons,\noptimization algorithms\nreduce loss function values\nby iteratively updating model parameters.\nHowever,\nwhen we train high-capacity models,\nsuch as deep neural networks, we run the risk of overfitting.\nThus, we will need to provide your first rigorous introduction\nto the notions of overfitting, underfitting, and model selection.\nTo help you combat these problems,\nwe will introduce regularization techniques such as weight decay and dropout.\nIn view of many failed machine learning *deployments*,\nit is necessary to\nexpose some common concerns\nand stimulate the critical thinking required to detect these situations early, mitigate damage, and use machine learning responsibly.\nThroughout, we aim to give you a firm grasp not just of the concepts\nbut also of the practice of using machine learning models.\nAt the end of this chapter,\nwe apply what we have introduced so far to a real case: house price prediction.\nWe punt matters relating to the computational performance,\nscalability, and efficiency of our models to subsequent chapters.\n\n```toc\n:maxdepth: 2\n\nmodel-selection\nunderfit-overfit\n```"
    },
    {
      "chunk_id": "cf680da85e05_0",
      "chapter": "model-selection",
      "heading": "model-selection",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow'])\n```\n\n# Model Selection\n:label:`sec_model_selection`\n\nAs machine learning scientists,\nour goal is to discover *patterns*. But how can we be sure that we have\ntruly discovered a *general* pattern\nand not simply memorized our data? For example, imagine that we wanted to hunt\nfor patterns among genetic markers\nlinking patients to their dementia status,\nwhere the labels are drawn from the set\n$\\{\\text{dementia}, \\text{mild cognitive impairment}, \\text{healthy}\\}$. Because each person's genes identify them uniquely\n(ignoring identical siblings),\nit is possible to memorize the entire dataset. We do not want our model to say\n*\"That's Bob! I remember him! He has dementia!\"*\nThe reason why is simple. When we deploy the model in the future,\nwe will encounter patients\nthat the model has never seen before. Our predictions will only be useful\nif our model has truly discovered a *general* pattern. To recapitulate more formally,\nour goal is to discover patterns\nthat capture regularities in the underlying population\nfrom which our training set was drawn. If we are successful in this endeavor,\nthen we could successfully assess risk\neven for individuals that we have never encountered before. This problem---how to discover patterns that *generalize*---is\nthe fundamental problem of machine learning. The danger is that when we train models,\nwe access just a small sample of data. The largest *labeled* public image datasets, such as Imagenet :cite:`Deng.Dong.Socher.ea.2009` contain\nroughly one million images. Unlabeled image collections such as the Flickr YFC100M dataset can be significantly larger, containing 100 million images :cite:`Thomee.Shamma.Friedland.ea.2016`. While both numbers seem large, they are tiny compared to the space of all possible images that one could take at, say, 1 Megapixel resolution. Worse still, we frequently must learn from only hundreds of examples."
    },
    {
      "chunk_id": "cf680da85e05_1",
      "chapter": "model-selection",
      "heading": "model-selection",
      "text": "While both numbers seem large, they are tiny compared to the space of all possible images that one could take at, say, 1 Megapixel resolution. Worse still, we frequently must learn from only hundreds of examples. For instance,\na hospital might only have data of 100 occurrences of an infrequent disease. When working with finite samples, we run the risk\nthat we might discover apparent associations\nthat turn out not to hold up when we collect more data. The phenomenon of fitting our training data\nmore closely than we fit the underlying distribution is called *overfitting*, and the techniques used to combat overfitting are called *regularization*. While the following is no substitute for a proper introduction to statistical learning theory :cite:`Vapnik.1998,Boucheron.Bousquet.Lugosi.2005`, it should at least make you aware of some of the phenomena that arise in learning. Overfitting is really quite prevalent. In the previous sections, you might have observed\noverfitting while experimenting with the Fashion-MNIST dataset. If you altered the model structure or the hyperparameters during the experiment, you might have noticed that with enough neurons, layers, and training epochs, the model can eventually reach perfect accuracy on the training set, while the accuracy on test data deteriorates."
    },
    {
      "chunk_id": "a30a9f264efc_0",
      "chapter": "model-selection",
      "heading": "Training Error and Generalization Error",
      "text": "In order to discuss this phenomenon more formally,\nwe need to differentiate between training error and generalization error. The *training error* $R_\\mathrm{emp}$ is the error of our model\nas calculated on the training dataset,\nwhile *generalization error* $R$ is the expectation of our model's error\nwere we to apply it to an infinite stream of additional data examples\ndrawn from the same underlying data distribution as our original sample. They are defined as follows:\n\n$$R_\\mathrm{emp}[\\mathbf{X}, \\mathbf{Y}, f] = \\frac{1}{m} \\sum_{i=1}^m l(\\mathbf{x}_i, \\mathbf{y}_i, f(\\mathbf{x}_i))\n\\text{ and }\nR[p, f] = E_{(\\mathbf{x}, \\mathbf{y}) \\sim p} [l(\\mathbf{x}, \\mathbf{y}, f(\\mathbf{x}))]$$\n\nProblematically, we can never calculate the generalization error $R$ exactly. That is because the stream of infinite data is an imaginary object. In practice, we must *estimate* the generalization error\nby applying our model to an independent test set\nconstituted of a random selection of examples $\\mathbf{X}'$ and labels $\\mathbf{Y}'$\nthat were withheld from our training set. This yields $R_\\mathrm{emp}[\\mathbf{X}', \\mathbf{Y}', f]$. The following three thought experiments\nwill help illustrate this situation better. Consider a college student trying to prepare for his final exam. A diligent student will strive to practice well\nand test his abilities using exams from previous years. Nonetheless, doing well on past exams is no guarantee\nthat he will excel when it matters. For instance, the student might try to prepare\nby rote learning the answers to the exam questions. This requires the student to memorize many things. She might even remember the answers for past exams perfectly. Another student might prepare by trying to understand\nthe reasons for giving certain answers. While this tends to work well for drivers license exams,\nit has poor outcomes when the set of exam questions is more\nvaried and drawn from a larger, possibly infinite pool. Likewise, consider a model that simply uses a lookup table to answer questions."
    },
    {
      "chunk_id": "a30a9f264efc_1",
      "chapter": "model-selection",
      "heading": "Training Error and Generalization Error",
      "text": "While this tends to work well for drivers license exams,\nit has poor outcomes when the set of exam questions is more\nvaried and drawn from a larger, possibly infinite pool. Likewise, consider a model that simply uses a lookup table to answer questions. If the set of allowable inputs is discrete and reasonably small, then perhaps after viewing *many* training examples, this approach would perform well. Still this model has no ability to do better than random guessing when faced with examples that it has never seen before. In reality the input spaces are far too large to memorize the answers corresponding to every conceivable input. For example, consider the black and white $28\\times28$ images. If each pixel can take one among $256$ grayscale values, then there are $256^{784} \\approx 10^{1888}$ possible images. That means that there are far more low-resolution grayscale thumbnail-sized images than the approximately $10^{82}$ atoms in the universe. Even if we could encounter such data, we could never afford to store the lookup table. This explosion in the number of required samples is closely related to the curse of dimensionality where simple problems become rather difficult once the data is high dimensional :cite:`Friedman.1997`. Last, consider the problem of trying\nto classify the outcomes of coin tosses (class 0: heads, class 1: tails)\nbased on some contextual features that might be available. Suppose that the coin is fair. No matter what algorithm we come up with,\nthe generalization error will always be $\\frac{1}{2}$. However, for most algorithms,\nwe should expect our training error to be considerably lower,\ndepending on the luck of the draw,\neven if we did not have any features! Consider the dataset {0, 1, 1, 1, 0, 1}. Our feature-less algorithm would have to fall back on always predicting\nthe *majority class*, which appears from our limited sample to be *1*. In this case, the model that always predicts class 1\nwill incur an error of $\\frac{1}{3}$,\nconsiderably better than our generalization error."
    },
    {
      "chunk_id": "a30a9f264efc_2",
      "chapter": "model-selection",
      "heading": "Training Error and Generalization Error",
      "text": "In this case, the model that always predicts class 1\nwill incur an error of $\\frac{1}{3}$,\nconsiderably better than our generalization error. As we increase the amount of data,\nthe probability that the fraction of heads\nwill deviate significantly from $\\frac{1}{2}$ diminishes,\nand our training error would come to match the generalization error."
    },
    {
      "chunk_id": "91e789b0a380_0",
      "chapter": "model-selection",
      "heading": "Statistical Learning Theory",
      "text": "Since generalization is the fundamental problem in machine learning,\nyou might not be surprised to learn\nthat many mathematicians and theorists have dedicated their lives\nto developing formal theories to describe this phenomenon. In their [epoynmous theorem](https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem) Glivenko and Cantelli derived the rate at which the training error\nconverges to the generalization error :cite:`Glivenko.1933,Cantelli.1933`. In a series of seminal papers, Vapnik and Chervonenkis\nextended this theory to more general classes of functions\n:cite:`Vapnik.Chervonenkis.1964,Vapnik.Chervonenkis.1968,Vapnik.Chervonenkis.1971,Vapnik.Chervonenkis.1981,Vapnik.Chervonenkis.1991,Vapnik.Chervonenkis.1974*1`. This work laid the foundations of statistical learning theory. In the standard supervised learning setting, which we have addressed\nup until now and will stick with throughout most of this book,\nwe assume that both the training data and the test data\nare drawn *independently* from *identical* distributions. This is commonly called the *IID assumption*. It means that\nall samples are drawn from the same distribution. It also means that,\nknowing all $n-1$ samples makes it no easier for us to predict the $n$-th sample\nthan it is to predict the first one. Being a good machine learning scientist requires thinking critically,\nand already you should be poking holes in this assumption,\ncoming up with common cases where the assumption fails. What if we train a mortality risk predictor\non data collected from patients at UCSF Medical Center,\nand apply it on patients at Massachusetts General Hospital? These distributions are simply not identical. This is a well-studied\nproblem in statistics :cite:`Rosenbaum.Rubin.1983`. Moreover, draws might be correlated in time. What if we are classifying the topics of Tweets? The news cycle would create temporal dependencies\nin the topics being discussed, violating any assumptions of independence."
    },
    {
      "chunk_id": "91e789b0a380_1",
      "chapter": "model-selection",
      "heading": "Statistical Learning Theory",
      "text": "Moreover, draws might be correlated in time. What if we are classifying the topics of Tweets? The news cycle would create temporal dependencies\nin the topics being discussed, violating any assumptions of independence. Sometimes we can get away with minor violations of the IID assumption\nand our models will continue to work remarkably well. After all, nearly every real-world application\ninvolves at least some minor violation of the IID assumption,\nand yet we have many useful tools for\nvarious applications such as\nface recognition,\nspeech recognition, and language translation. :cite:`Yu.1994` provides\na quantitative handle on this behavior. Other violations are sure to cause trouble. Imagine, for example, if we try to train\na face recognition system by training it\nexclusively on university students\nand then want to deploy it as a tool\nfor monitoring geriatrics in a nursing home population. This is unlikely to work well since college students\ntend to look considerably different from the elderly. In subsequent chapters, we will discuss problems\narising from violations of the IID assumption. For now, even taking the IID assumption for granted,\nunderstanding generalization is a formidable problem. Moreover, elucidating the precise theoretical foundations\nthat might explain why deep neural networks generalize as well as they do\ncontinues to vex the greatest minds in learning theory :cite:`Frankle.Carbin.2018,Bartlett.Montanari.Rakhlin.2021,Nagarajan.Kolter.2019,Kawaguchi.Kaelbling.Bengio.2017`. When we train our models, we attempt to search for a function\nthat fits the training data as well as possible. If the function is so flexible that it can catch on to spurious patterns\njust as easily as to true associations,\nthen it might perform *too well* without producing a model\nthat generalizes well to unseen data. This is precisely what we want to avoid or at least control."
    },
    {
      "chunk_id": "91e789b0a380_2",
      "chapter": "model-selection",
      "heading": "Statistical Learning Theory",
      "text": "This is precisely what we want to avoid or at least control. Many of the techniques in deep learning are heuristics and tricks\naimed at guarding against overfitting (:numref:`sec_weight_decay`, :numref:`sec_dropout`, :numref:`sec_batch_norm`)."
    },
    {
      "chunk_id": "0b48332e2ccd_0",
      "chapter": "model-selection",
      "heading": "Model Complexity",
      "text": "When we have simple models and abundant data,\nwe expect the generalization error to resemble the training error. When we work with more complex models and fewer examples,\nwe expect the training error to go down but the generalization gap to grow. What precisely constitutes model complexity is a complex matter. Many factors govern whether a model will generalize well. For example a model with more parameters might be considered more complex in general. Note, though, that this is not necessarily true. For instance, kernel methods operate in spaces with infinite numbers of parameters, yet they exhibit very well-controlled model complexity :cite:`Scholkopf.Smola.2002`. Instead, a better way to think about this is that a\nmodel whose parameters can take a wider range of values\nmight be more complex. Often with neural networks, we think of a model\nthat takes more training iterations as more complex,\nand one subject to *early stopping* (fewer training iterations) as less complex :cite:`Prechelt.1998`. It can be difficult to compare the complexity among members\nof substantially different model classes\n(say, decision trees vs. neural networks). For now, a simple rule of thumb is quite useful:\na model that can readily explain arbitrary facts\nis what statisticians view as complex,\nwhereas one that has only a limited expressive power\nbut still manages to explain the data well\nis probably closer to the truth :cite:`Vapnik.Levin.Le-Cun.1994`. In philosophy, this is closely related to Popper's\ncriterion of falsifiability\nof a scientific theory: a theory is good if it fits data\nand if there are specific tests that can be used to disprove it. This is important since all statistical estimation is\n*post hoc*,\ni.e., we estimate after we observe the facts,\nhence vulnerable to the associated fallacy :cite:`Corfield.Scholkopf.Vapnik.2009`. For now, we will put the philosophy aside and stick to more tangible issues."
    },
    {
      "chunk_id": "0b48332e2ccd_1",
      "chapter": "model-selection",
      "heading": "Model Complexity",
      "text": "For now, we will put the philosophy aside and stick to more tangible issues. In this section, to give you some intuition,\nwe will focus on a few factors that tend\nto influence the generalizability of a model class:\n\n1. The number of tunable parameters. When the number of tunable parameters, sometimes called the *degrees of freedom*, is large, models tend to be more susceptible to overfitting :cite:`Murata.Yoshizawa.Amari.1994`. 1. The values taken by the parameters. When weights can take a wider range of values, models can be more susceptible to overfitting :cite:`Krogh.Hertz.1992`. 1. The number of training examples. It is trivially easy to overfit a dataset containing only one or two examples even if your model is simple. But overfitting a dataset with millions of examples requires an extremely flexible model :cite:`Henighan.Kaplan.Katz.ea.2020`."
    },
    {
      "chunk_id": "ba7d82cf5c9f_0",
      "chapter": "model-selection",
      "heading": "Model Selection",
      "text": "In machine learning, we usually select our final model\nafter evaluating several candidate models.\nThis process is called *model selection*.\nSometimes the models subject to comparison\nare fundamentally different in nature\n(say, decision trees vs. linear models).\nAt other times, we are comparing\nmembers of the same class of models\nthat have been trained with different hyperparameter settings.\n\nWith MLPs, for example,\nwe may wish to compare models with\ndifferent numbers of hidden layers,\ndifferent numbers of hidden units,\nand various choices of the activation functions\napplied to each hidden layer. For a particularly elegant\nstrategy to accomplish this for computer vision see :cite:`Radosavovic.Kosaraju.Girshick.ea.2020`.\nIn order to determine the best among our candidate models,\nwe will typically employ a validation dataset."
    },
    {
      "chunk_id": "dd374da52fef_0",
      "chapter": "model-selection",
      "heading": "Validation Dataset",
      "text": "In principle we should not touch our test set\nuntil after we have chosen all our hyperparameters. Were we to use the test data in the model selection process,\nthere is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data,\nthere is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know? See e.g. :cite:`Ong.Smola.Williamson.ea.2005` for an example how\nthis can lead to absurd results even for models where the complexity\ncan be tightly controlled. Thus, we should never rely on the test data for model selection. And yet we cannot rely solely on the training data\nfor model selection either because\nwe cannot estimate the generalization error\non the very data that we use to train the model. In practical applications, the picture gets muddier. While ideally we would only touch the test data once,\nto assess the very best model or to compare\na small number of models to each other,\nreal-world test data is seldom discarded after just one use. We can seldom afford a new test set for each round of experiments. In fact, recycling benchmark data for decades can have a significant impact on the\ndevelopment of algorithms, e.g. for [image classification](https://paperswithcode.com/sota/image-classification-on-imagenet) and [optical character recognition](https://paperswithcode.com/sota/image-classification-on-mnist). The common practice to address the problem of `training on the test set`\nis to split our data three ways,\nincorporating a *validation set*\nin addition to the training and test datasets. The result is a murky practice where the boundaries\nbetween validation and test data are worryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book\nwe are really working with what should rightly be called\ntraining data and validation data, with no true test sets. Therefore, the accuracy reported in each experiment of the book is really\nthe validation accuracy and not a true test set accuracy."
    },
    {
      "chunk_id": "20a8135b2185_0",
      "chapter": "model-selection",
      "heading": "$K$-Fold Cross-Validation",
      "text": "When training data is scarce,\nwe might not even be able to afford to hold out\nenough data to constitute a proper validation set.\nOne popular solution to this problem is to employ\n$K$*-fold cross-validation*.\nHere, the original training data is split into $K$ non-overlapping subsets.\nThen model training and validation are executed $K$ times,\neach time training on $K-1$ subsets and validating\non a different subset (the one not used for training in that round).\nFinally, the training and validation errors are estimated\nby averaging over the results from the $K$ experiments."
    },
    {
      "chunk_id": "eb1ac0a7e49c_0",
      "chapter": "model-selection",
      "heading": "Underfitting or Overfitting?",
      "text": "When we compare the training and validation errors,\nwe want to be mindful of two common situations.\nFirst, we want to watch out for cases\nwhen our training error and validation error are both substantial\nbut there is a little gap between them.\nIf the model is unable to reduce the training error,\nthat could mean that our model is too simple\n(i.e., insufficiently expressive)\nto capture the pattern that we are trying to model.\nMoreover, since the *generalization gap*\nbetween our training and validation errors is small,\nwe have reason to believe that we could get away with a more complex model.\nThis phenomenon is known as *underfitting* (note, though, that it could also\nmean that the problem is simply very difficult).\n\nOn the other hand, as we discussed above,\nwe want to watch out for the cases\nwhen our training error is significantly lower\nthan our validation error, indicating severe *overfitting*.\nNote that overfitting is not always a bad thing.\nWith deep learning especially, it is well known\nthat the best predictive models often perform\nfar better on training data than on holdout data.\nUltimately, we usually care more about the validation error\nthan about the gap between the training and validation errors.\n\nWhether we overfit or underfit can depend\nboth on the complexity of our model\nand the size of the available training datasets,\ntwo topics that we discuss below."
    },
    {
      "chunk_id": "0b48332e2ccd_0",
      "chapter": "model-selection",
      "heading": "Model Complexity",
      "text": "To illustrate some classical intuition\nabout overfitting and model complexity,\nwe give an example using polynomials. Given training data consisting of a single feature $x$\nand a corresponding real-valued label $y$,\nwe try to find the polynomial of degree $d$\n\n$$\\hat{y}= \\sum_{i=0}^d x^i w_i$$\n\nto estimate the labels $y$. This is just a linear regression problem\nwhere our features are given by the powers of $x$,\nthe model's weights are given by $w_i$,\nand the bias is given by $w_0$ since $x^0 = 1$ for all $x$. Since this is just a linear regression problem,\nwe can use the squared error as our loss function. A higher-order polynomial function is more complex\nthan a lower-order polynomial function,\nsince the higher-order polynomial has more parameters\nand the model function's selection range is wider. Fixing the training dataset,\nhigher-order polynomial functions should always\nachieve lower (at worst, equal) training error\nrelative to lower degree polynomials. In fact, whenever the data examples each have a distinct value of $x$,\na polynomial function with degree equal to the number of data examples\ncan fit the training set perfectly. We visualize the relationship between polynomial degree\nand underfitting vs. overfitting in :numref:`fig_capacity_error`. ![Influence of model complexity on underfitting and overfitting](../img/capacity-vs-error.svg)\n:label:`fig_capacity_error`\n\nMuch of the intuition of this arises from Statistical Learning Theory. One of the guarantees it\nprovides :cite:`Vapnik.1998` is that the gap between empirical risk and expected risk is bounded by\n\n$$\\Pr\\left(R[p, f] - R_\\mathrm{emp}[\\mathbf{X}, \\mathbf{Y}, f] < \\epsilon\\right) \\geq 1-\\delta\n\\ \\text{for}\\ \\epsilon \\geq c \\sqrt{(\\mathrm{VC} - \\log \\delta)/n}.$$\n\nHere $\\delta > 0$ is the probability that the bound is violated and $\\mathrm{VC}$ is the Vapnik-Chervonenkis (VC)\ndimension of the set of functions that we want to fit. For instance, for polynomials of degree $d$ the VC dimension is $d+1$."
    },
    {
      "chunk_id": "0b48332e2ccd_1",
      "chapter": "model-selection",
      "heading": "Model Complexity",
      "text": "For instance, for polynomials of degree $d$ the VC dimension is $d+1$. Lastly, $c > 0$ is a constant that depends only on the scale of the loss that can be incurred. In short, this shows that our bound becomes increasingly loose as we pick more complex models and that the number of free parameters should not increase more rapidly than the dataset size $n$ increases. See :cite:`Boucheron.Bousquet.Lugosi.2005` for a detailed discussion and for much more advanced ways of measuring function complexity."
    },
    {
      "chunk_id": "ad78a127758d_0",
      "chapter": "model-selection",
      "heading": "Dataset Size",
      "text": "As the above bound already indicates, the other big consideration to bear in mind is the dataset size.\nFixing our model, the fewer samples we have in the training dataset,\nthe more likely (and more severely) we are to encounter overfitting.\nAs we increase the amount of training data,\nthe generalization error typically decreases.\nMoreover, in general, more data never hurts.\nFor a fixed task and data distribution, model complexity should not\nincrease more rapidly than the amount of data does.\nGiven more data, we might profitably attempt to fit a more complex model.\nAbsent sufficient data, simpler models may be more difficult to beat.\nFor many tasks, deep learning only outperforms linear models\nwhen many thousands of training examples are available.\nIn part, the current success of deep learning\nowes to the current abundance of massive datasets\ndue to Internet companies, cheap storage, connected devices,\nand the broad digitization of the economy."
    },
    {
      "chunk_id": "acc193dff28a_0",
      "chapter": "model-selection",
      "heading": "Summary",
      "text": "This section explored some of the theoretical underpinnings of machine learning. Making these work for modern deep learning is still very much a work in progress. Simply minimizing the training error will not necessarily mean a reduction in the generalization error. Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error. Nonetheless, we provided some basic intuition how to control the generalization error. For instance, we can to resort to validation sets or statistical bounds.\n\nA few rules of thumb: 1) A validation set can be used for model selection, provided that it is not used too liberally. 2) A more complex model requires more data, where the amount of data should scale up at least as rapidly as the model complexity. 3) More parameters can mean more complex models, but there are ways where this need not be the case, e.g. by controlling the magnitude. 4) More data makes everything better. As long as the data is drawn from the same distribution. 5) Check your assumptions."
    },
    {
      "chunk_id": "104724307201_0",
      "chapter": "model-selection",
      "heading": "Exercises",
      "text": "1. Can you solve the problem of polynomial regression exactly?\n1. Give at least five examples where dependent random variables make treating the problem as IID data inadvisable.\n1. Can you ever expect to see zero training error? Under which circumstances would you see zero generalization error?\n1. Why is $k$-fold crossvalidation very expensive to compute?\n1. Why is the $k$-fold crossvalidation error estimate biased?\n1. The VC dimension is defined as the maximum number of points that can be classified with arbitrary labels $\\{\\pm 1\\}$ by a function of a class of functions. Why might this not be a good idea to measure how complex the class of functions is? Hint: what about the magnitude of the functions?\n1. Your manager gives you a difficult dataset on which your current algorithm doesn't perform so well. How would you justify to him that you need more data? Hint: you cannot increase the data but you can decrease it.\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/96)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/97)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/234)\n:end_tab:"
    },
    {
      "chunk_id": "27ce52cf21ee_0",
      "chapter": "underfit-overfit",
      "heading": "underfit-overfit",
      "text": "```{.python .input  n=1}\n%load_ext d2lbook.tab\ntab.interact_select(['mxnet', 'pytorch', 'tensorflow'])\n```\n\n# Underfitting and Overfitting \n:label:`sec_polynomial`\n\nIn this section we test out some of the concepts that we saw previously. To keep matters simple, we use polynomial regression as our toy example.\n\n```{.python .input  n=3}\n%%tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import gluon, np, npx\nfrom mxnet.gluon import nn\nimport math\nnpx.set_np()\n```\n\n```{.python .input  n=4}\n%%tab pytorch\nfrom d2l import torch as d2l\nimport torch\nfrom torch import nn\nimport math\n```\n\n```{.python .input  n=5}\n%%tab tensorflow\nfrom d2l import tensorflow as d2l\nimport tensorflow as tf\nimport math\n```"
    },
    {
      "chunk_id": "26aed3cc614b_0",
      "chapter": "underfit-overfit",
      "heading": "Generating the Dataset",
      "text": "First we need data. Given $x$, we will [**use the following cubic polynomial to generate the labels**] on training and test data:\n\n(**$$y = 5 + 1.2x - 3.4\\frac{x^2}{2!} + 5.6 \\frac{x^3}{3!} + \\epsilon \\text{ where }\n\\epsilon \\sim \\mathcal{N}(0, 0.1^2).$$**)\n\nThe noise term $\\epsilon$ obeys a normal distribution\nwith a mean of 0 and a standard deviation of 0.1.\nFor optimization, we typically want to avoid\nvery large values of gradients or losses.\nThis is why the *features*\nare rescaled from $x^i$ to $\\frac{x^i}{i!}$.\nIt allows us to avoid very large values for large exponents $i$.\nWe will synthesize 100 samples each for the training set and test set.\n\n```{.python .input  n=6}\n%%tab all\nclass Data(d2l.DataModule):\n    def __init__(self, num_train, num_val, num_inputs, batch_size):\n        self.save_hyperparameters()        \n        p, n = max(3, self.num_inputs), num_train + num_val\n        w = d2l.tensor([1.2, -3.4, 5.6] + [0]*(p-3))\n        if tab.selected('mxnet') or tab.selected('pytorch'):\n            x = d2l.randn(n, 1)\n            noise = d2l.randn(n, 1) * 0.1\n        if tab.selected('tensorflow'):\n            x = d2l.normal((n, 1))\n            noise = d2l.normal((n, 1)) * 0.1\n        X = d2l.concat([x ** (i+1) / math.gamma(i+2) for i in range(p)], 1)\n        self.y = d2l.matmul(X, d2l.reshape(w, (-1, 1))) + noise\n        self.X = X[:,:num_inputs]\n        \n    def get_dataloader(self, train):\n        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n        return self.get_tensorloader([self.X, self.y], train, i)\n```\n\nAgain, monomials stored in `poly_features`\nare rescaled by the gamma function,\nwhere $\\Gamma(n)=(n-1)!$.\n[**Take a look at the first 2 samples**] from the generated dataset.\nThe value 1 is technically a feature,\nnamely the constant feature corresponding to the bias."
    },
    {
      "chunk_id": "fafd876b1078_0",
      "chapter": "underfit-overfit",
      "heading": "[**Third-Order Polynomial Function Fitting (Normal)**]",
      "text": "We will begin by first using a third-order polynomial function, which is the same order as that of the data generation function.\nThe results show that this model's training and test losses can be both effectively reduced.\nThe learned model parameters are also close\nto the true values $w = [1.2, -3.4, 5.6], b=5$.\n\n```{.python .input  n=7}\n%%tab all\ndef train(p):\n    if tab.selected('mxnet') or tab.selected('tensorflow'):\n        model = d2l.LinearRegression(lr=0.01)\n    if tab.selected('pytorch'):\n        model = d2l.LinearRegression(p, lr=0.01)\n    model.board.ylim = [1, 1e2]\n    data = Data(200, 200, p, 20)\n    trainer = d2l.Trainer(max_epochs=10)\n    trainer.fit(model, data)\n    print(model.get_w_b())\n    \ntrain(p=3)\n```"
    },
    {
      "chunk_id": "a7dbf7110e6f_0",
      "chapter": "underfit-overfit",
      "heading": "[**Linear Function Fitting (Underfitting)**]",
      "text": "Let's take another look at linear function fitting.\nAfter the decline in early epochs,\nit becomes difficult to further decrease\nthis model's training loss.\nAfter the last epoch iteration has been completed,\nthe training loss is still high.\nWhen used to fit nonlinear patterns\n(like the third-order polynomial function here)\nlinear models are liable to underfit.\n\n```{.python .input  n=8}\n%%tab all\ntrain(p=1)\n```"
    },
    {
      "chunk_id": "786f915062bd_0",
      "chapter": "underfit-overfit",
      "heading": "[**Higher-Order Polynomial Function Fitting  (Overfitting)**]",
      "text": "Now let's try to train the model\nusing a polynomial of too high degree.\nHere, there is insufficient data to learn that\nthe higher-degree coefficients should have values close to zero.\nAs a result, our overly-complex model\nis so susceptible that it is being influenced\nby noise in the training data.\nThough the training loss can be effectively reduced,\nthe test loss is still much higher.\nIt shows that\nthe complex model overfits the data.\n\n```{.python .input  n=9}\n%%tab all\ntrain(p=10)\n```\n\nIn the subsequent sections, we will continue\nto discuss overfitting problems\nand methods for dealing with them,\nsuch as weight decay and dropout."
    },
    {
      "chunk_id": "476cab9e53db_0",
      "chapter": "underfit-overfit",
      "heading": "Summary",
      "text": "* Since the generalization error cannot be estimated based on the training error, simply minimizing the training error will not necessarily mean a reduction in the generalization error. Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error.\n* A validation set can be used for model selection, provided that it is not used too liberally.\n* Underfitting means that a model is not able to reduce the training error. When training error is much lower than validation error, there is overfitting.\n* We should choose an appropriately complex model and avoid using insufficient training samples."
    },
    {
      "chunk_id": "a4d06ea65e31_0",
      "chapter": "underfit-overfit",
      "heading": "Exercises",
      "text": "1. Can you solve the polynomial regression problem exactly? Hint: use linear algebra.\n1. Consider model selection for polynomials:\n    1. Plot the training loss vs. model complexity (degree of the polynomial). What do you observe? What degree of polynomial do you need to reduce the training loss to 0?\n    1. Plot the test loss in this case.\n    1. Generate the same plot as a function of the amount of data.\n1. What happens if you drop the normalization ($1/i!$) of the polynomial features $x^i$? Can you fix this in some other way?\n1. Can you ever expect to see zero generalization error?\n\n:begin_tab:`mxnet`\n[Discussions](https://discuss.d2l.ai/t/96)\n:end_tab:\n\n:begin_tab:`pytorch`\n[Discussions](https://discuss.d2l.ai/t/97)\n:end_tab:\n\n:begin_tab:`tensorflow`\n[Discussions](https://discuss.d2l.ai/t/234)\n:end_tab:"
    },
    {
      "chunk_id": "991811812ee2_0",
      "chapter": "fasttext-pretraining",
      "heading": "fasttext-pretraining",
      "text": "# Pretraining fastText\n:label:`sec_word2vec_gluon`\n\nIn this section, we will\ntrain a skip-gram model defined in\n:numref:`sec_word2vec`. First, import the\npackages and modules required for the experiment, and load the PTB dataset."
    },
    {
      "chunk_id": "991811812ee2_1",
      "chapter": "fasttext-pretraining",
      "heading": "fasttext-pretraining",
      "text": "# Pretraining fastText\n:label:`sec_word2vec_gluon`\n\nIn this section, we will\ntrain a skip-gram model defined in\n:numref:`sec_word2vec`. First, import the\npackages and modules required for the experiment, and load the PTB dataset. ```{.python .input  n=1}\n#@tab mxnet\nfrom collections import defaultdict\nfrom d2l import mxnet as d2l\nfrom functools import partial\nfrom mxnet import autograd, gluon, init, np, npx, cpu\nfrom mxnet.gluon import nn\nimport random\n\nnpx.set_np()\n```\n\n```{.python .input  n=2}\n#@tab mxnet\ndef compute_subword(token):\n    if token[0] != '<' and token[-1] != '>':\n        token = '<' + token + '>'\n        subwords = {token}\n        for i in range(len(token)-3):\n            for j in range(i + 3, len(token)+1):\n                if j - i <= 6:\n                    subwords.add(token[i:j])\n        return subwords\n    else:\n        return [token]\n```\n\n```{.python .input  n=3}\n#@tab mxnet\ndef get_subword_map(vocab):\n    tokenid_to_subword, subword_to_idx = defaultdict(list), defaultdict(int)\n    for token, tokenid in vocab.token_to_idx.items():\n        subwords = compute_subword(token)\n        for subword in subwords:\n            if subword not in subword_to_idx:\n                subword_to_idx[subword] = len(subword_to_idx)\n            tokenid_to_subword[tokenid].append(subword_to_idx[subword])\n    return tokenid_to_subword, subword_to_idx\n```\n\n```{.python .input  n=4}\n#@tab mxnet\ndef token_transform(tokens, vocab, subword_map):\n    if not isinstance(tokens, (list, tuple)):\n        return d2l.truncate_pad(subword_map[tokens],\n                                 64, vocab['<pad>'])\n    return [token_transform(token, vocab, subword_map) for token in tokens]\n```\n\n```{.python .input  n=5}\n#@tab mxnet\ndef batchify(data, vocab, subword_map):\n    max_len = max(len(c) + len(n) for _, c, n in data)\n    centers, contexts_negatives, masks, labels = [], [], [], []\n    for center, context, negative in data:\n        cur_len = len(context) + len(negative)\n        centers += [token_transform([center], vocab, subword_map)]\n        contexts_negatives += [token_transform(context + negative + \\\n                               [1] * (max_len - cur_len), vocab, subword_map)]\n        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n    return (np.array(centers), np.array(contexts_negatives),\n            np.array(masks), np.array(labels))\n```\n\n```{.python .input  n=6}\n#@tab mxnet\ndef load_data_ptb(batch_size, max_window_size, num_noise_words):\n    num_workers = d2l.get_dataloader_workers()\n    sentences = d2l.read_ptb()\n    vocab = d2l.Vocab(sentences, min_freq=10, reserved_tokens=['<pad>'])\n    subsampled = d2l.subsampling(sentences, vocab)\n    corpus = [vocab[line] for line in subsampled]\n    all_centers, all_contexts = d2l.get_centers_and_contexts(\n        corpus, max_window_size)\n    all_negatives = d2l.get_negatives(all_contexts, corpus, num_noise_words)\n    dataset = gluon.data.ArrayDataset(\n        all_centers, all_contexts, all_negatives)\n    subword_map, subword_to_idx = get_subword_map(vocab)\n    data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True,\n                                      batchify_fn=partial(batchify, vocab=vocab, subword_map=subword_map),\n                                      num_workers=num_workers)\n    return data_iter, vocab, subword_to_idx\n```\n\n```{.python .input  n=7}\n#@tab mxnet\nbatch_size, max_window_size, num_noise_words = 512, 5, 5\ndata_iter, vocab, subword_to_idx = load_data_ptb(batch_size, max_window_size, num_noise_words)\n```\n\n```{.python .input  n=8}\n#@tab mxnet\nnames = ['centers', 'contexts_negatives', 'masks', 'labels']\nfor batch in data_iter:\n    for name, data in zip(names, batch):\n        print(name, 'shape:', data.shape)\n    break\n```"
    },
    {
      "chunk_id": "1db750633340_0",
      "chapter": "fasttext-pretraining",
      "heading": "The Skip-Gram Model",
      "text": "We will implement the skip-gram model by using embedding\nlayers and minibatch multiplication. These methods are also often used to\nimplement other natural language processing applications."
    },
    {
      "chunk_id": "c033cf6cfada_0",
      "chapter": "fasttext-pretraining",
      "heading": "Embedding Layer",
      "text": "The layer in which the obtained word is embedded is called the embedding layer,\nwhich can be obtained by creating an `nn.Embedding` instance in Gluon. The\nweight of the embedding layer is a matrix whose number of rows is the dictionary\nsize (`input_dim`) and whose number of columns is the dimension of each word\nvector (`output_dim`). We set the dictionary size to $20$ and the word vector\ndimension to $4$.\n\n```{.python .input  n=9}\n#@tab mxnet\nembed = nn.Embedding(input_dim=20, output_dim=4)\nembed.initialize()\nembed.weight\n```"
    },
    {
      "chunk_id": "7be51a28f7a6_0",
      "chapter": "fasttext-pretraining",
      "heading": "Skip-gram Model Forward Calculation",
      "text": "In forward calculation, the input of\nthe skip-gram model contains the central target word index `center` and the\nconcatenated context and noise word index `contexts_and_negatives`. In which,\nthe `center` variable has the shape (batch size, 1), while the\n`contexts_and_negatives` variable has the shape (batch size, `max_len`). These\ntwo variables are first transformed from word indexes to word vectors by the\nword embedding layer, and then the output of shape (batch size, 1, `max_len`) is\nobtained by minibatch multiplication. Each element in the output is the inner\nproduct of the central target word vector and the context word vector or noise\nword vector.\n\n```{.python .input  n=10}\n#@tab mxnet\ndef skip_gram(center, contexts_and_negatives, embed_v, embed_u, padding):\n    v_embedding = embed_v(center)\n    v_mask = (center!=padding).astype('float32')\n    v = (v_embedding * np.expand_dims(v_mask, axis=-1)).sum(-1)/(np.expand_dims(v_mask.sum(-1), axis=-1)+1e-5)\n    u_embedding = embed_u(contexts_and_negatives)\n    u_mask = (contexts_and_negatives!=padding).astype('float32')\n    u = (u_embedding * np.expand_dims(u_mask, axis=-1)).sum(-1)/(np.expand_dims(u_mask.sum(-1), axis=-1)+1e-5)\n    pred = npx.batch_dot(v, u.swapaxes(1, 2))\n    return pred\n```\n\nVerify that the output shape should be (batch size, 1, `max_len`).\n\n```{.python .input  n=12}\n#@tab mxnet\nskip_gram(np.ones((2, 1, 64)), np.ones((2, 6, 64)), embed, embed, vocab['<pad>']).shape\n```"
    },
    {
      "chunk_id": "6618489c0fb7_0",
      "chapter": "fasttext-pretraining",
      "heading": "Training",
      "text": "Before training the word embedding model, we need to define the\nloss function of the model."
    },
    {
      "chunk_id": "0e9d8aebf34c_0",
      "chapter": "fasttext-pretraining",
      "heading": "Binary Cross Entropy Loss Function",
      "text": "According\nto the definition of the loss function in negative sampling, we can directly use\nGluon's binary cross-entropy loss function `SigmoidBinaryCrossEntropyLoss`.\n\n```{.python .input  n=13}\n#@tab mxnet\nloss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n```\n\nIt is worth mentioning that we can use the mask variable to specify the partial\npredicted value and label that participate in loss function calculation in the\nminibatch: when the mask is 1, the predicted value and label of the\ncorresponding position will participate in the calculation of the loss function;\nWhen the mask is 0, the predicted value and label of the corresponding position\ndo not participate in the calculation of the loss function. As we mentioned\nearlier, mask variables can be used to avoid the effect of padding on loss\nfunction calculations.\n\nGiven two identical examples, different masks lead to\ndifferent loss values.\n\n```{.python .input  n=14}\n#@tab mxnet\npred = np.array([[.5]*4]*2)\nlabel = np.array([[1, 0, 1, 0]]*2)\nmask = np.array([[1, 1, 1, 1], [1, 1, 0, 0]])\nloss(pred, label, mask)\n```\n\nWe can normalize the loss in each example due to various lengths in each\nexample.\n\n```{.python .input  n=15}\n#@tab mxnet\nloss(pred, label, mask) / mask.sum(axis=1) * mask.shape[1]\n```"
    },
    {
      "chunk_id": "0512fd76eda5_0",
      "chapter": "fasttext-pretraining",
      "heading": "Initializing Model Parameters",
      "text": "We construct the embedding layers of the\ncentral and context words, respectively, and set the hyperparameter word vector\ndimension `embed_size` to 100.\n\n```{.python .input  n=16}\n#@tab mxnet\nembed_size = 100\nnet = nn.Sequential()\nnet.add(nn.Embedding(input_dim=len(subword_to_idx), output_dim=embed_size),\n        nn.Embedding(input_dim=len(subword_to_idx), output_dim=embed_size))\n```"
    },
    {
      "chunk_id": "6618489c0fb7_0",
      "chapter": "fasttext-pretraining",
      "heading": "Training",
      "text": "The training function is defined below. Because of the existence\nof padding, the calculation of the loss function is slightly different compared\nto the previous training functions. ```{.python .input  n=17}\n#@tab mxnet\ndef train(net, data_iter, lr, num_epochs, ctx=d2l.try_gpu()):\n    net.initialize(ctx=ctx, force_reinit=True)\n    trainer = gluon.Trainer(net.collect_params(), 'adam',\n                            {'learning_rate': lr})\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs])\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(2)  # loss_sum, num_tokens\n        for i, batch in enumerate(data_iter):\n            center, context_negative, mask, label = [\n                data.as_in_ctx(ctx) for data in batch]\n            with autograd.record():\n                pred = skip_gram(center, context_negative, net[0], net[1], vocab['<pad>'])\n                l = (loss(pred.reshape(label.shape), label, mask)\n                     / mask.sum(axis=1) * mask.shape[1])\n            l.backward()\n            trainer.step(batch_size)\n            metric.add(l.sum(), l.size)\n            if (i+1) % 50 == 0:\n                animator.add(epoch+(i+1)/len(data_iter),\n                             (metric[0]/metric[1],))\n            npx.waitall()\n    print('loss %.3f, %d tokens/sec on %s ' % (\n        metric[0]/metric[1], metric[1]/timer.stop(), ctx))\n```\n\nNow, we can train a skip-gram model using negative sampling."
    },
    {
      "chunk_id": "6618489c0fb7_1",
      "chapter": "fasttext-pretraining",
      "heading": "Training",
      "text": "```{.python .input  n=20}\n#@tab mxnet\nlr, num_epochs = 0.01, 5\ntrain(net, data_iter, lr, num_epochs)\n```\n\n```{.python .input}\n#@tab mxnet\ndef get_similar_tokens(query_token, k, embed, vocab, subword_to_idx):\n    W = embed.weight.data()\n    x = W[vocab[query_token]]\n    all_v = []\n    for token in vocab.idx_to_token:\n        subword = compute_subword(token)\n        w_v = W[[subword_to_idx[s] for s in subword]].sum(0)\n        all_v.append(np.expand_dims(w_v, 0))\n    all_v = np.concatenate(all_v, 0)\n    # Compute the cosine similarity. Add 1e-9 for numerical stability\n    cos = np.dot(all_v, x) / np.sqrt(np.sum(all_v * all_v, axis=1) * np.sum(x * x) + 1e-9)\n    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n    for i in topk[1:]:  # Remove the input words\n        print('cosine sim=%.3f: %s' % (cos[i], (vocab.idx_to_token[i])))\n```\n\n```{.python .input}\n#@tab mxnet\nget_similar_tokens('chip', 3, net[0], vocab, subword_to_idx)\n```"
    },
    {
      "chunk_id": "c6524cbf191e_0",
      "chapter": "glove-pretraining",
      "heading": "glove-pretraining",
      "text": "# Pretraining GloVe\n:label:`sec_GloVe_gluon`\n\nIn this section, we will train a\nGloVe model defined in\n:numref:`sec_glove`.\n\nFirst, import the\npackages and\nmodules required for the experiment.\n\n```{.python .input  n=1}\n#@tab mxnet\nfrom collections import defaultdict\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx, cpu\nfrom mxnet.gluon import nn\nimport random\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "29304544dd27_0",
      "chapter": "glove-pretraining",
      "heading": "Preprocessing Dataset",
      "text": "We will train GloVe model on PTB dataset.\n\nFirst, we\nread the PTB dataset, build a vocabulary with words and map each token into an\nindex to construct the corpus.\n\n```{.python .input  n=2}\n#@tab mxnet\nsentences = d2l.read_ptb()\nvocab = d2l.Vocab(sentences, min_freq=10)\ncorpus = [vocab[line] for line in sentences]\n```"
    },
    {
      "chunk_id": "35e5ba1c5284_0",
      "chapter": "glove-pretraining",
      "heading": "Construct Cooccurrence Counts",
      "text": "Let the word-word cooccurrence counts be\ndenoted by $X$, whose entries $x_{ij}$ tabulate the number of times word $j$\noccurs in the context of word $i$. Next, we define following function to\nextracts all the central target words and their context words. It use a\ndecreasing weighting function, so that word pairs that are $d$ words apart\ncontribute $1/d$ to the total count. This is one way to account for the fact\nthat very distant word pairs are expected to contain less relevant information\nabout the words\u2019 relationship to one another. ```{.python .input  n=3}\n#@tab mxnet\ndef get_coocurrence_counts(corpus, window_size):\n    centers, contexts = [], []\n    cooccurence_counts = defaultdict(float)\n    for line in corpus:\n        # Each sentence needs at least 2 words to form a\n        # \"central target word - context word\" pair\n        if len(line) < 2:\n            continue\n        centers += line\n        for i in range(len(line)):  # Context window centered at i\n            left_indices = list(range(max(0, i - window_size), i))\n            right_indices = list(range(i + 1,\n                                       min(len(line), i + 1 + window_size)))\n            left_context = [line[idx] for idx in left_indices]\n            right_context = [line[idx] for idx in right_indices]\n            for distance, word in enumerate(left_context[::-1]):\n                cooccurence_counts[line[i], word] += 1 / (distance + 1)\n            for distance, word in enumerate(right_context):\n                cooccurence_counts[line[i], word] += 1 / (distance + 1)\n    cooccurence_counts = [(word[0], word[1], count)\n                          for word, count in cooccurence_counts.items()]\n    return cooccurence_counts\n```\n\nWe create an artificial dataset containing two sentences of 5 and 2 words,\nrespectively. Assume the maximum context window is 4. Then, we print the\ncooccurrence counts of all the central target words and context words."
    },
    {
      "chunk_id": "35e5ba1c5284_1",
      "chapter": "glove-pretraining",
      "heading": "Construct Cooccurrence Counts",
      "text": "Assume the maximum context window is 4. Then, we print the\ncooccurrence counts of all the central target words and context words. ```{.python .input  n=4}\n#@tab mxnet\ntiny_dataset = [list(range(5)), list(range(5, 7))]\nprint('dataset', tiny_dataset)\nfor center, context, coocurrence in get_coocurrence_counts(tiny_dataset, 4):\n        print('center: %s, context: %s, coocurrence: %.2f' %\n          (center, context, coocurrence))\n```\n\nWe set the maximum context window size to 5. The following extracts all the\ncentral target words and their context words in the dataset, and calculate their\ncooccurrence counts\n\n```{.python .input  n=5}\n#@tab mxnet\ncoocurrence_matrix = get_coocurrence_counts(corpus, 5)\n'# center-context pairs: %d' % len(coocurrence_matrix)\n```"
    },
    {
      "chunk_id": "ca2df1ae0e7b_0",
      "chapter": "glove-pretraining",
      "heading": "Putting All Things Together",
      "text": "Last, We define the load_data_ptb_glove\nfunction that read the PTB dataset and return the data loader.\n\n```{.python .input  n=16}\n#@tab mxnet\ndef load_data_ptb_glove(batch_size, window_size):\n    num_workers = d2l.get_dataloader_workers()\n    sentences = d2l.read_ptb()\n    vocab = d2l.Vocab(sentences, min_freq=5)\n    corpus = [vocab[line] for line in sentences]\n    coocurrence_matrix = get_coocurrence_counts(corpus, window_size)\n    dataset = gluon.data.ArrayDataset(coocurrence_matrix)\n    data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True,\n                                      num_workers=num_workers)\n    return data_iter, vocab\n\nbatch_size, window_size = 1024, 10\ndata_iter, vocab = load_data_ptb_glove(batch_size, window_size)\n```\n\nLet\u2019s print the first minibatch of the data iterator.\n\n```{.python .input  n=17}\n#@tab mxnet\nnames = ['center', 'context', 'Cooccurence']\nfor batch in data_iter:\n    for name, data in zip(names, batch):\n        print(name, 'shape:', data.shape)\n    break\n```"
    },
    {
      "chunk_id": "f6970ff549f5_0",
      "chapter": "glove-pretraining",
      "heading": "The GloVe Model",
      "text": "In section 15.1, we introduced the goal of GloVe is to\nminimize the loss function.\n\n$$\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}}\nh(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j -\n\\log\\,x_{ij}\\right)^2.$$\n\nWe will implement the GloVe model by implementing each\npart of the loss function."
    },
    {
      "chunk_id": "a7be25716ebc_0",
      "chapter": "glove-pretraining",
      "heading": "Weight function",
      "text": "GloVe introduced a weighting\nfunction $h(x_{ij})$ into the loss function.\n\n$$h(x_{ij})=\\begin{cases}\n(\\frac{x}{x_{max}})^\\alpha & x_{ij}<x_{max}\\\\\n1 & otherwise\n\\end{cases}$$\n\n\nWe\nimplement the weighting function $h(x_{ij})$. Since $x_{ij}<x_{max}$is\nequivalent to $(\\frac{x}{x_{max}})^\\alpha < 1$, we can give the following\nimplementation.\n\n```{.python .input  n=18}\n#@tab mxnet\ndef compute_weight(x, x_max = 30, alpha = 0.75):\n    w = (x / x_max) ** alpha\n    return np.minimum(w, 1)\n```\n\nThe following prints the weight of the cooccurrence counts of all the central\ntarget words and context words when the $x_{max}$ set to 2 and $\\alpha$ to 0.75\n\n```{.python .input  n=19}\n#@tab mxnet\nfor center, context, coocurrence in get_coocurrence_counts(tiny_dataset, 4)[:5]:\n    print('center: %s, context: %s, coocurrence: %.2f, weight: %.2f' %\n          (center, context, coocurrence, compute_weight(coocurrence, x_max = 2, alpha = 0.75)))\n```"
    },
    {
      "chunk_id": "3acfdc863880_0",
      "chapter": "glove-pretraining",
      "heading": "Bias Term",
      "text": "GloVe has two scalar model parameters for each word $w_i$ : the\nbias terms $b_i$ (for central target words) and  $c_i$ (for context words).\nBias term can be realized by embedding layer. The weight of the embedding layer\nis a matrix whose number of rows is the dictionary size (input_dim) and whose\nnumber of columns is one.\n\nWe set the dictionary size to  20.\n\n```{.python .input}\n#@tab mxnet\nembed_bias = nn.Embedding(input_dim=20, output_dim=1)\nembed_bias.initialize()\nembed_bias.weight\n```\n\nThe input of the embedding layer is the index of the word. When we enter the\nindex $i$ of a word, the embedding layer returns the $i$ th row of the weight\nvalue as its bias term.\n\n```{.python .input}\n#@tab mxnet\nx = np.array([1, 2, 3])\nembed_bias(x)\n```"
    },
    {
      "chunk_id": "09bb11be76ee_0",
      "chapter": "glove-pretraining",
      "heading": "GloVe Model Forward Calculation",
      "text": "In forward calculation, the input of\nthe\nGloVe model contains the central\ntarget word index `center` and the context word\nindex\n`context`. In which,\nthe `center` variable has the shape (batch\nsize, 1),\nwhile the\n`context` variable has the shape (batch size,\n1). These\ntwo variables\nare first transformed from word indexes to word\nvectors by the\nword embedding\nlayer.\n\n```{.python .input  n=20}\n#@tab mxnet\ndef GloVe(center, context, coocurrence, embed_v, embed_u,\n          bias_v, bias_u, x_max, alpha):\n    # Shape of v: (batch_size, embed_size)\n    v = embed_v(center)\n    # Shape of u: (batch_size, embed_size)\n    u = embed_u(context)\n    # Shape of b: (batch_size, )\n    b = bias_v(center).squeeze()\n    # Shape of c: (batch_size, )\n    c = bias_u(context).squeeze()\n    # Shape of embed_products: (batch_size,)\n    embed_products = npx.batch_dot(np.expand_dims(v, 1),\n                                   np.expand_dims(u, 2)).squeeze()\n    # Shape of distance_expr: (batch_size,)\n    distance_expr = np.power(embed_products + b +\n                     c - np.log(coocurrence), 2)\n    # Shape of weight: (batch_size,)\n    weight = compute_weight(coocurrence)\n    return weight * distance_expr\n```\n\nVerify that the output shape should be (batch size, ).\n\n```{.python .input  n=21}\n#@tab mxnet\nembed_word = nn.Embedding(input_dim=20, output_dim=4)\nembed_word.initialize()\nGloVe(np.ones((2)), np.ones((2)), np.ones((2)), embed_word, embed_word,\n      embed_bias, embed_bias, x_max = 2, alpha = 0.75).shape\n```"
    },
    {
      "chunk_id": "d2908c6ac648_0",
      "chapter": "glove-pretraining",
      "heading": "Training",
      "text": "Before training the word embedding model, we need to define the\nloss function of\nthe model."
    },
    {
      "chunk_id": "6de77404d31d_0",
      "chapter": "glove-pretraining",
      "heading": "Initializing Model Parameters",
      "text": "We construct the\nembedding layers of words and\nadditional biases,\nand set the hyperparameter word\nvector\ndimension `embed_size`\nto\n100.\n\n```{.python .input  n=22}\n#@tab mxnet\nembed_size = 100\nnet = nn.Sequential()\nnet.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),\n        nn.Embedding(input_dim=len(vocab), output_dim=embed_size),\n        nn.Embedding(input_dim=len(vocab), output_dim=1),\n        nn.Embedding(input_dim=len(vocab), output_dim=1))\n```"
    },
    {
      "chunk_id": "d2908c6ac648_0",
      "chapter": "glove-pretraining",
      "heading": "Training",
      "text": "The training function is defined below.\n\n```{.python .input  n=23}\n#@tab mxnet\ndef train(net, data_iter, lr, num_epochs, x_max, alpha, ctx=d2l.try_gpu()):\n    net.initialize(ctx=ctx, force_reinit=True)\n    trainer = gluon.Trainer(net.collect_params(), 'AdaGrad',\n                            {'learning_rate': lr})\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[0, num_epochs])\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(2)  # loss_sum, num_tokens\n        for i, batch in enumerate(data_iter):\n            center, context, coocurrence = [\n                data.as_in_context(ctx) for data in batch]\n            with autograd.record():\n                l = GloVe(center, context, coocurrence.astype('float32'),\n                          net[0], net[1], net[2], net[3], x_max, alpha)\n            l.backward()\n            trainer.step(batch_size)\n            metric.add(l.sum(), l.size)\n            if (i+1) % 50 == 0:\n                animator.add(epoch+(i+1)/len(data_iter),\n                             (metric[0]/metric[1],))\n    print('loss %.3f, %d tokens/sec on %s ' % (\n        metric[0]/metric[1], metric[1]/timer.stop(), ctx))\n```\n\nNow, we can train a GloVe model.\n\n```{.python .input  n=12}\n#@tab mxnet\nlr, num_epochs = 0.1, 5\nx_max, alpha = 100, 0.75\ntrain(net, data_iter, lr, num_epochs, x_max, alpha)\n```"
    },
    {
      "chunk_id": "b01e0420fe47_0",
      "chapter": "glove-pretraining",
      "heading": "Applying the GloVe Model",
      "text": "GloVe model generates two sets of word vectors,\n`embed_v` and `embed_u` . `embed_v` and `embed_u` are\nequivalent and differ only\nas a result of their random initializations; the two sets of vectors should\nperform equivalently.Generally, we choose to use the sum `embed_v`+`embed_u` as\nour word vectors.\n\n\nAfter training the GloVe model, we can still represent\nsimilarity in meaning between words based on the cosine similarity of two word\nvectors.\n\n```{.python .input  n=13}\n#@tab mxnet\ndef get_similar_tokens(query_token, k, embed_v, embed_u):\n    W = embed_v.weight.data() + embed_u.weight.data()\n    x = W[vocab[query_token]]\n    # Compute the cosine similarity. Add 1e-9 for numerical stability\n    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\n    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n    for i in topk[1:]:  # Remove the input words\n        print('cosine sim=%.3f: %s' % (cos[i], (vocab.idx_to_token[i])))\n\nget_similar_tokens('chip', 3, net[0], net[1])\n```"
    },
    {
      "chunk_id": "b9bae7bf605c_0",
      "chapter": "key-value",
      "heading": "key-value",
      "text": "# Distributed Key-Value Store\n:label:`sec_key_value`\n\nKVStore is a place for data sharing. Think of it as a single object shared across different devices (GPUs and computers), where each device can push data in and pull data out."
    },
    {
      "chunk_id": "64e400254e27_0",
      "chapter": "key-value",
      "heading": "Initialization",
      "text": "Let us consider a simple example: initializing a (int, NDArray) pair into the store, and then pulling the value out:\n\n```{.python .input  n=1}\n#@tab mxnet\nfrom mxnet import np, npx, kv\nnpx.set_np()\n```\n\n```{.python .input  n=2}\n#@tab mxnet\nnp.ones((2,3))\n```\n\n```{.python .input  n=11}\n#@tab mxnet\nhelp(kv)\n```\n\n```{.python .input  n=3}\n#@tab mxnet\nkv = kv.create('local')  # Create a local kv store.\nshape = (2,3)\nkv.init(3, np.ones(shape) * 2)\na = np.zeros(shape)\nkv.pull(3, out = a)\nprint(a)\n```"
    },
    {
      "chunk_id": "465b94c67ad1_0",
      "chapter": "key-value",
      "heading": "Push, Aggregate, and Update",
      "text": "For any key that has been initialized, you can push a new value with the same shape to the key:\n\n```{.python .input  n=4}\n#@tab mxnet\nkv.push(3, np.ones(shape)*8)\nkv.pull(3, out = a)  # Pull out the value\nprint(a.asnumpy())\n```\n\nThe data for pushing can be stored on any device. Furthermore, you can push multiple values into the same key, where KVStore will first sum all of these values and then push the aggregated value. Here we will just demonstrate pushing a list of values on CPU. Please note summation only happens if the value list is longer than one\n\n```{.python .input  n=5}\n#@tab mxnet\ndevices = [npx.cpu(i) for i in range(4)]\nb = [np.ones(shape, ctx=device) for device in devices]\nkv.push(3, b)\nkv.pull(3, out = a)\nprint(a)\n```\n\nFor each push, KVStore combines the pushed value with the value stored using an updater. The default updater is ASSIGN. You can replace the default to control how data is merged:\n\n```{.python .input  n=6}\n#@tab mxnet\ndef update(key, input, stored):\n    print(f'update on key: {key}')\n    stored += input * 2\nkv._set_updater(update)\nkv.pull(3, out=a)\nprint(a)\n```\n\n```{.python .input  n=7}\n#@tab mxnet\nkv.push(3, np.ones(shape))\nkv.pull(3, out=a)\nprint(a)\n```"
    },
    {
      "chunk_id": "97c13db00a9e_0",
      "chapter": "key-value",
      "heading": "Pull",
      "text": "You have already seen how to pull a single key-value pair. Similarly, to push, you can pull the value onto several devices with a single call:\n\n```{.python .input  n=8}\n#@tab mxnet\nb = [np.ones(shape, ctx=device) for device in devices]\nkv.pull(3, out = b)\nprint(b[1])\n```"
    },
    {
      "chunk_id": "502a8030f154_0",
      "chapter": "key-value",
      "heading": "Handle a List of Key-Value Pairs",
      "text": "All operations introduced so far involve a single key. KVStore also provides an interface for a list of key-value pairs.\n\nFor a single device:\n\n```{.python .input  n=9}\n#@tab mxnet\nkeys = [5, 7, 9]\nkv.init(keys, [np.ones(shape)]*len(keys))\nkv.push(keys, [np.ones(shape)]*len(keys))\nb = [np.zeros(shape)]*len(keys)\nkv.pull(keys, out = b)\nprint(b[1])\n```\n\nFor multiple devices:\n\n```{.python .input  n=10}\n#@tab mxnet\nb = [[np.ones(shape, ctx=device) for device in devices]] * len(keys)\nkv.push(keys, b)\nkv.pull(keys, out = b)\nprint(b[1][1])\n```"
    },
    {
      "chunk_id": "78acf2eb134e_0",
      "chapter": "seq2seq",
      "heading": "seq2seq",
      "text": "#  Sequence to Sequence\n:label:`sec_seq2seq`\n\nThe sequence to sequence (seq2seq) model is based on the encoder-decoder architecture to generate a sequence output for a sequence input, as demonstrated in :numref:`fig_seq2seq`. Both the encoder and the decoder use recurrent neural networks (RNNs) to handle sequence inputs of variable length. The hidden state of the encoder is used directly to initialize the decoder hidden state to pass information from the encoder to the decoder.\n\n![The sequence to sequence model architecture.](../img/seq2seq.svg)\n:label:`fig_seq2seq`\n\nThe layers in the encoder and the decoder are illustrated in :numref:`fig_seq2seq_details`.\n\n![Layers in the encoder and the decoder.](../img/seq2seq-details.svg)\n:label:`fig_seq2seq_details`\n\nIn this section we will explain and implement the seq2seq model to train on the machine translation dataset.\n\n```{.python .input  n=1}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nfrom mxnet import np, npx, init, gluon, autograd\nfrom mxnet.gluon import nn, rnn\nfrom queue import PriorityQueue\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "8e624e2d4a2d_0",
      "chapter": "seq2seq",
      "heading": "Encoder",
      "text": "Recall that the encoder of seq2seq can transform the inputs of variable length to a fixed-length context vector $\\mathbf{c}$ by encoding the sequence information into $\\mathbf{c}$. We usually use RNN layers within the encoder. Suppose that we have an input sequence $x_1, \\ldots, x_T$, where $x_t$ is the $t^\\mathrm{th}$ word. At timestep $t$, the RNN will have two vectors as the input: the feature vector $\\mathbf{x}_t$ of $x_t$ and the hidden state of the last timestep $\\mathbf{h}_{t-1}$. Let us denote the transformation of the RNN's hidden states by a function $f$:\n\n$$\\mathbf{h}_t = f (\\mathbf{x}_t, \\mathbf{h}_{t-1}).$$\n\nNext, the encoder captures information of all the hidden states and encodes it into the context vector $\\mathbf{c}$ with a function $q$:\n\n$$\\mathbf{c} = q (\\mathbf{h}_1, \\ldots, \\mathbf{h}_T).$$\n\nFor example, if we choose $q$ as $q (\\mathbf{h}_1, \\ldots, \\mathbf{h}_T) = \\mathbf{h}_T$, then the context vector will be the final hidden state $\\mathbf{h}_T$. So far what we describe above is a unidirectional RNN, where each timestep's hidden state depends only on the previous timesteps'. We can also use other forms of RNNs such as GRUs, LSTMs, and bidirectional RNNs to encode the sequential input. Now let us implement the seq2seq's encoder. Here we use the word embedding layer to obtain the feature vector\naccording to the word index of the input language. Those feature vectors will be fed to a multi-layer LSTM. The input for the encoder is a batch of sequences, which is 2-D tensor with shape (batch size, sequence length). The encoder returns both the LSTM outputs, i.e., hidden states of all the timesteps, as well as the hidden state and the memory cell of the final timestep."
    },
    {
      "chunk_id": "8e624e2d4a2d_1",
      "chapter": "seq2seq",
      "heading": "Encoder",
      "text": "The encoder returns both the LSTM outputs, i.e., hidden states of all the timesteps, as well as the hidden state and the memory cell of the final timestep. ```{.python .input  n=2}\n#@tab mxnet\n#@save\nclass Seq2SeqEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n\n    def forward(self, X, *args):\n        X = self.embedding(X)  # X shape: (batch_size, seq_len, embed_size)\n        # RNN needs first axes to be timestep, i.e., seq_len\n        X = X.swapaxes(0, 1)\n        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n        out, state = self.rnn(X, state)\n        # out shape: (seq_len, batch_size, num_hiddens)\n        # state shape: (num_layers, batch_size, num_hiddens),\n        # where \"state\" contains the hidden state and the memory cell\n        return out, state\n```\n\nNext, we will create a minibatch sequence input with a batch size of 4 and 7 timesteps. We assume the number of hidden layers of the LSTM unit is 2 and the number of hidden units is 16. The output shape returned by the encoder after performing forward calculation on the input is (number of timesteps, batch size, number of hidden units). The shape of the multi-layer hidden state of the gated recurrent unit in the final timestep is (number of hidden layers, batch size, number of hidden units). For the gated recurrent unit, the `state` list contains only one element, which is the hidden state. If long short-term memory is used, the `state` list will also contain another element, which is the memory cell."
    },
    {
      "chunk_id": "8e624e2d4a2d_2",
      "chapter": "seq2seq",
      "heading": "Encoder",
      "text": "For the gated recurrent unit, the `state` list contains only one element, which is the hidden state. If long short-term memory is used, the `state` list will also contain another element, which is the memory cell. ```{.python .input  n=3}\n#@tab mxnet\nencoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n                         num_layers=2)\nencoder.initialize()\nX = np.zeros((4, 7))\noutput, state = encoder(X)\noutput.shape\n```\n\nSince an LSTM is used, the `state` list will contain both the hidden state and the memory cell with same shape (number of hidden layers, batch size, number of hidden units). However, if a GRU is used, the `state` list will contain only one element---the hidden state in the final timestep with shape (number of hidden layers, batch size, number of hidden units). ```{.python .input  n=4}\n#@tab mxnet\nlen(state), state[0].shape, state[1].shape\n```"
    },
    {
      "chunk_id": "f0a6ef5a0a52_0",
      "chapter": "seq2seq",
      "heading": "Decoder",
      "text": ":label:`sec_seq2seq_decoder`\n\nAs we just introduced, the context vector $\\mathbf{c}$ encodes the information from the whole input sequence $x_1, \\ldots, x_T$. Suppose that the given outputs in the training set are $y_1, \\ldots, y_{T'}$. At each timestep $t'$, the conditional probability of output $y_{t'}$ will depend on the previous output sequence $y_1, \\ldots, y_{t'-1}$ and the context vector $\\mathbf{c}$, i.e.,\n\n$$P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c}).$$\n\nHence, we can use another RNN as the decoder. At timestep $t'$, the decoder will update its hidden state $\\mathbf{s}_{t'}$ using three inputs: the feature vector $\\mathbf{y}_{t'-1}$ of $y_{t'-1}$, the context vector $\\mathbf{c}$, and the hidden state of the last timestep $\\mathbf{s}_{t'-1}$. Let us denote the transformation of the RNN's hidden states within the decoder by a function $g$:\n\n$$\\mathbf{s}_{t'} = g(\\mathbf{y}_{t'-1}, \\mathbf{c}, \\mathbf{s}_{t'-1}).$$\n\n\nWhen implementing the decoder, we directly use the hidden state of the encoder in the final timestep as the initial hidden state of the decoder. This requires that the encoder and decoder RNNs have the same numbers of layers and hidden units. The LSTM forward calculation of the decoder is similar to that of the encoder. The only difference is that we add a dense layer after the LSTM layers, where the hidden size is the vocabulary size. The dense layer will predict the confidence score for each word."
    },
    {
      "chunk_id": "f0a6ef5a0a52_1",
      "chapter": "seq2seq",
      "heading": "Decoder",
      "text": "The only difference is that we add a dense layer after the LSTM layers, where the hidden size is the vocabulary size. The dense layer will predict the confidence score for each word. ```{.python .input  n=5}\n#@tab mxnet\n#@save\nclass Seq2SeqDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqDecoder, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Dense(vocab_size, flatten=False)\n\n    def init_state(self, enc_outputs, *args):\n        return enc_outputs[1]\n\n    def forward(self, X, state):\n        X = self.embedding(X).swapaxes(0, 1)\n        out, state = self.rnn(X, state)\n        # Make the batch to be the first dimension to simplify loss\n        # computation\n        out = self.dense(out).swapaxes(0, 1)\n        return out, state\n```\n\nWe create a decoder with the same hyper-parameters as the encoder. As we can see, the output shape is changed to (batch size, the sequence length, vocabulary size). ```{.python .input  n=6}\n#@tab mxnet\ndecoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,\n                         num_hiddens=16, num_layers=2)\ndecoder.initialize()\nstate = decoder.init_state(encoder(X))\nout, state = decoder(X, state)\nout.shape, len(state), state[0].shape, state[1].shape\n```"
    },
    {
      "chunk_id": "a4300978219a_0",
      "chapter": "seq2seq",
      "heading": "The Loss Function",
      "text": "For each timestep, the decoder outputs a vocabulary-size confidence score vector to predict words. Similar to language modeling, we can apply softmax to obtain the probabilities and then use cross-entropy loss to calculate the loss. Note that we padded the target sentences to make them have the same length, but we do not need to compute the loss on the padding symbols. To implement the loss function that filters out some entries, we will use an operator called `SequenceMask`. It can specify to mask the first dimension (`axis=0`) or the second one (`axis=1`). If the second one is chosen, given a valid length vector `len` and 2-dim input `X`, this operator sets `X[i, len[i]:] = 0` for all $i$'s. ```{.python .input  n=7}\n#@tab mxnet\nX = np.array([[1, 2, 3], [4, 5, 6]])\nnpx.sequence_mask(X, np.array([1, 2]), True, axis=1)\n```\n\nApply to $n$-dim tensor $X$, it sets `X[i, len[i]:, :, ..., :] = 0`. In addition, we can specify the filling value such as $-1$ as shown below. ```{.python .input  n=8}\n#@tab mxnet\nX = np.ones((2, 3, 4))\nnpx.sequence_mask(X, np.array([1, 2]), True, value=-1, axis=1)\n```\n\nNow we can implement the masked version of the softmax cross-entropy loss. Note that each Gluon loss function allows to specify per-example weights, in default they are 1s. Then we can just use a zero weight for each example we would like to remove. So our customized loss function accepts an additional `valid_len` argument to ignore some failing elements in each sequence."
    },
    {
      "chunk_id": "a4300978219a_1",
      "chapter": "seq2seq",
      "heading": "The Loss Function",
      "text": "Then we can just use a zero weight for each example we would like to remove. So our customized loss function accepts an additional `valid_len` argument to ignore some failing elements in each sequence. ```{.python .input  n=9}\n#@tab mxnet\n#@save\nclass MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n    # pred shape: (batch_size, seq_len, vocab_size)\n    # label shape: (batch_size, seq_len)\n    # valid_len shape: (batch_size, )\n    def forward(self, pred, label, valid_len):\n        # weights shape: (batch_size, seq_len, 1)\n        weights = np.expand_dims(np.ones_like(label), axis=-1)\n        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)\n```\n\nFor a sanity check, we create identical three sequences, keep 4 elements for the first sequence, 2 elements for the second sequence, and none for the last one. Then the first example loss should be 2 times larger than the second one, and the last loss should be 0. ```{.python .input  n=10}\n#@tab mxnet\nloss = MaskedSoftmaxCELoss()\nloss(np.ones((3, 4, 10)), np.ones((3, 4)), np.array([4, 2, 0]))\n```"
    },
    {
      "chunk_id": "50fa98b1d18e_0",
      "chapter": "seq2seq",
      "heading": "Training",
      "text": ":label:`sec_seq2seq_training`\n\nDuring training, if the target sequence has length $n$, we feed the first $n-1$ tokens into the decoder as inputs, and the last $n-1$ tokens are used as ground truth label. ```{.python .input  n=11}\n#@tab mxnet\n#@save\ndef train_s2s_ch9(model, data_iter, lr, num_epochs, ctx):\n    model.initialize(init.Xavier(), force_reinit=True, ctx=ctx)\n    trainer = gluon.Trainer(model.collect_params(),\n                            'adam', {'learning_rate': lr})\n    loss = MaskedSoftmaxCELoss()\n    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n                            xlim=[1, num_epochs], ylim=[0, 0.25])\n    for epoch in range(1, num_epochs + 1):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(2)  # loss_sum, num_tokens\n        for batch in data_iter:\n            X, X_vlen, Y, Y_vlen = [x.as_in_ctx(ctx) for x in batch]\n            Y_input, Y_label, Y_vlen = Y[:, :-1], Y[:, 1:], Y_vlen-1\n            with autograd.record():\n                Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n                l = loss(Y_hat, Y_label, Y_vlen)\n            l.backward()\n            d2l.grad_clipping(model, 1)\n            num_tokens = Y_vlen.sum()\n            trainer.step(num_tokens)\n            metric.add(l.sum(), num_tokens)\n        if epoch % 10 == 0:\n            animator.add(epoch, (metric[0]/metric[1],))\n    print('loss %.3f, %d tokens/sec on %s ' % (\n        metric[0]/metric[1], metric[1]/timer.stop(), ctx))\n```\n\nNext, we create a model instance and set hyper-parameters. Then, we can train the model."
    },
    {
      "chunk_id": "50fa98b1d18e_1",
      "chapter": "seq2seq",
      "heading": "Training",
      "text": "Then, we can train the model. ```{.python .input  n=12}\n#@tab mxnet\nembed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\nbatch_size, num_steps = 64, 10\nlr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()\n\nsrc_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(batch_size, num_steps)\nencoder = Seq2SeqEncoder(\n    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder = Seq2SeqDecoder(\n    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel = d2l.EncoderDecoder(encoder, decoder)\ntrain_s2s_ch9(model, train_iter, lr, num_epochs, ctx)\n```"
    },
    {
      "chunk_id": "b7021ed39958_0",
      "chapter": "seq2seq",
      "heading": "Predicting",
      "text": "Here we implement the simplest method, greedy search, to generate an output\nsequence. As illustrated in :numref:`fig_seq2seq_predict`, during predicting, we feed the same \"&lt;bos&gt;\" token to the decoder as training at timestep 0. But the input token for a later timestep is the predicted token from the previous timestep."
    },
    {
      "chunk_id": "b7021ed39958_1",
      "chapter": "seq2seq",
      "heading": "Predicting",
      "text": "As illustrated in :numref:`fig_seq2seq_predict`, during predicting, we feed the same \"&lt;bos&gt;\" token to the decoder as training at timestep 0. But the input token for a later timestep is the predicted token from the previous timestep. ![Sequence to sequence model predicting with greedy search](../img/seq2seq_predict.svg)\n:label:`fig_seq2seq_predict`\n\n```{.python .input  n=16}\n#@tab mxnet\n#@save\nclass BeamSearchNode(object):\n    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n        self.h = hiddenstate\n        self.prevNode = previousNode\n        self.wordid = wordId\n        self.logp = logProb\n        self.length = length\n\n    def eval(self, alpha=1.0):\n        reward = 0\n        return self.logp / float(self.length - 1 + 1e-6) + alpha * reward\n#@save\ndef predict_s2s_ch9_beam(model, src_sentence, src_vocab, tgt_vocab, num_steps,\n                         beam_width, ctx):\n    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n    enc_valid_len = np.array([len(src_tokens)], ctx=ctx)\n    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n    enc_X = np.array(src_tokens, ctx=ctx)\n    # Add the batch_size dimension\n    enc_outputs = model.encoder(np.expand_dims(enc_X, axis=0),\n                                enc_valid_len)\n    dec_state = model.decoder.init_state(enc_outputs, enc_valid_len)\n    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=ctx), axis=0)\n    \n    node = BeamSearchNode(dec_state, None, dec_X, 0, 1)\n    nodes = PriorityQueue()\n    decoded_batch = []\n    nodes.put((-node.eval(), node))\n    #while True:\n    for _ in range(num_steps):\n        # give up when decoding takes too long\n        score, n = nodes.get()\n        dec_X = n.wordid\n        dec_state = n.h\n        if n.wordid.item() == tgt_vocab['<eos>'] and n.prevNode != None:\n            endnodes = (score, n)\n            break\n        Y, dec_state = model.decoder(dec_X, dec_state)\n        indexes = npx.topk(Y, k=beam_width)\n        nextnodes = []\n        for new_k in range(beam_width):\n            decoded_t = indexes[:,:,new_k]\n            log_p = Y.reshape(-1)[decoded_t].item()\n            node = BeamSearchNode(dec_state, n, decoded_t, n.logp + log_p, n.length + 1)\n            score = -node.eval()\n            nextnodes.append((score, node))\n        for i in range(len(nextnodes)):\n            score, nn = nextnodes[i]\n            nodes.put((score, nn))\n            \n    if len(endnodes) == 0:\n        endnodes = nodes.get()\n    score, n = endnodes\n    predict_tokens = []\n    if int(n.wordid) != tgt_vocab['<eos>']:\n        predict_tokens.append(int(n.wordid))\n    # back trace\n    while n.prevNode != None:\n        n = n.prevNode\n        if int(n.wordid) != tgt_vocab['<bos>']:\n            predict_tokens.append(int(n.wordid))\n    predict_tokens = predict_tokens[::-1]\n    return ' '.join(tgt_vocab.to_tokens(predict_tokens))\n```\n\nTry several examples:\n\n```{.python .input  n=204}\n#@tab mxnet\nfor sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n    print(sentence + ' => ' + predict_s2s_ch9_beam(\n        model, sentence, src_vocab, tgt_vocab, num_steps, 3, ctx))\n```"
    },
    {
      "chunk_id": "09e56f8a84b2_0",
      "chapter": "seq2seq",
      "heading": "Summary",
      "text": "* The sequence to sequence (seq2seq) model is based on the encoder-decoder architecture to generate a sequence output from a sequence input.\n* We use multiple LSTM layers for both the encoder and the decoder."
    },
    {
      "chunk_id": "3ce322d3a228_0",
      "chapter": "seq2seq",
      "heading": "Exercises",
      "text": "1. Can you think of other use cases of seq2seq besides neural machine translation?\n1. What if the input sequence in the example of this section is longer?\n1. If we do not use the `SequenceMask` in the loss function, what may happen?"
    },
    {
      "chunk_id": "56310530b141_0",
      "chapter": "similarity-analogy",
      "heading": "similarity-analogy",
      "text": "# Finding Synonyms and Analogies\n:label:`sec_synonyms`\n\nIn :numref:`sec_word2vec_gluon` we trained a word2vec word embedding model\non a small-scale dataset and searched for synonyms using the cosine similarity\nof word vectors. In practice, word vectors pretrained on a large-scale corpus\ncan often be applied to downstream natural language processing tasks. This\nsection will demonstrate how to use these pretrained word vectors to find\nsynonyms and analogies. We will continue to apply pretrained word vectors in\nsubsequent sections.\n\n```{.python .input  n=1}\n#@tab mxnet\nfrom d2l import mxnet as d2l\nimport matplotlib.pyplot as plt\nfrom mxnet import np, npx\nimport numpy\nimport os\nfrom sklearn.decomposition import PCA\n\nnpx.set_np()\n```"
    },
    {
      "chunk_id": "1db299c04f9b_0",
      "chapter": "similarity-analogy",
      "heading": "Using Pretrained Word Vectors",
      "text": "Below lists pretrained GloVe embeddings of dimensions 50, 100, and 300,\nwhich can be downloaded from the [GloVe website](https://nlp.stanford.edu/projects/glove/). The pretrained fastText embeddings are available in multiple languages. Here we consider one English version (300-dimensional \"wiki.en\") that can be downloaded from the\n[fastText website](https://fasttext.cc/). ```{.python .input  n=2}\n#@tab mxnet\n#@save\nd2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n\n#@save\nd2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n\n#@save\nd2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n\n#@save\nd2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n                           'c1816da3821ae9f43899be655002f6c723e91b88')\n```\n\nWe define the following `TokenEmbedding` class to load the above pretrained Glove and fastText embeddings."
    },
    {
      "chunk_id": "1db299c04f9b_1",
      "chapter": "similarity-analogy",
      "heading": "Using Pretrained Word Vectors",
      "text": "```{.python .input  n=3}\n#@tab mxnet\n#@save\nclass TokenEmbedding:\n    \"\"\"Token Embedding.\"\"\"\n    def __init__(self, embedding_name):\n        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n            embedding_name)\n        self.unknown_idx = 0\n        self.token_to_idx = {token: idx for idx, token in \n                             enumerate(self.idx_to_token)}\n\n    def _load_embedding(self, embedding_name):\n        idx_to_token, idx_to_vec = ['<unk>'], []\n        data_dir = d2l.download_extract(embedding_name)\n        # GloVe website: https://nlp.stanford.edu/projects/glove/\n        # fastText website: https://fasttext.cc/\n        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n            for line in f:\n                elems = line.rstrip().split(' ')\n                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n                # Skip header information, such as the top row in fastText\n                if len(elems) > 1:\n                    idx_to_token.append(token)\n                    idx_to_vec.append(elems)\n        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n        return idx_to_token, np.array(idx_to_vec)\n\n    def __getitem__(self, tokens):\n        indices = [self.token_to_idx.get(token, self.unknown_idx)\n                   for token in tokens]\n        vecs = self.idx_to_vec[np.array(indices)]\n        return vecs\n\n    def __len__(self):\n        return len(self.idx_to_token)\n```\n\nNext, we use 50-dimensional GloVe embeddings pretrained on a subset of the Wikipedia. The corresponding word embedding is automatically downloaded the first time we create a pretrained word embedding instance. ```{.python .input  n=4}\n#@tab mxnet\nglove_6b50d = TokenEmbedding('glove.6b.50d')\n```\n\nOutput the dictionary size. The dictionary contains $400,000$ words and a special unknown token. ```{.python .input  n=5}\n#@tab mxnet\nlen(glove_6b50d)\n```\n\nWe can use a word to get its index in the dictionary, or we can get the word from its index."
    },
    {
      "chunk_id": "1db299c04f9b_2",
      "chapter": "similarity-analogy",
      "heading": "Using Pretrained Word Vectors",
      "text": "The dictionary contains $400,000$ words and a special unknown token. ```{.python .input  n=5}\n#@tab mxnet\nlen(glove_6b50d)\n```\n\nWe can use a word to get its index in the dictionary, or we can get the word from its index. ```{.python .input  n=6}\n#@tab mxnet\nglove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]\n```"
    },
    {
      "chunk_id": "50fa060f1211_0",
      "chapter": "similarity-analogy",
      "heading": "Applying Pretrained Word Vectors",
      "text": "Below, we demonstrate the application of pretrained word vectors, using GloVe as an example."
    },
    {
      "chunk_id": "aff471827f7d_0",
      "chapter": "similarity-analogy",
      "heading": "Finding Synonyms",
      "text": "Here, we re-implement the algorithm used to search for synonyms by cosine\nsimilarity introduced in :numref:`sec_word2vec`\n\nIn order to reuse the logic for seeking the $k$ nearest neighbors when\nseeking analogies, we encapsulate this part of the logic separately in the `knn`\n($k$-nearest neighbors) function.\n\n```{.python .input  n=7}\n#@tab mxnet\ndef knn(W, x, k):\n    # The added 1e-9 is for numerical stability\n    cos = np.dot(W, x.reshape(-1,)) / (\n        np.sqrt(np.sum(W * W, axis=1) + 1e-9) * np.sqrt((x * x).sum()))\n    topk = npx.topk(cos, k=k, ret_typ='indices')\n    return topk, [cos[int(i)] for i in topk]\n```\n\nThen, we search for synonyms by pre-training the word vector instance `embed`.\n\n```{.python .input  n=8}\n#@tab mxnet\ndef get_similar_tokens(query_token, k, embed):\n    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n    for i, c in zip(topk[1:], cos[1:]):  # Remove input words\n        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')\n```\n\nThe dictionary of pretrained word vector instance `glove_6b50d` already created contains 400,000 words and a special unknown token. Excluding input words and unknown words, we search for the three words that are the most similar in meaning to \"chip\".\n\n```{.python .input  n=9}\n#@tab mxnet\nget_similar_tokens('chip', 3, glove_6b50d)\n```\n\nNext, we search for the synonyms of \"baby\" and \"beautiful\".\n\n```{.python .input  n=10}\n#@tab mxnet\nget_similar_tokens('baby', 3, glove_6b50d)\n```\n\n```{.python .input  n=11}\n#@tab mxnet\nget_similar_tokens('beautiful', 3, glove_6b50d)\n```"
    },
    {
      "chunk_id": "45d6c390182b_0",
      "chapter": "similarity-analogy",
      "heading": "Finding Analogies",
      "text": "In addition to seeking synonyms, we can also use the pretrained word vector to seek the analogies between words. For example, \u201cman\u201d:\u201cwoman\u201d::\u201cson\u201d:\u201cdaughter\u201d is an example of analogy, \u201cman\u201d is to \u201cwoman\u201d as \u201cson\u201d is to \u201cdaughter\u201d. The problem of seeking analogies can be defined as follows: for four words in the analogical relationship $a : b :: c : d$, given the first three words, $a$, $b$ and $c$, we want to find $d$. Assume the word vector for the word $w$ is $\\text{vec}(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$. ```{.python .input  n=12}\n#@tab mxnet\ndef get_analogy(token_a, token_b, token_c, embed):\n    vecs = embed[[token_a, token_b, token_c]]\n    x = vecs[1] - vecs[0] + vecs[2]\n    topk, cos = knn(embed.idx_to_vec, x, 1)\n    return embed.idx_to_token[int(topk[0])]  # Remove unknown words\n```\n\nVerify the \"male-female\" analogy. ```{.python .input  n=13}\n#@tab mxnet\nget_analogy('man', 'woman', 'son', glove_6b50d)\n```\n\n\u201cCapital-country\u201d analogy: \"beijing\" is to \"china\" as \"tokyo\" is to what? The answer should be \"japan\". ```{.python .input  n=14}\n#@tab mxnet\nget_analogy('beijing', 'china', 'tokyo', glove_6b50d)\n```\n\n\"Adjective-superlative adjective\" analogy: \"bad\" is to \"worst\" as \"big\" is to what? The answer should be \"biggest\". ```{.python .input  n=15}\n#@tab mxnet\nget_analogy('bad', 'worst', 'big', glove_6b50d)\n```\n\n\"Present tense verb-past tense verb\" analogy: \"do\" is to \"did\" as \"go\" is to what? The answer should be \"went\"."
    },
    {
      "chunk_id": "45d6c390182b_1",
      "chapter": "similarity-analogy",
      "heading": "Finding Analogies",
      "text": "The answer should be \"biggest\". ```{.python .input  n=15}\n#@tab mxnet\nget_analogy('bad', 'worst', 'big', glove_6b50d)\n```\n\n\"Present tense verb-past tense verb\" analogy: \"do\" is to \"did\" as \"go\" is to what? The answer should be \"went\". ```{.python .input  n=16}\n#@tab mxnet\nget_analogy('do', 'did', 'go', glove_6b50d)\n```\n\n```{.python .input  n=51}\n#@tab mxnet\ndef visualization(token_pairs, embed):\n    plt.figure(figsize=(7, 5))\n    vecs = np.concatenate([embed[pair] for pair in token_pairs])\n    vecs_pca = PCA(n_components=2).fit_transform(numpy.array(vecs))\n    for i, pair in enumerate(token_pairs):\n        x1, y1 = vecs_pca[2 * i]\n        x2, y2 = vecs_pca[2 * i + 1]\n        plt.scatter(x1, y1)\n        plt.scatter(x2, y2)\n        plt.annotate(pair[0], xy=(x1, y1))\n        plt.annotate(pair[1], xy=(x2, y2))\n        plt.plot([x1, x2], [y1, y2])\n    plt.show()\n```\n\n```{.python .input  n=57}\n#@tab mxnet\ntoken_pairs = [['man', 'woman'], ['son', 'daughter'], ['king', 'queen'],\n              ['uncle', 'aunt'], ['sir', 'madam'], ['sister', 'brother']]\nvisualization(token_pairs, glove_6b50d)\n```"
    },
    {
      "chunk_id": "605896649629_0",
      "chapter": "similarity-analogy",
      "heading": "Summary",
      "text": "* Word vectors pre-trained on a large-scale corpus can often be applied to downstream natural language processing tasks.\n* We can use pre-trained word vectors to seek synonyms and analogies."
    },
    {
      "chunk_id": "3f3113fad766_0",
      "chapter": "similarity-analogy",
      "heading": "Exercises",
      "text": "1. Test the fastText results using `TokenEmbedding('wiki.en')`.\n1. If the dictionary is extremely large, how can we accelerate finding synonyms and analogies?"
    },
    {
      "chunk_id": "01f4e33118cb_0",
      "chapter": "index",
      "heading": "index",
      "text": "Dive into Deep Learning\n========================\n\n```eval_rst\n.. raw:: html\n   :file: frontpage.html\n```\n\n\n```toc\n:maxdepth: 1\n\nchapter_preface/index\nchapter_installation/index\nchapter_notation/index\n```\n\n\n```toc\n:maxdepth: 2\n:numbered:\n\nchapter_introduction/index\nchapter_preliminaries/index\nchapter_linear-regression/index\nchapter_linear-classification/index\nchapter_multilayer-perceptrons/index\nchapter_builders-guide/index\nchapter_convolutional-neural-networks/index\nchapter_convolutional-modern/index\nchapter_recurrent-neural-networks/index\nchapter_recurrent-modern/index\nchapter_attention-mechanisms-and-transformers/index\nchapter_optimization/index\nchapter_computational-performance/index\nchapter_computer-vision/index\nchapter_natural-language-processing-pretraining/index\nchapter_natural-language-processing-applications/index\nchapter_reinforcement-learning/index\nchapter_gaussian-processes/index\nchapter_hyperparameter-optimization/index\nchapter_generative-adversarial-networks/index\nchapter_recommender-systems/index\nchapter_appendix-mathematics-for-deep-learning/index\nchapter_appendix-tools-for-deep-learning/index\n\n```\n\n\n```toc\n:maxdepth: 1\n\nchapter_references/zreferences\n```"
    },
    {
      "chunk_id": "819f1b145acc_0",
      "chapter": "root-index",
      "heading": "root-index",
      "text": "Dive into Deep Learning\n========================\n\n```eval_rst\n.. raw:: html\n   :file: frontpage.html\n```\n\n\n```toc\n:maxdepth: 1\n\nchapter_preface/index\nchapter_installation/index\nchapter_notation/index\n```\n\n\n```toc\n:maxdepth: 2\n:numbered:\n\nchapter_introduction/index\nchapter_preliminaries/index\nchapter_linear-regression/index\nchapter_linear-classification/index\nchapter_multilayer-perceptrons/index\nchapter_builders-guide/index\nchapter_convolutional-neural-networks/index\nchapter_convolutional-modern/index\nchapter_recurrent-neural-networks/index\nchapter_recurrent-modern/index\nchapter_attention-mechanisms-and-transformers/index\nchapter_appendix-tools-for-deep-learning/index\n\n```\n\n\n```toc\n:maxdepth: 1\n\nchapter_references/zreferences\n```"
    },
    {
      "chunk_id": "f5a24a1db3c6_0",
      "chapter": "tools-index",
      "heading": "tools-index",
      "text": "# Appendix: Tools for Deep Learning\n:label:`chap_appendix_tools`\n\n\nTo get the most out of *Dive into Deep Learning*,\nwe will talk you through different tools\nin this appendix,\nsuch as\nfor running and contributing to this\ninteractive open-source book.\n\n```toc\n:maxdepth: 2\n\njupyter\nsagemaker\naws\ncolab\nselecting-servers-gpus\ncontributing\nd2l\n```"
    }
  ],
  "questions": [
    {
      "question_id": "d2l_q_0000",
      "question": "What is the formula for a parabola as mentioned in the passage?",
      "gold_answer": "The formula for a parabola is y=ax^2+bx+c.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "The formula for a parabola is y=ax^2+bx+c.",
      "gold_chunk_ids": [
        "6d0a4a01e336_0"
      ],
      "chapter": "chapter-one-problem-set",
      "section": "Problems from Automatic differentiation with ``autograd``"
    },
    {
      "question_id": "d2l_q_0001",
      "question": "What problems arise when trying to evaluate a new model after already using a test set for another model?",
      "gold_answer": "The problems that arise include the need for a test set to evaluate the new model and the fact that the original test set was collected under the assumption of evaluating a single classifier.",
      "answer_type": "procedural",
      "difficulty": "medium",
      "supporting_quote": "you now face two formidable problems. First, when you collected your test set, you determined the required level of precision under the assumption that you were evaluating a single classifier $f$.",
      "gold_chunk_ids": [
        "cbf8d7c93871_0"
      ],
      "chapter": "generalization-classification",
      "section": "Test Set Reuse"
    },
    {
      "question_id": "d2l_q_0002",
      "question": "How does the `BasicScheduler` class implement the `suggest` method?",
      "gold_answer": "The `suggest` method in the `BasicScheduler` class calls the `sample_configuration` method of the `searcher` to return a new configuration.",
      "answer_type": "code_based",
      "difficulty": "medium",
      "supporting_quote": "def suggest(self) -> dict: return self.searcher.sample_configuration()",
      "gold_chunk_ids": [
        "e9d08511aaae_0"
      ],
      "chapter": "hyperopt-api",
      "section": "Scheduler"
    },
    {
      "question_id": "d2l_q_0003",
      "question": "What does the pull function do in the context of key-value stores?",
      "gold_answer": "The pull function retrieves an aggregate value from common storage after combining the gradients from all workers.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "pull(key, value) retrieves an aggregate value from common storage, e.g., after combining the gradients from all workers.",
      "gold_chunk_ids": [
        "59b22ce69ca4_1"
      ],
      "chapter": "parameterserver",
      "section": "Key--Value Stores"
    },
    {
      "question_id": "d2l_q_0004",
      "question": "What is an example of a sentence that can be split using NLTK?",
      "gold_answer": "An example of a sentence that can be split using NLTK is 'This is great ! Why not ?'.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "To split sentences such as `sentences = 'This is great ! Why not ?'`...",
      "gold_chunk_ids": [
        "f7806c643140_0"
      ],
      "chapter": "bert-dataset",
      "section": "Exercises"
    },
    {
      "question_id": "d2l_q_0005",
      "question": "How do you calculate the probability that a random variable falls within a specific interval using a probability density function?",
      "gold_answer": "To calculate the probability that a random variable falls within a specific interval (a, b], you use the formula: $$P(X\\in(a, b]) = \\int _ {a}^{b} p(x) \\; dx.$$",
      "answer_type": "procedural",
      "difficulty": "medium",
      "supporting_quote": "$$P(X\\in(a, b]) = \\int _ {a}^{b} p(x) \\; dx.$$",
      "gold_chunk_ids": [
        "af5a1e21f602_2"
      ],
      "chapter": "random-variables",
      "section": "Probability Density Functions"
    },
    {
      "question_id": "d2l_q_0006",
      "question": "What happens to the encoder attention weights if the conditions are met?",
      "gold_answer": "If the conditions are met, the encoder attention weights are saved in the intermediates collection and assigned to the variable 'enc_attention_weights'.",
      "answer_type": "factual",
      "difficulty": "medium",
      "supporting_quote": "enc_attention_weights = inter_enc_vars['intermediates']['enc_attention_weights'][0]",
      "gold_chunk_ids": [
        "d253741a9d64_2"
      ],
      "chapter": "seq2seq",
      "section": "[**Prediction**]"
    },
    {
      "question_id": "d2l_q_0007",
      "question": "What does it mean for the generator to win in the context of GANs?",
      "gold_answer": "For the generator to win, it means that the discriminator is unable to distinguish between the two distributions on finite samples.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "the discriminator ends up unable to distinguish the two distributions on finite samples",
      "gold_chunk_ids": [
        "3a04b91abe58_0"
      ],
      "chapter": "gan",
      "section": "Exercises"
    },
    {
      "question_id": "d2l_q_0008",
      "question": "How do we read the banana detection dataset?",
      "gold_answer": "We read the banana detection dataset using the `read_data_bananas` function.",
      "answer_type": "procedural",
      "difficulty": "medium",
      "supporting_quote": "We are going to [**read the banana detection dataset**] in the `read_data_bananas` function below.",
      "gold_chunk_ids": [
        "2e8c6dd0922a_0"
      ],
      "chapter": "object-detection-dataset",
      "section": "Reading the Dataset"
    },
    {
      "question_id": "d2l_q_0009",
      "question": "Why is it important to pay attention to data quality in machine learning?",
      "gold_answer": "Data quality is crucial because real-world datasets often contain outliers, faulty measurements, and recording errors that must be addressed before feeding the data into any model.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "we must pay attention to data quality... Real-world datasets are often plagued by outliers, faulty measurements from sensors, and recording errors...",
      "gold_chunk_ids": [
        "e79510af145b_0"
      ],
      "chapter": "pandas",
      "section": "Discussion"
    },
    {
      "question_id": "d2l_q_0010",
      "question": "What is the computational complexity of a convolutional layer in a CNN?",
      "gold_answer": "The computational complexity of the convolutional layer is \\( \\mathcal{O}(knd^2) \\).",
      "answer_type": "mathematical",
      "difficulty": "easy",
      "supporting_quote": "the computational complexity of the convolutional layer is \\( \\mathcal{O}(knd^2) \\)",
      "gold_chunk_ids": [
        "e3088f338128_0"
      ],
      "chapter": "self-attention-and-positional-encoding",
      "section": "Comparing CNNs, RNNs, and Self-Attention"
    },
    {
      "question_id": "d2l_q_0011",
      "question": "How do you convert a bounding box from its original format to the matplotlib format using the `bbox_to_rect` function?",
      "gold_answer": "You convert the bounding box from the format (upper-left x, upper-left y, lower-right x, lower-right y) to the matplotlib format: ((upper-left x, upper-left y), width, height).",
      "answer_type": "procedural",
      "difficulty": "medium",
      "supporting_quote": "Convert the bounding box (upper-left x, upper-left y, lower-right x, lower-right y) format to the matplotlib format: ((upper-left x, upper-left y), width, height)",
      "gold_chunk_ids": [
        "7621af610c9f_1"
      ],
      "chapter": "bounding-box",
      "section": "Bounding Boxes"
    },
    {
      "question_id": "d2l_q_0012",
      "question": "What does the passage suggest about using libraries for RNNs in production?",
      "gold_answer": "The passage suggests that using libraries for RNNs can cut down on both implementation time and computation time.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "you will want to rely more on libraries that cut down on both implementation time and computation time",
      "gold_chunk_ids": [
        "3f7abfa99f3b_0"
      ],
      "chapter": "rnn-concise",
      "section": "rnn-concise"
    },
    {
      "question_id": "d2l_q_0013",
      "question": "What is recommended for the CPU when building a server with 4 GPUs?",
      "gold_answer": "You need to buy a CPU with relatively fast single-thread speed and a larger number of PCIe lanes, such as an AMD Threadripper.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "You will probably need a CPU with a larger number of PCIe lanes, such as an AMD Threadripper.",
      "gold_chunk_ids": [
        "ba99032aafa2_1"
      ],
      "chapter": "selecting-servers-gpus",
      "section": "Selecting Servers"
    },
    {
      "question_id": "d2l_q_0014",
      "question": "How can we ensure that the output and input have the same height and width when using a convolution kernel with different dimensions?",
      "gold_answer": "We can achieve this by setting different padding numbers for height and width.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "we can make the output and input have the same height and width by [**setting different padding numbers for height and width.**]",
      "gold_chunk_ids": [
        "75cb4b632f89_3"
      ],
      "chapter": "padding-and-strides",
      "section": "Padding"
    },
    {
      "question_id": "d2l_q_0015",
      "question": "What is a textbook case of overfitting as described in the passage?",
      "gold_answer": "A textbook case of overfitting is characterized by decreasing training error while the validation error does not decrease.",
      "answer_type": "conceptual",
      "difficulty": "easy",
      "supporting_quote": "a textbook case of overfitting.",
      "gold_chunk_ids": [
        "182814de38f8_0"
      ],
      "chapter": "weight-decay",
      "section": "[**Training without Regularization**]"
    },
    {
      "question_id": "d2l_q_0016",
      "question": "What does the `d2l` section of the textbook provide?",
      "gold_answer": "The `d2l` section displays classes and functions in the `d2l` package, showing where they are defined in the book for more detailed implementations and explanations.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "This section displays classes and functions (sorted alphabetically) in the `d2l` package, showing where they are defined in the book so you can find more detailed implementations and explanations.",
      "gold_chunk_ids": [
        "8c95bd9be1ec_0"
      ],
      "chapter": "d2l",
      "section": "d2l"
    },
    {
      "question_id": "d2l_q_0017",
      "question": "How does the approach to plotting the distributions differ from the Poisson case?",
      "gold_answer": "In the Gaussian case, we are dividing by the standard deviation, which means we are squeezing the possible outcomes into smaller and smaller areas.",
      "answer_type": "comparative",
      "difficulty": "medium",
      "supporting_quote": "compared to the Poisson case, we are now dividing by the standard deviation which means that we are squeezing the possible outcomes into smaller and smaller areas.",
      "gold_chunk_ids": [
        "b09f695018b3_1"
      ],
      "chapter": "distributions",
      "section": "Gaussian"
    },
    {
      "question_id": "d2l_q_0018",
      "question": "How do we transform the number of output channels into the number of classes for the Pascal VOC2012 dataset?",
      "gold_answer": "We use a $1\\times 1$ convolutional layer to transform the number of output channels into the number of classes (21) of the Pascal VOC2012 dataset.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "use a $1\\times 1$ convolutional layer to transform the number of output channels into the number of classes (21) of the Pascal VOC2012 dataset.",
      "gold_chunk_ids": [
        "152fc413808e_1"
      ],
      "chapter": "fcn",
      "section": "The Model"
    },
    {
      "question_id": "d2l_q_0019",
      "question": "How does the output of the 2x2 max-pooling layer behave when the input values are different?",
      "gold_answer": "Regardless of whether or not the values of `X[i, j]`, `X[i, j + 1]`, `X[i+1, j]` and `X[i+1, j + 1]` are different, the pooling layer always outputs `Y[i, j] = 1`.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "the pooling layer always outputs `Y[i, j] = 1`.",
      "gold_chunk_ids": [
        "6c04484493ed_1"
      ],
      "chapter": "pooling",
      "section": "Maximum Pooling and Average Pooling"
    },
    {
      "question_id": "d2l_q_0020",
      "question": "What is the purpose of the `save_hyperparameters` function in the utility functions section?",
      "gold_answer": "The `save_hyperparameters` function is designed to save function arguments into class attributes.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "Save function arguments into class attributes.",
      "gold_chunk_ids": [
        "cd327ed47fcc_0"
      ],
      "chapter": "utils",
      "section": "utils"
    },
    {
      "question_id": "d2l_q_0021",
      "question": "What are the three main classes proposed for object-oriented design in deep learning?",
      "gold_answer": "The three main classes are `Module`, `DataModule`, and `Trainer`. `Module` contains models, losses, and optimization methods, while `DataModule` provides data loaders for training and validation.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "we wish to have three classes: (i) `Module` contains models, losses, and optimization methods; (ii) `DataModule` provides data loaders for training and validation; (iii) both classes are combined using the `Trainer` class.",
      "gold_chunk_ids": [
        "504f494116e0_0"
      ],
      "chapter": "oo-design",
      "section": "oo-design"
    },
    {
      "question_id": "d2l_q_0022",
      "question": "What is the relationship between low training error and generalization error?",
      "gold_answer": "Low training error does not necessarily imply low generalization error, nor does it imply high generalization error; it is insufficient to certify low generalization error.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "low training error alone is not enough to certify low generalization error",
      "gold_chunk_ids": [
        "1e552de70c22_1"
      ],
      "chapter": "generalization",
      "section": "Model Complexity"
    },
    {
      "question_id": "d2l_q_0023",
      "question": "What might happen to validation accuracy if you increase the number of epochs for training?",
      "gold_answer": "The validation accuracy might decrease after a while due to overfitting.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "Why might the validation accuracy decrease after a while? How could we fix this?",
      "gold_chunk_ids": [
        "5dcc945c2880_0"
      ],
      "chapter": "softmax-regression-concise",
      "section": "Exercises"
    },
    {
      "question_id": "d2l_q_0024",
      "question": "How does the `NextSentencePred` class in BERT determine if the second sentence follows the first?",
      "gold_answer": "The `NextSentencePred` class uses a one-hidden-layer MLP to predict whether the second sentence is the next sentence of the first in the BERT input sequence.",
      "answer_type": "procedural",
      "difficulty": "medium",
      "supporting_quote": "The following `NextSentencePred` class uses a one-hidden-layer MLP to predict whether the second sentence is the next sentence of the first in the BERT input sequence.",
      "gold_chunk_ids": [
        "a3b17633c704_0"
      ],
      "chapter": "bert",
      "section": "[**Next Sentence Prediction**]"
    },
    {
      "question_id": "d2l_q_0025",
      "question": "What should be done to ensure the libraries' versions are consistent?",
      "gold_answer": "The libraries' versions should be consistent between config.ini and build.yml.",
      "answer_type": "procedural",
      "difficulty": "medium",
      "supporting_quote": "ensure libs (e.g., under sagemaker) version consistent between config.ini and build.yml",
      "gold_chunk_ids": [
        "298a4918ccfa_0"
      ],
      "chapter": "INFO",
      "section": "d2l-en"
    },
    {
      "question_id": "d2l_q_0026",
      "question": "What does the faster R-CNN model replace selective search with?",
      "gold_answer": "The faster R-CNN model replaces selective search with a region proposal network.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "the faster R-CNN proposes to replace selective search with a region proposal network.",
      "gold_chunk_ids": [
        "2e544fc8196d_0"
      ],
      "chapter": "rcnn",
      "section": "Faster R-CNN"
    },
    {
      "question_id": "d2l_q_0027",
      "question": "What do the printed coordinate axes of the images indicate?",
      "gold_answer": "The printed coordinate axes indicate that the content and style images have different sizes.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "we can tell that these images have different sizes.",
      "gold_chunk_ids": [
        "4294f19c8f38_0"
      ],
      "chapter": "neural-style",
      "section": "[**Reading the Content and Style Images**]"
    },
    {
      "question_id": "d2l_q_0028",
      "question": "What happens to the width and height of the inputs when using a basic D_block with default settings?",
      "gold_answer": "A basic block with default settings will halve the width and height of the inputs.",
      "answer_type": "conceptual",
      "difficulty": "easy",
      "supporting_quote": "A basic block with default settings will halve the width and height of the inputs.",
      "gold_chunk_ids": [
        "3054aa8d0336_1"
      ],
      "chapter": "dcgan",
      "section": "Discriminator"
    },
    {
      "question_id": "d2l_q_0029",
      "question": "What is the purpose of the objective function in the AutoRec model?",
      "gold_answer": "The objective function aims to minimize the reconstruction error by adjusting the weights and biases.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "The following objective function aims to minimize the reconstruction error.",
      "gold_chunk_ids": [
        "4799b0505223_0"
      ],
      "chapter": "autorec",
      "section": "Model"
    },
    {
      "question_id": "d2l_q_0030",
      "question": "What is the maximum likelihood estimate for alpha when the observation is 3?",
      "gold_answer": "The maximum likelihood estimate for alpha is 1/3.",
      "answer_type": "mathematical",
      "difficulty": "medium",
      "supporting_quote": "What is the maximum likelihood estimate for \u03b1?",
      "gold_chunk_ids": [
        "726faf75d1b4_0"
      ],
      "chapter": "maximum-likelihood",
      "section": "Exercises"
    },
    {
      "question_id": "d2l_q_0031",
      "question": "What is a drawback of stochastic gradient descent (SGD) compared to full batch updates?",
      "gold_answer": "One drawback of SGD is that it takes a lot longer to process one sample at a time compared to a full batch due to the efficiency of matrix-vector multiplications over vector-vector operations.",
      "answer_type": "comparative",
      "difficulty": "medium",
      "supporting_quote": "It is up to an order of magnitude more efficient to perform a matrix--vector multiplication than a corresponding number of vector--vector operations.",
      "gold_chunk_ids": [
        "d7108bba2f17_0"
      ],
      "chapter": "linear-regression",
      "section": "Minibatch Stochastic Gradient Descent"
    },
    {
      "question_id": "d2l_q_0032",
      "question": "What issue can arise when using Adam according to the passage?",
      "gold_answer": "Adam can diverge due to poor variance control in certain situations.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "there are situations where Adam can diverge due to poor variance control.",
      "gold_chunk_ids": [
        "c87fbf86d320_0"
      ],
      "chapter": "adam",
      "section": "adam"
    },
    {
      "question_id": "d2l_q_0033",
      "question": "Why is feature interaction important in prediction tasks?",
      "gold_answer": "Feature interaction is important for prediction tasks because it helps to model the 2-way interaction efficiently with FM.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "Feature interaction/crossing is important for prediction tasks and the 2-way interaction can be efficiently modeled with FM.",
      "gold_chunk_ids": [
        "ed0eaba8f3ec_0"
      ],
      "chapter": "fm",
      "section": "Summary"
    },
    {
      "question_id": "d2l_q_0034",
      "question": "What is the relationship between the input and output dimensions in a $1 \\times 1$ convolution?",
      "gold_answer": "The input and output have the same height and width.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "The input and output have the same height and width.",
      "gold_chunk_ids": [
        "2cf13df9dc69_1"
      ],
      "chapter": "channels",
      "section": "$1\\times 1$ Convolutional Layer"
    },
    {
      "question_id": "d2l_q_0035",
      "question": "What does the function f represent in the context of hyperparameter optimization?",
      "gold_answer": "The function f represents the performance of a learning algorithm, mapping from the hyperparameter space to the validation loss.",
      "answer_type": "conceptual",
      "difficulty": "easy",
      "supporting_quote": "The performance of a learning algorithm can be seen as a function f: \\mathcal{X} \\rightarrow \\mathbb{R} that maps from the hyperparameter space \\mathbf{x} \\in \\mathcal{X} to the validation loss.",
      "gold_chunk_ids": [
        "d5641b3b3f50_0"
      ],
      "chapter": "hyperopt-intro",
      "section": "The Objective Function"
    },
    {
      "question_id": "d2l_q_0036",
      "question": "What does `encoded_text[:, 0, :]` represent in BERT?",
      "gold_answer": "`encoded_text[:, 0, :]` is the BERT representation of the entire input sentence.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "Since zero is the index of the \u201c<cls>\u201d token, `encoded_text[:, 0, :]` is the BERT representation of the entire input sentence.",
      "gold_chunk_ids": [
        "b5c8694edf02_1"
      ],
      "chapter": "bert-pretraining",
      "section": "[**Representing Text with BERT**]"
    },
    {
      "question_id": "d2l_q_0037",
      "question": "What is a potential solution for combining squared loss and absolute value loss?",
      "gold_answer": "A cheap solution might involve avoiding really large gradient values.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "Can you think of a cheap solution for combining the best aspects of squared loss and absolute value loss? Hint: how can you avoid really large gradient values?",
      "gold_chunk_ids": [
        "0775b8fc67e4_1"
      ],
      "chapter": "linear-regression-scratch",
      "section": "Exercises"
    },
    {
      "question_id": "d2l_q_0038",
      "question": "How can the gradient of the attention with respect to the query be expressed?",
      "gold_answer": "The gradient of the attention with respect to the query is expressed as $\\nabla_{\\mathbf{q}} \\mathop{\\textrm{Attention}}(\\mathbf{q}, \\mathcal{D}) = \\textrm{Cov}_{p(\\mathbf{k}_i; \\mathbf{q})}[\\mathbf{k}_i]$.",
      "answer_type": "mathematical",
      "difficulty": "medium",
      "supporting_quote": "prove that $\\nabla_{\\mathbf{q}} \\mathop{\\textrm{Attention}}(\\mathbf{q}, \\mathcal{D}) = \\textrm{Cov}_{p(\\mathbf{k}_i; \\mathbf{q})}[\\mathbf{k}_i]$",
      "gold_chunk_ids": [
        "90f0b325ac8d_0"
      ],
      "chapter": "queries-keys-values",
      "section": "Exercises"
    },
    {
      "question_id": "d2l_q_0039",
      "question": "What are the two subnetworks that the NeuMF model fuses?",
      "gold_answer": "The NeuMF model fuses the GMF (Generalized Matrix Factorization) and MLP (Multi-Layer Perceptron) subnetworks.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "NeuMF fuses two subnetworks. The GMF is a generic neural network version of matrix factorization... Another component of this model is MLP.",
      "gold_chunk_ids": [
        "d60639674e4c_0"
      ],
      "chapter": "neumf",
      "section": "The NeuMF model"
    },
    {
      "question_id": "d2l_q_0040",
      "question": "Where can I download the notebooks associated with the book?",
      "gold_answer": "You can download the notebooks from the D2L.ai website and from GitHub.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "All our notebooks can be downloaded from the [D2L.ai website](https://d2l.ai) and from [GitHub](https://github.com/d2l-ai/d2l-en).",
      "gold_chunk_ids": [
        "c3f9b99a4ac9_0"
      ],
      "chapter": "index",
      "section": "Notebooks, Website, GitHub, and Forum"
    },
    {
      "question_id": "d2l_q_0041",
      "question": "How does the GRU update its hidden state H during the forward computation?",
      "gold_answer": "The GRU updates its hidden state H using the equation H = Z * H + (1 - Z) * H_tilde, where Z is computed using the sigmoid function.",
      "answer_type": "mathematical",
      "difficulty": "medium",
      "supporting_quote": "H = Z * H + (1 - Z) * H_tilde",
      "gold_chunk_ids": [
        "2b1ee8057766_1"
      ],
      "chapter": "gru",
      "section": "Defining the Model"
    },
    {
      "question_id": "d2l_q_0042",
      "question": "What does the `Dropout` layer do when the model is not in training mode?",
      "gold_answer": "When not in training mode, the `Dropout` layer simply passes the data through during testing.",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "When not in training mode, the `Dropout` layer simply passes the data through during testing.",
      "gold_chunk_ids": [
        "a2f1100eaef6_1"
      ],
      "chapter": "dropout",
      "section": "[**Concise Implementation**]"
    },
    {
      "question_id": "d2l_q_0043",
      "question": "Why is it important to select the kernel and initialize hyperparameters carefully in Gaussian Processes?",
      "gold_answer": "It is crucial to put careful thought into selecting the kernel and initializing the hyperparameters because marginal likelihood optimization is not immune to poor initializations.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "In general, it is crucial to put careful thought into selecting the kernel and initializing the hyperparameters.",
      "gold_chunk_ids": [
        "e26d9a5711aa_2"
      ],
      "chapter": "gp-inference",
      "section": "Worked Example from Scratch"
    },
    {
      "question_id": "d2l_q_0044",
      "question": "How does 'Dive into Deep Learning' balance its teaching approach?",
      "gold_answer": "'Dive into Deep Learning' strikes an excellent balance between hands-on learning and in-depth explanation.",
      "answer_type": "conceptual",
      "difficulty": "easy",
      "supporting_quote": "Dive into Deep Learning strikes an excellent balance between hands-on learning and in-depth explanation.",
      "gold_chunk_ids": [
        "785d825e0925_0"
      ],
      "chapter": "README",
      "section": "Endorsements"
    },
    {
      "question_id": "d2l_q_0045",
      "question": "What is the significance of translation invariance in convolutional neural networks?",
      "gold_answer": "Translation invariance implies that all patches of an image will be treated in the same manner, which is essential for effective image processing and computer vision algorithms.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "In particular, translation invariance in images implies that all patches of an image will be treated in the same manner.",
      "gold_chunk_ids": [
        "bf13fba9cd02_0"
      ],
      "chapter": "why-conv",
      "section": "Summary and Discussion"
    },
    {
      "question_id": "d2l_q_0046",
      "question": "What did the research by Xiao et al. (2018) demonstrate regarding neural networks?",
      "gold_answer": "They demonstrated the possibility of training 10,000-layer neural networks without architectural tricks by using a carefully-designed initialization method.",
      "answer_type": "factual",
      "difficulty": "medium",
      "supporting_quote": "Xiao.Bahri.Sohl-Dickstein.ea.2018 demonstrated the possibility of training 10,000-layer neural networks without architectural tricks by using a carefully-designed initialization method.",
      "gold_chunk_ids": [
        "a3e86dcd5323_0"
      ],
      "chapter": "numerical-stability-and-init",
      "section": "Beyond"
    },
    {
      "question_id": "d2l_q_0047",
      "question": "What are the model parameters of the output layer in a deep bidirectional RNN?",
      "gold_answer": "The model parameters of the output layer are the weight matrix \\(\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{2h \\times q}\\) and the bias \\(\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}\\).",
      "answer_type": "factual",
      "difficulty": "easy",
      "supporting_quote": "the weight matrix $\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{2h \\times q}$ and the bias $\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}$ are the model parameters of the output layer.",
      "gold_chunk_ids": [
        "8c6158e8c5fb_2"
      ],
      "chapter": "bi-rnn",
      "section": "bi-rnn"
    },
    {
      "question_id": "d2l_q_0048",
      "question": "Why is regularization important in deep learning models?",
      "gold_answer": "Regularization is important because it helps reduce overfitting, which is the only avenue for further gains in generalization error when models can achieve zero training error on the training set.",
      "answer_type": "conceptual",
      "difficulty": "medium",
      "supporting_quote": "*the only avenue for further gains is to reduce overfitting*.",
      "gold_chunk_ids": [
        "8e91e09c739f_1"
      ],
      "chapter": "generalization-deep",
      "section": "Revisiting Overfitting and Regularization"
    },
    {
      "question_id": "d2l_q_0049",
      "question": "What are some preprocessing steps applied to the raw text data?",
      "gold_answer": "The preprocessing steps include replacing non-breaking spaces with regular spaces, converting uppercase letters to lowercase, and inserting spaces between words and punctuation marks.",
      "answer_type": "procedural",
      "difficulty": "medium",
      "supporting_quote": "we proceed with several preprocessing steps for the raw text data. For instance, we replace non-breaking space with space, convert uppercase letters to lowercase ones, and insert space between words and punctuation marks.",
      "gold_chunk_ids": [
        "0c680d1ae392_0"
      ],
      "chapter": "machine-translation-and-dataset",
      "section": "[**Downloading and Preprocessing the Dataset**]"
    }
  ]
}